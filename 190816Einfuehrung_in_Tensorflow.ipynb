{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/190816Einfuehrung_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8llTbsXJHd6N",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEc-ybFoYoka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "91a4d934-35d7-43d2-d1bc-e6ad3cb66f54"
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-16 11:24:54--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  72.1MB/s    in 2.3s    \n",
            "\n",
            "2019-08-16 11:24:56 (72.1 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCDYM21uY7iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf /content/cifar-10-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYGWyjw5MwVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CifarLoader(object):\n",
        "  def __init__(self, source_files):\n",
        "    self._source = source_files\n",
        "    self._i = 0\n",
        "    self.images = None\n",
        "    self.labels = None\n",
        "\n",
        "  def load(self):\n",
        "    data = [unpickle(f) for f in self._source]\n",
        "    images = np.vstack([d[b\"data\"] for d in data])\n",
        "    n = len(images)\n",
        "    self.images = images.reshape(n, 3, 32, 32).transpose(0, 2, 3, 1).astype(float) / 255\n",
        "    self.labels = one_hot(np.hstack([d[b\"labels\"] for d in data]), 10)\n",
        "    return self\n",
        "\n",
        "  def next_batch(self, batch_size):\n",
        "    x, y = self.images[self._i:self._i+batch_size], self.labels[self._i:self._i+batch_size]\n",
        "    self._i = (self._i + batch_size) % len(self.images)\n",
        "    return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUJZcf8GVUpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = '/content/cifar-10-batches-py'\n",
        "def unpickle(file):\n",
        "  with open(os.path.join(DATA_PATH, file), 'rb') as fo:\n",
        "    dict = pickle.load(fo, encoding=\"bytes\")\n",
        "  return dict\n",
        "\n",
        "def one_hot(vec, vals=10):\n",
        "  n = len(vec)\n",
        "  out = np.zeros((n, vals))\n",
        "  out[range(n), vec] = 1\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMDAeEEjR4i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CifarDataManager(object):\n",
        "  def __init__(self):\n",
        "    self.train = CifarLoader([\"data_batch_{}\".format(i)\n",
        "    for i in range(1, 6)]).load()\n",
        "    self.test = CifarLoader([\"test_batch\"].load())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXavo0F3TIS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "083aca1f-4617-4d64-e0d2-9400ca112e86"
      },
      "source": [
        "def display_cifar(images, size):\n",
        "  n = len(images)\n",
        "  plt.figure()\n",
        "  plt.gca().set_axis_off()\n",
        "  im = np.vstack([np.hstack([images[np.random.choice(n)] for i in range(size)]) for i in range(size)])\n",
        "  plt.imshow(im)\n",
        "  plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "d = CifarDataManager()\n",
        "print(\"Anzahl der Bilder zum Anlernen: {}\".format(len(d.train.images)))\n",
        "images = d.train.images\n",
        "display_cifar(images, 10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-17180697c0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCifarDataManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anzahl der Bilder zum Anlernen: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-70b298556602>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     self.train = CifarLoader([\"data_batch_{}\".format(i)\n\u001b[0;32m----> 4\u001b[0;31m     for i in range(1, 6)]).load()\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCifarLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8597e2ca15c0>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8597e2ca15c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-750be2b270ac>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWhJ1ZERTmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "def conv_layer(input, shape):\n",
        "  W = weight_variable(shape)\n",
        "  b = bias_variable([shape[3]])\n",
        "  return tf.nn.relu(conv2d(input, W) +b)\n",
        "\n",
        "def full_layer(input, size):\n",
        "  in_size = int(input.get_shape()[1])\n",
        "  W = weight_variable([in_size, size])\n",
        "  b = bias_variable([size])\n",
        "  return tf.matmul(input, W) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM-URTpBNsFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "ede15c47-b211-4276-d414-470e3bfec3c2"
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "conv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\n",
        "conv1_pool = max_pool_2x2(conv1)\n",
        "\n",
        "conv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\n",
        "conv2_pool = max_pool_2x2(conv2)\n",
        "\n",
        "conv2_flat = tf. reshape(conv2_pool, [-1, 7*7*64])\n",
        "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
        "\n",
        "y_conv = full_layer(full1_drop, 10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 11:25:01.591935 140411875473280 deprecation.py:506] From <ipython-input-6-4c5dd5b73f32>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB9x1jHViRU",
        "colab_type": "code",
        "outputId": "78df73ce-ffca-4a03-a3bb-6848063f7878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for i in range(NUM_STEPS):  \n",
        "    batch = mnist.train.next_batch(50)  \n",
        "    \n",
        "    if i % 100 == 0:\n",
        "      train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
        "      print(\"Schritt {}, Genauigkeit Anlernen {}\".format(i, train_accuracy))\n",
        "      \n",
        "    sess.run(train_step, feed_dict={x: batch[0], y_:batch[1], keep_prob: 0.5})\n",
        "    \n",
        "  X = mnist.test.images.reshape(10, 1000, 784)\n",
        "  Y = mnist.test.labels.reshape(10, 1000, 10)\n",
        "  test_accuracy = np.mean([sess.run(accuracy, feed_dict={x:X[i], y_:Y[i], keep_prob:1.0})for i in range(10)])\n",
        "  \n",
        "  print(\"Genauigkeit Test: {}\".format(test_accuracy))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0816 11:25:01.635628 140411875473280 deprecation.py:323] From <ipython-input-7-a7a2399f2694>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0816 11:25:01.637301 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0816 11:25:01.638909 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0816 11:25:02.147055 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 11:25:02.486081 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0816 11:25:02.488548 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 11:25:02.762781 140411875473280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 11:25:02.984872 140411875473280 deprecation.py:323] From <ipython-input-7-a7a2399f2694>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Schritt 0, Genauigkeit Anlernen 0.019999999552965164\n",
            "Schritt 100, Genauigkeit Anlernen 0.8600000143051147\n",
            "Schritt 200, Genauigkeit Anlernen 0.7799999713897705\n",
            "Schritt 300, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 400, Genauigkeit Anlernen 0.9200000166893005\n",
            "Schritt 500, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 600, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 700, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 800, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 900, Genauigkeit Anlernen 0.8999999761581421\n",
            "Schritt 1000, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 1100, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 1200, Genauigkeit Anlernen 1.0\n",
            "Schritt 1300, Genauigkeit Anlernen 1.0\n",
            "Schritt 1400, Genauigkeit Anlernen 1.0\n",
            "Schritt 1500, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 1600, Genauigkeit Anlernen 1.0\n",
            "Schritt 1700, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 1800, Genauigkeit Anlernen 1.0\n",
            "Schritt 1900, Genauigkeit Anlernen 1.0\n",
            "Schritt 2000, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 2100, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 2200, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 2300, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 2400, Genauigkeit Anlernen 1.0\n",
            "Schritt 2500, Genauigkeit Anlernen 1.0\n",
            "Schritt 2600, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 2700, Genauigkeit Anlernen 1.0\n",
            "Schritt 2800, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 2900, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 3000, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 3100, Genauigkeit Anlernen 1.0\n",
            "Schritt 3200, Genauigkeit Anlernen 1.0\n",
            "Schritt 3300, Genauigkeit Anlernen 1.0\n",
            "Schritt 3400, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 3500, Genauigkeit Anlernen 1.0\n",
            "Schritt 3600, Genauigkeit Anlernen 1.0\n",
            "Schritt 3700, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 3800, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 3900, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 4000, Genauigkeit Anlernen 0.9200000166893005\n",
            "Schritt 4100, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 4200, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 4300, Genauigkeit Anlernen 1.0\n",
            "Schritt 4400, Genauigkeit Anlernen 1.0\n",
            "Schritt 4500, Genauigkeit Anlernen 1.0\n",
            "Schritt 4600, Genauigkeit Anlernen 1.0\n",
            "Schritt 4700, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 4800, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 4900, Genauigkeit Anlernen 1.0\n",
            "Schritt 5000, Genauigkeit Anlernen 1.0\n",
            "Schritt 5100, Genauigkeit Anlernen 1.0\n",
            "Schritt 5200, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 5300, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 5400, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 5500, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 5600, Genauigkeit Anlernen 1.0\n",
            "Schritt 5700, Genauigkeit Anlernen 1.0\n",
            "Schritt 5800, Genauigkeit Anlernen 1.0\n",
            "Schritt 5900, Genauigkeit Anlernen 1.0\n",
            "Schritt 6000, Genauigkeit Anlernen 1.0\n",
            "Schritt 6100, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 6200, Genauigkeit Anlernen 1.0\n",
            "Schritt 6300, Genauigkeit Anlernen 1.0\n",
            "Schritt 6400, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 6500, Genauigkeit Anlernen 1.0\n",
            "Schritt 6600, Genauigkeit Anlernen 1.0\n",
            "Schritt 6700, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 6800, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 6900, Genauigkeit Anlernen 1.0\n",
            "Schritt 7000, Genauigkeit Anlernen 1.0\n",
            "Schritt 7100, Genauigkeit Anlernen 1.0\n",
            "Schritt 7200, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 7300, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 7400, Genauigkeit Anlernen 1.0\n",
            "Schritt 7500, Genauigkeit Anlernen 1.0\n",
            "Schritt 7600, Genauigkeit Anlernen 1.0\n",
            "Schritt 7700, Genauigkeit Anlernen 1.0\n",
            "Schritt 7800, Genauigkeit Anlernen 1.0\n",
            "Schritt 7900, Genauigkeit Anlernen 1.0\n",
            "Schritt 8000, Genauigkeit Anlernen 1.0\n",
            "Schritt 8100, Genauigkeit Anlernen 0.9399999976158142\n",
            "Schritt 8200, Genauigkeit Anlernen 0.9599999785423279\n",
            "Schritt 8300, Genauigkeit Anlernen 1.0\n",
            "Schritt 8400, Genauigkeit Anlernen 1.0\n",
            "Schritt 8500, Genauigkeit Anlernen 1.0\n",
            "Schritt 8600, Genauigkeit Anlernen 1.0\n",
            "Schritt 8700, Genauigkeit Anlernen 1.0\n",
            "Schritt 8800, Genauigkeit Anlernen 1.0\n",
            "Schritt 8900, Genauigkeit Anlernen 1.0\n",
            "Schritt 9000, Genauigkeit Anlernen 1.0\n",
            "Schritt 9100, Genauigkeit Anlernen 1.0\n",
            "Schritt 9200, Genauigkeit Anlernen 1.0\n",
            "Schritt 9300, Genauigkeit Anlernen 1.0\n",
            "Schritt 9400, Genauigkeit Anlernen 1.0\n",
            "Schritt 9500, Genauigkeit Anlernen 1.0\n",
            "Schritt 9600, Genauigkeit Anlernen 1.0\n",
            "Schritt 9700, Genauigkeit Anlernen 1.0\n",
            "Schritt 9800, Genauigkeit Anlernen 0.9800000190734863\n",
            "Schritt 9900, Genauigkeit Anlernen 1.0\n",
            "Genauigkeit Test: 0.990899920463562\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}