{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/210422_objectdetectionclassifier_tflite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5X4nbEyIxJ9"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e69125-cdf8-48f9-c24f-7d6cf0d8d4e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D_ZSeMHekeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe84751-f986-44fd-c6e7-20d8ad4494f7"
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c0838a-3116-440b-a3de-63417afdc575"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 56287, done.\u001b[K\n",
            "remote: Counting objects: 100% (327/327), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 56287 (delta 163), reused 240 (delta 97), pack-reused 55960\u001b[K\n",
            "Receiving objects: 100% (56287/56287), 572.32 MiB | 37.47 MiB/s, done.\n",
            "Resolving deltas: 100% (38776/38776), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vflaf6n4pNoU"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxs7TrfgpOPa"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riuP9RX0fREn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb125800-c3f8-46d0-9e94-b618f55de2f7"
      },
      "source": [
        "%cd /content/models/research"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jycm9v1qfXdg"
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71hCx5YNfbOp"
      },
      "source": [
        "!cp object_detection/packages/tf2/setup.py ."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peYD_5JOfeyf"
      },
      "source": [
        "!python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNBxZN3HgVfV"
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/dataset/20200916/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/dataset/20200916/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/dataset/20200916/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/dataset/20200916/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qAysqFiKNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f74e46-dcd0-4d17-98f6-f38d0ce3287e"
      },
      "source": [
        "%cd /content/models/research/object_detection/\n",
        "!git clone https://github.com/aalpatya/detect_hands.git"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n",
            "fatal: destination path 'detect_hands' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0U5AkXZfE0n"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq"
      },
      "source": [
        "!mkdir TFLite_model_210419"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv99g_MdfhZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ee3d40-9c15-4eb3-b649-1053facfc3b1"
      },
      "source": [
        "!cat /content/detect_hands/generate_tfrecord.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\"\"\n",
            "ORIGINAL SOURCE: https://github.com/datitran/raccoon_dataset/master/generate_tfrecord.py\n",
            "All I have done here is to change line 19 from import tensorflow as tf \n",
            "to import tensorflow.compat.v1 as tf\n",
            "and line 36 from racoon to hand\n",
            "Usage:\n",
            "  # From tensorflow/models/\n",
            "  # Create train data:\n",
            "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\n",
            "\n",
            "  # Create test data:\n",
            "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\n",
            "\"\"\"\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "from __future__ import absolute_import\n",
            "\n",
            "import os\n",
            "import io\n",
            "import pandas as pd\n",
            "import tensorflow.compat.v1 as tf\n",
            "\n",
            "from PIL import Image\n",
            "from object_detection.utils import dataset_util\n",
            "from collections import namedtuple, OrderedDict\n",
            "\n",
            "flags = tf.app.flags\n",
            "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
            "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
            "flags.DEFINE_string('image_dir', '', 'Path to images')\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "\n",
            "# TO-DO replace this with label map\n",
            "def class_text_to_int(row_label):\n",
            "    if row_label == 'hand':\n",
            "        return 1\n",
            "    else:\n",
            "        None\n",
            "\n",
            "\n",
            "def split(df, group):\n",
            "    data = namedtuple('data', ['filename', 'object'])\n",
            "    gb = df.groupby(group)\n",
            "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
            "\n",
            "\n",
            "def create_tf_example(group, path):\n",
            "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
            "        encoded_jpg = fid.read()\n",
            "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
            "    image = Image.open(encoded_jpg_io)\n",
            "    width, height = image.size\n",
            "\n",
            "    filename = group.filename.encode('utf8')\n",
            "    image_format = b'jpg'\n",
            "    xmins = []\n",
            "    xmaxs = []\n",
            "    ymins = []\n",
            "    ymaxs = []\n",
            "    classes_text = []\n",
            "    classes = []\n",
            "\n",
            "    for index, row in group.object.iterrows():\n",
            "        xmins.append(row['xmin'] / width)\n",
            "        xmaxs.append(row['xmax'] / width)\n",
            "        ymins.append(row['ymin'] / height)\n",
            "        ymaxs.append(row['ymax'] / height)\n",
            "        classes_text.append(row['class'].encode('utf8'))\n",
            "        classes.append(class_text_to_int(row['class']))\n",
            "\n",
            "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
            "        'image/height': dataset_util.int64_feature(height),\n",
            "        'image/width': dataset_util.int64_feature(width),\n",
            "        'image/filename': dataset_util.bytes_feature(filename),\n",
            "        'image/source_id': dataset_util.bytes_feature(filename),\n",
            "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
            "        'image/format': dataset_util.bytes_feature(image_format),\n",
            "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
            "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
            "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
            "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
            "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
            "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
            "    }))\n",
            "    return tf_example\n",
            "\n",
            "\n",
            "def main(_):\n",
            "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
            "    path = os.path.join(FLAGS.image_dir)\n",
            "    examples = pd.read_csv(FLAGS.csv_input)\n",
            "    grouped = split(examples, 'filename')\n",
            "    for group in grouped:\n",
            "        tf_example = create_tf_example(group, path)\n",
            "        writer.write(tf_example.SerializeToString())\n",
            "\n",
            "    writer.close()\n",
            "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
            "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    tf.app.run()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a718863-8779-4fe4-af18-cf711b723ea1"
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "ORIGINAL SOURCE: https://github.com/datitran/raccoon_dataset/master/generate_tfrecord.py\n",
        "All I have done here is to change line 19 from import tensorflow as tf \n",
        "to import tensorflow.compat.v1 as tf\n",
        "and line 36 from racoon to hand\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to images')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a3396c-4f64-425c-df49-b523577c998a"
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 13:01:13.241302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82bbcd1-1e10-466c-cf62-54dbcc9aa54f"
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 13:01:18.751916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acce7be-b64d-48de-e6d2-7b2486488d56"
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config': No such file or directory\n",
            "rm: cannot remove '/content/models/research/object_detection/training/labelmap.pbtxt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEtcIrlvim0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b9b9bc-4088-43c5-8a2f-dce26762ceac"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYJyKhfsvwDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad252aa-c026-4c34-f99e-bb142293d3e3"
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "# Unzip\n",
        "!tar -xzvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-22 13:01:38--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.164.144, 2607:f8b0:4004:814::2010\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.164.144|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20515344 (20M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’\n",
            "\n",
            "\r          ssd_mobil   0%[                    ]       0  --.-KB/s               \rssd_mobilenet_v2_fp 100%[===================>]  19.56M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-22 13:01:38 (201 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’ saved [20515344/20515344]\n",
            "\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7116c538-63e5-4e59-be7d-233971f5dac4"
      },
      "source": [
        "%%writefile label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MQi6uNpxI8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c162ca7-fe5d-4ed7-abf7-2f19df76c92f"
      },
      "source": [
        "%%writefile pipeline.config\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 320\n",
        "        width: 320\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
        "      depth_multiplier: 1.0\n",
        "      min_depth: 16\n",
        "      conv_hyperparams {\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 4e-05\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          random_normal_initializer {\n",
        "            mean: 0.0\n",
        "            stddev: 0.01\n",
        "          }\n",
        "        }\n",
        "        activation: RELU_6\n",
        "        batch_norm {\n",
        "          decay: 0.997\n",
        "          scale: true\n",
        "          epsilon: 0.001\n",
        "        }\n",
        "      }\n",
        "      use_depthwise: true\n",
        "      override_base_feature_extractor_hyperparams: true\n",
        "      fpn {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "        additional_layer_depth: 128\n",
        "      }\n",
        "    }\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "        use_matmul_gather: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      weight_shared_convolutional_box_predictor {\n",
        "        conv_hyperparams {\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 4e-05\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            random_normal_initializer {\n",
        "              mean: 0.0\n",
        "              stddev: 0.01\n",
        "            }\n",
        "          }\n",
        "          activation: RELU_6\n",
        "          batch_norm {\n",
        "            decay: 0.997\n",
        "            scale: true\n",
        "            epsilon: 0.001\n",
        "          }\n",
        "        }\n",
        "        depth: 128\n",
        "        num_layers_before_predictor: 4\n",
        "        kernel_size: 3\n",
        "        class_prediction_bias_init: -4.6\n",
        "        share_prediction_tower: true\n",
        "        use_depthwise: true\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      multiscale_anchor_generator {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "        anchor_scale: 4.0\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        scales_per_octave: 2\n",
        "      }\n",
        "    }\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-08\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "        use_static_shapes: false\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    loss {\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      classification_loss {\n",
        "        weighted_sigmoid_focal {\n",
        "          gamma: 2.0\n",
        "          alpha: 0.25\n",
        "        }\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    encode_background_as_zeros: true\n",
        "    normalize_loc_loss_by_codesize: true\n",
        "    inplace_batchnorm_update: true\n",
        "    freeze_batchnorm: false\n",
        "  }\n",
        "}\n",
        "train_config {\n",
        "  batch_size: 4\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    random_crop_image {\n",
        "      min_object_covered: 0.0\n",
        "      min_aspect_ratio: 0.75\n",
        "      max_aspect_ratio: 3.0\n",
        "      min_area: 0.75\n",
        "      max_area: 1.0\n",
        "      overlap_thresh: 0.0\n",
        "    }\n",
        "  }\n",
        "  sync_replicas: true\n",
        "  optimizer {\n",
        "    momentum_optimizer {\n",
        "      learning_rate {\n",
        "        cosine_decay_learning_rate {\n",
        "          learning_rate_base: 0.08\n",
        "          total_steps: 50000\n",
        "          warmup_learning_rate: 0.026666\n",
        "          warmup_steps: 1000\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
        "  num_steps: 50000\n",
        "  startup_delay_steps: 0.0\n",
        "  replicas_to_aggregate: 8\n",
        "  max_number_of_boxes: 100\n",
        "  unpad_groundtruth_tensors: false\n",
        "  fine_tune_checkpoint_type: \"detection\"\n",
        "  fine_tune_checkpoint_version: V2\n",
        "}\n",
        "train_input_reader {\n",
        "  label_map_path: \"/content/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "}\n",
        "eval_config {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  use_moving_averages: false\n",
        "}\n",
        "eval_input_reader {\n",
        "  label_map_path: \"/content/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_epochs: 1\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/test.record\"\n",
        "  }\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62YPivoizzNi"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=/content/output_training/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgoKmyMT0i_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915c2a1d-d6c4-4c24-f5d4-5f79b23d062d"
      },
      "source": [
        "%cd /content/models/research/object_detection/"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gT4uv_O0p2A"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3QPNgzO0xZb"
      },
      "source": [
        "#train \n",
        "!python model_main_tf2.py \\\n",
        "--pipeline_config_path=/content/pipeline.config \\\n",
        "--model_dir=/content/output_training --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "timUd_I25ojF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d6c5cd-3ede-4590-bfe3-50934367e07c"
      },
      "source": [
        "%cd /content/models/research/object_detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUNzm_RU6Djc"
      },
      "source": [
        "!python exporter_main_v2.py \\\n",
        "--trained_checkpoint_dir=/content/output_training \\\n",
        "--pipeline_config_path=/content/pipeline.config \\\n",
        "--output_directory /content/models/research/object_detection/TFLite_model_210419"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn-AQPeuDmll"
      },
      "source": [
        "!python export_tflite_graph_tf2.py \\\n",
        "    --pipeline_config_path pipeline.config \\\n",
        "    --trained_checkpoint_dir /content/models/research/object_detection/TFLite_model_210419/checkpoint/ \\\n",
        "    --output_directory /content/models/research/object_detection/TFLite_model_210419/endresult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QQKZ-i9wULj"
      },
      "source": [
        "!tflite_convert \\\n",
        "  --saved_model_dir=/content/models/research/object_detection/TFLite_model_210419/endresult/saved_model \\\n",
        "  --output_file=/tmp/mobilenet.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3"
      },
      "source": [
        "cp -r /content/models/research/object_detection/TFLite_model_20200916 /gdrive/My\\ Drive/colabfiles/inference_graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}