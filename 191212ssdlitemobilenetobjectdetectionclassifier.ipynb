{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/191212ssdlitemobilenetobjectdetectionclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "1286c94e-f679-4ad5-ec63-f655c34f625d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "44e813fd-6be0-47c0-ac7b-b01082a9bc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 33176, done.\u001b[K\n",
            "remote: Total 33176 (delta 0), reused 0 (delta 0), pack-reused 33176\u001b[K\n",
            "Receiving objects: 100% (33176/33176), 511.87 MiB | 33.78 MiB/s, done.\n",
            "Resolving deltas: 100% (21169/21169), done.\n",
            "Checking out files: 100% (3188/3188), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "49603ce0-3aa1-4b30-fbf9-08f040c5583d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "ca0847ea-59e2-40f9-de65-8fc4dccbaf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-12 08:09:30--  http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.218.128, 2a00:1450:4013:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.218.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51025348 (49M) [application/x-tar]\n",
            "Saving to: ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’\n",
            "\n",
            "\r          ssdlite_m   0%[                    ]       0  --.-KB/s               \r         ssdlite_mo   8%[>                   ]   4.01M  17.4MB/s               \r        ssdlite_mob  65%[============>       ]  32.01M  65.4MB/s               \rssdlite_mobilenet_v 100%[===================>]  48.66M  93.7MB/s    in 0.5s    \n",
            "\n",
            "2019-12-12 08:09:30 (93.7 MB/s) - ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’ saved [51025348/51025348]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "4cadd6a3-e089-462a-942f-cec6698d2929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!git clone https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10'...\n",
            "remote: Enumerating objects: 1129, done.\u001b[K\n",
            "remote: Total 1129 (delta 0), reused 0 (delta 0), pack-reused 1129\u001b[K\n",
            "Receiving objects: 100% (1129/1129), 57.63 MiB | 26.70 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/doc /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/inference_graph /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/training /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/translate /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_webcam.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/resizer.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test.mov /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test1.JPG /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/README.md /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "7e33b48d-3bec-4f34-95b6-1290ed9dfef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "outputId": "d8273c58-391d-42b2-f729-c09fbb25d27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images\n",
        "!mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6n8GqZS-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98GmqrIhe3z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm /content/models/research/object_detection/images/test_labels.csv\n",
        "#!rm /content/models/research/object_detection/images/train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "outputId": "ba87296d-e365-4428-d048-91291208d707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!python3 xml_to_csv.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv99g_MdfhZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/models/research/object_detection/generate_tfrecord.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab_type": "code",
        "outputId": "2cc442ca-d27c-49c0-adda-8abf5fe0c88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "c16a7c70-068e-471b-a18d-8ca0db57fcd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1212 08:13:58.733636 140657677166464 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1212 08:13:58.793049 140657677166464 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "bc74fb18-a1d2-4ba8-a110-ca25c0aa566a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1212 08:14:06.806991 140661656606592 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1212 08:14:06.856883 140661656606592 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "fb78e92e-5434-4753-d877-cc8285d50732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiYCKlkF2eOO",
        "colab_type": "code",
        "outputId": "e7606775-4c5b-4818-d7a0-e295500df82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /content/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 90\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 3\n",
            "        use_depthwise: true\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      use_depthwise: true\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 24\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 200000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n",
            "  }\n",
            "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n",
            "  }\n",
            "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ayapmyjQt4W",
        "colab_type": "code",
        "outputId": "46919886-c825-4f16-d18a-c98bbed528bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/ssdlite_mobilenet_v2_coco.config\n",
        "\n",
        "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 3\n",
        "        use_depthwise: true\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      use_depthwise: true\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/ssdlite_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "1bd5dac5-3f01-47f1-fc13-b6d8b1cffa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "1f16f16a-5742-402e-860b-93b2a1137908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W1212 08:16:08.001720 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "WARNING:tensorflow:From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1212 08:16:08.001915 140053320693632 module_wrapper.py:139] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1212 08:16:08.002170 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W1212 08:16:08.005424 140053320693632 module_wrapper.py:139] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W1212 08:16:08.017889 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W1212 08:16:08.024690 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W1212 08:16:08.024976 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W1212 08:16:08.047349 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W1212 08:16:08.049178 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1212 08:16:08.049375 140053320693632 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W1212 08:16:08.059988 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W1212 08:16:08.060209 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1212 08:16:08.087685 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W1212 08:16:08.661217 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1212 08:16:08.668303 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W1212 08:16:08.675047 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1212 08:16:08.722495 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1212 08:16:08.731735 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W1212 08:16:09.334505 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1212 08:16:09.337808 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1212 08:16:09.338793 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W1212 08:16:09.343331 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W1212 08:16:09.346343 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W1212 08:16:09.349150 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1212 08:16:09.349512 140053320693632 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W1212 08:16:09.349636 140053320693632 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1212 08:16:10.104209 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1212 08:16:10.364449 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1212 08:16:13.035711 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1212 08:16:13.045528 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.045739 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.141084 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.353772 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.447865 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.545651 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:16:13.642932 140053320693632 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W1212 08:16:13.879817 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W1212 08:16:17.530264 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W1212 08:16:17.531558 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W1212 08:16:17.532708 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W1212 08:16:18.062933 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W1212 08:16:18.063816 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W1212 08:16:18.064086 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W1212 08:16:18.072695 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "W1212 08:16:20.296636 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W1212 08:16:20.298788 140053320693632 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W1212 08:16:22.513810 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W1212 08:16:26.867421 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W1212 08:16:27.211816 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W1212 08:16:27.213758 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W1212 08:16:27.218264 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1212 08:16:27.226429 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1212 08:16:27.226614 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W1212 08:16:28.248898 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1212 08:16:28.251980 140053320693632 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1212 08:16:28.255119 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.255233 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.255293 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.255352 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.255442 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.255498 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.255565 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.255625 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.255680 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.255736 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.255788 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.255838 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.255892 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.255948 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.255999 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256061 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256117 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256169 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256225 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256276 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256326 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256394 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256446 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256496 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256558 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256610 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256659 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256725 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256772 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256818 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.256869 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.256922 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.256968 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257027 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257074 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257121 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257171 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257218 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257265 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257316 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257376 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257425 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257483 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257531 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257578 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257629 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257677 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257723 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257773 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257819 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.257865 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.257928 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.257982 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258031 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258083 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258130 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258177 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258228 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258275 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258321 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258390 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258440 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258487 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258539 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258591 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258638 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258688 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258735 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258781 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.258845 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.258915 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.258968 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.259023 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.259073 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.259122 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.259176 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.259226 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.259275 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.259337 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.259402 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.259455 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.259510 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.320088 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.320228 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.320312 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.320409 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.320491 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.320569 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.320629 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.320696 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.320763 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.320825 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.320886 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.320966 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.321029 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.321091 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.321169 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.321233 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.321296 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.321378 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.321446 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.321511 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.321580 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.321644 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.321707 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.321786 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.321853 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.321924 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.321997 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.322060 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.322123 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.322191 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.322255 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.322317 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.322411 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.322483 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.322546 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.322613 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.322679 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.322742 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.322810 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.322875 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.322948 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.323029 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.323096 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.323162 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.323231 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.323303 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.323393 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.323470 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.323536 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.323599 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.323678 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.323745 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.323828 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.323902 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.323981 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.324048 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.324122 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.324202 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.324264 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.324343 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.324429 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.324492 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.324561 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.324626 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.324688 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.324755 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.324820 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.324884 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.324972 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.325039 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.325102 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.325175 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.325261 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.325325 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.325417 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.325489 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.325557 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.325641 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.325711 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.325795 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.325876 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.325960 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.326032 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.326111 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.326186 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.326256 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.326342 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.326447 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.326520 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.326601 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.326675 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.326746 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.326826 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.326916 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.326985 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.327067 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.327139 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.327206 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.327278 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.327346 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.327450 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.327540 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.327627 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.327697 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.327781 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.327883 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.327967 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.328045 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.328125 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.328190 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.328263 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.328333 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.328417 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.328502 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.328573 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.328640 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.328712 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.328816 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.328880 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.328963 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.329032 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.329099 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.329182 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.329271 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.329344 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.329442 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.329517 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.329589 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.329688 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.329773 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.329843 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.329940 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.330019 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.330090 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.330168 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.330240 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.330311 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.330406 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.330489 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.330555 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.330636 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.330705 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.330770 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.330843 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.330919 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.330988 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.331063 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.331129 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.331195 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.331278 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.331349 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.331436 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.331510 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.331578 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.331644 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.331737 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.331808 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.331878 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.331977 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.332054 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.332125 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.332201 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.332274 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.332371 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.332484 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.332552 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.332618 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.332703 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.332774 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.332839 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.332921 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.332993 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.333060 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.333133 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.333200 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.333266 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.333349 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.333440 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.333508 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.333582 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.333650 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.333718 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.333790 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.333858 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.333927 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.334006 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.334075 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.334142 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.334213 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.334280 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.334345 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.334444 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.334509 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.334571 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.334650 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.334718 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.334781 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.334850 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.334921 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.334986 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.335056 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.335120 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.335182 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.335258 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.335324 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.335404 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.335475 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.335539 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.335601 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.335670 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.335752 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.335818 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.335901 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.336010 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.336078 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.336151 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.336219 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.336286 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.336383 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.336450 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.336513 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.336591 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.336676 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.336744 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.336818 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.336887 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.336964 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.337040 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.337109 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.337174 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.337258 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.337329 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.337415 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.337491 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.337560 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.337635 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.337704 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.337766 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.337826 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.337913 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.338002 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.338069 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.338141 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.338210 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.338287 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.338389 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.338462 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.338529 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.338612 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.338683 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.338749 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.338823 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.338893 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.338972 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.339046 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.339114 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.339180 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.339262 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.339334 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.339418 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.339495 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.339583 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.339656 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.339735 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.339807 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.339878 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.339977 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.340053 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.340134 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.340207 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.340275 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.340341 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.340433 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.340514 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.340577 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.340656 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.340734 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.340797 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.340931 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.341002 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.341089 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.341184 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.341259 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.341332 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.341442 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.341520 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.341592 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.341668 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.341753 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.341825 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.341910 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.341986 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.342057 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.342165 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.342252 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.342322 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.342426 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.342505 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.342567 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.342657 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.342724 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.342792 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.342875 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.342953 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.343019 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.343094 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.343162 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.343227 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.343299 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.343382 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.343453 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.343558 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.343643 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.343710 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.343784 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.343854 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.343925 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.344000 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.344067 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.344134 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.344226 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.344291 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.344349 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.344456 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.344525 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.344589 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.344683 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.344757 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.344828 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.344924 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.345000 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.345071 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.345151 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.345224 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.345294 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.345387 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.345464 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.345537 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.345623 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.345697 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.345778 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.345852 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.345925 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.345992 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.346064 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.346133 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.346199 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.346281 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.346352 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.346438 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.346513 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.346581 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.346647 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.346720 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.346789 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.346856 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.346948 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.347020 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.347088 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.347161 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.347230 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.347297 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.347392 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.347462 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.347524 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.347618 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.347701 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.347764 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.347834 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.347897 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.347996 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.348072 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.348140 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.348205 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.348288 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.348374 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.348448 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.348523 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.348591 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.348658 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.348733 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.348800 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.348865 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.348957 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.349038 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.349102 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.349171 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.349236 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.349300 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.349384 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.349452 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.349514 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.349593 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.349660 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.349722 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.349812 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.349881 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.349958 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.350034 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.350100 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.350166 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.350247 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.350318 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.350400 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.350476 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.350545 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.350611 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.350685 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.350752 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.350819 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.350900 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.350982 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.351047 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.351120 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.351190 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.351257 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.351330 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.351418 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.351487 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.351570 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.351650 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.351712 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.351781 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.351847 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.351917 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.351988 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.352053 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.352115 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.352192 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.352258 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.352319 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.352404 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.352473 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.352536 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.352604 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.352668 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.352729 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.352807 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.352875 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.352948 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.353018 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.353082 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.353143 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.353212 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.353276 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.353339 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.353434 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.353503 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.353566 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.353635 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.353699 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.353761 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.353828 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.353918 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.353995 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.354074 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.354140 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.354203 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.354272 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.354338 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.354417 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.354487 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.354552 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.354615 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.354692 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.354757 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.354819 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.354890 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.354965 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.355029 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.355098 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.355163 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.355226 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.355303 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.355384 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.355450 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.355521 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.355585 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.355647 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.355716 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.355782 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.355847 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.355931 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.355999 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.356064 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.356134 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.356199 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.356260 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.356331 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.356413 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.356477 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1212 08:16:28.356555 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1212 08:16:28.356622 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1212 08:16:28.356685 140053320693632 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "W1212 08:16:29.254087 140053320693632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-12-12 08:16:30.620253: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-12 08:16:30.632208: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000185000 Hz\n",
            "2019-12-12 08:16:30.633983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b0a6140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 08:16:30.634048: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-12 08:16:30.640228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-12 08:16:30.811904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:30.812596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b0a6680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 08:16:30.812620: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-12 08:16:30.813499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:30.814050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 08:16:30.831123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:16:31.065876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:16:31.195133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 08:16:31.220144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 08:16:31.403253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 08:16:31.421949: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 08:16:31.823102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:16:31.823306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:31.823919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:31.824477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 08:16:31.827285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:16:31.828733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 08:16:31.828765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 08:16:31.828779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 08:16:31.829663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:31.830249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:16:31.830880: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-12 08:16:31.830913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "I1212 08:16:35.936941 140053320693632 saver.py:1284] Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I1212 08:16:36.670033 140053320693632 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I1212 08:16:37.264121 140053320693632 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "I1212 08:16:47.409002 140053320693632 learning.py:754] Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1212 08:16:47.881581 140049589327616 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "I1212 08:16:47.884440 140053320693632 learning.py:768] Starting Queues.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "I1212 08:17:05.011785 140049622898432 supervisor.py:1099] global_step/sec: 0\n",
            "2019-12-12 08:17:06.854247: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-12 08:17:07.834352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:17:08.922986: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-12 08:17:11.433301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:17:13.141734: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-12 08:17:14.253350: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-12 08:17:15.184214: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Recording summary at step 1.\n",
            "I1212 08:17:17.024187 140049614505728 supervisor.py:1050] Recording summary at step 1.\n",
            "INFO:tensorflow:global step 1: loss = 11.2139 (29.136 sec/step)\n",
            "I1212 08:17:17.861294 140053320693632 learning.py:507] global step 1: loss = 11.2139 (29.136 sec/step)\n",
            "INFO:tensorflow:global step 2: loss = 11.0391 (3.093 sec/step)\n",
            "I1212 08:17:22.288381 140053320693632 learning.py:507] global step 2: loss = 11.0391 (3.093 sec/step)\n",
            "INFO:tensorflow:global step 3: loss = 10.4327 (3.089 sec/step)\n",
            "I1212 08:17:26.099453 140053320693632 learning.py:507] global step 3: loss = 10.4327 (3.089 sec/step)\n",
            "INFO:tensorflow:global step 4: loss = 10.0644 (0.836 sec/step)\n",
            "I1212 08:17:26.965701 140053320693632 learning.py:507] global step 4: loss = 10.0644 (0.836 sec/step)\n",
            "INFO:tensorflow:global step 5: loss = 9.2020 (1.569 sec/step)\n",
            "I1212 08:17:28.537040 140053320693632 learning.py:507] global step 5: loss = 9.2020 (1.569 sec/step)\n",
            "INFO:tensorflow:global step 6: loss = 8.8183 (0.548 sec/step)\n",
            "I1212 08:17:29.177518 140053320693632 learning.py:507] global step 6: loss = 8.8183 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 7: loss = 9.2348 (2.020 sec/step)\n",
            "I1212 08:17:31.455304 140053320693632 learning.py:507] global step 7: loss = 9.2348 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 8: loss = 8.9002 (0.698 sec/step)\n",
            "I1212 08:17:32.194517 140053320693632 learning.py:507] global step 8: loss = 8.9002 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 9: loss = 8.8042 (1.319 sec/step)\n",
            "I1212 08:17:33.831536 140053320693632 learning.py:507] global step 9: loss = 8.8042 (1.319 sec/step)\n",
            "INFO:tensorflow:global step 10: loss = 8.3737 (0.654 sec/step)\n",
            "I1212 08:17:34.486959 140053320693632 learning.py:507] global step 10: loss = 8.3737 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 11: loss = 8.5435 (1.861 sec/step)\n",
            "I1212 08:17:36.350138 140053320693632 learning.py:507] global step 11: loss = 8.5435 (1.861 sec/step)\n",
            "INFO:tensorflow:global step 12: loss = 7.9740 (0.743 sec/step)\n",
            "I1212 08:17:37.379251 140053320693632 learning.py:507] global step 12: loss = 7.9740 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 13: loss = 7.9752 (0.785 sec/step)\n",
            "I1212 08:17:38.398537 140053320693632 learning.py:507] global step 13: loss = 7.9752 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 14: loss = 7.6618 (0.629 sec/step)\n",
            "I1212 08:17:39.252397 140053320693632 learning.py:507] global step 14: loss = 7.6618 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 15: loss = 7.6774 (0.738 sec/step)\n",
            "I1212 08:17:40.370307 140053320693632 learning.py:507] global step 15: loss = 7.6774 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 16: loss = 7.6230 (1.086 sec/step)\n",
            "I1212 08:17:41.549214 140053320693632 learning.py:507] global step 16: loss = 7.6230 (1.086 sec/step)\n",
            "INFO:tensorflow:global step 17: loss = 7.8682 (1.031 sec/step)\n",
            "I1212 08:17:42.582477 140053320693632 learning.py:507] global step 17: loss = 7.8682 (1.031 sec/step)\n",
            "INFO:tensorflow:global step 18: loss = 7.8899 (0.607 sec/step)\n",
            "I1212 08:17:43.369582 140053320693632 learning.py:507] global step 18: loss = 7.8899 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 19: loss = 7.5300 (1.858 sec/step)\n",
            "I1212 08:17:45.339415 140053320693632 learning.py:507] global step 19: loss = 7.5300 (1.858 sec/step)\n",
            "INFO:tensorflow:global step 20: loss = 7.6557 (1.101 sec/step)\n",
            "I1212 08:17:46.442459 140053320693632 learning.py:507] global step 20: loss = 7.6557 (1.101 sec/step)\n",
            "INFO:tensorflow:global step 21: loss = 7.0645 (0.727 sec/step)\n",
            "I1212 08:17:47.204711 140053320693632 learning.py:507] global step 21: loss = 7.0645 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 22: loss = 7.4870 (0.677 sec/step)\n",
            "I1212 08:17:48.169472 140053320693632 learning.py:507] global step 22: loss = 7.4870 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 23: loss = 7.2576 (1.225 sec/step)\n",
            "I1212 08:17:49.611868 140053320693632 learning.py:507] global step 23: loss = 7.2576 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 24: loss = 7.5343 (2.014 sec/step)\n",
            "I1212 08:17:51.627921 140053320693632 learning.py:507] global step 24: loss = 7.5343 (2.014 sec/step)\n",
            "INFO:tensorflow:global step 25: loss = 7.3274 (1.165 sec/step)\n",
            "I1212 08:17:52.794821 140053320693632 learning.py:507] global step 25: loss = 7.3274 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 26: loss = 7.1802 (0.635 sec/step)\n",
            "I1212 08:17:53.694136 140053320693632 learning.py:507] global step 26: loss = 7.1802 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 27: loss = 6.7883 (1.282 sec/step)\n",
            "I1212 08:17:55.130337 140053320693632 learning.py:507] global step 27: loss = 6.7883 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 28: loss = 6.7117 (0.637 sec/step)\n",
            "I1212 08:17:56.015558 140053320693632 learning.py:507] global step 28: loss = 6.7117 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 29: loss = 6.9439 (1.233 sec/step)\n",
            "I1212 08:17:57.355638 140053320693632 learning.py:507] global step 29: loss = 6.9439 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 30: loss = 6.9526 (0.647 sec/step)\n",
            "I1212 08:17:58.234675 140053320693632 learning.py:507] global step 30: loss = 6.9526 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 31: loss = 7.0566 (0.703 sec/step)\n",
            "I1212 08:17:59.271623 140053320693632 learning.py:507] global step 31: loss = 7.0566 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 32: loss = 6.6990 (1.223 sec/step)\n",
            "I1212 08:18:00.509668 140053320693632 learning.py:507] global step 32: loss = 6.6990 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 33: loss = 6.7201 (0.610 sec/step)\n",
            "I1212 08:18:01.429743 140053320693632 learning.py:507] global step 33: loss = 6.7201 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 34: loss = 6.4585 (0.725 sec/step)\n",
            "I1212 08:18:02.521714 140053320693632 learning.py:507] global step 34: loss = 6.4585 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 35: loss = 7.0239 (0.672 sec/step)\n",
            "I1212 08:18:03.432425 140053320693632 learning.py:507] global step 35: loss = 7.0239 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 36: loss = 5.9602 (1.148 sec/step)\n",
            "I1212 08:18:04.717989 140053320693632 learning.py:507] global step 36: loss = 5.9602 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 37: loss = 6.7879 (1.108 sec/step)\n",
            "I1212 08:18:05.827869 140053320693632 learning.py:507] global step 37: loss = 6.7879 (1.108 sec/step)\n",
            "INFO:tensorflow:global step 38: loss = 6.6081 (1.096 sec/step)\n",
            "I1212 08:18:06.926003 140053320693632 learning.py:507] global step 38: loss = 6.6081 (1.096 sec/step)\n",
            "INFO:tensorflow:global step 39: loss = 6.5368 (1.120 sec/step)\n",
            "I1212 08:18:08.047195 140053320693632 learning.py:507] global step 39: loss = 6.5368 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 40: loss = 6.2766 (0.589 sec/step)\n",
            "I1212 08:18:08.637662 140053320693632 learning.py:507] global step 40: loss = 6.2766 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 41: loss = 6.4576 (1.017 sec/step)\n",
            "I1212 08:18:09.805938 140053320693632 learning.py:507] global step 41: loss = 6.4576 (1.017 sec/step)\n",
            "INFO:tensorflow:global step 42: loss = 5.8133 (1.539 sec/step)\n",
            "I1212 08:18:11.611341 140053320693632 learning.py:507] global step 42: loss = 5.8133 (1.539 sec/step)\n",
            "INFO:tensorflow:global step 43: loss = 6.8620 (1.213 sec/step)\n",
            "I1212 08:18:12.826404 140053320693632 learning.py:507] global step 43: loss = 6.8620 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 44: loss = 6.5916 (0.670 sec/step)\n",
            "I1212 08:18:13.885390 140053320693632 learning.py:507] global step 44: loss = 6.5916 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 45: loss = 6.8956 (1.282 sec/step)\n",
            "I1212 08:18:15.204107 140053320693632 learning.py:507] global step 45: loss = 6.8956 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 46: loss = 6.3273 (0.639 sec/step)\n",
            "I1212 08:18:16.162640 140053320693632 learning.py:507] global step 46: loss = 6.3273 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 47: loss = 5.6950 (1.217 sec/step)\n",
            "I1212 08:18:17.381153 140053320693632 learning.py:507] global step 47: loss = 5.6950 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 48: loss = 6.1331 (1.042 sec/step)\n",
            "I1212 08:18:18.424742 140053320693632 learning.py:507] global step 48: loss = 6.1331 (1.042 sec/step)\n",
            "INFO:tensorflow:global step 49: loss = 5.5162 (0.581 sec/step)\n",
            "I1212 08:18:19.229546 140053320693632 learning.py:507] global step 49: loss = 5.5162 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 50: loss = 5.3234 (1.131 sec/step)\n",
            "I1212 08:18:20.560370 140053320693632 learning.py:507] global step 50: loss = 5.3234 (1.131 sec/step)\n",
            "INFO:tensorflow:global step 51: loss = 5.4944 (0.514 sec/step)\n",
            "I1212 08:18:21.076552 140053320693632 learning.py:507] global step 51: loss = 5.4944 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 52: loss = 5.5232 (1.662 sec/step)\n",
            "I1212 08:18:22.740713 140053320693632 learning.py:507] global step 52: loss = 5.5232 (1.662 sec/step)\n",
            "INFO:tensorflow:global step 53: loss = 5.9533 (0.525 sec/step)\n",
            "I1212 08:18:23.547698 140053320693632 learning.py:507] global step 53: loss = 5.9533 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 54: loss = 5.5953 (1.103 sec/step)\n",
            "I1212 08:18:24.781269 140053320693632 learning.py:507] global step 54: loss = 5.5953 (1.103 sec/step)\n",
            "INFO:tensorflow:global step 55: loss = 5.9092 (1.065 sec/step)\n",
            "I1212 08:18:25.847684 140053320693632 learning.py:507] global step 55: loss = 5.9092 (1.065 sec/step)\n",
            "INFO:tensorflow:global step 56: loss = 5.4156 (0.653 sec/step)\n",
            "I1212 08:18:26.751538 140053320693632 learning.py:507] global step 56: loss = 5.4156 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 57: loss = 5.5093 (1.241 sec/step)\n",
            "I1212 08:18:28.014098 140053320693632 learning.py:507] global step 57: loss = 5.5093 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 58: loss = 5.5246 (0.574 sec/step)\n",
            "I1212 08:18:28.910707 140053320693632 learning.py:507] global step 58: loss = 5.5246 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 59: loss = 5.8591 (0.619 sec/step)\n",
            "I1212 08:18:29.639598 140053320693632 learning.py:507] global step 59: loss = 5.8591 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 60: loss = 6.0532 (1.628 sec/step)\n",
            "I1212 08:18:31.271662 140053320693632 learning.py:507] global step 60: loss = 6.0532 (1.628 sec/step)\n",
            "INFO:tensorflow:global step 61: loss = 5.5736 (0.523 sec/step)\n",
            "I1212 08:18:31.795669 140053320693632 learning.py:507] global step 61: loss = 5.5736 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 62: loss = 5.2666 (0.675 sec/step)\n",
            "I1212 08:18:32.560875 140053320693632 learning.py:507] global step 62: loss = 5.2666 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 63: loss = 5.4014 (1.515 sec/step)\n",
            "I1212 08:18:34.372318 140053320693632 learning.py:507] global step 63: loss = 5.4014 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 64: loss = 5.0676 (0.646 sec/step)\n",
            "I1212 08:18:35.173806 140053320693632 learning.py:507] global step 64: loss = 5.0676 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 65: loss = 5.0242 (1.710 sec/step)\n",
            "I1212 08:18:36.886065 140053320693632 learning.py:507] global step 65: loss = 5.0242 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 66: loss = 5.1437 (1.085 sec/step)\n",
            "I1212 08:18:37.973109 140053320693632 learning.py:507] global step 66: loss = 5.1437 (1.085 sec/step)\n",
            "INFO:tensorflow:global step 67: loss = 5.3204 (0.661 sec/step)\n",
            "I1212 08:18:38.811107 140053320693632 learning.py:507] global step 67: loss = 5.3204 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 68: loss = 6.0640 (0.600 sec/step)\n",
            "I1212 08:18:39.930565 140053320693632 learning.py:507] global step 68: loss = 6.0640 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 69: loss = 5.3009 (1.275 sec/step)\n",
            "I1212 08:18:41.207224 140053320693632 learning.py:507] global step 69: loss = 5.3009 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 70: loss = 5.3867 (0.568 sec/step)\n",
            "I1212 08:18:42.045700 140053320693632 learning.py:507] global step 70: loss = 5.3867 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 71: loss = 5.1133 (0.650 sec/step)\n",
            "I1212 08:18:43.051558 140053320693632 learning.py:507] global step 71: loss = 5.1133 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 72: loss = 5.4960 (0.605 sec/step)\n",
            "I1212 08:18:44.050069 140053320693632 learning.py:507] global step 72: loss = 5.4960 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 73: loss = 5.1811 (0.516 sec/step)\n",
            "I1212 08:18:44.766934 140053320693632 learning.py:507] global step 73: loss = 5.1811 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 74: loss = 5.3608 (1.633 sec/step)\n",
            "I1212 08:18:46.401998 140053320693632 learning.py:507] global step 74: loss = 5.3608 (1.633 sec/step)\n",
            "INFO:tensorflow:global step 75: loss = 5.3609 (0.601 sec/step)\n",
            "I1212 08:18:47.371605 140053320693632 learning.py:507] global step 75: loss = 5.3609 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 76: loss = 5.2257 (0.929 sec/step)\n",
            "I1212 08:18:48.848913 140053320693632 learning.py:507] global step 76: loss = 5.2257 (0.929 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 76.\n",
            "I1212 08:18:50.421018 140049614505728 supervisor.py:1050] Recording summary at step 76.\n",
            "INFO:tensorflow:global step 77: loss = 5.0063 (1.647 sec/step)\n",
            "I1212 08:18:50.633784 140053320693632 learning.py:507] global step 77: loss = 5.0063 (1.647 sec/step)\n",
            "INFO:tensorflow:global step 78: loss = 4.8000 (0.663 sec/step)\n",
            "I1212 08:18:51.406135 140053320693632 learning.py:507] global step 78: loss = 4.8000 (0.663 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.724833\n",
            "I1212 08:18:52.622829 140049622898432 supervisor.py:1099] global_step/sec: 0.724833\n",
            "INFO:tensorflow:global step 79: loss = 5.5412 (1.208 sec/step)\n",
            "I1212 08:18:52.798213 140053320693632 learning.py:507] global step 79: loss = 5.5412 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 80: loss = 5.2726 (1.138 sec/step)\n",
            "I1212 08:18:53.937869 140053320693632 learning.py:507] global step 80: loss = 5.2726 (1.138 sec/step)\n",
            "INFO:tensorflow:global step 81: loss = 5.1210 (1.014 sec/step)\n",
            "I1212 08:18:54.953265 140053320693632 learning.py:507] global step 81: loss = 5.1210 (1.014 sec/step)\n",
            "INFO:tensorflow:global step 82: loss = 4.6506 (0.618 sec/step)\n",
            "I1212 08:18:55.829953 140053320693632 learning.py:507] global step 82: loss = 4.6506 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 83: loss = 4.7911 (1.174 sec/step)\n",
            "I1212 08:18:57.103411 140053320693632 learning.py:507] global step 83: loss = 4.7911 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 84: loss = 5.0994 (1.071 sec/step)\n",
            "I1212 08:18:58.175719 140053320693632 learning.py:507] global step 84: loss = 5.0994 (1.071 sec/step)\n",
            "INFO:tensorflow:global step 85: loss = 4.5113 (0.708 sec/step)\n",
            "I1212 08:18:59.100801 140053320693632 learning.py:507] global step 85: loss = 4.5113 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 86: loss = 4.7154 (1.672 sec/step)\n",
            "I1212 08:19:00.863667 140053320693632 learning.py:507] global step 86: loss = 4.7154 (1.672 sec/step)\n",
            "INFO:tensorflow:global step 87: loss = 5.0814 (1.093 sec/step)\n",
            "I1212 08:19:01.958088 140053320693632 learning.py:507] global step 87: loss = 5.0814 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 88: loss = 4.3296 (1.055 sec/step)\n",
            "I1212 08:19:03.014396 140053320693632 learning.py:507] global step 88: loss = 4.3296 (1.055 sec/step)\n",
            "INFO:tensorflow:global step 89: loss = 4.7804 (0.717 sec/step)\n",
            "I1212 08:19:03.951663 140053320693632 learning.py:507] global step 89: loss = 4.7804 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 90: loss = 4.5631 (0.676 sec/step)\n",
            "I1212 08:19:04.710447 140053320693632 learning.py:507] global step 90: loss = 4.5631 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 91: loss = 4.9816 (0.686 sec/step)\n",
            "I1212 08:19:05.689535 140053320693632 learning.py:507] global step 91: loss = 4.9816 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 92: loss = 4.8310 (1.621 sec/step)\n",
            "I1212 08:19:07.576853 140053320693632 learning.py:507] global step 92: loss = 4.8310 (1.621 sec/step)\n",
            "INFO:tensorflow:global step 93: loss = 4.9986 (1.055 sec/step)\n",
            "I1212 08:19:08.633149 140053320693632 learning.py:507] global step 93: loss = 4.9986 (1.055 sec/step)\n",
            "INFO:tensorflow:global step 94: loss = 4.9956 (0.742 sec/step)\n",
            "I1212 08:19:09.382436 140053320693632 learning.py:507] global step 94: loss = 4.9956 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 95: loss = 4.5342 (0.532 sec/step)\n",
            "I1212 08:19:09.916393 140053320693632 learning.py:507] global step 95: loss = 4.5342 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 96: loss = 4.4179 (0.588 sec/step)\n",
            "I1212 08:19:10.506808 140053320693632 learning.py:507] global step 96: loss = 4.4179 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 97: loss = 4.3693 (1.115 sec/step)\n",
            "I1212 08:19:11.653585 140053320693632 learning.py:507] global step 97: loss = 4.3693 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 98: loss = 4.7497 (2.905 sec/step)\n",
            "I1212 08:19:15.143171 140053320693632 learning.py:507] global step 98: loss = 4.7497 (2.905 sec/step)\n",
            "INFO:tensorflow:global step 99: loss = 4.7690 (0.638 sec/step)\n",
            "I1212 08:19:15.905830 140053320693632 learning.py:507] global step 99: loss = 4.7690 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 100: loss = 5.0638 (2.063 sec/step)\n",
            "I1212 08:19:18.136538 140053320693632 learning.py:507] global step 100: loss = 5.0638 (2.063 sec/step)\n",
            "INFO:tensorflow:global step 101: loss = 4.7565 (1.589 sec/step)\n",
            "I1212 08:19:19.733127 140053320693632 learning.py:507] global step 101: loss = 4.7565 (1.589 sec/step)\n",
            "INFO:tensorflow:global step 102: loss = 5.1666 (0.507 sec/step)\n",
            "I1212 08:19:20.241644 140053320693632 learning.py:507] global step 102: loss = 5.1666 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 103: loss = 4.1832 (1.651 sec/step)\n",
            "I1212 08:19:21.894033 140053320693632 learning.py:507] global step 103: loss = 4.1832 (1.651 sec/step)\n",
            "INFO:tensorflow:global step 104: loss = 4.5296 (0.567 sec/step)\n",
            "I1212 08:19:22.762564 140053320693632 learning.py:507] global step 104: loss = 4.5296 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 105: loss = 4.7253 (1.225 sec/step)\n",
            "I1212 08:19:24.090388 140053320693632 learning.py:507] global step 105: loss = 4.7253 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 106: loss = 4.5051 (0.508 sec/step)\n",
            "I1212 08:19:24.806918 140053320693632 learning.py:507] global step 106: loss = 4.5051 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 107: loss = 4.4263 (0.636 sec/step)\n",
            "I1212 08:19:25.637172 140053320693632 learning.py:507] global step 107: loss = 4.4263 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 108: loss = 4.9335 (0.875 sec/step)\n",
            "I1212 08:19:26.733412 140053320693632 learning.py:507] global step 108: loss = 4.9335 (0.875 sec/step)\n",
            "INFO:tensorflow:global step 109: loss = 4.5530 (1.605 sec/step)\n",
            "I1212 08:19:28.539471 140053320693632 learning.py:507] global step 109: loss = 4.5530 (1.605 sec/step)\n",
            "INFO:tensorflow:global step 110: loss = 4.1228 (0.609 sec/step)\n",
            "I1212 08:19:29.366948 140053320693632 learning.py:507] global step 110: loss = 4.1228 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 111: loss = 4.8397 (1.216 sec/step)\n",
            "I1212 08:19:30.653016 140053320693632 learning.py:507] global step 111: loss = 4.8397 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 112: loss = 4.5086 (0.693 sec/step)\n",
            "I1212 08:19:31.445565 140053320693632 learning.py:507] global step 112: loss = 4.5086 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 113: loss = 3.9444 (0.759 sec/step)\n",
            "I1212 08:19:32.584143 140053320693632 learning.py:507] global step 113: loss = 3.9444 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 114: loss = 4.3793 (1.072 sec/step)\n",
            "I1212 08:19:33.693274 140053320693632 learning.py:507] global step 114: loss = 4.3793 (1.072 sec/step)\n",
            "INFO:tensorflow:global step 115: loss = 4.5763 (1.074 sec/step)\n",
            "I1212 08:19:34.769125 140053320693632 learning.py:507] global step 115: loss = 4.5763 (1.074 sec/step)\n",
            "INFO:tensorflow:global step 116: loss = 4.5897 (0.733 sec/step)\n",
            "I1212 08:19:35.593290 140053320693632 learning.py:507] global step 116: loss = 4.5897 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 117: loss = 4.2661 (1.106 sec/step)\n",
            "I1212 08:19:36.845956 140053320693632 learning.py:507] global step 117: loss = 4.2661 (1.106 sec/step)\n",
            "INFO:tensorflow:global step 118: loss = 4.3870 (0.633 sec/step)\n",
            "I1212 08:19:37.680530 140053320693632 learning.py:507] global step 118: loss = 4.3870 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 119: loss = 4.2529 (0.503 sec/step)\n",
            "I1212 08:19:38.458047 140053320693632 learning.py:507] global step 119: loss = 4.2529 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 120: loss = 4.6059 (0.678 sec/step)\n",
            "I1212 08:19:39.463488 140053320693632 learning.py:507] global step 120: loss = 4.6059 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 121: loss = 4.3857 (1.248 sec/step)\n",
            "I1212 08:19:40.822188 140053320693632 learning.py:507] global step 121: loss = 4.3857 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 122: loss = 4.6135 (0.577 sec/step)\n",
            "I1212 08:19:41.603600 140053320693632 learning.py:507] global step 122: loss = 4.6135 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 123: loss = 4.3497 (0.656 sec/step)\n",
            "I1212 08:19:42.610641 140053320693632 learning.py:507] global step 123: loss = 4.3497 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 124: loss = 4.1622 (0.523 sec/step)\n",
            "I1212 08:19:43.506598 140053320693632 learning.py:507] global step 124: loss = 4.1622 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 125: loss = 4.4760 (0.692 sec/step)\n",
            "I1212 08:19:44.647276 140053320693632 learning.py:507] global step 125: loss = 4.4760 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 126: loss = 4.0141 (1.400 sec/step)\n",
            "I1212 08:19:46.121999 140053320693632 learning.py:507] global step 126: loss = 4.0141 (1.400 sec/step)\n",
            "INFO:tensorflow:global step 127: loss = 4.1765 (0.600 sec/step)\n",
            "I1212 08:19:46.997954 140053320693632 learning.py:507] global step 127: loss = 4.1765 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 128: loss = 3.6482 (1.288 sec/step)\n",
            "I1212 08:19:48.306818 140053320693632 learning.py:507] global step 128: loss = 3.6482 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 129: loss = 4.9366 (0.590 sec/step)\n",
            "I1212 08:19:49.126194 140053320693632 learning.py:507] global step 129: loss = 4.9366 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 130: loss = 4.5857 (0.495 sec/step)\n",
            "I1212 08:19:49.971988 140053320693632 learning.py:507] global step 130: loss = 4.5857 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 131: loss = 4.2385 (0.656 sec/step)\n",
            "I1212 08:19:51.173391 140053320693632 learning.py:507] global step 131: loss = 4.2385 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 132: loss = 4.1964 (0.497 sec/step)\n",
            "I1212 08:19:51.941903 140053320693632 learning.py:507] global step 132: loss = 4.1964 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 133: loss = 3.9585 (0.549 sec/step)\n",
            "I1212 08:19:52.812295 140053320693632 learning.py:507] global step 133: loss = 3.9585 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 134: loss = 4.1004 (0.624 sec/step)\n",
            "I1212 08:19:53.438421 140053320693632 learning.py:507] global step 134: loss = 4.1004 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 135: loss = 3.7745 (2.601 sec/step)\n",
            "I1212 08:19:56.041567 140053320693632 learning.py:507] global step 135: loss = 3.7745 (2.601 sec/step)\n",
            "INFO:tensorflow:global step 136: loss = 3.9041 (0.539 sec/step)\n",
            "I1212 08:19:56.765088 140053320693632 learning.py:507] global step 136: loss = 3.9041 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 137: loss = 4.3913 (0.501 sec/step)\n",
            "I1212 08:19:57.771146 140053320693632 learning.py:507] global step 137: loss = 4.3913 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 138: loss = 3.9384 (1.253 sec/step)\n",
            "I1212 08:19:59.182925 140053320693632 learning.py:507] global step 138: loss = 3.9384 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 139: loss = 4.2298 (0.505 sec/step)\n",
            "I1212 08:19:59.908598 140053320693632 learning.py:507] global step 139: loss = 4.2298 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 140: loss = 3.9143 (1.197 sec/step)\n",
            "I1212 08:20:01.266795 140053320693632 learning.py:507] global step 140: loss = 3.9143 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 141: loss = 4.2069 (0.592 sec/step)\n",
            "I1212 08:20:01.914105 140053320693632 learning.py:507] global step 141: loss = 4.2069 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 142: loss = 3.6986 (0.783 sec/step)\n",
            "I1212 08:20:03.100338 140053320693632 learning.py:507] global step 142: loss = 3.6986 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 143: loss = 4.1642 (0.631 sec/step)\n",
            "I1212 08:20:04.062038 140053320693632 learning.py:507] global step 143: loss = 4.1642 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 144: loss = 4.7589 (0.706 sec/step)\n",
            "I1212 08:20:05.000102 140053320693632 learning.py:507] global step 144: loss = 4.7589 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 145: loss = 4.0118 (0.663 sec/step)\n",
            "I1212 08:20:06.117030 140053320693632 learning.py:507] global step 145: loss = 4.0118 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 146: loss = 3.6011 (0.606 sec/step)\n",
            "I1212 08:20:06.782946 140053320693632 learning.py:507] global step 146: loss = 3.6011 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 147: loss = 3.8441 (1.435 sec/step)\n",
            "I1212 08:20:08.439144 140053320693632 learning.py:507] global step 147: loss = 3.8441 (1.435 sec/step)\n",
            "INFO:tensorflow:global step 148: loss = 3.9081 (1.654 sec/step)\n",
            "I1212 08:20:10.407630 140053320693632 learning.py:507] global step 148: loss = 3.9081 (1.654 sec/step)\n",
            "INFO:tensorflow:global step 149: loss = 4.3192 (0.678 sec/step)\n",
            "I1212 08:20:11.358255 140053320693632 learning.py:507] global step 149: loss = 4.3192 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 150: loss = 3.7721 (1.201 sec/step)\n",
            "I1212 08:20:12.663502 140053320693632 learning.py:507] global step 150: loss = 3.7721 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 151: loss = 4.0061 (0.642 sec/step)\n",
            "I1212 08:20:13.400317 140053320693632 learning.py:507] global step 151: loss = 4.0061 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 152: loss = 4.2051 (0.737 sec/step)\n",
            "I1212 08:20:14.594969 140053320693632 learning.py:507] global step 152: loss = 4.2051 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 153: loss = 3.6071 (1.227 sec/step)\n",
            "I1212 08:20:15.875205 140053320693632 learning.py:507] global step 153: loss = 3.6071 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 154: loss = 4.2090 (0.593 sec/step)\n",
            "I1212 08:20:16.693986 140053320693632 learning.py:507] global step 154: loss = 4.2090 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 155: loss = 3.8457 (1.182 sec/step)\n",
            "I1212 08:20:18.037824 140053320693632 learning.py:507] global step 155: loss = 3.8457 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 156: loss = 3.5707 (0.654 sec/step)\n",
            "I1212 08:20:18.802716 140053320693632 learning.py:507] global step 156: loss = 3.5707 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 157: loss = 3.7669 (1.324 sec/step)\n",
            "I1212 08:20:20.153131 140053320693632 learning.py:507] global step 157: loss = 3.7669 (1.324 sec/step)\n",
            "INFO:tensorflow:global step 158: loss = 3.8952 (0.748 sec/step)\n",
            "I1212 08:20:20.940188 140053320693632 learning.py:507] global step 158: loss = 3.8952 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 159: loss = 3.9465 (1.295 sec/step)\n",
            "I1212 08:20:22.297158 140053320693632 learning.py:507] global step 159: loss = 3.9465 (1.295 sec/step)\n",
            "INFO:tensorflow:global step 160: loss = 3.3900 (0.736 sec/step)\n",
            "I1212 08:20:23.405550 140053320693632 learning.py:507] global step 160: loss = 3.3900 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 161: loss = 3.8842 (0.687 sec/step)\n",
            "I1212 08:20:24.286296 140053320693632 learning.py:507] global step 161: loss = 3.8842 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 162: loss = 3.5545 (0.719 sec/step)\n",
            "I1212 08:20:25.375925 140053320693632 learning.py:507] global step 162: loss = 3.5545 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 163: loss = 3.7527 (1.242 sec/step)\n",
            "I1212 08:20:26.669406 140053320693632 learning.py:507] global step 163: loss = 3.7527 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 164: loss = 4.8912 (0.721 sec/step)\n",
            "I1212 08:20:27.516521 140053320693632 learning.py:507] global step 164: loss = 4.8912 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 165: loss = 3.9579 (1.234 sec/step)\n",
            "I1212 08:20:28.769686 140053320693632 learning.py:507] global step 165: loss = 3.9579 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 166: loss = 3.8959 (0.577 sec/step)\n",
            "I1212 08:20:29.545732 140053320693632 learning.py:507] global step 166: loss = 3.8959 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 167: loss = 3.8343 (0.695 sec/step)\n",
            "I1212 08:20:30.631736 140053320693632 learning.py:507] global step 167: loss = 3.8343 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 168: loss = 3.7251 (0.684 sec/step)\n",
            "I1212 08:20:31.572983 140053320693632 learning.py:507] global step 168: loss = 3.7251 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 169: loss = 4.0516 (0.803 sec/step)\n",
            "I1212 08:20:32.816226 140053320693632 learning.py:507] global step 169: loss = 4.0516 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 170: loss = 3.5876 (1.095 sec/step)\n",
            "I1212 08:20:33.914670 140053320693632 learning.py:507] global step 170: loss = 3.5876 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 171: loss = 3.5488 (0.638 sec/step)\n",
            "I1212 08:20:34.854146 140053320693632 learning.py:507] global step 171: loss = 3.5488 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 172: loss = 3.4694 (1.214 sec/step)\n",
            "I1212 08:20:36.110530 140053320693632 learning.py:507] global step 172: loss = 3.4694 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 173: loss = 3.1523 (0.531 sec/step)\n",
            "I1212 08:20:36.892196 140053320693632 learning.py:507] global step 173: loss = 3.1523 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 174: loss = 4.4401 (0.643 sec/step)\n",
            "I1212 08:20:37.772889 140053320693632 learning.py:507] global step 174: loss = 4.4401 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 175: loss = 4.1392 (1.376 sec/step)\n",
            "I1212 08:20:39.177152 140053320693632 learning.py:507] global step 175: loss = 4.1392 (1.376 sec/step)\n",
            "INFO:tensorflow:global step 176: loss = 4.4736 (1.175 sec/step)\n",
            "I1212 08:20:40.354059 140053320693632 learning.py:507] global step 176: loss = 4.4736 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 177: loss = 3.8008 (0.607 sec/step)\n",
            "I1212 08:20:41.099828 140053320693632 learning.py:507] global step 177: loss = 3.8008 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 178: loss = 4.0969 (1.344 sec/step)\n",
            "I1212 08:20:42.639632 140053320693632 learning.py:507] global step 178: loss = 4.0969 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 179: loss = 3.6705 (0.591 sec/step)\n",
            "I1212 08:20:43.511179 140053320693632 learning.py:507] global step 179: loss = 3.6705 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 180: loss = 4.1685 (1.155 sec/step)\n",
            "I1212 08:20:44.852713 140053320693632 learning.py:507] global step 180: loss = 4.1685 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 181: loss = 4.2158 (0.647 sec/step)\n",
            "I1212 08:20:45.519525 140053320693632 learning.py:507] global step 181: loss = 4.2158 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 182: loss = 3.4192 (1.296 sec/step)\n",
            "I1212 08:20:46.998072 140053320693632 learning.py:507] global step 182: loss = 3.4192 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 183: loss = 3.9813 (0.679 sec/step)\n",
            "I1212 08:20:47.857614 140053320693632 learning.py:507] global step 183: loss = 3.9813 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 184: loss = 4.1826 (2.362 sec/step)\n",
            "I1212 08:20:50.431758 140053320693632 learning.py:507] global step 184: loss = 4.1826 (2.362 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 184.\n",
            "I1212 08:20:50.446846 140049614505728 supervisor.py:1050] Recording summary at step 184.\n",
            "INFO:tensorflow:global step 185: loss = 3.7051 (0.650 sec/step)\n",
            "I1212 08:20:51.300483 140053320693632 learning.py:507] global step 185: loss = 3.7051 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 186: loss = 3.8035 (1.214 sec/step)\n",
            "I1212 08:20:52.522257 140053320693632 learning.py:507] global step 186: loss = 3.8035 (1.214 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.899724\n",
            "I1212 08:20:52.659628 140049622898432 supervisor.py:1099] global_step/sec: 0.899724\n",
            "INFO:tensorflow:global step 187: loss = 4.4337 (0.636 sec/step)\n",
            "I1212 08:20:53.255177 140053320693632 learning.py:507] global step 187: loss = 4.4337 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 188: loss = 3.7918 (0.759 sec/step)\n",
            "I1212 08:20:54.507530 140053320693632 learning.py:507] global step 188: loss = 3.7918 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 189: loss = 3.5307 (1.103 sec/step)\n",
            "I1212 08:20:55.722224 140053320693632 learning.py:507] global step 189: loss = 3.5307 (1.103 sec/step)\n",
            "INFO:tensorflow:global step 190: loss = 4.1736 (0.663 sec/step)\n",
            "I1212 08:20:56.580843 140053320693632 learning.py:507] global step 190: loss = 4.1736 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 191: loss = 3.8262 (0.657 sec/step)\n",
            "I1212 08:20:57.565921 140053320693632 learning.py:507] global step 191: loss = 3.8262 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 192: loss = 3.4008 (1.156 sec/step)\n",
            "I1212 08:20:58.862476 140053320693632 learning.py:507] global step 192: loss = 3.4008 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 193: loss = 3.9064 (0.578 sec/step)\n",
            "I1212 08:20:59.546322 140053320693632 learning.py:507] global step 193: loss = 3.9064 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 194: loss = 3.5428 (1.287 sec/step)\n",
            "I1212 08:21:01.119658 140053320693632 learning.py:507] global step 194: loss = 3.5428 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 195: loss = 3.5289 (0.674 sec/step)\n",
            "I1212 08:21:01.922791 140053320693632 learning.py:507] global step 195: loss = 3.5289 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 196: loss = 3.5156 (1.217 sec/step)\n",
            "I1212 08:21:03.285509 140053320693632 learning.py:507] global step 196: loss = 3.5156 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 197: loss = 3.9891 (0.858 sec/step)\n",
            "I1212 08:21:04.189901 140053320693632 learning.py:507] global step 197: loss = 3.9891 (0.858 sec/step)\n",
            "INFO:tensorflow:global step 198: loss = 3.4658 (1.134 sec/step)\n",
            "I1212 08:21:05.398148 140053320693632 learning.py:507] global step 198: loss = 3.4658 (1.134 sec/step)\n",
            "INFO:tensorflow:global step 199: loss = 3.7954 (0.647 sec/step)\n",
            "I1212 08:21:06.281150 140053320693632 learning.py:507] global step 199: loss = 3.7954 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 3.6946 (1.515 sec/step)\n",
            "I1212 08:21:07.824806 140053320693632 learning.py:507] global step 200: loss = 3.6946 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 201: loss = 3.7043 (0.666 sec/step)\n",
            "I1212 08:21:08.736701 140053320693632 learning.py:507] global step 201: loss = 3.7043 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 202: loss = 3.7727 (0.678 sec/step)\n",
            "I1212 08:21:09.966790 140053320693632 learning.py:507] global step 202: loss = 3.7727 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 203: loss = 3.1420 (0.703 sec/step)\n",
            "I1212 08:21:10.960237 140053320693632 learning.py:507] global step 203: loss = 3.1420 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 204: loss = 3.5457 (1.285 sec/step)\n",
            "I1212 08:21:12.264321 140053320693632 learning.py:507] global step 204: loss = 3.5457 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 205: loss = 2.9771 (0.515 sec/step)\n",
            "I1212 08:21:12.977527 140053320693632 learning.py:507] global step 205: loss = 2.9771 (0.515 sec/step)\n",
            "INFO:tensorflow:global step 206: loss = 3.7738 (1.387 sec/step)\n",
            "I1212 08:21:14.574755 140053320693632 learning.py:507] global step 206: loss = 3.7738 (1.387 sec/step)\n",
            "INFO:tensorflow:global step 207: loss = 3.7015 (0.698 sec/step)\n",
            "I1212 08:21:15.611139 140053320693632 learning.py:507] global step 207: loss = 3.7015 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 208: loss = 3.1637 (1.100 sec/step)\n",
            "I1212 08:21:16.769169 140053320693632 learning.py:507] global step 208: loss = 3.1637 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 209: loss = 3.5038 (1.086 sec/step)\n",
            "I1212 08:21:17.856245 140053320693632 learning.py:507] global step 209: loss = 3.5038 (1.086 sec/step)\n",
            "INFO:tensorflow:global step 210: loss = 2.8654 (0.605 sec/step)\n",
            "I1212 08:21:18.534635 140053320693632 learning.py:507] global step 210: loss = 2.8654 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 211: loss = 3.7820 (0.769 sec/step)\n",
            "I1212 08:21:19.766981 140053320693632 learning.py:507] global step 211: loss = 3.7820 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 212: loss = 3.6802 (1.570 sec/step)\n",
            "I1212 08:21:21.399874 140053320693632 learning.py:507] global step 212: loss = 3.6802 (1.570 sec/step)\n",
            "INFO:tensorflow:global step 213: loss = 3.4268 (0.617 sec/step)\n",
            "I1212 08:21:22.136040 140053320693632 learning.py:507] global step 213: loss = 3.4268 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 214: loss = 3.4561 (1.349 sec/step)\n",
            "I1212 08:21:23.754150 140053320693632 learning.py:507] global step 214: loss = 3.4561 (1.349 sec/step)\n",
            "INFO:tensorflow:global step 215: loss = 3.6029 (0.616 sec/step)\n",
            "I1212 08:21:24.553747 140053320693632 learning.py:507] global step 215: loss = 3.6029 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 216: loss = 3.3030 (0.547 sec/step)\n",
            "I1212 08:21:25.523256 140053320693632 learning.py:507] global step 216: loss = 3.3030 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 217: loss = 3.4247 (0.677 sec/step)\n",
            "I1212 08:21:26.459981 140053320693632 learning.py:507] global step 217: loss = 3.4247 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 218: loss = 3.5024 (1.068 sec/step)\n",
            "I1212 08:21:27.642865 140053320693632 learning.py:507] global step 218: loss = 3.5024 (1.068 sec/step)\n",
            "INFO:tensorflow:global step 219: loss = 3.3012 (1.450 sec/step)\n",
            "I1212 08:21:29.298034 140053320693632 learning.py:507] global step 219: loss = 3.3012 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 220: loss = 4.3375 (0.618 sec/step)\n",
            "I1212 08:21:30.204310 140053320693632 learning.py:507] global step 220: loss = 4.3375 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 221: loss = 3.3298 (1.562 sec/step)\n",
            "I1212 08:21:31.824719 140053320693632 learning.py:507] global step 221: loss = 3.3298 (1.562 sec/step)\n",
            "INFO:tensorflow:global step 222: loss = 3.1414 (0.696 sec/step)\n",
            "I1212 08:21:32.762605 140053320693632 learning.py:507] global step 222: loss = 3.1414 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 223: loss = 3.7657 (0.786 sec/step)\n",
            "I1212 08:21:33.839624 140053320693632 learning.py:507] global step 223: loss = 3.7657 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 224: loss = 3.4032 (0.812 sec/step)\n",
            "I1212 08:21:34.920027 140053320693632 learning.py:507] global step 224: loss = 3.4032 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 225: loss = 3.9281 (1.256 sec/step)\n",
            "I1212 08:21:36.248926 140053320693632 learning.py:507] global step 225: loss = 3.9281 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 226: loss = 3.4821 (0.648 sec/step)\n",
            "I1212 08:21:37.164623 140053320693632 learning.py:507] global step 226: loss = 3.4821 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 227: loss = 3.8096 (1.168 sec/step)\n",
            "I1212 08:21:38.372638 140053320693632 learning.py:507] global step 227: loss = 3.8096 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 228: loss = 4.2069 (0.792 sec/step)\n",
            "I1212 08:21:39.174258 140053320693632 learning.py:507] global step 228: loss = 4.2069 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 229: loss = 3.5651 (1.683 sec/step)\n",
            "I1212 08:21:40.872081 140053320693632 learning.py:507] global step 229: loss = 3.5651 (1.683 sec/step)\n",
            "INFO:tensorflow:global step 230: loss = 3.3583 (0.647 sec/step)\n",
            "I1212 08:21:41.669991 140053320693632 learning.py:507] global step 230: loss = 3.3583 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 231: loss = 3.6937 (1.272 sec/step)\n",
            "I1212 08:21:43.091610 140053320693632 learning.py:507] global step 231: loss = 3.6937 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 232: loss = 3.4583 (0.573 sec/step)\n",
            "I1212 08:21:43.956525 140053320693632 learning.py:507] global step 232: loss = 3.4583 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 233: loss = 3.9243 (0.585 sec/step)\n",
            "I1212 08:21:44.906213 140053320693632 learning.py:507] global step 233: loss = 3.9243 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 234: loss = 3.7730 (1.608 sec/step)\n",
            "I1212 08:21:46.553614 140053320693632 learning.py:507] global step 234: loss = 3.7730 (1.608 sec/step)\n",
            "INFO:tensorflow:global step 235: loss = 3.0767 (0.752 sec/step)\n",
            "I1212 08:21:47.364642 140053320693632 learning.py:507] global step 235: loss = 3.0767 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 236: loss = 3.0950 (1.550 sec/step)\n",
            "I1212 08:21:48.978961 140053320693632 learning.py:507] global step 236: loss = 3.0950 (1.550 sec/step)\n",
            "INFO:tensorflow:global step 237: loss = 4.1142 (0.579 sec/step)\n",
            "I1212 08:21:49.904633 140053320693632 learning.py:507] global step 237: loss = 4.1142 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 238: loss = 3.7049 (0.633 sec/step)\n",
            "I1212 08:21:50.894669 140053320693632 learning.py:507] global step 238: loss = 3.7049 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 239: loss = 3.6599 (0.677 sec/step)\n",
            "I1212 08:21:51.750419 140053320693632 learning.py:507] global step 239: loss = 3.6599 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 240: loss = 3.3793 (1.577 sec/step)\n",
            "I1212 08:21:53.329210 140053320693632 learning.py:507] global step 240: loss = 3.3793 (1.577 sec/step)\n",
            "INFO:tensorflow:global step 241: loss = 3.2903 (0.614 sec/step)\n",
            "I1212 08:21:53.981661 140053320693632 learning.py:507] global step 241: loss = 3.2903 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 242: loss = 3.6281 (0.867 sec/step)\n",
            "I1212 08:21:55.111408 140053320693632 learning.py:507] global step 242: loss = 3.6281 (0.867 sec/step)\n",
            "INFO:tensorflow:global step 243: loss = 4.0695 (0.544 sec/step)\n",
            "I1212 08:21:55.924538 140053320693632 learning.py:507] global step 243: loss = 4.0695 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 244: loss = 4.4205 (1.485 sec/step)\n",
            "I1212 08:21:57.466558 140053320693632 learning.py:507] global step 244: loss = 4.4205 (1.485 sec/step)\n",
            "INFO:tensorflow:global step 245: loss = 3.3341 (1.367 sec/step)\n",
            "I1212 08:21:58.851764 140053320693632 learning.py:507] global step 245: loss = 3.3341 (1.367 sec/step)\n",
            "INFO:tensorflow:global step 246: loss = 3.5288 (0.681 sec/step)\n",
            "I1212 08:21:59.739544 140053320693632 learning.py:507] global step 246: loss = 3.5288 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 247: loss = 3.1669 (0.594 sec/step)\n",
            "I1212 08:22:00.548311 140053320693632 learning.py:507] global step 247: loss = 3.1669 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 248: loss = 3.3243 (1.425 sec/step)\n",
            "I1212 08:22:02.273847 140053320693632 learning.py:507] global step 248: loss = 3.3243 (1.425 sec/step)\n",
            "INFO:tensorflow:global step 249: loss = 3.8466 (0.631 sec/step)\n",
            "I1212 08:22:03.174865 140053320693632 learning.py:507] global step 249: loss = 3.8466 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 250: loss = 3.4784 (0.552 sec/step)\n",
            "I1212 08:22:03.871744 140053320693632 learning.py:507] global step 250: loss = 3.4784 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 251: loss = 3.4164 (1.645 sec/step)\n",
            "I1212 08:22:05.517980 140053320693632 learning.py:507] global step 251: loss = 3.4164 (1.645 sec/step)\n",
            "INFO:tensorflow:global step 252: loss = 2.7588 (0.742 sec/step)\n",
            "I1212 08:22:06.499940 140053320693632 learning.py:507] global step 252: loss = 2.7588 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 253: loss = 3.3742 (1.321 sec/step)\n",
            "I1212 08:22:07.839255 140053320693632 learning.py:507] global step 253: loss = 3.3742 (1.321 sec/step)\n",
            "INFO:tensorflow:global step 254: loss = 3.4317 (0.713 sec/step)\n",
            "I1212 08:22:08.746883 140053320693632 learning.py:507] global step 254: loss = 3.4317 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 255: loss = 3.6433 (1.122 sec/step)\n",
            "I1212 08:22:09.996609 140053320693632 learning.py:507] global step 255: loss = 3.6433 (1.122 sec/step)\n",
            "INFO:tensorflow:global step 256: loss = 3.0670 (0.684 sec/step)\n",
            "I1212 08:22:10.979720 140053320693632 learning.py:507] global step 256: loss = 3.0670 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 257: loss = 4.1409 (0.826 sec/step)\n",
            "I1212 08:22:11.966737 140053320693632 learning.py:507] global step 257: loss = 4.1409 (0.826 sec/step)\n",
            "INFO:tensorflow:global step 258: loss = 4.3403 (0.690 sec/step)\n",
            "I1212 08:22:13.043596 140053320693632 learning.py:507] global step 258: loss = 4.3403 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 259: loss = 3.1492 (1.482 sec/step)\n",
            "I1212 08:22:14.635155 140053320693632 learning.py:507] global step 259: loss = 3.1492 (1.482 sec/step)\n",
            "INFO:tensorflow:global step 260: loss = 4.2389 (0.596 sec/step)\n",
            "I1212 08:22:15.611049 140053320693632 learning.py:507] global step 260: loss = 4.2389 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 261: loss = 3.2282 (0.695 sec/step)\n",
            "I1212 08:22:16.543803 140053320693632 learning.py:507] global step 261: loss = 3.2282 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 262: loss = 4.2208 (0.711 sec/step)\n",
            "I1212 08:22:17.717319 140053320693632 learning.py:507] global step 262: loss = 4.2208 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 263: loss = 3.2117 (0.672 sec/step)\n",
            "I1212 08:22:18.779253 140053320693632 learning.py:507] global step 263: loss = 3.2117 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 264: loss = 4.3996 (0.607 sec/step)\n",
            "I1212 08:22:19.389127 140053320693632 learning.py:507] global step 264: loss = 4.3996 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 265: loss = 3.9047 (1.843 sec/step)\n",
            "I1212 08:22:21.234212 140053320693632 learning.py:507] global step 265: loss = 3.9047 (1.843 sec/step)\n",
            "INFO:tensorflow:global step 266: loss = 3.5227 (0.608 sec/step)\n",
            "I1212 08:22:21.981075 140053320693632 learning.py:507] global step 266: loss = 3.5227 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 267: loss = 3.7037 (0.705 sec/step)\n",
            "I1212 08:22:23.207228 140053320693632 learning.py:507] global step 267: loss = 3.7037 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 268: loss = 3.9855 (0.547 sec/step)\n",
            "I1212 08:22:24.072129 140053320693632 learning.py:507] global step 268: loss = 3.9855 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 269: loss = 3.7076 (0.660 sec/step)\n",
            "I1212 08:22:24.978032 140053320693632 learning.py:507] global step 269: loss = 3.7076 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 270: loss = 4.0765 (1.462 sec/step)\n",
            "I1212 08:22:26.633781 140053320693632 learning.py:507] global step 270: loss = 4.0765 (1.462 sec/step)\n",
            "INFO:tensorflow:global step 271: loss = 3.9162 (0.494 sec/step)\n",
            "I1212 08:22:27.130013 140053320693632 learning.py:507] global step 271: loss = 3.9162 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 272: loss = 3.8959 (1.645 sec/step)\n",
            "I1212 08:22:28.776980 140053320693632 learning.py:507] global step 272: loss = 3.8959 (1.645 sec/step)\n",
            "INFO:tensorflow:global step 273: loss = 3.4684 (0.627 sec/step)\n",
            "I1212 08:22:29.634166 140053320693632 learning.py:507] global step 273: loss = 3.4684 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 274: loss = 3.3407 (0.611 sec/step)\n",
            "I1212 08:22:30.710548 140053320693632 learning.py:507] global step 274: loss = 3.3407 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 275: loss = 3.4361 (0.573 sec/step)\n",
            "I1212 08:22:31.425086 140053320693632 learning.py:507] global step 275: loss = 3.4361 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 276: loss = 3.3095 (1.705 sec/step)\n",
            "I1212 08:22:33.131589 140053320693632 learning.py:507] global step 276: loss = 3.3095 (1.705 sec/step)\n",
            "INFO:tensorflow:global step 277: loss = 3.7907 (0.781 sec/step)\n",
            "I1212 08:22:34.072906 140053320693632 learning.py:507] global step 277: loss = 3.7907 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 278: loss = 3.6392 (0.660 sec/step)\n",
            "I1212 08:22:35.001554 140053320693632 learning.py:507] global step 278: loss = 3.6392 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 279: loss = 3.3280 (0.619 sec/step)\n",
            "I1212 08:22:35.777503 140053320693632 learning.py:507] global step 279: loss = 3.3280 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 280: loss = 2.8989 (1.424 sec/step)\n",
            "I1212 08:22:37.250029 140053320693632 learning.py:507] global step 280: loss = 2.8989 (1.424 sec/step)\n",
            "INFO:tensorflow:global step 281: loss = 3.4007 (0.636 sec/step)\n",
            "I1212 08:22:38.193549 140053320693632 learning.py:507] global step 281: loss = 3.4007 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 282: loss = 2.9782 (1.357 sec/step)\n",
            "I1212 08:22:39.572391 140053320693632 learning.py:507] global step 282: loss = 2.9782 (1.357 sec/step)\n",
            "INFO:tensorflow:global step 283: loss = 3.4564 (0.763 sec/step)\n",
            "I1212 08:22:40.394193 140053320693632 learning.py:507] global step 283: loss = 3.4564 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 284: loss = 3.2739 (1.246 sec/step)\n",
            "I1212 08:22:41.696424 140053320693632 learning.py:507] global step 284: loss = 3.2739 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 285: loss = 3.2027 (0.805 sec/step)\n",
            "I1212 08:22:42.708481 140053320693632 learning.py:507] global step 285: loss = 3.2027 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 286: loss = 4.4235 (0.832 sec/step)\n",
            "I1212 08:22:43.663731 140053320693632 learning.py:507] global step 286: loss = 4.4235 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 287: loss = 3.4826 (0.686 sec/step)\n",
            "I1212 08:22:44.553551 140053320693632 learning.py:507] global step 287: loss = 3.4826 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 288: loss = 3.6559 (0.722 sec/step)\n",
            "I1212 08:22:45.522507 140053320693632 learning.py:507] global step 288: loss = 3.6559 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 289: loss = 3.3944 (1.061 sec/step)\n",
            "I1212 08:22:46.707384 140053320693632 learning.py:507] global step 289: loss = 3.3944 (1.061 sec/step)\n",
            "INFO:tensorflow:global step 290: loss = 4.4116 (0.655 sec/step)\n",
            "I1212 08:22:47.520042 140053320693632 learning.py:507] global step 290: loss = 4.4116 (0.655 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 290.\n",
            "I1212 08:22:49.564127 140049614505728 supervisor.py:1050] Recording summary at step 290.\n",
            "INFO:tensorflow:global step 291: loss = 3.3527 (2.126 sec/step)\n",
            "I1212 08:22:49.782144 140053320693632 learning.py:507] global step 291: loss = 3.3527 (2.126 sec/step)\n",
            "INFO:tensorflow:global step 292: loss = 3.4538 (1.047 sec/step)\n",
            "I1212 08:22:50.830084 140053320693632 learning.py:507] global step 292: loss = 3.4538 (1.047 sec/step)\n",
            "INFO:tensorflow:global step 293: loss = 3.1720 (0.609 sec/step)\n",
            "I1212 08:22:51.792552 140053320693632 learning.py:507] global step 293: loss = 3.1720 (0.609 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.891838\n",
            "I1212 08:22:52.636553 140049622898432 supervisor.py:1099] global_step/sec: 0.891838\n",
            "INFO:tensorflow:global step 294: loss = 3.4160 (1.027 sec/step)\n",
            "I1212 08:22:52.843531 140053320693632 learning.py:507] global step 294: loss = 3.4160 (1.027 sec/step)\n",
            "INFO:tensorflow:global step 295: loss = 3.2572 (0.602 sec/step)\n",
            "I1212 08:22:53.593079 140053320693632 learning.py:507] global step 295: loss = 3.2572 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 296: loss = 3.0877 (1.216 sec/step)\n",
            "I1212 08:22:55.017328 140053320693632 learning.py:507] global step 296: loss = 3.0877 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 297: loss = 3.2785 (0.729 sec/step)\n",
            "I1212 08:22:55.934226 140053320693632 learning.py:507] global step 297: loss = 3.2785 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 298: loss = 3.0887 (1.233 sec/step)\n",
            "I1212 08:22:57.198264 140053320693632 learning.py:507] global step 298: loss = 3.0887 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 299: loss = 3.7761 (0.647 sec/step)\n",
            "I1212 08:22:58.094609 140053320693632 learning.py:507] global step 299: loss = 3.7761 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 3.2013 (1.161 sec/step)\n",
            "I1212 08:22:59.346698 140053320693632 learning.py:507] global step 300: loss = 3.2013 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 301: loss = 3.7194 (0.480 sec/step)\n",
            "I1212 08:23:00.013821 140053320693632 learning.py:507] global step 301: loss = 3.7194 (0.480 sec/step)\n",
            "INFO:tensorflow:global step 302: loss = 3.3937 (0.647 sec/step)\n",
            "I1212 08:23:01.213542 140053320693632 learning.py:507] global step 302: loss = 3.3937 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 303: loss = 3.5432 (1.122 sec/step)\n",
            "I1212 08:23:02.340730 140053320693632 learning.py:507] global step 303: loss = 3.5432 (1.122 sec/step)\n",
            "INFO:tensorflow:global step 304: loss = 2.9008 (0.605 sec/step)\n",
            "I1212 08:23:03.029194 140053320693632 learning.py:507] global step 304: loss = 2.9008 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 305: loss = 3.1934 (1.457 sec/step)\n",
            "I1212 08:23:04.499936 140053320693632 learning.py:507] global step 305: loss = 3.1934 (1.457 sec/step)\n",
            "INFO:tensorflow:global step 306: loss = 4.0840 (0.666 sec/step)\n",
            "I1212 08:23:05.375993 140053320693632 learning.py:507] global step 306: loss = 4.0840 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 307: loss = 3.1690 (1.006 sec/step)\n",
            "I1212 08:23:06.463870 140053320693632 learning.py:507] global step 307: loss = 3.1690 (1.006 sec/step)\n",
            "INFO:tensorflow:global step 308: loss = 3.4873 (0.601 sec/step)\n",
            "I1212 08:23:07.274194 140053320693632 learning.py:507] global step 308: loss = 3.4873 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 309: loss = 3.2200 (1.039 sec/step)\n",
            "I1212 08:23:08.474481 140053320693632 learning.py:507] global step 309: loss = 3.2200 (1.039 sec/step)\n",
            "INFO:tensorflow:global step 310: loss = 3.5368 (0.634 sec/step)\n",
            "I1212 08:23:09.179707 140053320693632 learning.py:507] global step 310: loss = 3.5368 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 311: loss = 3.6788 (0.609 sec/step)\n",
            "I1212 08:23:10.336772 140053320693632 learning.py:507] global step 311: loss = 3.6788 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 312: loss = 3.5636 (0.607 sec/step)\n",
            "I1212 08:23:11.247546 140053320693632 learning.py:507] global step 312: loss = 3.5636 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 313: loss = 4.1400 (1.082 sec/step)\n",
            "I1212 08:23:12.493053 140053320693632 learning.py:507] global step 313: loss = 4.1400 (1.082 sec/step)\n",
            "INFO:tensorflow:global step 314: loss = 3.0375 (0.590 sec/step)\n",
            "I1212 08:23:13.385405 140053320693632 learning.py:507] global step 314: loss = 3.0375 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 315: loss = 4.0661 (1.205 sec/step)\n",
            "I1212 08:23:14.592534 140053320693632 learning.py:507] global step 315: loss = 4.0661 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 316: loss = 3.8945 (0.585 sec/step)\n",
            "I1212 08:23:15.293712 140053320693632 learning.py:507] global step 316: loss = 3.8945 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 317: loss = 3.3077 (1.482 sec/step)\n",
            "I1212 08:23:17.008636 140053320693632 learning.py:507] global step 317: loss = 3.3077 (1.482 sec/step)\n",
            "INFO:tensorflow:global step 318: loss = 2.6744 (0.604 sec/step)\n",
            "I1212 08:23:17.757257 140053320693632 learning.py:507] global step 318: loss = 2.6744 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 319: loss = 3.7621 (1.282 sec/step)\n",
            "I1212 08:23:19.175060 140053320693632 learning.py:507] global step 319: loss = 3.7621 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 320: loss = 3.8618 (0.598 sec/step)\n",
            "I1212 08:23:19.976345 140053320693632 learning.py:507] global step 320: loss = 3.8618 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 321: loss = 3.3136 (1.374 sec/step)\n",
            "I1212 08:23:21.483888 140053320693632 learning.py:507] global step 321: loss = 3.3136 (1.374 sec/step)\n",
            "INFO:tensorflow:global step 322: loss = 2.9649 (0.728 sec/step)\n",
            "I1212 08:23:22.535763 140053320693632 learning.py:507] global step 322: loss = 2.9649 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 323: loss = 3.3449 (0.641 sec/step)\n",
            "I1212 08:23:23.467160 140053320693632 learning.py:507] global step 323: loss = 3.3449 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 324: loss = 3.6475 (0.647 sec/step)\n",
            "I1212 08:23:24.463947 140053320693632 learning.py:507] global step 324: loss = 3.6475 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 325: loss = 3.3804 (0.681 sec/step)\n",
            "I1212 08:23:25.284497 140053320693632 learning.py:507] global step 325: loss = 3.3804 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 326: loss = 4.2445 (1.690 sec/step)\n",
            "I1212 08:23:26.976261 140053320693632 learning.py:507] global step 326: loss = 4.2445 (1.690 sec/step)\n",
            "INFO:tensorflow:global step 327: loss = 3.5692 (0.547 sec/step)\n",
            "I1212 08:23:27.668992 140053320693632 learning.py:507] global step 327: loss = 3.5692 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 328: loss = 3.5976 (1.224 sec/step)\n",
            "I1212 08:23:29.034679 140053320693632 learning.py:507] global step 328: loss = 3.5976 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 329: loss = 3.9096 (0.750 sec/step)\n",
            "I1212 08:23:30.000545 140053320693632 learning.py:507] global step 329: loss = 3.9096 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 330: loss = 3.1763 (0.680 sec/step)\n",
            "I1212 08:23:30.978789 140053320693632 learning.py:507] global step 330: loss = 3.1763 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 331: loss = 3.1595 (1.356 sec/step)\n",
            "I1212 08:23:32.369327 140053320693632 learning.py:507] global step 331: loss = 3.1595 (1.356 sec/step)\n",
            "INFO:tensorflow:global step 332: loss = 3.7586 (1.075 sec/step)\n",
            "I1212 08:23:33.445839 140053320693632 learning.py:507] global step 332: loss = 3.7586 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 333: loss = 3.4282 (1.130 sec/step)\n",
            "I1212 08:23:34.577128 140053320693632 learning.py:507] global step 333: loss = 3.4282 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 334: loss = 3.3949 (0.596 sec/step)\n",
            "I1212 08:23:35.417997 140053320693632 learning.py:507] global step 334: loss = 3.3949 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 335: loss = 3.3636 (0.824 sec/step)\n",
            "I1212 08:23:36.845106 140053320693632 learning.py:507] global step 335: loss = 3.3636 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 336: loss = 2.7093 (0.492 sec/step)\n",
            "I1212 08:23:37.358055 140053320693632 learning.py:507] global step 336: loss = 2.7093 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 337: loss = 4.0209 (1.783 sec/step)\n",
            "I1212 08:23:39.142632 140053320693632 learning.py:507] global step 337: loss = 4.0209 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 338: loss = 3.1197 (1.079 sec/step)\n",
            "I1212 08:23:40.223306 140053320693632 learning.py:507] global step 338: loss = 3.1197 (1.079 sec/step)\n",
            "INFO:tensorflow:global step 339: loss = 3.1439 (0.650 sec/step)\n",
            "I1212 08:23:40.969963 140053320693632 learning.py:507] global step 339: loss = 3.1439 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 340: loss = 3.2503 (0.794 sec/step)\n",
            "I1212 08:23:42.245558 140053320693632 learning.py:507] global step 340: loss = 3.2503 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 341: loss = 3.7018 (1.346 sec/step)\n",
            "I1212 08:23:43.598953 140053320693632 learning.py:507] global step 341: loss = 3.7018 (1.346 sec/step)\n",
            "INFO:tensorflow:global step 342: loss = 3.0428 (0.587 sec/step)\n",
            "I1212 08:23:44.263979 140053320693632 learning.py:507] global step 342: loss = 3.0428 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 343: loss = 3.5390 (1.414 sec/step)\n",
            "I1212 08:23:45.994171 140053320693632 learning.py:507] global step 343: loss = 3.5390 (1.414 sec/step)\n",
            "INFO:tensorflow:global step 344: loss = 3.1407 (0.601 sec/step)\n",
            "I1212 08:23:46.823139 140053320693632 learning.py:507] global step 344: loss = 3.1407 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 345: loss = 3.7524 (1.177 sec/step)\n",
            "I1212 08:23:48.109115 140053320693632 learning.py:507] global step 345: loss = 3.7524 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 346: loss = 4.0746 (0.749 sec/step)\n",
            "I1212 08:23:49.041882 140053320693632 learning.py:507] global step 346: loss = 4.0746 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 347: loss = 3.2976 (0.662 sec/step)\n",
            "I1212 08:23:49.962155 140053320693632 learning.py:507] global step 347: loss = 3.2976 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 348: loss = 3.1090 (0.789 sec/step)\n",
            "I1212 08:23:51.352476 140053320693632 learning.py:507] global step 348: loss = 3.1090 (0.789 sec/step)\n",
            "INFO:tensorflow:global step 349: loss = 3.5265 (0.725 sec/step)\n",
            "I1212 08:23:52.311128 140053320693632 learning.py:507] global step 349: loss = 3.5265 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 350: loss = 3.1604 (1.432 sec/step)\n",
            "I1212 08:23:53.768110 140053320693632 learning.py:507] global step 350: loss = 3.1604 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 351: loss = 4.0959 (0.747 sec/step)\n",
            "I1212 08:23:54.740087 140053320693632 learning.py:507] global step 351: loss = 4.0959 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 352: loss = 3.4016 (1.241 sec/step)\n",
            "I1212 08:23:56.084302 140053320693632 learning.py:507] global step 352: loss = 3.4016 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 353: loss = 3.1474 (0.525 sec/step)\n",
            "I1212 08:23:56.611579 140053320693632 learning.py:507] global step 353: loss = 3.1474 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 354: loss = 3.2968 (1.632 sec/step)\n",
            "I1212 08:23:58.244934 140053320693632 learning.py:507] global step 354: loss = 3.2968 (1.632 sec/step)\n",
            "INFO:tensorflow:global step 355: loss = 3.2200 (0.699 sec/step)\n",
            "I1212 08:23:59.011628 140053320693632 learning.py:507] global step 355: loss = 3.2200 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 356: loss = 3.5106 (0.615 sec/step)\n",
            "I1212 08:24:00.185001 140053320693632 learning.py:507] global step 356: loss = 3.5106 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 357: loss = 3.6042 (0.737 sec/step)\n",
            "I1212 08:24:01.265965 140053320693632 learning.py:507] global step 357: loss = 3.6042 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 358: loss = 3.6706 (0.813 sec/step)\n",
            "I1212 08:24:02.456247 140053320693632 learning.py:507] global step 358: loss = 3.6706 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 359: loss = 3.1798 (0.599 sec/step)\n",
            "I1212 08:24:03.402832 140053320693632 learning.py:507] global step 359: loss = 3.1798 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 360: loss = 3.0468 (0.641 sec/step)\n",
            "I1212 08:24:04.372676 140053320693632 learning.py:507] global step 360: loss = 3.0468 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 361: loss = 4.1548 (0.738 sec/step)\n",
            "I1212 08:24:05.465981 140053320693632 learning.py:507] global step 361: loss = 4.1548 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 362: loss = 3.7204 (0.725 sec/step)\n",
            "I1212 08:24:06.531601 140053320693632 learning.py:507] global step 362: loss = 3.7204 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 363: loss = 3.2391 (1.165 sec/step)\n",
            "I1212 08:24:07.741142 140053320693632 learning.py:507] global step 363: loss = 3.2391 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 364: loss = 2.9557 (0.529 sec/step)\n",
            "I1212 08:24:08.271898 140053320693632 learning.py:507] global step 364: loss = 2.9557 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 365: loss = 3.6468 (1.604 sec/step)\n",
            "I1212 08:24:09.878408 140053320693632 learning.py:507] global step 365: loss = 3.6468 (1.604 sec/step)\n",
            "INFO:tensorflow:global step 366: loss = 3.5255 (0.613 sec/step)\n",
            "I1212 08:24:10.846221 140053320693632 learning.py:507] global step 366: loss = 3.5255 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 367: loss = 3.9401 (1.158 sec/step)\n",
            "I1212 08:24:12.011187 140053320693632 learning.py:507] global step 367: loss = 3.9401 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 368: loss = 3.5505 (0.636 sec/step)\n",
            "I1212 08:24:12.940444 140053320693632 learning.py:507] global step 368: loss = 3.5505 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 369: loss = 3.3603 (0.680 sec/step)\n",
            "I1212 08:24:13.758289 140053320693632 learning.py:507] global step 369: loss = 3.3603 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 370: loss = 3.4429 (1.326 sec/step)\n",
            "I1212 08:24:15.228583 140053320693632 learning.py:507] global step 370: loss = 3.4429 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 371: loss = 2.5670 (0.632 sec/step)\n",
            "I1212 08:24:16.090342 140053320693632 learning.py:507] global step 371: loss = 2.5670 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 372: loss = 3.7085 (1.227 sec/step)\n",
            "I1212 08:24:17.453616 140053320693632 learning.py:507] global step 372: loss = 3.7085 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 373: loss = 3.9991 (0.498 sec/step)\n",
            "I1212 08:24:18.191903 140053320693632 learning.py:507] global step 373: loss = 3.9991 (0.498 sec/step)\n",
            "INFO:tensorflow:global step 374: loss = 3.0046 (0.691 sec/step)\n",
            "I1212 08:24:19.137540 140053320693632 learning.py:507] global step 374: loss = 3.0046 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 375: loss = 3.2812 (1.404 sec/step)\n",
            "I1212 08:24:20.542756 140053320693632 learning.py:507] global step 375: loss = 3.2812 (1.404 sec/step)\n",
            "INFO:tensorflow:global step 376: loss = 3.3456 (0.618 sec/step)\n",
            "I1212 08:24:21.257207 140053320693632 learning.py:507] global step 376: loss = 3.3456 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 377: loss = 3.4174 (0.688 sec/step)\n",
            "I1212 08:24:22.380192 140053320693632 learning.py:507] global step 377: loss = 3.4174 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 378: loss = 4.2079 (0.546 sec/step)\n",
            "I1212 08:24:23.066784 140053320693632 learning.py:507] global step 378: loss = 4.2079 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 379: loss = 4.0624 (1.409 sec/step)\n",
            "I1212 08:24:24.552138 140053320693632 learning.py:507] global step 379: loss = 4.0624 (1.409 sec/step)\n",
            "INFO:tensorflow:global step 380: loss = 3.3572 (0.640 sec/step)\n",
            "I1212 08:24:25.354842 140053320693632 learning.py:507] global step 380: loss = 3.3572 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 381: loss = 3.3402 (0.598 sec/step)\n",
            "I1212 08:24:26.412801 140053320693632 learning.py:507] global step 381: loss = 3.3402 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 382: loss = 3.1141 (1.163 sec/step)\n",
            "I1212 08:24:27.660955 140053320693632 learning.py:507] global step 382: loss = 3.1141 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 383: loss = 3.3478 (0.541 sec/step)\n",
            "I1212 08:24:28.414187 140053320693632 learning.py:507] global step 383: loss = 3.3478 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 384: loss = 2.7288 (1.230 sec/step)\n",
            "I1212 08:24:29.792663 140053320693632 learning.py:507] global step 384: loss = 2.7288 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 385: loss = 3.9139 (0.586 sec/step)\n",
            "I1212 08:24:30.509589 140053320693632 learning.py:507] global step 385: loss = 3.9139 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 386: loss = 3.2909 (0.858 sec/step)\n",
            "I1212 08:24:31.808170 140053320693632 learning.py:507] global step 386: loss = 3.2909 (0.858 sec/step)\n",
            "INFO:tensorflow:global step 387: loss = 3.3437 (0.699 sec/step)\n",
            "I1212 08:24:32.748067 140053320693632 learning.py:507] global step 387: loss = 3.3437 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 388: loss = 2.8616 (0.657 sec/step)\n",
            "I1212 08:24:33.560868 140053320693632 learning.py:507] global step 388: loss = 2.8616 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 389: loss = 3.7746 (0.655 sec/step)\n",
            "I1212 08:24:34.600548 140053320693632 learning.py:507] global step 389: loss = 3.7746 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 390: loss = 3.3567 (1.081 sec/step)\n",
            "I1212 08:24:35.839709 140053320693632 learning.py:507] global step 390: loss = 3.3567 (1.081 sec/step)\n",
            "INFO:tensorflow:global step 391: loss = 3.6561 (0.723 sec/step)\n",
            "I1212 08:24:36.592056 140053320693632 learning.py:507] global step 391: loss = 3.6561 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 392: loss = 3.1292 (1.402 sec/step)\n",
            "I1212 08:24:38.057745 140053320693632 learning.py:507] global step 392: loss = 3.1292 (1.402 sec/step)\n",
            "INFO:tensorflow:global step 393: loss = 3.1342 (0.656 sec/step)\n",
            "I1212 08:24:38.923342 140053320693632 learning.py:507] global step 393: loss = 3.1342 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 394: loss = 2.9058 (0.696 sec/step)\n",
            "I1212 08:24:39.948330 140053320693632 learning.py:507] global step 394: loss = 2.9058 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 395: loss = 3.1717 (1.062 sec/step)\n",
            "I1212 08:24:41.073767 140053320693632 learning.py:507] global step 395: loss = 3.1717 (1.062 sec/step)\n",
            "INFO:tensorflow:global step 396: loss = 3.6374 (0.630 sec/step)\n",
            "I1212 08:24:41.940654 140053320693632 learning.py:507] global step 396: loss = 3.6374 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 397: loss = 3.2754 (1.095 sec/step)\n",
            "I1212 08:24:43.174155 140053320693632 learning.py:507] global step 397: loss = 3.2754 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 398: loss = 3.4255 (0.991 sec/step)\n",
            "I1212 08:24:44.166423 140053320693632 learning.py:507] global step 398: loss = 3.4255 (0.991 sec/step)\n",
            "INFO:tensorflow:global step 399: loss = 3.1010 (0.634 sec/step)\n",
            "I1212 08:24:45.038604 140053320693632 learning.py:507] global step 399: loss = 3.1010 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 3.3080 (0.602 sec/step)\n",
            "I1212 08:24:46.067126 140053320693632 learning.py:507] global step 400: loss = 3.3080 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 401: loss = 3.2895 (1.338 sec/step)\n",
            "I1212 08:24:47.430943 140053320693632 learning.py:507] global step 401: loss = 3.2895 (1.338 sec/step)\n",
            "INFO:tensorflow:global step 402: loss = 3.0162 (0.975 sec/step)\n",
            "I1212 08:24:48.609730 140053320693632 learning.py:507] global step 402: loss = 3.0162 (0.975 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 402.\n",
            "I1212 08:24:50.332794 140049614505728 supervisor.py:1050] Recording summary at step 402.\n",
            "INFO:tensorflow:global step 403: loss = 3.0290 (1.634 sec/step)\n",
            "I1212 08:24:50.536310 140053320693632 learning.py:507] global step 403: loss = 3.0290 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 404: loss = 2.5338 (0.614 sec/step)\n",
            "I1212 08:24:51.338063 140053320693632 learning.py:507] global step 404: loss = 2.5338 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 405: loss = 3.1440 (0.826 sec/step)\n",
            "I1212 08:24:52.479535 140053320693632 learning.py:507] global step 405: loss = 3.1440 (0.826 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.932973\n",
            "I1212 08:24:52.682841 140049622898432 supervisor.py:1099] global_step/sec: 0.932973\n",
            "INFO:tensorflow:global step 406: loss = 3.1055 (0.585 sec/step)\n",
            "I1212 08:24:53.379539 140053320693632 learning.py:507] global step 406: loss = 3.1055 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 407: loss = 3.5323 (1.057 sec/step)\n",
            "I1212 08:24:54.559990 140053320693632 learning.py:507] global step 407: loss = 3.5323 (1.057 sec/step)\n",
            "INFO:tensorflow:global step 408: loss = 2.7209 (0.669 sec/step)\n",
            "I1212 08:24:55.455475 140053320693632 learning.py:507] global step 408: loss = 2.7209 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 409: loss = 3.5434 (1.258 sec/step)\n",
            "I1212 08:24:56.779449 140053320693632 learning.py:507] global step 409: loss = 3.5434 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 410: loss = 3.5835 (0.722 sec/step)\n",
            "I1212 08:24:57.505089 140053320693632 learning.py:507] global step 410: loss = 3.5835 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 411: loss = 2.9394 (1.363 sec/step)\n",
            "I1212 08:24:58.869687 140053320693632 learning.py:507] global step 411: loss = 2.9394 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 412: loss = 3.0574 (0.491 sec/step)\n",
            "I1212 08:24:59.362622 140053320693632 learning.py:507] global step 412: loss = 3.0574 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 413: loss = 3.0554 (1.655 sec/step)\n",
            "I1212 08:25:01.020164 140053320693632 learning.py:507] global step 413: loss = 3.0554 (1.655 sec/step)\n",
            "INFO:tensorflow:global step 414: loss = 3.5062 (0.662 sec/step)\n",
            "I1212 08:25:01.988712 140053320693632 learning.py:507] global step 414: loss = 3.5062 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 415: loss = 3.2739 (1.215 sec/step)\n",
            "I1212 08:25:03.205197 140053320693632 learning.py:507] global step 415: loss = 3.2739 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 416: loss = 2.4167 (0.633 sec/step)\n",
            "I1212 08:25:04.003738 140053320693632 learning.py:507] global step 416: loss = 2.4167 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 417: loss = 2.9041 (0.632 sec/step)\n",
            "I1212 08:25:05.122718 140053320693632 learning.py:507] global step 417: loss = 2.9041 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 418: loss = 3.4956 (1.223 sec/step)\n",
            "I1212 08:25:06.350560 140053320693632 learning.py:507] global step 418: loss = 3.4956 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 419: loss = 2.9020 (0.697 sec/step)\n",
            "I1212 08:25:07.295450 140053320693632 learning.py:507] global step 419: loss = 2.9020 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 420: loss = 3.3216 (0.593 sec/step)\n",
            "I1212 08:25:08.142323 140053320693632 learning.py:507] global step 420: loss = 3.3216 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 421: loss = 2.9537 (1.174 sec/step)\n",
            "I1212 08:25:09.494866 140053320693632 learning.py:507] global step 421: loss = 2.9537 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 422: loss = 4.1118 (0.612 sec/step)\n",
            "I1212 08:25:10.282347 140053320693632 learning.py:507] global step 422: loss = 4.1118 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 423: loss = 2.8543 (0.527 sec/step)\n",
            "I1212 08:25:11.011116 140053320693632 learning.py:507] global step 423: loss = 2.8543 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 424: loss = 3.2937 (1.651 sec/step)\n",
            "I1212 08:25:12.664387 140053320693632 learning.py:507] global step 424: loss = 3.2937 (1.651 sec/step)\n",
            "INFO:tensorflow:global step 425: loss = 3.0572 (0.607 sec/step)\n",
            "I1212 08:25:13.480669 140053320693632 learning.py:507] global step 425: loss = 3.0572 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 426: loss = 3.1020 (1.063 sec/step)\n",
            "I1212 08:25:14.757816 140053320693632 learning.py:507] global step 426: loss = 3.1020 (1.063 sec/step)\n",
            "INFO:tensorflow:global step 427: loss = 2.9214 (0.634 sec/step)\n",
            "I1212 08:25:15.666725 140053320693632 learning.py:507] global step 427: loss = 2.9214 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 428: loss = 3.0036 (0.689 sec/step)\n",
            "I1212 08:25:16.578784 140053320693632 learning.py:507] global step 428: loss = 3.0036 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 429: loss = 3.7549 (0.621 sec/step)\n",
            "I1212 08:25:17.713207 140053320693632 learning.py:507] global step 429: loss = 3.7549 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 430: loss = 3.0547 (1.136 sec/step)\n",
            "I1212 08:25:18.911963 140053320693632 learning.py:507] global step 430: loss = 3.0547 (1.136 sec/step)\n",
            "INFO:tensorflow:global step 431: loss = 3.8893 (0.677 sec/step)\n",
            "I1212 08:25:19.890296 140053320693632 learning.py:507] global step 431: loss = 3.8893 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 432: loss = 3.6431 (1.105 sec/step)\n",
            "I1212 08:25:21.003024 140053320693632 learning.py:507] global step 432: loss = 3.6431 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 433: loss = 3.0219 (0.724 sec/step)\n",
            "I1212 08:25:21.903189 140053320693632 learning.py:507] global step 433: loss = 3.0219 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 434: loss = 2.6922 (0.617 sec/step)\n",
            "I1212 08:25:22.656943 140053320693632 learning.py:507] global step 434: loss = 2.6922 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 435: loss = 2.8852 (0.625 sec/step)\n",
            "I1212 08:25:23.954633 140053320693632 learning.py:507] global step 435: loss = 2.8852 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 436: loss = 3.1552 (0.629 sec/step)\n",
            "I1212 08:25:24.821807 140053320693632 learning.py:507] global step 436: loss = 3.1552 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 437: loss = 2.9514 (1.236 sec/step)\n",
            "I1212 08:25:26.185395 140053320693632 learning.py:507] global step 437: loss = 2.9514 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 438: loss = 3.2707 (0.520 sec/step)\n",
            "I1212 08:25:26.802046 140053320693632 learning.py:507] global step 438: loss = 3.2707 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 439: loss = 2.8263 (1.506 sec/step)\n",
            "I1212 08:25:28.353625 140053320693632 learning.py:507] global step 439: loss = 2.8263 (1.506 sec/step)\n",
            "INFO:tensorflow:global step 440: loss = 3.1897 (0.550 sec/step)\n",
            "I1212 08:25:28.973989 140053320693632 learning.py:507] global step 440: loss = 3.1897 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 441: loss = 3.6692 (0.715 sec/step)\n",
            "I1212 08:25:30.170684 140053320693632 learning.py:507] global step 441: loss = 3.6692 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 442: loss = 3.3331 (1.394 sec/step)\n",
            "I1212 08:25:31.764674 140053320693632 learning.py:507] global step 442: loss = 3.3331 (1.394 sec/step)\n",
            "INFO:tensorflow:global step 443: loss = 2.6635 (0.578 sec/step)\n",
            "I1212 08:25:32.645752 140053320693632 learning.py:507] global step 443: loss = 2.6635 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 444: loss = 3.6556 (1.155 sec/step)\n",
            "I1212 08:25:33.945111 140053320693632 learning.py:507] global step 444: loss = 3.6556 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 445: loss = 2.8754 (1.122 sec/step)\n",
            "I1212 08:25:35.076294 140053320693632 learning.py:507] global step 445: loss = 2.8754 (1.122 sec/step)\n",
            "INFO:tensorflow:global step 446: loss = 2.9870 (0.678 sec/step)\n",
            "I1212 08:25:35.758729 140053320693632 learning.py:507] global step 446: loss = 2.9870 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 447: loss = 3.2029 (1.448 sec/step)\n",
            "I1212 08:25:37.430051 140053320693632 learning.py:507] global step 447: loss = 3.2029 (1.448 sec/step)\n",
            "INFO:tensorflow:global step 448: loss = 3.2973 (0.483 sec/step)\n",
            "I1212 08:25:37.914453 140053320693632 learning.py:507] global step 448: loss = 3.2973 (0.483 sec/step)\n",
            "INFO:tensorflow:global step 449: loss = 3.0139 (1.689 sec/step)\n",
            "I1212 08:25:39.604920 140053320693632 learning.py:507] global step 449: loss = 3.0139 (1.689 sec/step)\n",
            "INFO:tensorflow:global step 450: loss = 2.7246 (0.623 sec/step)\n",
            "I1212 08:25:40.467076 140053320693632 learning.py:507] global step 450: loss = 2.7246 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 451: loss = 3.3406 (1.298 sec/step)\n",
            "I1212 08:25:41.793969 140053320693632 learning.py:507] global step 451: loss = 3.3406 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 452: loss = 3.5250 (0.726 sec/step)\n",
            "I1212 08:25:42.652501 140053320693632 learning.py:507] global step 452: loss = 3.5250 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 453: loss = 3.3600 (0.645 sec/step)\n",
            "I1212 08:25:43.628036 140053320693632 learning.py:507] global step 453: loss = 3.3600 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 454: loss = 2.6254 (1.341 sec/step)\n",
            "I1212 08:25:44.973239 140053320693632 learning.py:507] global step 454: loss = 2.6254 (1.341 sec/step)\n",
            "INFO:tensorflow:global step 455: loss = 2.6823 (0.662 sec/step)\n",
            "I1212 08:25:45.869386 140053320693632 learning.py:507] global step 455: loss = 2.6823 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 456: loss = 3.8000 (1.219 sec/step)\n",
            "I1212 08:25:47.141298 140053320693632 learning.py:507] global step 456: loss = 3.8000 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 457: loss = 2.6893 (1.026 sec/step)\n",
            "I1212 08:25:48.168746 140053320693632 learning.py:507] global step 457: loss = 2.6893 (1.026 sec/step)\n",
            "INFO:tensorflow:global step 458: loss = 3.4346 (0.633 sec/step)\n",
            "I1212 08:25:49.032178 140053320693632 learning.py:507] global step 458: loss = 3.4346 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 459: loss = 3.0661 (0.521 sec/step)\n",
            "I1212 08:25:49.785774 140053320693632 learning.py:507] global step 459: loss = 3.0661 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 460: loss = 3.1683 (1.366 sec/step)\n",
            "I1212 08:25:51.247783 140053320693632 learning.py:507] global step 460: loss = 3.1683 (1.366 sec/step)\n",
            "INFO:tensorflow:global step 461: loss = 2.8839 (0.640 sec/step)\n",
            "I1212 08:25:52.130270 140053320693632 learning.py:507] global step 461: loss = 2.8839 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 462: loss = 2.8670 (0.604 sec/step)\n",
            "I1212 08:25:52.972351 140053320693632 learning.py:507] global step 462: loss = 2.8670 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 463: loss = 3.1878 (0.626 sec/step)\n",
            "I1212 08:25:54.221557 140053320693632 learning.py:507] global step 463: loss = 3.1878 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 464: loss = 3.5779 (0.716 sec/step)\n",
            "I1212 08:25:55.172794 140053320693632 learning.py:507] global step 464: loss = 3.5779 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 465: loss = 3.3634 (0.642 sec/step)\n",
            "I1212 08:25:56.059526 140053320693632 learning.py:507] global step 465: loss = 3.3634 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 466: loss = 3.5271 (1.377 sec/step)\n",
            "I1212 08:25:57.469881 140053320693632 learning.py:507] global step 466: loss = 3.5271 (1.377 sec/step)\n",
            "INFO:tensorflow:global step 467: loss = 3.0450 (0.682 sec/step)\n",
            "I1212 08:25:58.192260 140053320693632 learning.py:507] global step 467: loss = 3.0450 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 468: loss = 2.6000 (0.664 sec/step)\n",
            "I1212 08:25:59.523133 140053320693632 learning.py:507] global step 468: loss = 2.6000 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 469: loss = 2.4668 (1.090 sec/step)\n",
            "I1212 08:26:00.619237 140053320693632 learning.py:507] global step 469: loss = 2.4668 (1.090 sec/step)\n",
            "INFO:tensorflow:global step 470: loss = 3.5642 (0.656 sec/step)\n",
            "I1212 08:26:01.459530 140053320693632 learning.py:507] global step 470: loss = 3.5642 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 471: loss = 2.8166 (0.885 sec/step)\n",
            "I1212 08:26:02.354003 140053320693632 learning.py:507] global step 471: loss = 2.8166 (0.885 sec/step)\n",
            "INFO:tensorflow:global step 472: loss = 2.6629 (1.433 sec/step)\n",
            "I1212 08:26:03.788987 140053320693632 learning.py:507] global step 472: loss = 2.6629 (1.433 sec/step)\n",
            "INFO:tensorflow:global step 473: loss = 3.0501 (0.685 sec/step)\n",
            "I1212 08:26:04.622949 140053320693632 learning.py:507] global step 473: loss = 3.0501 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 474: loss = 3.0980 (1.099 sec/step)\n",
            "I1212 08:26:05.840119 140053320693632 learning.py:507] global step 474: loss = 3.0980 (1.099 sec/step)\n",
            "INFO:tensorflow:global step 475: loss = 2.9364 (0.701 sec/step)\n",
            "I1212 08:26:06.740530 140053320693632 learning.py:507] global step 475: loss = 2.9364 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 476: loss = 3.0774 (1.255 sec/step)\n",
            "I1212 08:26:08.057169 140053320693632 learning.py:507] global step 476: loss = 3.0774 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 477: loss = 3.3320 (0.719 sec/step)\n",
            "I1212 08:26:09.034149 140053320693632 learning.py:507] global step 477: loss = 3.3320 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 478: loss = 2.8625 (1.163 sec/step)\n",
            "I1212 08:26:10.218169 140053320693632 learning.py:507] global step 478: loss = 2.8625 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 479: loss = 4.3445 (0.644 sec/step)\n",
            "I1212 08:26:11.089483 140053320693632 learning.py:507] global step 479: loss = 4.3445 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 480: loss = 2.9047 (1.139 sec/step)\n",
            "I1212 08:26:12.338959 140053320693632 learning.py:507] global step 480: loss = 2.9047 (1.139 sec/step)\n",
            "INFO:tensorflow:global step 481: loss = 2.5504 (0.594 sec/step)\n",
            "I1212 08:26:13.102653 140053320693632 learning.py:507] global step 481: loss = 2.5504 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 482: loss = 4.5414 (0.734 sec/step)\n",
            "I1212 08:26:14.127457 140053320693632 learning.py:507] global step 482: loss = 4.5414 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 483: loss = 4.0000 (0.838 sec/step)\n",
            "I1212 08:26:15.119808 140053320693632 learning.py:507] global step 483: loss = 4.0000 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 484: loss = 3.3241 (0.637 sec/step)\n",
            "I1212 08:26:16.173102 140053320693632 learning.py:507] global step 484: loss = 3.3241 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 485: loss = 3.4116 (1.081 sec/step)\n",
            "I1212 08:26:17.457713 140053320693632 learning.py:507] global step 485: loss = 3.4116 (1.081 sec/step)\n",
            "INFO:tensorflow:global step 486: loss = 2.9307 (0.697 sec/step)\n",
            "I1212 08:26:18.350539 140053320693632 learning.py:507] global step 486: loss = 2.9307 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 487: loss = 2.8833 (1.107 sec/step)\n",
            "I1212 08:26:19.576121 140053320693632 learning.py:507] global step 487: loss = 2.8833 (1.107 sec/step)\n",
            "INFO:tensorflow:global step 488: loss = 2.8938 (0.572 sec/step)\n",
            "I1212 08:26:20.545263 140053320693632 learning.py:507] global step 488: loss = 2.8938 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 489: loss = 3.1177 (0.616 sec/step)\n",
            "I1212 08:26:21.279082 140053320693632 learning.py:507] global step 489: loss = 3.1177 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 490: loss = 3.0920 (1.305 sec/step)\n",
            "I1212 08:26:22.821436 140053320693632 learning.py:507] global step 490: loss = 3.0920 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 491: loss = 3.2832 (0.807 sec/step)\n",
            "I1212 08:26:23.877020 140053320693632 learning.py:507] global step 491: loss = 3.2832 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 492: loss = 3.3798 (0.529 sec/step)\n",
            "I1212 08:26:24.557248 140053320693632 learning.py:507] global step 492: loss = 3.3798 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 493: loss = 3.0600 (0.705 sec/step)\n",
            "I1212 08:26:25.692533 140053320693632 learning.py:507] global step 493: loss = 3.0600 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 494: loss = 2.8645 (0.642 sec/step)\n",
            "I1212 08:26:26.716453 140053320693632 learning.py:507] global step 494: loss = 2.8645 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 495: loss = 2.5518 (1.660 sec/step)\n",
            "I1212 08:26:28.378027 140053320693632 learning.py:507] global step 495: loss = 2.5518 (1.660 sec/step)\n",
            "INFO:tensorflow:global step 496: loss = 3.0053 (0.686 sec/step)\n",
            "I1212 08:26:29.352799 140053320693632 learning.py:507] global step 496: loss = 3.0053 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 497: loss = 2.8167 (0.601 sec/step)\n",
            "I1212 08:26:30.208694 140053320693632 learning.py:507] global step 497: loss = 2.8167 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 498: loss = 3.2898 (0.675 sec/step)\n",
            "I1212 08:26:31.360441 140053320693632 learning.py:507] global step 498: loss = 3.2898 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 499: loss = 3.0215 (0.671 sec/step)\n",
            "I1212 08:26:32.303762 140053320693632 learning.py:507] global step 499: loss = 3.0215 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 500: loss = 3.0709 (1.193 sec/step)\n",
            "I1212 08:26:33.754053 140053320693632 learning.py:507] global step 500: loss = 3.0709 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 501: loss = 3.0676 (0.712 sec/step)\n",
            "I1212 08:26:34.522641 140053320693632 learning.py:507] global step 501: loss = 3.0676 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 502: loss = 2.7089 (1.276 sec/step)\n",
            "I1212 08:26:35.929476 140053320693632 learning.py:507] global step 502: loss = 2.7089 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 503: loss = 3.0561 (0.760 sec/step)\n",
            "I1212 08:26:36.953166 140053320693632 learning.py:507] global step 503: loss = 3.0561 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 504: loss = 3.3401 (1.255 sec/step)\n",
            "I1212 08:26:38.209924 140053320693632 learning.py:507] global step 504: loss = 3.3401 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 505: loss = 2.6430 (0.608 sec/step)\n",
            "I1212 08:26:39.032970 140053320693632 learning.py:507] global step 505: loss = 2.6430 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 506: loss = 2.8697 (1.144 sec/step)\n",
            "I1212 08:26:40.364996 140053320693632 learning.py:507] global step 506: loss = 2.8697 (1.144 sec/step)\n",
            "INFO:tensorflow:global step 507: loss = 3.9848 (1.058 sec/step)\n",
            "I1212 08:26:41.424769 140053320693632 learning.py:507] global step 507: loss = 3.9848 (1.058 sec/step)\n",
            "INFO:tensorflow:global step 508: loss = 3.3844 (0.657 sec/step)\n",
            "I1212 08:26:42.216762 140053320693632 learning.py:507] global step 508: loss = 3.3844 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 509: loss = 2.9616 (0.657 sec/step)\n",
            "I1212 08:26:43.180232 140053320693632 learning.py:507] global step 509: loss = 2.9616 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 510: loss = 2.6567 (0.666 sec/step)\n",
            "I1212 08:26:44.406267 140053320693632 learning.py:507] global step 510: loss = 2.6567 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 511: loss = 2.6993 (0.733 sec/step)\n",
            "I1212 08:26:45.387977 140053320693632 learning.py:507] global step 511: loss = 2.6993 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 512: loss = 3.1197 (0.574 sec/step)\n",
            "I1212 08:26:46.352436 140053320693632 learning.py:507] global step 512: loss = 3.1197 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 513: loss = 3.2422 (0.652 sec/step)\n",
            "I1212 08:26:47.381907 140053320693632 learning.py:507] global step 513: loss = 3.2422 (0.652 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1212 08:26:47.881978 140049589327616 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 513.\n",
            "I1212 08:26:49.519636 140049614505728 supervisor.py:1050] Recording summary at step 513.\n",
            "INFO:tensorflow:global step 514: loss = 2.5929 (2.405 sec/step)\n",
            "I1212 08:26:50.017535 140053320693632 learning.py:507] global step 514: loss = 2.5929 (2.405 sec/step)\n",
            "INFO:tensorflow:global step 515: loss = 3.3362 (1.302 sec/step)\n",
            "I1212 08:26:51.486226 140053320693632 learning.py:507] global step 515: loss = 3.3362 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 516: loss = 3.3817 (0.943 sec/step)\n",
            "I1212 08:26:52.830998 140053320693632 learning.py:507] global step 516: loss = 3.3817 (0.943 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.921297\n",
            "I1212 08:26:53.165159 140049622898432 supervisor.py:1099] global_step/sec: 0.921297\n",
            "INFO:tensorflow:global step 517: loss = 2.8394 (1.435 sec/step)\n",
            "I1212 08:26:54.600648 140053320693632 learning.py:507] global step 517: loss = 2.8394 (1.435 sec/step)\n",
            "INFO:tensorflow:global step 518: loss = 2.6706 (0.611 sec/step)\n",
            "I1212 08:26:55.439934 140053320693632 learning.py:507] global step 518: loss = 2.6706 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 519: loss = 2.9878 (0.687 sec/step)\n",
            "I1212 08:26:56.517616 140053320693632 learning.py:507] global step 519: loss = 2.9878 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 520: loss = 3.3683 (1.288 sec/step)\n",
            "I1212 08:26:57.830463 140053320693632 learning.py:507] global step 520: loss = 3.3683 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 521: loss = 2.9663 (0.862 sec/step)\n",
            "I1212 08:26:58.863789 140053320693632 learning.py:507] global step 521: loss = 2.9663 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 522: loss = 3.6980 (0.544 sec/step)\n",
            "I1212 08:26:59.512510 140053320693632 learning.py:507] global step 522: loss = 3.6980 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 523: loss = 3.4644 (1.901 sec/step)\n",
            "I1212 08:27:01.415593 140053320693632 learning.py:507] global step 523: loss = 3.4644 (1.901 sec/step)\n",
            "INFO:tensorflow:global step 524: loss = 3.1799 (0.695 sec/step)\n",
            "I1212 08:27:02.252882 140053320693632 learning.py:507] global step 524: loss = 3.1799 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 525: loss = 3.6292 (1.602 sec/step)\n",
            "I1212 08:27:03.886579 140053320693632 learning.py:507] global step 525: loss = 3.6292 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 526: loss = 3.4017 (0.590 sec/step)\n",
            "I1212 08:27:04.661010 140053320693632 learning.py:507] global step 526: loss = 3.4017 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 527: loss = 2.7256 (0.644 sec/step)\n",
            "I1212 08:27:05.726133 140053320693632 learning.py:507] global step 527: loss = 2.7256 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 528: loss = 2.4890 (0.848 sec/step)\n",
            "I1212 08:27:06.743652 140053320693632 learning.py:507] global step 528: loss = 2.4890 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 529: loss = 2.8854 (1.567 sec/step)\n",
            "I1212 08:27:08.418919 140053320693632 learning.py:507] global step 529: loss = 2.8854 (1.567 sec/step)\n",
            "INFO:tensorflow:global step 530: loss = 2.8236 (0.706 sec/step)\n",
            "I1212 08:27:09.223182 140053320693632 learning.py:507] global step 530: loss = 2.8236 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 531: loss = 2.7735 (0.760 sec/step)\n",
            "I1212 08:27:10.494498 140053320693632 learning.py:507] global step 531: loss = 2.7735 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 532: loss = 2.6544 (1.206 sec/step)\n",
            "I1212 08:27:11.773878 140053320693632 learning.py:507] global step 532: loss = 2.6544 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 533: loss = 2.8160 (0.643 sec/step)\n",
            "I1212 08:27:12.678964 140053320693632 learning.py:507] global step 533: loss = 2.8160 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 534: loss = 2.9505 (0.579 sec/step)\n",
            "I1212 08:27:13.883518 140053320693632 learning.py:507] global step 534: loss = 2.9505 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 535: loss = 2.4876 (1.275 sec/step)\n",
            "I1212 08:27:15.208564 140053320693632 learning.py:507] global step 535: loss = 2.4876 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 536: loss = 3.5715 (0.536 sec/step)\n",
            "I1212 08:27:15.986698 140053320693632 learning.py:507] global step 536: loss = 3.5715 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 537: loss = 3.0496 (0.604 sec/step)\n",
            "I1212 08:27:17.130708 140053320693632 learning.py:507] global step 537: loss = 3.0496 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 538: loss = 2.8664 (1.191 sec/step)\n",
            "I1212 08:27:18.382112 140053320693632 learning.py:507] global step 538: loss = 2.8664 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 539: loss = 3.3652 (0.698 sec/step)\n",
            "I1212 08:27:19.285068 140053320693632 learning.py:507] global step 539: loss = 3.3652 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 540: loss = 2.6811 (0.613 sec/step)\n",
            "I1212 08:27:20.356930 140053320693632 learning.py:507] global step 540: loss = 2.6811 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 541: loss = 3.4558 (1.497 sec/step)\n",
            "I1212 08:27:21.996635 140053320693632 learning.py:507] global step 541: loss = 3.4558 (1.497 sec/step)\n",
            "INFO:tensorflow:global step 542: loss = 3.0130 (0.508 sec/step)\n",
            "I1212 08:27:22.727985 140053320693632 learning.py:507] global step 542: loss = 3.0130 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 543: loss = 2.9765 (1.380 sec/step)\n",
            "I1212 08:27:24.253877 140053320693632 learning.py:507] global step 543: loss = 2.9765 (1.380 sec/step)\n",
            "INFO:tensorflow:global step 544: loss = 2.5120 (1.153 sec/step)\n",
            "I1212 08:27:25.409186 140053320693632 learning.py:507] global step 544: loss = 2.5120 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 545: loss = 2.9683 (0.663 sec/step)\n",
            "I1212 08:27:26.373480 140053320693632 learning.py:507] global step 545: loss = 2.9683 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 546: loss = 3.5800 (0.812 sec/step)\n",
            "I1212 08:27:27.414902 140053320693632 learning.py:507] global step 546: loss = 3.5800 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 547: loss = 3.1576 (0.684 sec/step)\n",
            "I1212 08:27:28.459512 140053320693632 learning.py:507] global step 547: loss = 3.1576 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 548: loss = 3.3668 (0.813 sec/step)\n",
            "I1212 08:27:29.543463 140053320693632 learning.py:507] global step 548: loss = 3.3668 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 549: loss = 2.3853 (0.562 sec/step)\n",
            "I1212 08:27:30.107237 140053320693632 learning.py:507] global step 549: loss = 2.3853 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 550: loss = 2.8191 (1.749 sec/step)\n",
            "I1212 08:27:31.857663 140053320693632 learning.py:507] global step 550: loss = 2.8191 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 551: loss = 3.1674 (0.621 sec/step)\n",
            "I1212 08:27:32.826948 140053320693632 learning.py:507] global step 551: loss = 3.1674 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 552: loss = 2.5592 (0.512 sec/step)\n",
            "I1212 08:27:33.455530 140053320693632 learning.py:507] global step 552: loss = 2.5592 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 553: loss = 2.5159 (1.817 sec/step)\n",
            "I1212 08:27:35.273823 140053320693632 learning.py:507] global step 553: loss = 2.5159 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 554: loss = 2.9806 (0.624 sec/step)\n",
            "I1212 08:27:36.082477 140053320693632 learning.py:507] global step 554: loss = 2.9806 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 555: loss = 3.0760 (1.564 sec/step)\n",
            "I1212 08:27:37.692400 140053320693632 learning.py:507] global step 555: loss = 3.0760 (1.564 sec/step)\n",
            "INFO:tensorflow:global step 556: loss = 2.7700 (0.674 sec/step)\n",
            "I1212 08:27:38.556438 140053320693632 learning.py:507] global step 556: loss = 2.7700 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 557: loss = 3.3767 (0.832 sec/step)\n",
            "I1212 08:27:39.758074 140053320693632 learning.py:507] global step 557: loss = 3.3767 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 558: loss = 3.1305 (0.713 sec/step)\n",
            "I1212 08:27:40.642827 140053320693632 learning.py:507] global step 558: loss = 3.1305 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 559: loss = 2.7985 (0.688 sec/step)\n",
            "I1212 08:27:41.805544 140053320693632 learning.py:507] global step 559: loss = 2.7985 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 560: loss = 3.3792 (0.702 sec/step)\n",
            "I1212 08:27:42.924527 140053320693632 learning.py:507] global step 560: loss = 3.3792 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 561: loss = 3.5529 (1.413 sec/step)\n",
            "I1212 08:27:44.379144 140053320693632 learning.py:507] global step 561: loss = 3.5529 (1.413 sec/step)\n",
            "INFO:tensorflow:global step 562: loss = 2.8811 (0.692 sec/step)\n",
            "I1212 08:27:45.366668 140053320693632 learning.py:507] global step 562: loss = 2.8811 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 563: loss = 2.9668 (0.699 sec/step)\n",
            "I1212 08:27:46.286464 140053320693632 learning.py:507] global step 563: loss = 2.9668 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 564: loss = 2.3213 (1.207 sec/step)\n",
            "I1212 08:27:47.667273 140053320693632 learning.py:507] global step 564: loss = 2.3213 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 565: loss = 3.0092 (0.750 sec/step)\n",
            "I1212 08:27:48.492564 140053320693632 learning.py:507] global step 565: loss = 3.0092 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 566: loss = 3.5533 (0.619 sec/step)\n",
            "I1212 08:27:49.435497 140053320693632 learning.py:507] global step 566: loss = 3.5533 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 567: loss = 3.8948 (1.503 sec/step)\n",
            "I1212 08:27:51.009577 140053320693632 learning.py:507] global step 567: loss = 3.8948 (1.503 sec/step)\n",
            "INFO:tensorflow:global step 568: loss = 2.9773 (0.600 sec/step)\n",
            "I1212 08:27:51.611212 140053320693632 learning.py:507] global step 568: loss = 2.9773 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 569: loss = 3.2252 (1.680 sec/step)\n",
            "I1212 08:27:53.293027 140053320693632 learning.py:507] global step 569: loss = 3.2252 (1.680 sec/step)\n",
            "INFO:tensorflow:global step 570: loss = 3.0379 (0.630 sec/step)\n",
            "I1212 08:27:54.170657 140053320693632 learning.py:507] global step 570: loss = 3.0379 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 571: loss = 3.1085 (0.672 sec/step)\n",
            "I1212 08:27:55.117536 140053320693632 learning.py:507] global step 571: loss = 3.1085 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 572: loss = 3.1059 (0.762 sec/step)\n",
            "I1212 08:27:56.183028 140053320693632 learning.py:507] global step 572: loss = 3.1059 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 573: loss = 2.5097 (0.710 sec/step)\n",
            "I1212 08:27:57.358941 140053320693632 learning.py:507] global step 573: loss = 2.5097 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 574: loss = 2.6678 (0.726 sec/step)\n",
            "I1212 08:27:58.369517 140053320693632 learning.py:507] global step 574: loss = 2.6678 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 575: loss = 2.9763 (0.803 sec/step)\n",
            "I1212 08:27:59.487512 140053320693632 learning.py:507] global step 575: loss = 2.9763 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 576: loss = 3.0583 (1.362 sec/step)\n",
            "I1212 08:28:01.013982 140053320693632 learning.py:507] global step 576: loss = 3.0583 (1.362 sec/step)\n",
            "INFO:tensorflow:global step 577: loss = 3.1297 (0.621 sec/step)\n",
            "I1212 08:28:01.940247 140053320693632 learning.py:507] global step 577: loss = 3.1297 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 578: loss = 2.9257 (0.591 sec/step)\n",
            "I1212 08:28:02.834120 140053320693632 learning.py:507] global step 578: loss = 2.9257 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 579: loss = 3.1382 (0.554 sec/step)\n",
            "I1212 08:28:03.661528 140053320693632 learning.py:507] global step 579: loss = 3.1382 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 580: loss = 2.4787 (1.605 sec/step)\n",
            "I1212 08:28:05.436305 140053320693632 learning.py:507] global step 580: loss = 2.4787 (1.605 sec/step)\n",
            "INFO:tensorflow:global step 581: loss = 3.3659 (0.493 sec/step)\n",
            "I1212 08:28:06.270509 140053320693632 learning.py:507] global step 581: loss = 3.3659 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 582: loss = 3.0030 (0.660 sec/step)\n",
            "I1212 08:28:07.276122 140053320693632 learning.py:507] global step 582: loss = 3.0030 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 583: loss = 3.8510 (0.795 sec/step)\n",
            "I1212 08:28:08.390555 140053320693632 learning.py:507] global step 583: loss = 3.8510 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 584: loss = 3.0406 (0.610 sec/step)\n",
            "I1212 08:28:09.258370 140053320693632 learning.py:507] global step 584: loss = 3.0406 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 585: loss = 3.0248 (1.600 sec/step)\n",
            "I1212 08:28:10.860132 140053320693632 learning.py:507] global step 585: loss = 3.0248 (1.600 sec/step)\n",
            "INFO:tensorflow:global step 586: loss = 3.8318 (0.624 sec/step)\n",
            "I1212 08:28:11.701655 140053320693632 learning.py:507] global step 586: loss = 3.8318 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 587: loss = 3.1124 (1.293 sec/step)\n",
            "I1212 08:28:13.228578 140053320693632 learning.py:507] global step 587: loss = 3.1124 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 588: loss = 4.0977 (0.707 sec/step)\n",
            "I1212 08:28:13.960303 140053320693632 learning.py:507] global step 588: loss = 4.0977 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 589: loss = 2.7520 (1.499 sec/step)\n",
            "I1212 08:28:15.461953 140053320693632 learning.py:507] global step 589: loss = 2.7520 (1.499 sec/step)\n",
            "INFO:tensorflow:global step 590: loss = 2.8021 (0.695 sec/step)\n",
            "I1212 08:28:16.404444 140053320693632 learning.py:507] global step 590: loss = 2.8021 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 591: loss = 2.8772 (1.216 sec/step)\n",
            "I1212 08:28:17.716973 140053320693632 learning.py:507] global step 591: loss = 2.8772 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 592: loss = 3.0122 (0.730 sec/step)\n",
            "I1212 08:28:18.600478 140053320693632 learning.py:507] global step 592: loss = 3.0122 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 593: loss = 3.0788 (0.714 sec/step)\n",
            "I1212 08:28:19.622861 140053320693632 learning.py:507] global step 593: loss = 3.0788 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 594: loss = 2.3805 (1.250 sec/step)\n",
            "I1212 08:28:20.956037 140053320693632 learning.py:507] global step 594: loss = 2.3805 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 595: loss = 3.2729 (0.615 sec/step)\n",
            "I1212 08:28:21.833661 140053320693632 learning.py:507] global step 595: loss = 3.2729 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 596: loss = 3.4331 (1.231 sec/step)\n",
            "I1212 08:28:23.115207 140053320693632 learning.py:507] global step 596: loss = 3.4331 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 597: loss = 3.3450 (0.655 sec/step)\n",
            "I1212 08:28:23.846653 140053320693632 learning.py:507] global step 597: loss = 3.3450 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 598: loss = 3.1221 (0.760 sec/step)\n",
            "I1212 08:28:25.034973 140053320693632 learning.py:507] global step 598: loss = 3.1221 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 599: loss = 3.3821 (0.661 sec/step)\n",
            "I1212 08:28:26.212126 140053320693632 learning.py:507] global step 599: loss = 3.3821 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 2.9157 (0.643 sec/step)\n",
            "I1212 08:28:27.120486 140053320693632 learning.py:507] global step 600: loss = 2.9157 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 601: loss = 3.2106 (0.506 sec/step)\n",
            "I1212 08:28:28.003569 140053320693632 learning.py:507] global step 601: loss = 3.2106 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 602: loss = 2.4805 (1.144 sec/step)\n",
            "I1212 08:28:29.361539 140053320693632 learning.py:507] global step 602: loss = 2.4805 (1.144 sec/step)\n",
            "INFO:tensorflow:global step 603: loss = 2.3894 (0.595 sec/step)\n",
            "I1212 08:28:30.316849 140053320693632 learning.py:507] global step 603: loss = 2.3894 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 604: loss = 2.8409 (1.228 sec/step)\n",
            "I1212 08:28:31.564626 140053320693632 learning.py:507] global step 604: loss = 2.8409 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 605: loss = 2.7854 (0.735 sec/step)\n",
            "I1212 08:28:32.506518 140053320693632 learning.py:507] global step 605: loss = 2.7854 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 606: loss = 2.3187 (1.198 sec/step)\n",
            "I1212 08:28:33.770228 140053320693632 learning.py:507] global step 606: loss = 2.3187 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 607: loss = 3.6020 (0.676 sec/step)\n",
            "I1212 08:28:34.647639 140053320693632 learning.py:507] global step 607: loss = 3.6020 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 608: loss = 3.1837 (0.692 sec/step)\n",
            "I1212 08:28:35.696952 140053320693632 learning.py:507] global step 608: loss = 3.1837 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 609: loss = 2.7460 (1.307 sec/step)\n",
            "I1212 08:28:37.232918 140053320693632 learning.py:507] global step 609: loss = 2.7460 (1.307 sec/step)\n",
            "INFO:tensorflow:global step 610: loss = 2.7204 (0.664 sec/step)\n",
            "I1212 08:28:38.230736 140053320693632 learning.py:507] global step 610: loss = 2.7204 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 611: loss = 2.8117 (0.759 sec/step)\n",
            "I1212 08:28:39.173006 140053320693632 learning.py:507] global step 611: loss = 2.8117 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 612: loss = 3.2925 (0.662 sec/step)\n",
            "I1212 08:28:39.939055 140053320693632 learning.py:507] global step 612: loss = 3.2925 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 613: loss = 2.4210 (1.723 sec/step)\n",
            "I1212 08:28:41.663300 140053320693632 learning.py:507] global step 613: loss = 2.4210 (1.723 sec/step)\n",
            "INFO:tensorflow:global step 614: loss = 3.2166 (0.525 sec/step)\n",
            "I1212 08:28:42.479930 140053320693632 learning.py:507] global step 614: loss = 3.2166 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 615: loss = 3.4014 (0.707 sec/step)\n",
            "I1212 08:28:43.651529 140053320693632 learning.py:507] global step 615: loss = 3.4014 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 616: loss = 3.1165 (1.497 sec/step)\n",
            "I1212 08:28:45.153380 140053320693632 learning.py:507] global step 616: loss = 3.1165 (1.497 sec/step)\n",
            "INFO:tensorflow:global step 617: loss = 2.7207 (0.677 sec/step)\n",
            "I1212 08:28:45.874513 140053320693632 learning.py:507] global step 617: loss = 2.7207 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 618: loss = 3.0403 (0.576 sec/step)\n",
            "I1212 08:28:47.218302 140053320693632 learning.py:507] global step 618: loss = 3.0403 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 619: loss = 3.5005 (1.991 sec/step)\n",
            "I1212 08:28:49.383395 140053320693632 learning.py:507] global step 619: loss = 3.5005 (1.991 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 619.\n",
            "I1212 08:28:49.667444 140049614505728 supervisor.py:1050] Recording summary at step 619.\n",
            "INFO:tensorflow:global step 620: loss = 2.3854 (0.668 sec/step)\n",
            "I1212 08:28:50.329427 140053320693632 learning.py:507] global step 620: loss = 2.3854 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 621: loss = 3.1627 (1.273 sec/step)\n",
            "I1212 08:28:51.638435 140053320693632 learning.py:507] global step 621: loss = 3.1627 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 622: loss = 2.9023 (0.552 sec/step)\n",
            "I1212 08:28:52.496221 140053320693632 learning.py:507] global step 622: loss = 2.9023 (0.552 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.887231\n",
            "I1212 08:28:52.638006 140049622898432 supervisor.py:1099] global_step/sec: 0.887231\n",
            "INFO:tensorflow:global step 623: loss = 3.4005 (0.517 sec/step)\n",
            "I1212 08:28:53.176529 140053320693632 learning.py:507] global step 623: loss = 3.4005 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 624: loss = 2.6141 (1.743 sec/step)\n",
            "I1212 08:28:54.921294 140053320693632 learning.py:507] global step 624: loss = 2.6141 (1.743 sec/step)\n",
            "INFO:tensorflow:global step 625: loss = 3.2219 (0.616 sec/step)\n",
            "I1212 08:28:55.719324 140053320693632 learning.py:507] global step 625: loss = 3.2219 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 626: loss = 2.7724 (0.505 sec/step)\n",
            "I1212 08:28:56.601976 140053320693632 learning.py:507] global step 626: loss = 2.7724 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 627: loss = 2.1493 (1.332 sec/step)\n",
            "I1212 08:28:58.165034 140053320693632 learning.py:507] global step 627: loss = 2.1493 (1.332 sec/step)\n",
            "INFO:tensorflow:global step 628: loss = 3.2257 (0.482 sec/step)\n",
            "I1212 08:28:58.648875 140053320693632 learning.py:507] global step 628: loss = 3.2257 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 629: loss = 2.8801 (1.822 sec/step)\n",
            "I1212 08:29:00.472457 140053320693632 learning.py:507] global step 629: loss = 2.8801 (1.822 sec/step)\n",
            "INFO:tensorflow:global step 630: loss = 2.7036 (0.625 sec/step)\n",
            "I1212 08:29:01.422888 140053320693632 learning.py:507] global step 630: loss = 2.7036 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 631: loss = 2.9556 (0.736 sec/step)\n",
            "I1212 08:29:02.461379 140053320693632 learning.py:507] global step 631: loss = 2.9556 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 632: loss = 2.6381 (0.794 sec/step)\n",
            "I1212 08:29:03.465975 140053320693632 learning.py:507] global step 632: loss = 2.6381 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 633: loss = 3.1975 (1.331 sec/step)\n",
            "I1212 08:29:04.828742 140053320693632 learning.py:507] global step 633: loss = 3.1975 (1.331 sec/step)\n",
            "INFO:tensorflow:global step 634: loss = 2.8426 (0.626 sec/step)\n",
            "I1212 08:29:05.718424 140053320693632 learning.py:507] global step 634: loss = 2.8426 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 635: loss = 2.4513 (0.599 sec/step)\n",
            "I1212 08:29:06.548465 140053320693632 learning.py:507] global step 635: loss = 2.4513 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 636: loss = 3.6701 (1.296 sec/step)\n",
            "I1212 08:29:08.036102 140053320693632 learning.py:507] global step 636: loss = 3.6701 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 637: loss = 2.3171 (0.617 sec/step)\n",
            "I1212 08:29:08.937706 140053320693632 learning.py:507] global step 637: loss = 2.3171 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 638: loss = 3.1205 (0.678 sec/step)\n",
            "I1212 08:29:10.158978 140053320693632 learning.py:507] global step 638: loss = 3.1205 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 639: loss = 3.2985 (0.841 sec/step)\n",
            "I1212 08:29:11.289604 140053320693632 learning.py:507] global step 639: loss = 3.2985 (0.841 sec/step)\n",
            "INFO:tensorflow:global step 640: loss = 2.7667 (1.440 sec/step)\n",
            "I1212 08:29:12.734234 140053320693632 learning.py:507] global step 640: loss = 2.7667 (1.440 sec/step)\n",
            "INFO:tensorflow:global step 641: loss = 3.2702 (0.683 sec/step)\n",
            "I1212 08:29:13.545765 140053320693632 learning.py:507] global step 641: loss = 3.2702 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 642: loss = 3.4413 (1.173 sec/step)\n",
            "I1212 08:29:14.942523 140053320693632 learning.py:507] global step 642: loss = 3.4413 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 643: loss = 3.5968 (0.693 sec/step)\n",
            "I1212 08:29:15.646502 140053320693632 learning.py:507] global step 643: loss = 3.5968 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 644: loss = 2.5992 (1.335 sec/step)\n",
            "I1212 08:29:17.203535 140053320693632 learning.py:507] global step 644: loss = 2.5992 (1.335 sec/step)\n",
            "INFO:tensorflow:global step 645: loss = 2.9753 (0.547 sec/step)\n",
            "I1212 08:29:17.752279 140053320693632 learning.py:507] global step 645: loss = 2.9753 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 646: loss = 3.0825 (0.878 sec/step)\n",
            "I1212 08:29:18.858234 140053320693632 learning.py:507] global step 646: loss = 3.0825 (0.878 sec/step)\n",
            "INFO:tensorflow:global step 647: loss = 2.5622 (1.492 sec/step)\n",
            "I1212 08:29:20.609301 140053320693632 learning.py:507] global step 647: loss = 2.5622 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 648: loss = 2.6273 (0.522 sec/step)\n",
            "I1212 08:29:21.428556 140053320693632 learning.py:507] global step 648: loss = 2.6273 (0.522 sec/step)\n",
            "INFO:tensorflow:global step 649: loss = 2.4318 (1.523 sec/step)\n",
            "I1212 08:29:22.974881 140053320693632 learning.py:507] global step 649: loss = 2.4318 (1.523 sec/step)\n",
            "INFO:tensorflow:global step 650: loss = 3.1653 (0.499 sec/step)\n",
            "I1212 08:29:23.782516 140053320693632 learning.py:507] global step 650: loss = 3.1653 (0.499 sec/step)\n",
            "INFO:tensorflow:global step 651: loss = 2.5977 (1.281 sec/step)\n",
            "I1212 08:29:25.135846 140053320693632 learning.py:507] global step 651: loss = 2.5977 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 652: loss = 2.4817 (0.698 sec/step)\n",
            "I1212 08:29:26.054774 140053320693632 learning.py:507] global step 652: loss = 2.4817 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 653: loss = 2.8094 (1.346 sec/step)\n",
            "I1212 08:29:27.502093 140053320693632 learning.py:507] global step 653: loss = 2.8094 (1.346 sec/step)\n",
            "INFO:tensorflow:global step 654: loss = 3.9349 (0.621 sec/step)\n",
            "I1212 08:29:28.270229 140053320693632 learning.py:507] global step 654: loss = 3.9349 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 655: loss = 3.0766 (1.110 sec/step)\n",
            "I1212 08:29:29.616737 140053320693632 learning.py:507] global step 655: loss = 3.0766 (1.110 sec/step)\n",
            "INFO:tensorflow:global step 656: loss = 3.6040 (0.604 sec/step)\n",
            "I1212 08:29:30.394381 140053320693632 learning.py:507] global step 656: loss = 3.6040 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 657: loss = 3.1165 (0.530 sec/step)\n",
            "I1212 08:29:31.382789 140053320693632 learning.py:507] global step 657: loss = 3.1165 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 658: loss = 2.3071 (1.524 sec/step)\n",
            "I1212 08:29:33.179342 140053320693632 learning.py:507] global step 658: loss = 2.3071 (1.524 sec/step)\n",
            "INFO:tensorflow:global step 659: loss = 3.8115 (0.597 sec/step)\n",
            "I1212 08:29:33.943725 140053320693632 learning.py:507] global step 659: loss = 3.8115 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 660: loss = 2.5669 (0.586 sec/step)\n",
            "I1212 08:29:35.205380 140053320693632 learning.py:507] global step 660: loss = 2.5669 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 661: loss = 3.2104 (0.743 sec/step)\n",
            "I1212 08:29:36.089737 140053320693632 learning.py:507] global step 661: loss = 3.2104 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 662: loss = 3.0652 (0.951 sec/step)\n",
            "I1212 08:29:37.424829 140053320693632 learning.py:507] global step 662: loss = 3.0652 (0.951 sec/step)\n",
            "INFO:tensorflow:global step 663: loss = 2.8795 (1.092 sec/step)\n",
            "I1212 08:29:38.551540 140053320693632 learning.py:507] global step 663: loss = 2.8795 (1.092 sec/step)\n",
            "INFO:tensorflow:global step 664: loss = 3.4288 (0.610 sec/step)\n",
            "I1212 08:29:39.507960 140053320693632 learning.py:507] global step 664: loss = 3.4288 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 665: loss = 2.6840 (0.759 sec/step)\n",
            "I1212 08:29:40.676948 140053320693632 learning.py:507] global step 665: loss = 2.6840 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 666: loss = 3.1592 (0.628 sec/step)\n",
            "I1212 08:29:41.640316 140053320693632 learning.py:507] global step 666: loss = 3.1592 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 667: loss = 3.1243 (0.707 sec/step)\n",
            "I1212 08:29:42.697743 140053320693632 learning.py:507] global step 667: loss = 3.1243 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 668: loss = 2.4836 (0.590 sec/step)\n",
            "I1212 08:29:43.792593 140053320693632 learning.py:507] global step 668: loss = 2.4836 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 669: loss = 2.1584 (1.291 sec/step)\n",
            "I1212 08:29:45.222427 140053320693632 learning.py:507] global step 669: loss = 2.1584 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 670: loss = 2.9301 (0.675 sec/step)\n",
            "I1212 08:29:46.224043 140053320693632 learning.py:507] global step 670: loss = 2.9301 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 671: loss = 3.8535 (0.604 sec/step)\n",
            "I1212 08:29:47.183506 140053320693632 learning.py:507] global step 671: loss = 3.8535 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 672: loss = 3.8265 (0.589 sec/step)\n",
            "I1212 08:29:48.182282 140053320693632 learning.py:507] global step 672: loss = 3.8265 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 673: loss = 3.4874 (0.554 sec/step)\n",
            "I1212 08:29:48.994812 140053320693632 learning.py:507] global step 673: loss = 3.4874 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 674: loss = 2.8418 (1.781 sec/step)\n",
            "I1212 08:29:50.777869 140053320693632 learning.py:507] global step 674: loss = 2.8418 (1.781 sec/step)\n",
            "INFO:tensorflow:global step 675: loss = 2.5689 (0.562 sec/step)\n",
            "I1212 08:29:51.498416 140053320693632 learning.py:507] global step 675: loss = 2.5689 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 676: loss = 2.2859 (1.357 sec/step)\n",
            "I1212 08:29:53.030872 140053320693632 learning.py:507] global step 676: loss = 2.2859 (1.357 sec/step)\n",
            "INFO:tensorflow:global step 677: loss = 3.1229 (0.608 sec/step)\n",
            "I1212 08:29:53.743830 140053320693632 learning.py:507] global step 677: loss = 3.1229 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 678: loss = 2.9599 (1.383 sec/step)\n",
            "I1212 08:29:55.280606 140053320693632 learning.py:507] global step 678: loss = 2.9599 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 679: loss = 2.3227 (0.595 sec/step)\n",
            "I1212 08:29:56.134797 140053320693632 learning.py:507] global step 679: loss = 2.3227 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 680: loss = 4.3321 (0.651 sec/step)\n",
            "I1212 08:29:57.187932 140053320693632 learning.py:507] global step 680: loss = 4.3321 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 681: loss = 4.1830 (0.538 sec/step)\n",
            "I1212 08:29:57.923423 140053320693632 learning.py:507] global step 681: loss = 4.1830 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 682: loss = 2.6281 (1.311 sec/step)\n",
            "I1212 08:29:59.427223 140053320693632 learning.py:507] global step 682: loss = 2.6281 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 683: loss = 2.7586 (0.690 sec/step)\n",
            "I1212 08:30:00.432454 140053320693632 learning.py:507] global step 683: loss = 2.7586 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 684: loss = 3.4217 (0.553 sec/step)\n",
            "I1212 08:30:01.005823 140053320693632 learning.py:507] global step 684: loss = 3.4217 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 685: loss = 2.9144 (1.661 sec/step)\n",
            "I1212 08:30:02.667943 140053320693632 learning.py:507] global step 685: loss = 2.9144 (1.661 sec/step)\n",
            "INFO:tensorflow:global step 686: loss = 3.0630 (0.665 sec/step)\n",
            "I1212 08:30:03.531995 140053320693632 learning.py:507] global step 686: loss = 3.0630 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 687: loss = 2.7851 (1.355 sec/step)\n",
            "I1212 08:30:05.033503 140053320693632 learning.py:507] global step 687: loss = 2.7851 (1.355 sec/step)\n",
            "INFO:tensorflow:global step 688: loss = 2.8182 (0.715 sec/step)\n",
            "I1212 08:30:05.992087 140053320693632 learning.py:507] global step 688: loss = 2.8182 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 689: loss = 2.5346 (0.607 sec/step)\n",
            "I1212 08:30:06.825217 140053320693632 learning.py:507] global step 689: loss = 2.5346 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 690: loss = 2.7043 (1.257 sec/step)\n",
            "I1212 08:30:08.103427 140053320693632 learning.py:507] global step 690: loss = 2.7043 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 691: loss = 2.6074 (0.586 sec/step)\n",
            "I1212 08:30:08.988597 140053320693632 learning.py:507] global step 691: loss = 2.6074 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 692: loss = 2.5848 (0.633 sec/step)\n",
            "I1212 08:30:09.887417 140053320693632 learning.py:507] global step 692: loss = 2.5848 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 693: loss = 2.5018 (0.683 sec/step)\n",
            "I1212 08:30:10.840443 140053320693632 learning.py:507] global step 693: loss = 2.5018 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 694: loss = 2.4935 (0.723 sec/step)\n",
            "I1212 08:30:12.096456 140053320693632 learning.py:507] global step 694: loss = 2.4935 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 695: loss = 2.6778 (0.574 sec/step)\n",
            "I1212 08:30:12.870893 140053320693632 learning.py:507] global step 695: loss = 2.6778 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 696: loss = 2.9741 (1.480 sec/step)\n",
            "I1212 08:30:14.368188 140053320693632 learning.py:507] global step 696: loss = 2.9741 (1.480 sec/step)\n",
            "INFO:tensorflow:global step 697: loss = 3.1371 (0.549 sec/step)\n",
            "I1212 08:30:15.236792 140053320693632 learning.py:507] global step 697: loss = 3.1371 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 698: loss = 2.7685 (0.663 sec/step)\n",
            "I1212 08:30:16.440506 140053320693632 learning.py:507] global step 698: loss = 2.7685 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 699: loss = 2.7985 (0.793 sec/step)\n",
            "I1212 08:30:17.365601 140053320693632 learning.py:507] global step 699: loss = 2.7985 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 2.8850 (1.171 sec/step)\n",
            "I1212 08:30:18.764118 140053320693632 learning.py:507] global step 700: loss = 2.8850 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 701: loss = 2.2536 (0.499 sec/step)\n",
            "I1212 08:30:19.264305 140053320693632 learning.py:507] global step 701: loss = 2.2536 (0.499 sec/step)\n",
            "INFO:tensorflow:global step 702: loss = 2.8041 (1.633 sec/step)\n",
            "I1212 08:30:20.984117 140053320693632 learning.py:507] global step 702: loss = 2.8041 (1.633 sec/step)\n",
            "INFO:tensorflow:global step 703: loss = 3.3134 (0.722 sec/step)\n",
            "I1212 08:30:21.991028 140053320693632 learning.py:507] global step 703: loss = 3.3134 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 704: loss = 2.4480 (0.577 sec/step)\n",
            "I1212 08:30:22.768085 140053320693632 learning.py:507] global step 704: loss = 2.4480 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 705: loss = 3.2144 (0.634 sec/step)\n",
            "I1212 08:30:23.953590 140053320693632 learning.py:507] global step 705: loss = 3.2144 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 706: loss = 2.6470 (0.539 sec/step)\n",
            "I1212 08:30:24.766763 140053320693632 learning.py:507] global step 706: loss = 2.6470 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 707: loss = 2.7462 (0.842 sec/step)\n",
            "I1212 08:30:25.949301 140053320693632 learning.py:507] global step 707: loss = 2.7462 (0.842 sec/step)\n",
            "INFO:tensorflow:global step 708: loss = 2.8925 (1.379 sec/step)\n",
            "I1212 08:30:27.621908 140053320693632 learning.py:507] global step 708: loss = 2.8925 (1.379 sec/step)\n",
            "INFO:tensorflow:global step 709: loss = 3.3432 (0.606 sec/step)\n",
            "I1212 08:30:28.415048 140053320693632 learning.py:507] global step 709: loss = 3.3432 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 710: loss = 2.8867 (1.082 sec/step)\n",
            "I1212 08:30:29.756208 140053320693632 learning.py:507] global step 710: loss = 2.8867 (1.082 sec/step)\n",
            "INFO:tensorflow:global step 711: loss = 2.3369 (0.610 sec/step)\n",
            "I1212 08:30:30.702678 140053320693632 learning.py:507] global step 711: loss = 2.3369 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 712: loss = 2.0827 (0.696 sec/step)\n",
            "I1212 08:30:31.779612 140053320693632 learning.py:507] global step 712: loss = 2.0827 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 713: loss = 3.0080 (1.153 sec/step)\n",
            "I1212 08:30:33.046122 140053320693632 learning.py:507] global step 713: loss = 3.0080 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 714: loss = 3.1910 (0.644 sec/step)\n",
            "I1212 08:30:33.916262 140053320693632 learning.py:507] global step 714: loss = 3.1910 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 715: loss = 2.6065 (0.665 sec/step)\n",
            "I1212 08:30:34.686793 140053320693632 learning.py:507] global step 715: loss = 2.6065 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 716: loss = 2.6932 (1.738 sec/step)\n",
            "I1212 08:30:36.426676 140053320693632 learning.py:507] global step 716: loss = 2.6932 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 717: loss = 2.8048 (0.758 sec/step)\n",
            "I1212 08:30:37.269590 140053320693632 learning.py:507] global step 717: loss = 2.8048 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 718: loss = 2.7084 (0.684 sec/step)\n",
            "I1212 08:30:38.309900 140053320693632 learning.py:507] global step 718: loss = 2.7084 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 719: loss = 2.4222 (0.628 sec/step)\n",
            "I1212 08:30:39.564586 140053320693632 learning.py:507] global step 719: loss = 2.4222 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 720: loss = 3.1869 (0.870 sec/step)\n",
            "I1212 08:30:40.705521 140053320693632 learning.py:507] global step 720: loss = 3.1869 (0.870 sec/step)\n",
            "INFO:tensorflow:global step 721: loss = 2.7890 (1.360 sec/step)\n",
            "I1212 08:30:42.109894 140053320693632 learning.py:507] global step 721: loss = 2.7890 (1.360 sec/step)\n",
            "INFO:tensorflow:global step 722: loss = 3.0763 (0.596 sec/step)\n",
            "I1212 08:30:42.806668 140053320693632 learning.py:507] global step 722: loss = 3.0763 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 723: loss = 3.4459 (0.691 sec/step)\n",
            "I1212 08:30:44.072958 140053320693632 learning.py:507] global step 723: loss = 3.4459 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 724: loss = 3.1160 (1.336 sec/step)\n",
            "I1212 08:30:45.494542 140053320693632 learning.py:507] global step 724: loss = 3.1160 (1.336 sec/step)\n",
            "INFO:tensorflow:global step 725: loss = 2.1948 (0.511 sec/step)\n",
            "I1212 08:30:46.325072 140053320693632 learning.py:507] global step 725: loss = 2.1948 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 726: loss = 1.9349 (0.690 sec/step)\n",
            "I1212 08:30:47.185225 140053320693632 learning.py:507] global step 726: loss = 1.9349 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 727: loss = 3.6708 (1.229 sec/step)\n",
            "I1212 08:30:48.888944 140053320693632 learning.py:507] global step 727: loss = 3.6708 (1.229 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 727.\n",
            "I1212 08:30:50.817127 140049614505728 supervisor.py:1050] Recording summary at step 727.\n",
            "INFO:tensorflow:global step 728: loss = 3.3740 (1.809 sec/step)\n",
            "I1212 08:30:51.050454 140053320693632 learning.py:507] global step 728: loss = 3.3740 (1.809 sec/step)\n",
            "INFO:tensorflow:global step 729: loss = 2.7481 (0.661 sec/step)\n",
            "I1212 08:30:51.954467 140053320693632 learning.py:507] global step 729: loss = 2.7481 (0.661 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.891608\n",
            "I1212 08:30:52.645849 140049622898432 supervisor.py:1099] global_step/sec: 0.891608\n",
            "INFO:tensorflow:global step 730: loss = 3.1195 (1.288 sec/step)\n",
            "I1212 08:30:53.362168 140053320693632 learning.py:507] global step 730: loss = 3.1195 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 731: loss = 2.8420 (0.683 sec/step)\n",
            "I1212 08:30:54.197460 140053320693632 learning.py:507] global step 731: loss = 2.8420 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 732: loss = 3.3597 (0.599 sec/step)\n",
            "I1212 08:30:55.075432 140053320693632 learning.py:507] global step 732: loss = 3.3597 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 733: loss = 4.0299 (1.250 sec/step)\n",
            "I1212 08:30:56.546096 140053320693632 learning.py:507] global step 733: loss = 4.0299 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 734: loss = 2.8182 (0.564 sec/step)\n",
            "I1212 08:30:57.211344 140053320693632 learning.py:507] global step 734: loss = 2.8182 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 735: loss = 2.5886 (1.393 sec/step)\n",
            "I1212 08:30:58.786530 140053320693632 learning.py:507] global step 735: loss = 2.5886 (1.393 sec/step)\n",
            "INFO:tensorflow:global step 736: loss = 3.0148 (0.627 sec/step)\n",
            "I1212 08:30:59.635226 140053320693632 learning.py:507] global step 736: loss = 3.0148 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 737: loss = 3.0898 (1.477 sec/step)\n",
            "I1212 08:31:01.190901 140053320693632 learning.py:507] global step 737: loss = 3.0898 (1.477 sec/step)\n",
            "INFO:tensorflow:global step 738: loss = 2.9461 (0.717 sec/step)\n",
            "I1212 08:31:02.171588 140053320693632 learning.py:507] global step 738: loss = 2.9461 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 739: loss = 2.4983 (1.506 sec/step)\n",
            "I1212 08:31:03.690494 140053320693632 learning.py:507] global step 739: loss = 2.4983 (1.506 sec/step)\n",
            "INFO:tensorflow:global step 740: loss = 2.9129 (1.057 sec/step)\n",
            "I1212 08:31:04.749461 140053320693632 learning.py:507] global step 740: loss = 2.9129 (1.057 sec/step)\n",
            "INFO:tensorflow:global step 741: loss = 2.6653 (0.694 sec/step)\n",
            "I1212 08:31:05.654834 140053320693632 learning.py:507] global step 741: loss = 2.6653 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 742: loss = 2.4590 (0.725 sec/step)\n",
            "I1212 08:31:06.692626 140053320693632 learning.py:507] global step 742: loss = 2.4590 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 743: loss = 2.0132 (0.687 sec/step)\n",
            "I1212 08:31:07.814903 140053320693632 learning.py:507] global step 743: loss = 2.0132 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 744: loss = 2.8357 (0.578 sec/step)\n",
            "I1212 08:31:08.799399 140053320693632 learning.py:507] global step 744: loss = 2.8357 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 745: loss = 2.9518 (1.201 sec/step)\n",
            "I1212 08:31:10.060895 140053320693632 learning.py:507] global step 745: loss = 2.9518 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 746: loss = 2.7964 (0.624 sec/step)\n",
            "I1212 08:31:10.785837 140053320693632 learning.py:507] global step 746: loss = 2.7964 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 747: loss = 2.6614 (0.837 sec/step)\n",
            "I1212 08:31:11.848962 140053320693632 learning.py:507] global step 747: loss = 2.6614 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 748: loss = 2.8901 (0.836 sec/step)\n",
            "I1212 08:31:12.926001 140053320693632 learning.py:507] global step 748: loss = 2.8901 (0.836 sec/step)\n",
            "INFO:tensorflow:global step 749: loss = 2.9452 (1.646 sec/step)\n",
            "I1212 08:31:14.577189 140053320693632 learning.py:507] global step 749: loss = 2.9452 (1.646 sec/step)\n",
            "INFO:tensorflow:global step 750: loss = 3.2050 (0.708 sec/step)\n",
            "I1212 08:31:15.293289 140053320693632 learning.py:507] global step 750: loss = 3.2050 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 751: loss = 2.7898 (1.530 sec/step)\n",
            "I1212 08:31:16.841197 140053320693632 learning.py:507] global step 751: loss = 2.7898 (1.530 sec/step)\n",
            "INFO:tensorflow:global step 752: loss = 2.8101 (0.627 sec/step)\n",
            "I1212 08:31:17.618641 140053320693632 learning.py:507] global step 752: loss = 2.8101 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 753: loss = 2.7737 (1.305 sec/step)\n",
            "I1212 08:31:18.971647 140053320693632 learning.py:507] global step 753: loss = 2.7737 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 754: loss = 2.4099 (0.490 sec/step)\n",
            "I1212 08:31:19.463083 140053320693632 learning.py:507] global step 754: loss = 2.4099 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 755: loss = 2.3360 (1.588 sec/step)\n",
            "I1212 08:31:21.053212 140053320693632 learning.py:507] global step 755: loss = 2.3360 (1.588 sec/step)\n",
            "INFO:tensorflow:global step 756: loss = 3.0427 (0.706 sec/step)\n",
            "I1212 08:31:21.902844 140053320693632 learning.py:507] global step 756: loss = 3.0427 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 757: loss = 2.6521 (1.438 sec/step)\n",
            "I1212 08:31:23.391287 140053320693632 learning.py:507] global step 757: loss = 2.6521 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 758: loss = 3.2961 (0.479 sec/step)\n",
            "I1212 08:31:23.872340 140053320693632 learning.py:507] global step 758: loss = 3.2961 (0.479 sec/step)\n",
            "INFO:tensorflow:global step 759: loss = 2.6374 (1.564 sec/step)\n",
            "I1212 08:31:25.438273 140053320693632 learning.py:507] global step 759: loss = 2.6374 (1.564 sec/step)\n",
            "INFO:tensorflow:global step 760: loss = 3.3666 (0.484 sec/step)\n",
            "I1212 08:31:25.924083 140053320693632 learning.py:507] global step 760: loss = 3.3666 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 761: loss = 2.3854 (0.701 sec/step)\n",
            "I1212 08:31:26.868145 140053320693632 learning.py:507] global step 761: loss = 2.3854 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 762: loss = 2.6738 (1.508 sec/step)\n",
            "I1212 08:31:28.591607 140053320693632 learning.py:507] global step 762: loss = 2.6738 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 763: loss = 3.0464 (0.655 sec/step)\n",
            "I1212 08:31:29.536946 140053320693632 learning.py:507] global step 763: loss = 3.0464 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 764: loss = 2.7952 (0.677 sec/step)\n",
            "I1212 08:31:30.424508 140053320693632 learning.py:507] global step 764: loss = 2.7952 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 765: loss = 2.4031 (0.715 sec/step)\n",
            "I1212 08:31:31.571956 140053320693632 learning.py:507] global step 765: loss = 2.4031 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 766: loss = 3.7027 (1.281 sec/step)\n",
            "I1212 08:31:32.869347 140053320693632 learning.py:507] global step 766: loss = 3.7027 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 767: loss = 2.6721 (0.564 sec/step)\n",
            "I1212 08:31:33.798391 140053320693632 learning.py:507] global step 767: loss = 2.6721 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 768: loss = 3.0160 (1.232 sec/step)\n",
            "I1212 08:31:35.211241 140053320693632 learning.py:507] global step 768: loss = 3.0160 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 769: loss = 2.5814 (0.663 sec/step)\n",
            "I1212 08:31:36.119297 140053320693632 learning.py:507] global step 769: loss = 2.5814 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 770: loss = 2.9645 (1.355 sec/step)\n",
            "I1212 08:31:37.495161 140053320693632 learning.py:507] global step 770: loss = 2.9645 (1.355 sec/step)\n",
            "INFO:tensorflow:global step 771: loss = 2.6432 (0.697 sec/step)\n",
            "I1212 08:31:38.349572 140053320693632 learning.py:507] global step 771: loss = 2.6432 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 772: loss = 3.2889 (1.477 sec/step)\n",
            "I1212 08:31:39.846001 140053320693632 learning.py:507] global step 772: loss = 3.2889 (1.477 sec/step)\n",
            "INFO:tensorflow:global step 773: loss = 3.3109 (0.691 sec/step)\n",
            "I1212 08:31:40.717487 140053320693632 learning.py:507] global step 773: loss = 3.3109 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 774: loss = 3.0092 (0.743 sec/step)\n",
            "I1212 08:31:41.651517 140053320693632 learning.py:507] global step 774: loss = 3.0092 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 775: loss = 3.1835 (1.456 sec/step)\n",
            "I1212 08:31:43.109146 140053320693632 learning.py:507] global step 775: loss = 3.1835 (1.456 sec/step)\n",
            "INFO:tensorflow:global step 776: loss = 2.7913 (0.581 sec/step)\n",
            "I1212 08:31:44.001894 140053320693632 learning.py:507] global step 776: loss = 2.7913 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 777: loss = 2.6116 (1.426 sec/step)\n",
            "I1212 08:31:45.504880 140053320693632 learning.py:507] global step 777: loss = 2.6116 (1.426 sec/step)\n",
            "INFO:tensorflow:global step 778: loss = 2.4865 (0.557 sec/step)\n",
            "I1212 08:31:46.349839 140053320693632 learning.py:507] global step 778: loss = 2.4865 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 779: loss = 2.9967 (0.830 sec/step)\n",
            "I1212 08:31:47.548552 140053320693632 learning.py:507] global step 779: loss = 2.9967 (0.830 sec/step)\n",
            "INFO:tensorflow:global step 780: loss = 2.6643 (1.306 sec/step)\n",
            "I1212 08:31:49.060324 140053320693632 learning.py:507] global step 780: loss = 2.6643 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 781: loss = 3.1427 (0.631 sec/step)\n",
            "I1212 08:31:49.866586 140053320693632 learning.py:507] global step 781: loss = 3.1427 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 782: loss = 2.9257 (0.749 sec/step)\n",
            "I1212 08:31:51.005624 140053320693632 learning.py:507] global step 782: loss = 2.9257 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 783: loss = 2.3826 (0.865 sec/step)\n",
            "I1212 08:31:52.039021 140053320693632 learning.py:507] global step 783: loss = 2.3826 (0.865 sec/step)\n",
            "INFO:tensorflow:global step 784: loss = 2.7290 (0.638 sec/step)\n",
            "I1212 08:31:53.149798 140053320693632 learning.py:507] global step 784: loss = 2.7290 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 785: loss = 2.6375 (0.678 sec/step)\n",
            "I1212 08:31:54.187674 140053320693632 learning.py:507] global step 785: loss = 2.6375 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 786: loss = 2.4617 (0.891 sec/step)\n",
            "I1212 08:31:55.308757 140053320693632 learning.py:507] global step 786: loss = 2.4617 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 787: loss = 2.6045 (0.620 sec/step)\n",
            "I1212 08:31:56.099534 140053320693632 learning.py:507] global step 787: loss = 2.6045 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 788: loss = 3.5355 (1.284 sec/step)\n",
            "I1212 08:31:57.500794 140053320693632 learning.py:507] global step 788: loss = 3.5355 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 789: loss = 2.3119 (0.576 sec/step)\n",
            "I1212 08:31:58.392074 140053320693632 learning.py:507] global step 789: loss = 2.3119 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 790: loss = 2.7896 (0.679 sec/step)\n",
            "I1212 08:31:59.449095 140053320693632 learning.py:507] global step 790: loss = 2.7896 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 791: loss = 3.0734 (0.602 sec/step)\n",
            "I1212 08:32:00.425904 140053320693632 learning.py:507] global step 791: loss = 3.0734 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 792: loss = 3.3216 (0.610 sec/step)\n",
            "I1212 08:32:01.527746 140053320693632 learning.py:507] global step 792: loss = 3.3216 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 793: loss = 2.9264 (0.617 sec/step)\n",
            "I1212 08:32:02.549614 140053320693632 learning.py:507] global step 793: loss = 2.9264 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 794: loss = 2.4891 (0.608 sec/step)\n",
            "I1212 08:32:03.452653 140053320693632 learning.py:507] global step 794: loss = 2.4891 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 795: loss = 3.0637 (0.739 sec/step)\n",
            "I1212 08:32:04.529528 140053320693632 learning.py:507] global step 795: loss = 3.0637 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 796: loss = 3.1333 (1.365 sec/step)\n",
            "I1212 08:32:05.928187 140053320693632 learning.py:507] global step 796: loss = 3.1333 (1.365 sec/step)\n",
            "INFO:tensorflow:global step 797: loss = 2.6182 (1.093 sec/step)\n",
            "I1212 08:32:07.022606 140053320693632 learning.py:507] global step 797: loss = 2.6182 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 798: loss = 2.9107 (1.082 sec/step)\n",
            "I1212 08:32:08.106058 140053320693632 learning.py:507] global step 798: loss = 2.9107 (1.082 sec/step)\n",
            "INFO:tensorflow:global step 799: loss = 2.3984 (0.599 sec/step)\n",
            "I1212 08:32:08.970587 140053320693632 learning.py:507] global step 799: loss = 2.3984 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 2.3470 (0.590 sec/step)\n",
            "I1212 08:32:09.642555 140053320693632 learning.py:507] global step 800: loss = 2.3470 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 801: loss = 3.0601 (1.686 sec/step)\n",
            "I1212 08:32:11.330051 140053320693632 learning.py:507] global step 801: loss = 3.0601 (1.686 sec/step)\n",
            "INFO:tensorflow:global step 802: loss = 3.2219 (0.674 sec/step)\n",
            "I1212 08:32:12.210541 140053320693632 learning.py:507] global step 802: loss = 3.2219 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 803: loss = 2.2909 (1.194 sec/step)\n",
            "I1212 08:32:13.473779 140053320693632 learning.py:507] global step 803: loss = 2.2909 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 804: loss = 2.9689 (0.719 sec/step)\n",
            "I1212 08:32:14.320322 140053320693632 learning.py:507] global step 804: loss = 2.9689 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 805: loss = 2.4129 (0.585 sec/step)\n",
            "I1212 08:32:15.072990 140053320693632 learning.py:507] global step 805: loss = 2.4129 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 806: loss = 2.1127 (1.721 sec/step)\n",
            "I1212 08:32:16.795979 140053320693632 learning.py:507] global step 806: loss = 2.1127 (1.721 sec/step)\n",
            "INFO:tensorflow:global step 807: loss = 3.1592 (0.617 sec/step)\n",
            "I1212 08:32:17.703444 140053320693632 learning.py:507] global step 807: loss = 3.1592 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 808: loss = 2.5162 (0.612 sec/step)\n",
            "I1212 08:32:18.640116 140053320693632 learning.py:507] global step 808: loss = 2.5162 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 809: loss = 2.9935 (0.695 sec/step)\n",
            "I1212 08:32:19.774744 140053320693632 learning.py:507] global step 809: loss = 2.9935 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 810: loss = 2.8047 (0.676 sec/step)\n",
            "I1212 08:32:20.801559 140053320693632 learning.py:507] global step 810: loss = 2.8047 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 811: loss = 2.5823 (0.945 sec/step)\n",
            "I1212 08:32:22.035746 140053320693632 learning.py:507] global step 811: loss = 2.5823 (0.945 sec/step)\n",
            "INFO:tensorflow:global step 812: loss = 2.5767 (1.197 sec/step)\n",
            "I1212 08:32:23.274616 140053320693632 learning.py:507] global step 812: loss = 2.5767 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 813: loss = 2.7028 (0.607 sec/step)\n",
            "I1212 08:32:24.046205 140053320693632 learning.py:507] global step 813: loss = 2.7028 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 814: loss = 3.1891 (2.363 sec/step)\n",
            "I1212 08:32:26.619496 140053320693632 learning.py:507] global step 814: loss = 3.1891 (2.363 sec/step)\n",
            "INFO:tensorflow:global step 815: loss = 2.4371 (0.755 sec/step)\n",
            "I1212 08:32:27.721157 140053320693632 learning.py:507] global step 815: loss = 2.4371 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 816: loss = 2.3117 (0.655 sec/step)\n",
            "I1212 08:32:28.436954 140053320693632 learning.py:507] global step 816: loss = 2.3117 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 817: loss = 3.1953 (1.427 sec/step)\n",
            "I1212 08:32:29.973854 140053320693632 learning.py:507] global step 817: loss = 3.1953 (1.427 sec/step)\n",
            "INFO:tensorflow:global step 818: loss = 2.8874 (0.604 sec/step)\n",
            "I1212 08:32:30.579304 140053320693632 learning.py:507] global step 818: loss = 2.8874 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 819: loss = 2.8465 (0.891 sec/step)\n",
            "I1212 08:32:31.699168 140053320693632 learning.py:507] global step 819: loss = 2.8465 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 820: loss = 2.9375 (1.596 sec/step)\n",
            "I1212 08:32:33.518266 140053320693632 learning.py:507] global step 820: loss = 2.9375 (1.596 sec/step)\n",
            "INFO:tensorflow:global step 821: loss = 2.6012 (0.684 sec/step)\n",
            "I1212 08:32:34.510761 140053320693632 learning.py:507] global step 821: loss = 2.6012 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 822: loss = 2.6795 (0.748 sec/step)\n",
            "I1212 08:32:35.368399 140053320693632 learning.py:507] global step 822: loss = 2.6795 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 823: loss = 2.6946 (1.804 sec/step)\n",
            "I1212 08:32:37.321417 140053320693632 learning.py:507] global step 823: loss = 2.6946 (1.804 sec/step)\n",
            "INFO:tensorflow:global step 824: loss = 3.0124 (0.593 sec/step)\n",
            "I1212 08:32:37.916189 140053320693632 learning.py:507] global step 824: loss = 3.0124 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 825: loss = 3.0114 (1.807 sec/step)\n",
            "I1212 08:32:39.725214 140053320693632 learning.py:507] global step 825: loss = 3.0114 (1.807 sec/step)\n",
            "INFO:tensorflow:global step 826: loss = 2.5461 (0.752 sec/step)\n",
            "I1212 08:32:40.689697 140053320693632 learning.py:507] global step 826: loss = 2.5461 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 827: loss = 3.3677 (0.496 sec/step)\n",
            "I1212 08:32:41.312161 140053320693632 learning.py:507] global step 827: loss = 3.3677 (0.496 sec/step)\n",
            "INFO:tensorflow:global step 828: loss = 2.4560 (0.581 sec/step)\n",
            "I1212 08:32:42.496963 140053320693632 learning.py:507] global step 828: loss = 2.4560 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 829: loss = 2.9127 (2.417 sec/step)\n",
            "I1212 08:32:44.946822 140053320693632 learning.py:507] global step 829: loss = 2.9127 (2.417 sec/step)\n",
            "INFO:tensorflow:global step 830: loss = 2.9557 (1.091 sec/step)\n",
            "I1212 08:32:46.040127 140053320693632 learning.py:507] global step 830: loss = 2.9557 (1.091 sec/step)\n",
            "INFO:tensorflow:global step 831: loss = 2.7362 (0.526 sec/step)\n",
            "I1212 08:32:46.863986 140053320693632 learning.py:507] global step 831: loss = 2.7362 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 832: loss = 2.7340 (2.634 sec/step)\n",
            "I1212 08:32:49.575859 140053320693632 learning.py:507] global step 832: loss = 2.7340 (2.634 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 832.\n",
            "I1212 08:32:49.944122 140049614505728 supervisor.py:1050] Recording summary at step 832.\n",
            "INFO:tensorflow:global step 833: loss = 3.5042 (0.596 sec/step)\n",
            "I1212 08:32:50.601572 140053320693632 learning.py:507] global step 833: loss = 3.5042 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 834: loss = 3.2192 (1.286 sec/step)\n",
            "I1212 08:32:51.958510 140053320693632 learning.py:507] global step 834: loss = 3.2192 (1.286 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.875095\n",
            "I1212 08:32:52.632857 140049622898432 supervisor.py:1099] global_step/sec: 0.875095\n",
            "INFO:tensorflow:global step 835: loss = 2.2936 (0.736 sec/step)\n",
            "I1212 08:32:52.726953 140053320693632 learning.py:507] global step 835: loss = 2.2936 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 836: loss = 2.6072 (1.839 sec/step)\n",
            "I1212 08:32:54.584217 140053320693632 learning.py:507] global step 836: loss = 2.6072 (1.839 sec/step)\n",
            "INFO:tensorflow:global step 837: loss = 2.5888 (0.756 sec/step)\n",
            "I1212 08:32:55.551104 140053320693632 learning.py:507] global step 837: loss = 2.5888 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 838: loss = 3.4633 (0.650 sec/step)\n",
            "I1212 08:32:56.609538 140053320693632 learning.py:507] global step 838: loss = 3.4633 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 839: loss = 2.6947 (0.685 sec/step)\n",
            "I1212 08:32:57.663539 140053320693632 learning.py:507] global step 839: loss = 2.6947 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 840: loss = 3.1511 (1.389 sec/step)\n",
            "I1212 08:32:59.143658 140053320693632 learning.py:507] global step 840: loss = 3.1511 (1.389 sec/step)\n",
            "INFO:tensorflow:global step 841: loss = 3.1183 (0.718 sec/step)\n",
            "I1212 08:33:00.154757 140053320693632 learning.py:507] global step 841: loss = 3.1183 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 842: loss = 2.9427 (1.216 sec/step)\n",
            "I1212 08:33:01.399085 140053320693632 learning.py:507] global step 842: loss = 2.9427 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 843: loss = 2.9077 (0.650 sec/step)\n",
            "I1212 08:33:02.282586 140053320693632 learning.py:507] global step 843: loss = 2.9077 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 844: loss = 2.6806 (0.755 sec/step)\n",
            "I1212 08:33:03.340374 140053320693632 learning.py:507] global step 844: loss = 2.6806 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 845: loss = 2.9990 (1.548 sec/step)\n",
            "I1212 08:33:04.952940 140053320693632 learning.py:507] global step 845: loss = 2.9990 (1.548 sec/step)\n",
            "INFO:tensorflow:global step 846: loss = 2.6252 (0.479 sec/step)\n",
            "I1212 08:33:05.691689 140053320693632 learning.py:507] global step 846: loss = 2.6252 (0.479 sec/step)\n",
            "INFO:tensorflow:global step 847: loss = 2.4484 (0.985 sec/step)\n",
            "I1212 08:33:06.912750 140053320693632 learning.py:507] global step 847: loss = 2.4484 (0.985 sec/step)\n",
            "INFO:tensorflow:global step 848: loss = 3.1782 (0.636 sec/step)\n",
            "I1212 08:33:07.846846 140053320693632 learning.py:507] global step 848: loss = 3.1782 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 849: loss = 2.4823 (0.613 sec/step)\n",
            "I1212 08:33:08.738740 140053320693632 learning.py:507] global step 849: loss = 2.4823 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 850: loss = 2.7739 (0.697 sec/step)\n",
            "I1212 08:33:09.900249 140053320693632 learning.py:507] global step 850: loss = 2.7739 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 851: loss = 2.8099 (1.111 sec/step)\n",
            "I1212 08:33:11.199891 140053320693632 learning.py:507] global step 851: loss = 2.8099 (1.111 sec/step)\n",
            "INFO:tensorflow:global step 852: loss = 2.5968 (0.695 sec/step)\n",
            "I1212 08:33:11.913667 140053320693632 learning.py:507] global step 852: loss = 2.5968 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 853: loss = 2.7708 (1.564 sec/step)\n",
            "I1212 08:33:13.670498 140053320693632 learning.py:507] global step 853: loss = 2.7708 (1.564 sec/step)\n",
            "INFO:tensorflow:global step 854: loss = 2.6860 (0.781 sec/step)\n",
            "I1212 08:33:14.478886 140053320693632 learning.py:507] global step 854: loss = 2.6860 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 855: loss = 2.9894 (1.416 sec/step)\n",
            "I1212 08:33:15.944718 140053320693632 learning.py:507] global step 855: loss = 2.9894 (1.416 sec/step)\n",
            "INFO:tensorflow:global step 856: loss = 2.6099 (0.592 sec/step)\n",
            "I1212 08:33:16.841375 140053320693632 learning.py:507] global step 856: loss = 2.6099 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 857: loss = 2.3097 (1.293 sec/step)\n",
            "I1212 08:33:18.223766 140053320693632 learning.py:507] global step 857: loss = 2.3097 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 858: loss = 2.1811 (0.561 sec/step)\n",
            "I1212 08:33:18.786708 140053320693632 learning.py:507] global step 858: loss = 2.1811 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 859: loss = 2.2118 (1.095 sec/step)\n",
            "I1212 08:33:19.902278 140053320693632 learning.py:507] global step 859: loss = 2.2118 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 860: loss = 2.9626 (1.529 sec/step)\n",
            "I1212 08:33:21.496122 140053320693632 learning.py:507] global step 860: loss = 2.9626 (1.529 sec/step)\n",
            "INFO:tensorflow:global step 861: loss = 3.1169 (1.591 sec/step)\n",
            "I1212 08:33:23.141521 140053320693632 learning.py:507] global step 861: loss = 3.1169 (1.591 sec/step)\n",
            "INFO:tensorflow:global step 862: loss = 2.7758 (0.611 sec/step)\n",
            "I1212 08:33:24.014607 140053320693632 learning.py:507] global step 862: loss = 2.7758 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 863: loss = 2.8841 (1.385 sec/step)\n",
            "I1212 08:33:25.599856 140053320693632 learning.py:507] global step 863: loss = 2.8841 (1.385 sec/step)\n",
            "INFO:tensorflow:global step 864: loss = 3.0017 (0.527 sec/step)\n",
            "I1212 08:33:26.128349 140053320693632 learning.py:507] global step 864: loss = 3.0017 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 865: loss = 3.0608 (1.864 sec/step)\n",
            "I1212 08:33:27.994481 140053320693632 learning.py:507] global step 865: loss = 3.0608 (1.864 sec/step)\n",
            "INFO:tensorflow:global step 866: loss = 3.3089 (0.663 sec/step)\n",
            "I1212 08:33:28.749540 140053320693632 learning.py:507] global step 866: loss = 3.3089 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 867: loss = 2.6503 (1.495 sec/step)\n",
            "I1212 08:33:30.602334 140053320693632 learning.py:507] global step 867: loss = 2.6503 (1.495 sec/step)\n",
            "INFO:tensorflow:global step 868: loss = 2.2478 (0.514 sec/step)\n",
            "I1212 08:33:31.118081 140053320693632 learning.py:507] global step 868: loss = 2.2478 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 869: loss = 2.3737 (1.698 sec/step)\n",
            "I1212 08:33:32.817334 140053320693632 learning.py:507] global step 869: loss = 2.3737 (1.698 sec/step)\n",
            "INFO:tensorflow:global step 870: loss = 2.9769 (0.545 sec/step)\n",
            "I1212 08:33:33.364101 140053320693632 learning.py:507] global step 870: loss = 2.9769 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 871: loss = 3.2646 (1.896 sec/step)\n",
            "I1212 08:33:35.261471 140053320693632 learning.py:507] global step 871: loss = 3.2646 (1.896 sec/step)\n",
            "INFO:tensorflow:global step 872: loss = 2.2357 (0.516 sec/step)\n",
            "I1212 08:33:36.002973 140053320693632 learning.py:507] global step 872: loss = 2.2357 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 873: loss = 2.7869 (0.626 sec/step)\n",
            "I1212 08:33:37.131223 140053320693632 learning.py:507] global step 873: loss = 2.7869 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 874: loss = 2.7924 (1.405 sec/step)\n",
            "I1212 08:33:38.652240 140053320693632 learning.py:507] global step 874: loss = 2.7924 (1.405 sec/step)\n",
            "INFO:tensorflow:global step 875: loss = 2.4554 (0.678 sec/step)\n",
            "I1212 08:33:39.506422 140053320693632 learning.py:507] global step 875: loss = 2.4554 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 876: loss = 2.6674 (1.292 sec/step)\n",
            "I1212 08:33:40.934195 140053320693632 learning.py:507] global step 876: loss = 2.6674 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 877: loss = 2.5779 (0.710 sec/step)\n",
            "I1212 08:33:41.822972 140053320693632 learning.py:507] global step 877: loss = 2.5779 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 878: loss = 2.9694 (1.080 sec/step)\n",
            "I1212 08:33:43.114979 140053320693632 learning.py:507] global step 878: loss = 2.9694 (1.080 sec/step)\n",
            "INFO:tensorflow:global step 879: loss = 3.4231 (0.600 sec/step)\n",
            "I1212 08:33:43.985710 140053320693632 learning.py:507] global step 879: loss = 3.4231 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 880: loss = 2.3992 (0.607 sec/step)\n",
            "I1212 08:33:44.813205 140053320693632 learning.py:507] global step 880: loss = 2.3992 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 881: loss = 2.1956 (1.445 sec/step)\n",
            "I1212 08:33:46.349998 140053320693632 learning.py:507] global step 881: loss = 2.1956 (1.445 sec/step)\n",
            "INFO:tensorflow:global step 882: loss = 1.9837 (1.098 sec/step)\n",
            "I1212 08:33:47.449313 140053320693632 learning.py:507] global step 882: loss = 1.9837 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 883: loss = 2.4057 (0.603 sec/step)\n",
            "I1212 08:33:48.278676 140053320693632 learning.py:507] global step 883: loss = 2.4057 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 884: loss = 2.1268 (0.758 sec/step)\n",
            "I1212 08:33:49.471810 140053320693632 learning.py:507] global step 884: loss = 2.1268 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 885: loss = 2.9689 (0.671 sec/step)\n",
            "I1212 08:33:50.461891 140053320693632 learning.py:507] global step 885: loss = 2.9689 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 886: loss = 2.7181 (0.683 sec/step)\n",
            "I1212 08:33:51.372111 140053320693632 learning.py:507] global step 886: loss = 2.7181 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 887: loss = 2.8218 (0.830 sec/step)\n",
            "I1212 08:33:52.520978 140053320693632 learning.py:507] global step 887: loss = 2.8218 (0.830 sec/step)\n",
            "INFO:tensorflow:global step 888: loss = 3.0598 (1.328 sec/step)\n",
            "I1212 08:33:53.949442 140053320693632 learning.py:507] global step 888: loss = 3.0598 (1.328 sec/step)\n",
            "INFO:tensorflow:global step 889: loss = 2.1066 (0.630 sec/step)\n",
            "I1212 08:33:54.846701 140053320693632 learning.py:507] global step 889: loss = 2.1066 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 890: loss = 2.6883 (1.162 sec/step)\n",
            "I1212 08:33:56.133614 140053320693632 learning.py:507] global step 890: loss = 2.6883 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 891: loss = 2.3094 (0.693 sec/step)\n",
            "I1212 08:33:57.009129 140053320693632 learning.py:507] global step 891: loss = 2.3094 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 892: loss = 2.8179 (0.722 sec/step)\n",
            "I1212 08:33:57.990460 140053320693632 learning.py:507] global step 892: loss = 2.8179 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 893: loss = 2.5876 (1.341 sec/step)\n",
            "I1212 08:33:59.375789 140053320693632 learning.py:507] global step 893: loss = 2.5876 (1.341 sec/step)\n",
            "INFO:tensorflow:global step 894: loss = 2.7201 (0.567 sec/step)\n",
            "I1212 08:34:00.283754 140053320693632 learning.py:507] global step 894: loss = 2.7201 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 895: loss = 2.2273 (0.698 sec/step)\n",
            "I1212 08:34:01.115686 140053320693632 learning.py:507] global step 895: loss = 2.2273 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 896: loss = 1.8786 (2.474 sec/step)\n",
            "I1212 08:34:03.661120 140053320693632 learning.py:507] global step 896: loss = 1.8786 (2.474 sec/step)\n",
            "INFO:tensorflow:global step 897: loss = 2.5985 (0.665 sec/step)\n",
            "I1212 08:34:04.623531 140053320693632 learning.py:507] global step 897: loss = 2.5985 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 898: loss = 2.6370 (0.538 sec/step)\n",
            "I1212 08:34:05.509575 140053320693632 learning.py:507] global step 898: loss = 2.6370 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 899: loss = 2.5810 (1.638 sec/step)\n",
            "I1212 08:34:07.329973 140053320693632 learning.py:507] global step 899: loss = 2.5810 (1.638 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 2.9001 (0.702 sec/step)\n",
            "I1212 08:34:08.353310 140053320693632 learning.py:507] global step 900: loss = 2.9001 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 901: loss = 2.7159 (0.712 sec/step)\n",
            "I1212 08:34:09.152087 140053320693632 learning.py:507] global step 901: loss = 2.7159 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 902: loss = 4.1556 (0.697 sec/step)\n",
            "I1212 08:34:09.893484 140053320693632 learning.py:507] global step 902: loss = 4.1556 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 903: loss = 2.8390 (1.580 sec/step)\n",
            "I1212 08:34:11.680855 140053320693632 learning.py:507] global step 903: loss = 2.8390 (1.580 sec/step)\n",
            "INFO:tensorflow:global step 904: loss = 2.6491 (0.778 sec/step)\n",
            "I1212 08:34:12.646563 140053320693632 learning.py:507] global step 904: loss = 2.6491 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 905: loss = 2.9169 (0.724 sec/step)\n",
            "I1212 08:34:13.634394 140053320693632 learning.py:507] global step 905: loss = 2.9169 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 906: loss = 2.4992 (0.640 sec/step)\n",
            "I1212 08:34:14.514160 140053320693632 learning.py:507] global step 906: loss = 2.4992 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 907: loss = 2.4615 (1.642 sec/step)\n",
            "I1212 08:34:16.158265 140053320693632 learning.py:507] global step 907: loss = 2.4615 (1.642 sec/step)\n",
            "INFO:tensorflow:global step 908: loss = 3.5247 (0.663 sec/step)\n",
            "I1212 08:34:17.037891 140053320693632 learning.py:507] global step 908: loss = 3.5247 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 909: loss = 3.0134 (1.345 sec/step)\n",
            "I1212 08:34:18.489833 140053320693632 learning.py:507] global step 909: loss = 3.0134 (1.345 sec/step)\n",
            "INFO:tensorflow:global step 910: loss = 2.6970 (0.532 sec/step)\n",
            "I1212 08:34:19.023873 140053320693632 learning.py:507] global step 910: loss = 2.6970 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 911: loss = 2.4852 (0.732 sec/step)\n",
            "I1212 08:34:19.758208 140053320693632 learning.py:507] global step 911: loss = 2.4852 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 912: loss = 2.9395 (1.905 sec/step)\n",
            "I1212 08:34:21.903041 140053320693632 learning.py:507] global step 912: loss = 2.9395 (1.905 sec/step)\n",
            "INFO:tensorflow:global step 913: loss = 2.4546 (1.563 sec/step)\n",
            "I1212 08:34:23.512959 140053320693632 learning.py:507] global step 913: loss = 2.4546 (1.563 sec/step)\n",
            "INFO:tensorflow:global step 914: loss = 2.9940 (0.533 sec/step)\n",
            "I1212 08:34:24.047242 140053320693632 learning.py:507] global step 914: loss = 2.9940 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 915: loss = 2.3514 (1.685 sec/step)\n",
            "I1212 08:34:25.733965 140053320693632 learning.py:507] global step 915: loss = 2.3514 (1.685 sec/step)\n",
            "INFO:tensorflow:global step 916: loss = 2.4200 (0.602 sec/step)\n",
            "I1212 08:34:26.338412 140053320693632 learning.py:507] global step 916: loss = 2.4200 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 917: loss = 2.5871 (1.714 sec/step)\n",
            "I1212 08:34:28.054155 140053320693632 learning.py:507] global step 917: loss = 2.5871 (1.714 sec/step)\n",
            "INFO:tensorflow:global step 918: loss = 2.5379 (0.757 sec/step)\n",
            "I1212 08:34:29.109531 140053320693632 learning.py:507] global step 918: loss = 2.5379 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 919: loss = 3.1677 (0.623 sec/step)\n",
            "I1212 08:34:29.833534 140053320693632 learning.py:507] global step 919: loss = 3.1677 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 920: loss = 1.9360 (2.344 sec/step)\n",
            "I1212 08:34:32.419097 140053320693632 learning.py:507] global step 920: loss = 1.9360 (2.344 sec/step)\n",
            "INFO:tensorflow:global step 921: loss = 2.3159 (0.513 sec/step)\n",
            "I1212 08:34:32.933480 140053320693632 learning.py:507] global step 921: loss = 2.3159 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 922: loss = 2.2712 (0.719 sec/step)\n",
            "I1212 08:34:33.673021 140053320693632 learning.py:507] global step 922: loss = 2.2712 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 923: loss = 3.0491 (2.576 sec/step)\n",
            "I1212 08:34:36.252800 140053320693632 learning.py:507] global step 923: loss = 3.0491 (2.576 sec/step)\n",
            "INFO:tensorflow:global step 924: loss = 2.5619 (0.692 sec/step)\n",
            "I1212 08:34:37.059178 140053320693632 learning.py:507] global step 924: loss = 2.5619 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 925: loss = 3.3647 (0.672 sec/step)\n",
            "I1212 08:34:38.227968 140053320693632 learning.py:507] global step 925: loss = 3.3647 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 926: loss = 2.5510 (0.640 sec/step)\n",
            "I1212 08:34:38.991691 140053320693632 learning.py:507] global step 926: loss = 2.5510 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 927: loss = 2.6299 (1.458 sec/step)\n",
            "I1212 08:34:40.501288 140053320693632 learning.py:507] global step 927: loss = 2.6299 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 928: loss = 2.5615 (0.631 sec/step)\n",
            "I1212 08:34:41.427670 140053320693632 learning.py:507] global step 928: loss = 2.5615 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 929: loss = 3.3723 (0.614 sec/step)\n",
            "I1212 08:34:42.358751 140053320693632 learning.py:507] global step 929: loss = 3.3723 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 930: loss = 3.1897 (0.673 sec/step)\n",
            "I1212 08:34:43.296484 140053320693632 learning.py:507] global step 930: loss = 3.1897 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 931: loss = 2.9012 (0.734 sec/step)\n",
            "I1212 08:34:44.353147 140053320693632 learning.py:507] global step 931: loss = 2.9012 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 932: loss = 2.3934 (1.342 sec/step)\n",
            "I1212 08:34:45.750710 140053320693632 learning.py:507] global step 932: loss = 2.3934 (1.342 sec/step)\n",
            "INFO:tensorflow:global step 933: loss = 3.5842 (0.576 sec/step)\n",
            "I1212 08:34:46.479207 140053320693632 learning.py:507] global step 933: loss = 3.5842 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 934: loss = 3.3719 (1.218 sec/step)\n",
            "I1212 08:34:48.442995 140053320693632 learning.py:507] global step 934: loss = 3.3719 (1.218 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 934.\n",
            "I1212 08:34:49.093139 140049614505728 supervisor.py:1050] Recording summary at step 934.\n",
            "INFO:tensorflow:global step 935: loss = 3.7324 (0.761 sec/step)\n",
            "I1212 08:34:49.695542 140053320693632 learning.py:507] global step 935: loss = 3.7324 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 936: loss = 3.1096 (1.229 sec/step)\n",
            "I1212 08:34:50.964827 140053320693632 learning.py:507] global step 936: loss = 3.1096 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 937: loss = 2.6525 (0.632 sec/step)\n",
            "I1212 08:34:51.903411 140053320693632 learning.py:507] global step 937: loss = 2.6525 (0.632 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.866463\n",
            "I1212 08:34:52.660994 140049622898432 supervisor.py:1099] global_step/sec: 0.866463\n",
            "INFO:tensorflow:global step 938: loss = 2.8638 (0.630 sec/step)\n",
            "I1212 08:34:52.832597 140053320693632 learning.py:507] global step 938: loss = 2.8638 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 939: loss = 2.6115 (1.606 sec/step)\n",
            "I1212 08:34:54.505787 140053320693632 learning.py:507] global step 939: loss = 2.6115 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 940: loss = 2.5489 (0.538 sec/step)\n",
            "I1212 08:34:55.190158 140053320693632 learning.py:507] global step 940: loss = 2.5489 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 941: loss = 2.3588 (1.421 sec/step)\n",
            "I1212 08:34:56.695601 140053320693632 learning.py:507] global step 941: loss = 2.3588 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 942: loss = 2.4066 (0.709 sec/step)\n",
            "I1212 08:34:57.584206 140053320693632 learning.py:507] global step 942: loss = 2.4066 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 943: loss = 1.9566 (0.679 sec/step)\n",
            "I1212 08:34:58.446706 140053320693632 learning.py:507] global step 943: loss = 1.9566 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 944: loss = 2.5646 (1.315 sec/step)\n",
            "I1212 08:34:59.881506 140053320693632 learning.py:507] global step 944: loss = 2.5646 (1.315 sec/step)\n",
            "INFO:tensorflow:global step 945: loss = 2.1269 (0.503 sec/step)\n",
            "I1212 08:35:00.415103 140053320693632 learning.py:507] global step 945: loss = 2.1269 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 946: loss = 2.5540 (1.650 sec/step)\n",
            "I1212 08:35:02.066039 140053320693632 learning.py:507] global step 946: loss = 2.5540 (1.650 sec/step)\n",
            "INFO:tensorflow:global step 947: loss = 2.6493 (0.569 sec/step)\n",
            "I1212 08:35:02.636894 140053320693632 learning.py:507] global step 947: loss = 2.6493 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 948: loss = 2.7843 (1.709 sec/step)\n",
            "I1212 08:35:04.347624 140053320693632 learning.py:507] global step 948: loss = 2.7843 (1.709 sec/step)\n",
            "INFO:tensorflow:global step 949: loss = 3.0297 (0.708 sec/step)\n",
            "I1212 08:35:05.248322 140053320693632 learning.py:507] global step 949: loss = 3.0297 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 950: loss = 3.3950 (0.623 sec/step)\n",
            "I1212 08:35:06.380867 140053320693632 learning.py:507] global step 950: loss = 3.3950 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 951: loss = 2.2551 (2.719 sec/step)\n",
            "I1212 08:35:09.124939 140053320693632 learning.py:507] global step 951: loss = 2.2551 (2.719 sec/step)\n",
            "INFO:tensorflow:global step 952: loss = 2.4880 (0.621 sec/step)\n",
            "I1212 08:35:09.747798 140053320693632 learning.py:507] global step 952: loss = 2.4880 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 953: loss = 3.0247 (1.817 sec/step)\n",
            "I1212 08:35:11.566993 140053320693632 learning.py:507] global step 953: loss = 3.0247 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 954: loss = 2.6244 (0.500 sec/step)\n",
            "I1212 08:35:12.068175 140053320693632 learning.py:507] global step 954: loss = 2.6244 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 955: loss = 3.5358 (1.766 sec/step)\n",
            "I1212 08:35:13.835540 140053320693632 learning.py:507] global step 955: loss = 3.5358 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 956: loss = 2.7443 (0.632 sec/step)\n",
            "I1212 08:35:14.704208 140053320693632 learning.py:507] global step 956: loss = 2.7443 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 957: loss = 2.4859 (1.185 sec/step)\n",
            "I1212 08:35:16.059106 140053320693632 learning.py:507] global step 957: loss = 2.4859 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 958: loss = 2.8159 (0.618 sec/step)\n",
            "I1212 08:35:16.929848 140053320693632 learning.py:507] global step 958: loss = 2.8159 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 959: loss = 2.7470 (0.743 sec/step)\n",
            "I1212 08:35:18.174717 140053320693632 learning.py:507] global step 959: loss = 2.7470 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 960: loss = 2.7921 (0.671 sec/step)\n",
            "I1212 08:35:19.233578 140053320693632 learning.py:507] global step 960: loss = 2.7921 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 961: loss = 2.5696 (0.588 sec/step)\n",
            "I1212 08:35:20.203517 140053320693632 learning.py:507] global step 961: loss = 2.5696 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 962: loss = 2.7645 (1.512 sec/step)\n",
            "I1212 08:35:21.746810 140053320693632 learning.py:507] global step 962: loss = 2.7645 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 963: loss = 2.7493 (0.664 sec/step)\n",
            "I1212 08:35:22.635586 140053320693632 learning.py:507] global step 963: loss = 2.7493 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 964: loss = 2.6867 (1.083 sec/step)\n",
            "I1212 08:35:23.795248 140053320693632 learning.py:507] global step 964: loss = 2.6867 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 965: loss = 2.4835 (0.524 sec/step)\n",
            "I1212 08:35:24.653889 140053320693632 learning.py:507] global step 965: loss = 2.4835 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 966: loss = 3.1950 (0.605 sec/step)\n",
            "I1212 08:35:25.446992 140053320693632 learning.py:507] global step 966: loss = 3.1950 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 967: loss = 2.4982 (1.503 sec/step)\n",
            "I1212 08:35:26.970855 140053320693632 learning.py:507] global step 967: loss = 2.4982 (1.503 sec/step)\n",
            "INFO:tensorflow:global step 968: loss = 2.3604 (0.696 sec/step)\n",
            "I1212 08:35:27.899241 140053320693632 learning.py:507] global step 968: loss = 2.3604 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 969: loss = 2.3275 (0.659 sec/step)\n",
            "I1212 08:35:28.876752 140053320693632 learning.py:507] global step 969: loss = 2.3275 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 970: loss = 2.0357 (0.563 sec/step)\n",
            "I1212 08:35:29.565620 140053320693632 learning.py:507] global step 970: loss = 2.0357 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 971: loss = 2.5918 (1.548 sec/step)\n",
            "I1212 08:35:31.118496 140053320693632 learning.py:507] global step 971: loss = 2.5918 (1.548 sec/step)\n",
            "INFO:tensorflow:global step 972: loss = 2.2244 (1.287 sec/step)\n",
            "I1212 08:35:32.423612 140053320693632 learning.py:507] global step 972: loss = 2.2244 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 973: loss = 2.7811 (0.626 sec/step)\n",
            "I1212 08:35:33.373102 140053320693632 learning.py:507] global step 973: loss = 2.7811 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 974: loss = 2.2360 (0.634 sec/step)\n",
            "I1212 08:35:34.298517 140053320693632 learning.py:507] global step 974: loss = 2.2360 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 975: loss = 2.6092 (0.744 sec/step)\n",
            "I1212 08:35:35.283046 140053320693632 learning.py:507] global step 975: loss = 2.6092 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 976: loss = 2.7192 (0.689 sec/step)\n",
            "I1212 08:35:36.231810 140053320693632 learning.py:507] global step 976: loss = 2.7192 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 977: loss = 2.5993 (1.210 sec/step)\n",
            "I1212 08:35:37.640402 140053320693632 learning.py:507] global step 977: loss = 2.5993 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 978: loss = 2.2072 (0.524 sec/step)\n",
            "I1212 08:35:38.246144 140053320693632 learning.py:507] global step 978: loss = 2.2072 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 979: loss = 2.9890 (1.436 sec/step)\n",
            "I1212 08:35:39.772549 140053320693632 learning.py:507] global step 979: loss = 2.9890 (1.436 sec/step)\n",
            "INFO:tensorflow:global step 980: loss = 2.4173 (0.669 sec/step)\n",
            "I1212 08:35:40.451242 140053320693632 learning.py:507] global step 980: loss = 2.4173 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 981: loss = 1.7976 (1.475 sec/step)\n",
            "I1212 08:35:42.154066 140053320693632 learning.py:507] global step 981: loss = 1.7976 (1.475 sec/step)\n",
            "INFO:tensorflow:global step 982: loss = 2.0842 (0.637 sec/step)\n",
            "I1212 08:35:42.890038 140053320693632 learning.py:507] global step 982: loss = 2.0842 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 983: loss = 2.5462 (1.239 sec/step)\n",
            "I1212 08:35:44.374696 140053320693632 learning.py:507] global step 983: loss = 2.5462 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 984: loss = 1.8387 (0.722 sec/step)\n",
            "I1212 08:35:45.385565 140053320693632 learning.py:507] global step 984: loss = 1.8387 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 985: loss = 2.7964 (1.234 sec/step)\n",
            "I1212 08:35:46.685610 140053320693632 learning.py:507] global step 985: loss = 2.7964 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 986: loss = 1.7684 (0.709 sec/step)\n",
            "I1212 08:35:47.603634 140053320693632 learning.py:507] global step 986: loss = 1.7684 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 987: loss = 2.5459 (1.143 sec/step)\n",
            "I1212 08:35:48.807124 140053320693632 learning.py:507] global step 987: loss = 2.5459 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 988: loss = 2.4966 (0.662 sec/step)\n",
            "I1212 08:35:49.732882 140053320693632 learning.py:507] global step 988: loss = 2.4966 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 989: loss = 3.0571 (0.677 sec/step)\n",
            "I1212 08:35:50.546475 140053320693632 learning.py:507] global step 989: loss = 3.0571 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 990: loss = 3.2950 (1.434 sec/step)\n",
            "I1212 08:35:52.003334 140053320693632 learning.py:507] global step 990: loss = 3.2950 (1.434 sec/step)\n",
            "INFO:tensorflow:global step 991: loss = 3.1802 (0.636 sec/step)\n",
            "I1212 08:35:52.872008 140053320693632 learning.py:507] global step 991: loss = 3.1802 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 992: loss = 2.0037 (0.541 sec/step)\n",
            "I1212 08:35:53.514252 140053320693632 learning.py:507] global step 992: loss = 2.0037 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 993: loss = 2.9416 (1.750 sec/step)\n",
            "I1212 08:35:55.266034 140053320693632 learning.py:507] global step 993: loss = 2.9416 (1.750 sec/step)\n",
            "INFO:tensorflow:global step 994: loss = 3.0458 (0.469 sec/step)\n",
            "I1212 08:35:55.736875 140053320693632 learning.py:507] global step 994: loss = 3.0458 (0.469 sec/step)\n",
            "INFO:tensorflow:global step 995: loss = 2.1720 (0.723 sec/step)\n",
            "I1212 08:35:56.461878 140053320693632 learning.py:507] global step 995: loss = 2.1720 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 996: loss = 2.7080 (2.322 sec/step)\n",
            "I1212 08:35:58.835862 140053320693632 learning.py:507] global step 996: loss = 2.7080 (2.322 sec/step)\n",
            "INFO:tensorflow:global step 997: loss = 2.6114 (0.628 sec/step)\n",
            "I1212 08:35:59.649478 140053320693632 learning.py:507] global step 997: loss = 2.6114 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 998: loss = 2.7179 (1.242 sec/step)\n",
            "I1212 08:36:01.112776 140053320693632 learning.py:507] global step 998: loss = 2.7179 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 999: loss = 2.2235 (0.666 sec/step)\n",
            "I1212 08:36:02.063926 140053320693632 learning.py:507] global step 999: loss = 2.2235 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 2.2158 (0.692 sec/step)\n",
            "I1212 08:36:03.165550 140053320693632 learning.py:507] global step 1000: loss = 2.2158 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1001: loss = 2.5392 (0.762 sec/step)\n",
            "I1212 08:36:03.955399 140053320693632 learning.py:507] global step 1001: loss = 2.5392 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 1002: loss = 2.6724 (1.202 sec/step)\n",
            "I1212 08:36:05.519870 140053320693632 learning.py:507] global step 1002: loss = 2.6724 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1003: loss = 2.1914 (0.514 sec/step)\n",
            "I1212 08:36:06.362506 140053320693632 learning.py:507] global step 1003: loss = 2.1914 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 1004: loss = 2.2184 (0.501 sec/step)\n",
            "I1212 08:36:06.992673 140053320693632 learning.py:507] global step 1004: loss = 2.2184 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 1005: loss = 2.5631 (1.450 sec/step)\n",
            "I1212 08:36:08.472466 140053320693632 learning.py:507] global step 1005: loss = 2.5631 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 1006: loss = 2.6794 (0.732 sec/step)\n",
            "I1212 08:36:09.219389 140053320693632 learning.py:507] global step 1006: loss = 2.6794 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1007: loss = 2.8476 (1.464 sec/step)\n",
            "I1212 08:36:10.684776 140053320693632 learning.py:507] global step 1007: loss = 2.8476 (1.464 sec/step)\n",
            "INFO:tensorflow:global step 1008: loss = 2.5274 (0.683 sec/step)\n",
            "I1212 08:36:11.384123 140053320693632 learning.py:507] global step 1008: loss = 2.5274 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1009: loss = 2.2607 (1.551 sec/step)\n",
            "I1212 08:36:12.937101 140053320693632 learning.py:507] global step 1009: loss = 2.2607 (1.551 sec/step)\n",
            "INFO:tensorflow:global step 1010: loss = 1.9868 (0.508 sec/step)\n",
            "I1212 08:36:13.706271 140053320693632 learning.py:507] global step 1010: loss = 1.9868 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 1011: loss = 2.8520 (2.077 sec/step)\n",
            "I1212 08:36:15.988983 140053320693632 learning.py:507] global step 1011: loss = 2.8520 (2.077 sec/step)\n",
            "INFO:tensorflow:global step 1012: loss = 2.2813 (0.777 sec/step)\n",
            "I1212 08:36:17.052977 140053320693632 learning.py:507] global step 1012: loss = 2.2813 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 1013: loss = 3.6094 (0.593 sec/step)\n",
            "I1212 08:36:17.663417 140053320693632 learning.py:507] global step 1013: loss = 3.6094 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 1014: loss = 2.4290 (1.005 sec/step)\n",
            "I1212 08:36:18.776384 140053320693632 learning.py:507] global step 1014: loss = 2.4290 (1.005 sec/step)\n",
            "INFO:tensorflow:global step 1015: loss = 2.3416 (1.735 sec/step)\n",
            "I1212 08:36:20.779196 140053320693632 learning.py:507] global step 1015: loss = 2.3416 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 1016: loss = 2.3400 (0.521 sec/step)\n",
            "I1212 08:36:21.603841 140053320693632 learning.py:507] global step 1016: loss = 2.3400 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 1017: loss = 2.6710 (0.613 sec/step)\n",
            "I1212 08:36:22.610857 140053320693632 learning.py:507] global step 1017: loss = 2.6710 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 1018: loss = 2.2166 (1.612 sec/step)\n",
            "I1212 08:36:24.337450 140053320693632 learning.py:507] global step 1018: loss = 2.2166 (1.612 sec/step)\n",
            "INFO:tensorflow:global step 1019: loss = 2.3232 (0.638 sec/step)\n",
            "I1212 08:36:25.074706 140053320693632 learning.py:507] global step 1019: loss = 2.3232 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1020: loss = 1.9956 (0.676 sec/step)\n",
            "I1212 08:36:25.965755 140053320693632 learning.py:507] global step 1020: loss = 1.9956 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1021: loss = 2.6778 (1.096 sec/step)\n",
            "I1212 08:36:27.456840 140053320693632 learning.py:507] global step 1021: loss = 2.6778 (1.096 sec/step)\n",
            "INFO:tensorflow:global step 1022: loss = 2.7624 (0.620 sec/step)\n",
            "I1212 08:36:28.326699 140053320693632 learning.py:507] global step 1022: loss = 2.7624 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1023: loss = 2.6057 (1.743 sec/step)\n",
            "I1212 08:36:30.071181 140053320693632 learning.py:507] global step 1023: loss = 2.6057 (1.743 sec/step)\n",
            "INFO:tensorflow:global step 1024: loss = 2.5489 (0.674 sec/step)\n",
            "I1212 08:36:30.920063 140053320693632 learning.py:507] global step 1024: loss = 2.5489 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 1025: loss = 2.4773 (0.605 sec/step)\n",
            "I1212 08:36:31.816000 140053320693632 learning.py:507] global step 1025: loss = 2.4773 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 1026: loss = 2.0609 (1.542 sec/step)\n",
            "I1212 08:36:33.653902 140053320693632 learning.py:507] global step 1026: loss = 2.0609 (1.542 sec/step)\n",
            "INFO:tensorflow:global step 1027: loss = 3.1479 (0.597 sec/step)\n",
            "I1212 08:36:34.527057 140053320693632 learning.py:507] global step 1027: loss = 3.1479 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1028: loss = 2.1144 (1.527 sec/step)\n",
            "I1212 08:36:36.165049 140053320693632 learning.py:507] global step 1028: loss = 2.1144 (1.527 sec/step)\n",
            "INFO:tensorflow:global step 1029: loss = 2.8044 (0.643 sec/step)\n",
            "I1212 08:36:37.009607 140053320693632 learning.py:507] global step 1029: loss = 2.8044 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1030: loss = 2.6643 (1.329 sec/step)\n",
            "I1212 08:36:38.496246 140053320693632 learning.py:507] global step 1030: loss = 2.6643 (1.329 sec/step)\n",
            "INFO:tensorflow:global step 1031: loss = 2.6133 (0.603 sec/step)\n",
            "I1212 08:36:39.100307 140053320693632 learning.py:507] global step 1031: loss = 2.6133 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1032: loss = 2.4992 (1.672 sec/step)\n",
            "I1212 08:36:40.773941 140053320693632 learning.py:507] global step 1032: loss = 2.4992 (1.672 sec/step)\n",
            "INFO:tensorflow:global step 1033: loss = 2.7260 (0.637 sec/step)\n",
            "I1212 08:36:41.481511 140053320693632 learning.py:507] global step 1033: loss = 2.7260 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1034: loss = 2.6055 (1.490 sec/step)\n",
            "I1212 08:36:43.011166 140053320693632 learning.py:507] global step 1034: loss = 2.6055 (1.490 sec/step)\n",
            "INFO:tensorflow:global step 1035: loss = 2.5354 (0.638 sec/step)\n",
            "I1212 08:36:43.884524 140053320693632 learning.py:507] global step 1035: loss = 2.5354 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1036: loss = 1.8411 (0.542 sec/step)\n",
            "I1212 08:36:44.529125 140053320693632 learning.py:507] global step 1036: loss = 1.8411 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 1037: loss = 3.0113 (1.378 sec/step)\n",
            "I1212 08:36:45.956132 140053320693632 learning.py:507] global step 1037: loss = 3.0113 (1.378 sec/step)\n",
            "INFO:tensorflow:global step 1038: loss = 2.8214 (0.691 sec/step)\n",
            "I1212 08:36:46.911477 140053320693632 learning.py:507] global step 1038: loss = 2.8214 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1039: loss = 2.2054 (0.717 sec/step)\n",
            "I1212 08:36:47.753018 140053320693632 learning.py:507] global step 1039: loss = 2.2054 (0.717 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1212 08:36:47.881816 140049589327616 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 1039.\n",
            "I1212 08:36:48.797140 140049614505728 supervisor.py:1050] Recording summary at step 1039.\n",
            "INFO:tensorflow:global step 1040: loss = 2.7859 (1.194 sec/step)\n",
            "I1212 08:36:49.407284 140053320693632 learning.py:507] global step 1040: loss = 2.7859 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1041: loss = 2.4333 (1.505 sec/step)\n",
            "I1212 08:36:51.070496 140053320693632 learning.py:507] global step 1041: loss = 2.4333 (1.505 sec/step)\n",
            "INFO:tensorflow:global step 1042: loss = 2.8956 (1.077 sec/step)\n",
            "I1212 08:36:52.929590 140053320693632 learning.py:507] global step 1042: loss = 2.8956 (1.077 sec/step)\n",
            "INFO:tensorflow:global step 1043: loss = 2.7077 (1.929 sec/step)\n",
            "I1212 08:36:55.211756 140053320693632 learning.py:507] global step 1043: loss = 2.7077 (1.929 sec/step)\n",
            "INFO:tensorflow:global step 1044: loss = 3.0560 (0.585 sec/step)\n",
            "I1212 08:36:55.798907 140053320693632 learning.py:507] global step 1044: loss = 3.0560 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1045: loss = 3.0540 (1.843 sec/step)\n",
            "I1212 08:36:57.644230 140053320693632 learning.py:507] global step 1045: loss = 3.0540 (1.843 sec/step)\n",
            "INFO:tensorflow:global step 1046: loss = 2.4175 (0.589 sec/step)\n",
            "I1212 08:36:58.638266 140053320693632 learning.py:507] global step 1046: loss = 2.4175 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 1047: loss = 2.4908 (1.358 sec/step)\n",
            "I1212 08:37:00.050959 140053320693632 learning.py:507] global step 1047: loss = 2.4908 (1.358 sec/step)\n",
            "INFO:tensorflow:global step 1048: loss = 4.3587 (0.752 sec/step)\n",
            "I1212 08:37:01.098079 140053320693632 learning.py:507] global step 1048: loss = 4.3587 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 1049: loss = 2.2958 (0.724 sec/step)\n",
            "I1212 08:37:02.023323 140053320693632 learning.py:507] global step 1049: loss = 2.2958 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1050: loss = 2.2399 (0.660 sec/step)\n",
            "I1212 08:37:03.087856 140053320693632 learning.py:507] global step 1050: loss = 2.2399 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1051: loss = 2.9018 (0.637 sec/step)\n",
            "I1212 08:37:04.148746 140053320693632 learning.py:507] global step 1051: loss = 2.9018 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1052: loss = 2.0843 (1.506 sec/step)\n",
            "I1212 08:37:05.793403 140053320693632 learning.py:507] global step 1052: loss = 2.0843 (1.506 sec/step)\n",
            "INFO:tensorflow:global step 1053: loss = 2.0119 (0.658 sec/step)\n",
            "I1212 08:37:06.750196 140053320693632 learning.py:507] global step 1053: loss = 2.0119 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1054: loss = 1.9823 (1.393 sec/step)\n",
            "I1212 08:37:08.289412 140053320693632 learning.py:507] global step 1054: loss = 1.9823 (1.393 sec/step)\n",
            "INFO:tensorflow:global step 1055: loss = 2.9200 (1.132 sec/step)\n",
            "I1212 08:37:09.423249 140053320693632 learning.py:507] global step 1055: loss = 2.9200 (1.132 sec/step)\n",
            "INFO:tensorflow:global step 1056: loss = 3.1713 (1.173 sec/step)\n",
            "I1212 08:37:10.597748 140053320693632 learning.py:507] global step 1056: loss = 3.1713 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1057: loss = 2.3242 (0.677 sec/step)\n",
            "I1212 08:37:11.291501 140053320693632 learning.py:507] global step 1057: loss = 2.3242 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1058: loss = 2.3700 (1.535 sec/step)\n",
            "I1212 08:37:13.017348 140053320693632 learning.py:507] global step 1058: loss = 2.3700 (1.535 sec/step)\n",
            "INFO:tensorflow:global step 1059: loss = 2.1937 (0.543 sec/step)\n",
            "I1212 08:37:13.562062 140053320693632 learning.py:507] global step 1059: loss = 2.1937 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 1060: loss = 2.6533 (1.645 sec/step)\n",
            "I1212 08:37:15.208532 140053320693632 learning.py:507] global step 1060: loss = 2.6533 (1.645 sec/step)\n",
            "INFO:tensorflow:global step 1061: loss = 2.5229 (0.579 sec/step)\n",
            "I1212 08:37:15.920821 140053320693632 learning.py:507] global step 1061: loss = 2.5229 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 1062: loss = 2.1287 (0.666 sec/step)\n",
            "I1212 08:37:17.165803 140053320693632 learning.py:507] global step 1062: loss = 2.1287 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1063: loss = 2.2517 (0.764 sec/step)\n",
            "I1212 08:37:18.235846 140053320693632 learning.py:507] global step 1063: loss = 2.2517 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 1064: loss = 2.3250 (0.650 sec/step)\n",
            "I1212 08:37:19.133389 140053320693632 learning.py:507] global step 1064: loss = 2.3250 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1065: loss = 3.0018 (0.647 sec/step)\n",
            "I1212 08:37:20.170991 140053320693632 learning.py:507] global step 1065: loss = 3.0018 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1066: loss = 2.4790 (1.518 sec/step)\n",
            "I1212 08:37:21.739441 140053320693632 learning.py:507] global step 1066: loss = 2.4790 (1.518 sec/step)\n",
            "INFO:tensorflow:global step 1067: loss = 2.4983 (0.828 sec/step)\n",
            "I1212 08:37:22.791228 140053320693632 learning.py:507] global step 1067: loss = 2.4983 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 1068: loss = 2.4516 (0.676 sec/step)\n",
            "I1212 08:37:23.749853 140053320693632 learning.py:507] global step 1068: loss = 2.4516 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1069: loss = 2.6576 (1.061 sec/step)\n",
            "I1212 08:37:24.861490 140053320693632 learning.py:507] global step 1069: loss = 2.6576 (1.061 sec/step)\n",
            "INFO:tensorflow:global step 1070: loss = 2.7808 (0.745 sec/step)\n",
            "I1212 08:37:25.901114 140053320693632 learning.py:507] global step 1070: loss = 2.7808 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 1071: loss = 2.8170 (0.732 sec/step)\n",
            "I1212 08:37:26.993464 140053320693632 learning.py:507] global step 1071: loss = 2.8170 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1072: loss = 2.0823 (0.701 sec/step)\n",
            "I1212 08:37:27.930256 140053320693632 learning.py:507] global step 1072: loss = 2.0823 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1073: loss = 2.2423 (1.083 sec/step)\n",
            "I1212 08:37:29.174313 140053320693632 learning.py:507] global step 1073: loss = 2.2423 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 1074: loss = 2.9980 (0.684 sec/step)\n",
            "I1212 08:37:30.091585 140053320693632 learning.py:507] global step 1074: loss = 2.9980 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1075: loss = 2.5680 (0.678 sec/step)\n",
            "I1212 08:37:31.044292 140053320693632 learning.py:507] global step 1075: loss = 2.5680 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1076: loss = 2.5334 (0.730 sec/step)\n",
            "I1212 08:37:32.165180 140053320693632 learning.py:507] global step 1076: loss = 2.5334 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 1077: loss = 2.6774 (0.784 sec/step)\n",
            "I1212 08:37:33.207720 140053320693632 learning.py:507] global step 1077: loss = 2.6774 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 1078: loss = 2.2739 (1.225 sec/step)\n",
            "I1212 08:37:34.488451 140053320693632 learning.py:507] global step 1078: loss = 2.2739 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1079: loss = 3.1672 (0.599 sec/step)\n",
            "I1212 08:37:35.376454 140053320693632 learning.py:507] global step 1079: loss = 3.1672 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1080: loss = 1.9411 (1.148 sec/step)\n",
            "I1212 08:37:36.632555 140053320693632 learning.py:507] global step 1080: loss = 1.9411 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 1081: loss = 2.6791 (0.655 sec/step)\n",
            "I1212 08:37:37.349443 140053320693632 learning.py:507] global step 1081: loss = 2.6791 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1082: loss = 2.9252 (1.410 sec/step)\n",
            "I1212 08:37:38.867385 140053320693632 learning.py:507] global step 1082: loss = 2.9252 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 1083: loss = 2.5304 (0.746 sec/step)\n",
            "I1212 08:37:39.904531 140053320693632 learning.py:507] global step 1083: loss = 2.5304 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1084: loss = 2.6587 (0.643 sec/step)\n",
            "I1212 08:37:40.845642 140053320693632 learning.py:507] global step 1084: loss = 2.6587 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1085: loss = 2.5838 (1.266 sec/step)\n",
            "I1212 08:37:42.276048 140053320693632 learning.py:507] global step 1085: loss = 2.5838 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 1086: loss = 2.9858 (0.647 sec/step)\n",
            "I1212 08:37:43.169866 140053320693632 learning.py:507] global step 1086: loss = 2.9858 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1087: loss = 2.4705 (1.273 sec/step)\n",
            "I1212 08:37:44.617556 140053320693632 learning.py:507] global step 1087: loss = 2.4705 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 1088: loss = 3.0309 (0.599 sec/step)\n",
            "I1212 08:37:45.491761 140053320693632 learning.py:507] global step 1088: loss = 3.0309 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1089: loss = 2.7358 (1.210 sec/step)\n",
            "I1212 08:37:46.857192 140053320693632 learning.py:507] global step 1089: loss = 2.7358 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1090: loss = 2.6393 (0.576 sec/step)\n",
            "I1212 08:37:47.488886 140053320693632 learning.py:507] global step 1090: loss = 2.6393 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1091: loss = 2.4752 (1.410 sec/step)\n",
            "I1212 08:37:49.070290 140053320693632 learning.py:507] global step 1091: loss = 2.4752 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 1092: loss = 2.5107 (0.686 sec/step)\n",
            "I1212 08:37:49.855684 140053320693632 learning.py:507] global step 1092: loss = 2.5107 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1093: loss = 2.4805 (0.689 sec/step)\n",
            "I1212 08:37:51.024685 140053320693632 learning.py:507] global step 1093: loss = 2.4805 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1094: loss = 2.6160 (0.579 sec/step)\n",
            "I1212 08:37:51.914666 140053320693632 learning.py:507] global step 1094: loss = 2.6160 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 1095: loss = 2.9584 (1.224 sec/step)\n",
            "I1212 08:37:53.173702 140053320693632 learning.py:507] global step 1095: loss = 2.9584 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1096: loss = 2.7180 (0.484 sec/step)\n",
            "I1212 08:37:53.934147 140053320693632 learning.py:507] global step 1096: loss = 2.7180 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 1097: loss = 2.9309 (0.676 sec/step)\n",
            "I1212 08:37:54.989198 140053320693632 learning.py:507] global step 1097: loss = 2.9309 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1098: loss = 1.9968 (0.776 sec/step)\n",
            "I1212 08:37:56.135946 140053320693632 learning.py:507] global step 1098: loss = 1.9968 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1099: loss = 1.9836 (0.679 sec/step)\n",
            "I1212 08:37:56.926770 140053320693632 learning.py:507] global step 1099: loss = 1.9836 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1100: loss = 3.1790 (2.863 sec/step)\n",
            "I1212 08:37:59.969168 140053320693632 learning.py:507] global step 1100: loss = 3.1790 (2.863 sec/step)\n",
            "INFO:tensorflow:global step 1101: loss = 2.2160 (0.804 sec/step)\n",
            "I1212 08:38:00.967218 140053320693632 learning.py:507] global step 1101: loss = 2.2160 (0.804 sec/step)\n",
            "INFO:tensorflow:global step 1102: loss = 2.4131 (1.335 sec/step)\n",
            "I1212 08:38:02.463067 140053320693632 learning.py:507] global step 1102: loss = 2.4131 (1.335 sec/step)\n",
            "INFO:tensorflow:global step 1103: loss = 3.0169 (0.601 sec/step)\n",
            "I1212 08:38:03.065437 140053320693632 learning.py:507] global step 1103: loss = 3.0169 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1104: loss = 2.7241 (1.894 sec/step)\n",
            "I1212 08:38:04.961323 140053320693632 learning.py:507] global step 1104: loss = 2.7241 (1.894 sec/step)\n",
            "INFO:tensorflow:global step 1105: loss = 2.3015 (0.623 sec/step)\n",
            "I1212 08:38:05.585647 140053320693632 learning.py:507] global step 1105: loss = 2.3015 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1106: loss = 1.8342 (0.918 sec/step)\n",
            "I1212 08:38:06.516283 140053320693632 learning.py:507] global step 1106: loss = 1.8342 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 1107: loss = 2.3698 (1.851 sec/step)\n",
            "I1212 08:38:08.596705 140053320693632 learning.py:507] global step 1107: loss = 2.3698 (1.851 sec/step)\n",
            "INFO:tensorflow:global step 1108: loss = 2.8447 (0.738 sec/step)\n",
            "I1212 08:38:09.666025 140053320693632 learning.py:507] global step 1108: loss = 2.8447 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 1109: loss = 2.6338 (0.788 sec/step)\n",
            "I1212 08:38:10.486352 140053320693632 learning.py:507] global step 1109: loss = 2.6338 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 1110: loss = 3.0656 (1.223 sec/step)\n",
            "I1212 08:38:11.900074 140053320693632 learning.py:507] global step 1110: loss = 3.0656 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1111: loss = 1.9935 (0.643 sec/step)\n",
            "I1212 08:38:12.545350 140053320693632 learning.py:507] global step 1111: loss = 1.9935 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1112: loss = 2.2931 (0.939 sec/step)\n",
            "I1212 08:38:13.508682 140053320693632 learning.py:507] global step 1112: loss = 2.2931 (0.939 sec/step)\n",
            "INFO:tensorflow:global step 1113: loss = 2.8740 (2.088 sec/step)\n",
            "I1212 08:38:15.598604 140053320693632 learning.py:507] global step 1113: loss = 2.8740 (2.088 sec/step)\n",
            "INFO:tensorflow:global step 1114: loss = 1.9589 (0.626 sec/step)\n",
            "I1212 08:38:16.467978 140053320693632 learning.py:507] global step 1114: loss = 1.9589 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1115: loss = 2.7141 (0.641 sec/step)\n",
            "I1212 08:38:17.598007 140053320693632 learning.py:507] global step 1115: loss = 2.7141 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1116: loss = 2.4183 (1.237 sec/step)\n",
            "I1212 08:38:18.893584 140053320693632 learning.py:507] global step 1116: loss = 2.4183 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1117: loss = 3.1498 (0.686 sec/step)\n",
            "I1212 08:38:19.844393 140053320693632 learning.py:507] global step 1117: loss = 3.1498 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1118: loss = 2.3677 (0.636 sec/step)\n",
            "I1212 08:38:20.762580 140053320693632 learning.py:507] global step 1118: loss = 2.3677 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1119: loss = 3.5370 (0.614 sec/step)\n",
            "I1212 08:38:21.406909 140053320693632 learning.py:507] global step 1119: loss = 3.5370 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1120: loss = 2.9789 (0.553 sec/step)\n",
            "I1212 08:38:21.961854 140053320693632 learning.py:507] global step 1120: loss = 2.9789 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 1121: loss = 2.5223 (3.115 sec/step)\n",
            "I1212 08:38:25.079020 140053320693632 learning.py:507] global step 1121: loss = 2.5223 (3.115 sec/step)\n",
            "INFO:tensorflow:global step 1122: loss = 2.3075 (0.554 sec/step)\n",
            "I1212 08:38:25.634714 140053320693632 learning.py:507] global step 1122: loss = 2.3075 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 1123: loss = 2.6476 (1.056 sec/step)\n",
            "I1212 08:38:26.953725 140053320693632 learning.py:507] global step 1123: loss = 2.6476 (1.056 sec/step)\n",
            "INFO:tensorflow:global step 1124: loss = 2.7927 (1.303 sec/step)\n",
            "I1212 08:38:28.560537 140053320693632 learning.py:507] global step 1124: loss = 2.7927 (1.303 sec/step)\n",
            "INFO:tensorflow:global step 1125: loss = 2.4169 (0.519 sec/step)\n",
            "I1212 08:38:29.324779 140053320693632 learning.py:507] global step 1125: loss = 2.4169 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 1126: loss = 3.1521 (1.266 sec/step)\n",
            "I1212 08:38:30.780488 140053320693632 learning.py:507] global step 1126: loss = 3.1521 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 1127: loss = 2.3432 (0.721 sec/step)\n",
            "I1212 08:38:31.639374 140053320693632 learning.py:507] global step 1127: loss = 2.3432 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1128: loss = 2.0711 (0.689 sec/step)\n",
            "I1212 08:38:32.598177 140053320693632 learning.py:507] global step 1128: loss = 2.0711 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1129: loss = 2.3091 (1.519 sec/step)\n",
            "I1212 08:38:34.330636 140053320693632 learning.py:507] global step 1129: loss = 2.3091 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 1130: loss = 2.1464 (0.559 sec/step)\n",
            "I1212 08:38:34.891737 140053320693632 learning.py:507] global step 1130: loss = 2.1464 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1131: loss = 2.2189 (1.880 sec/step)\n",
            "I1212 08:38:36.773835 140053320693632 learning.py:507] global step 1131: loss = 2.2189 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 1132: loss = 3.4443 (0.486 sec/step)\n",
            "I1212 08:38:37.531050 140053320693632 learning.py:507] global step 1132: loss = 3.4443 (0.486 sec/step)\n",
            "INFO:tensorflow:global step 1133: loss = 2.4611 (1.412 sec/step)\n",
            "I1212 08:38:39.008776 140053320693632 learning.py:507] global step 1133: loss = 2.4611 (1.412 sec/step)\n",
            "INFO:tensorflow:global step 1134: loss = 2.6522 (0.811 sec/step)\n",
            "I1212 08:38:39.936782 140053320693632 learning.py:507] global step 1134: loss = 2.6522 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 1135: loss = 2.8276 (0.712 sec/step)\n",
            "I1212 08:38:40.968884 140053320693632 learning.py:507] global step 1135: loss = 2.8276 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 1136: loss = 2.0770 (0.598 sec/step)\n",
            "I1212 08:38:41.931493 140053320693632 learning.py:507] global step 1136: loss = 2.0770 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1137: loss = 2.8713 (1.470 sec/step)\n",
            "I1212 08:38:43.531576 140053320693632 learning.py:507] global step 1137: loss = 2.8713 (1.470 sec/step)\n",
            "INFO:tensorflow:global step 1138: loss = 2.0917 (0.621 sec/step)\n",
            "I1212 08:38:44.449888 140053320693632 learning.py:507] global step 1138: loss = 2.0917 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1139: loss = 2.2380 (0.617 sec/step)\n",
            "I1212 08:38:45.373404 140053320693632 learning.py:507] global step 1139: loss = 2.2380 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 1140: loss = 2.6692 (1.185 sec/step)\n",
            "I1212 08:38:46.820015 140053320693632 learning.py:507] global step 1140: loss = 2.6692 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1141: loss = 3.3498 (0.680 sec/step)\n",
            "I1212 08:38:47.748446 140053320693632 learning.py:507] global step 1141: loss = 3.3498 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1142: loss = 2.4297 (2.486 sec/step)\n",
            "I1212 08:38:50.295629 140053320693632 learning.py:507] global step 1142: loss = 2.4297 (2.486 sec/step)\n",
            "INFO:tensorflow:global step 1143: loss = 2.5315 (0.515 sec/step)\n",
            "I1212 08:38:50.812536 140053320693632 learning.py:507] global step 1143: loss = 2.5315 (0.515 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1143.\n",
            "I1212 08:38:50.846889 140049614505728 supervisor.py:1050] Recording summary at step 1143.\n",
            "INFO:tensorflow:global step 1144: loss = 2.5789 (1.632 sec/step)\n",
            "I1212 08:38:52.446121 140053320693632 learning.py:507] global step 1144: loss = 2.5789 (1.632 sec/step)\n",
            "INFO:tensorflow:global step 1145: loss = 2.0883 (0.502 sec/step)\n",
            "I1212 08:38:52.949520 140053320693632 learning.py:507] global step 1145: loss = 2.0883 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 1146: loss = 2.0439 (1.589 sec/step)\n",
            "I1212 08:38:54.540214 140053320693632 learning.py:507] global step 1146: loss = 2.0439 (1.589 sec/step)\n",
            "INFO:tensorflow:global step 1147: loss = 3.0168 (0.492 sec/step)\n",
            "I1212 08:38:55.033287 140053320693632 learning.py:507] global step 1147: loss = 3.0168 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 1148: loss = 1.7854 (0.816 sec/step)\n",
            "I1212 08:38:56.030723 140053320693632 learning.py:507] global step 1148: loss = 1.7854 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 1149: loss = 2.3108 (1.365 sec/step)\n",
            "I1212 08:38:57.702875 140053320693632 learning.py:507] global step 1149: loss = 2.3108 (1.365 sec/step)\n",
            "INFO:tensorflow:global step 1150: loss = 2.2492 (0.619 sec/step)\n",
            "I1212 08:38:58.587031 140053320693632 learning.py:507] global step 1150: loss = 2.2492 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1151: loss = 2.2749 (0.574 sec/step)\n",
            "I1212 08:38:59.297174 140053320693632 learning.py:507] global step 1151: loss = 2.2749 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 1152: loss = 2.5294 (1.423 sec/step)\n",
            "I1212 08:39:00.775688 140053320693632 learning.py:507] global step 1152: loss = 2.5294 (1.423 sec/step)\n",
            "INFO:tensorflow:global step 1153: loss = 2.4042 (1.234 sec/step)\n",
            "I1212 08:39:02.176985 140053320693632 learning.py:507] global step 1153: loss = 2.4042 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1154: loss = 2.1746 (0.633 sec/step)\n",
            "I1212 08:39:03.056704 140053320693632 learning.py:507] global step 1154: loss = 2.1746 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1155: loss = 2.5234 (0.583 sec/step)\n",
            "I1212 08:39:03.978712 140053320693632 learning.py:507] global step 1155: loss = 2.5234 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1156: loss = 2.3560 (1.301 sec/step)\n",
            "I1212 08:39:05.463373 140053320693632 learning.py:507] global step 1156: loss = 2.3560 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 1157: loss = 2.2356 (0.600 sec/step)\n",
            "I1212 08:39:06.248146 140053320693632 learning.py:507] global step 1157: loss = 2.2356 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 1158: loss = 2.5902 (1.264 sec/step)\n",
            "I1212 08:39:07.661800 140053320693632 learning.py:507] global step 1158: loss = 2.5902 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1159: loss = 2.6639 (0.641 sec/step)\n",
            "I1212 08:39:08.525393 140053320693632 learning.py:507] global step 1159: loss = 2.6639 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1160: loss = 2.4113 (0.688 sec/step)\n",
            "I1212 08:39:09.626557 140053320693632 learning.py:507] global step 1160: loss = 2.4113 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1161: loss = 2.2568 (1.962 sec/step)\n",
            "I1212 08:39:11.881733 140053320693632 learning.py:507] global step 1161: loss = 2.2568 (1.962 sec/step)\n",
            "INFO:tensorflow:global step 1162: loss = 2.3025 (0.727 sec/step)\n",
            "I1212 08:39:12.930669 140053320693632 learning.py:507] global step 1162: loss = 2.3025 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1163: loss = 2.6108 (0.696 sec/step)\n",
            "I1212 08:39:13.902836 140053320693632 learning.py:507] global step 1163: loss = 2.6108 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 1164: loss = 2.4609 (1.549 sec/step)\n",
            "I1212 08:39:15.454382 140053320693632 learning.py:507] global step 1164: loss = 2.4609 (1.549 sec/step)\n",
            "INFO:tensorflow:global step 1165: loss = 2.8132 (0.497 sec/step)\n",
            "I1212 08:39:15.952473 140053320693632 learning.py:507] global step 1165: loss = 2.8132 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 1166: loss = 2.8135 (1.581 sec/step)\n",
            "I1212 08:39:17.796839 140053320693632 learning.py:507] global step 1166: loss = 2.8135 (1.581 sec/step)\n",
            "INFO:tensorflow:global step 1167: loss = 2.5840 (1.754 sec/step)\n",
            "I1212 08:39:19.785091 140053320693632 learning.py:507] global step 1167: loss = 2.5840 (1.754 sec/step)\n",
            "INFO:tensorflow:global step 1168: loss = 2.5643 (0.759 sec/step)\n",
            "I1212 08:39:20.888691 140053320693632 learning.py:507] global step 1168: loss = 2.5643 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 1169: loss = 3.2216 (1.830 sec/step)\n",
            "I1212 08:39:22.765843 140053320693632 learning.py:507] global step 1169: loss = 3.2216 (1.830 sec/step)\n",
            "INFO:tensorflow:global step 1170: loss = 2.7325 (0.586 sec/step)\n",
            "I1212 08:39:23.353192 140053320693632 learning.py:507] global step 1170: loss = 2.7325 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 1171: loss = 2.8317 (0.917 sec/step)\n",
            "I1212 08:39:24.275489 140053320693632 learning.py:507] global step 1171: loss = 2.8317 (0.917 sec/step)\n",
            "INFO:tensorflow:global step 1172: loss = 2.1605 (1.924 sec/step)\n",
            "I1212 08:39:26.247302 140053320693632 learning.py:507] global step 1172: loss = 2.1605 (1.924 sec/step)\n",
            "INFO:tensorflow:global step 1173: loss = 1.9896 (0.587 sec/step)\n",
            "I1212 08:39:26.836037 140053320693632 learning.py:507] global step 1173: loss = 1.9896 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1174: loss = 3.2583 (1.036 sec/step)\n",
            "I1212 08:39:27.978130 140053320693632 learning.py:507] global step 1174: loss = 3.2583 (1.036 sec/step)\n",
            "INFO:tensorflow:global step 1175: loss = 3.2383 (2.402 sec/step)\n",
            "I1212 08:39:30.740989 140053320693632 learning.py:507] global step 1175: loss = 3.2383 (2.402 sec/step)\n",
            "INFO:tensorflow:global step 1176: loss = 2.6025 (0.515 sec/step)\n",
            "I1212 08:39:31.396560 140053320693632 learning.py:507] global step 1176: loss = 2.6025 (0.515 sec/step)\n",
            "INFO:tensorflow:global step 1177: loss = 2.6621 (1.471 sec/step)\n",
            "I1212 08:39:33.181333 140053320693632 learning.py:507] global step 1177: loss = 2.6621 (1.471 sec/step)\n",
            "INFO:tensorflow:global step 1178: loss = 2.5256 (0.729 sec/step)\n",
            "I1212 08:39:34.124664 140053320693632 learning.py:507] global step 1178: loss = 2.5256 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1179: loss = 2.5526 (0.707 sec/step)\n",
            "I1212 08:39:35.019543 140053320693632 learning.py:507] global step 1179: loss = 2.5526 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 1180: loss = 2.2871 (2.271 sec/step)\n",
            "I1212 08:39:37.292714 140053320693632 learning.py:507] global step 1180: loss = 2.2871 (2.271 sec/step)\n",
            "INFO:tensorflow:global step 1181: loss = 2.2429 (0.588 sec/step)\n",
            "I1212 08:39:37.882883 140053320693632 learning.py:507] global step 1181: loss = 2.2429 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 1182: loss = 2.9306 (1.821 sec/step)\n",
            "I1212 08:39:39.705487 140053320693632 learning.py:507] global step 1182: loss = 2.9306 (1.821 sec/step)\n",
            "INFO:tensorflow:global step 1183: loss = 2.3124 (0.775 sec/step)\n",
            "I1212 08:39:40.722079 140053320693632 learning.py:507] global step 1183: loss = 2.3124 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 1184: loss = 3.2796 (0.702 sec/step)\n",
            "I1212 08:39:41.739981 140053320693632 learning.py:507] global step 1184: loss = 3.2796 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 1185: loss = 2.0262 (1.222 sec/step)\n",
            "I1212 08:39:43.082808 140053320693632 learning.py:507] global step 1185: loss = 2.0262 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1186: loss = 3.0147 (0.690 sec/step)\n",
            "I1212 08:39:44.075500 140053320693632 learning.py:507] global step 1186: loss = 3.0147 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1187: loss = 2.5079 (0.629 sec/step)\n",
            "I1212 08:39:44.953472 140053320693632 learning.py:507] global step 1187: loss = 2.5079 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1188: loss = 1.9657 (0.578 sec/step)\n",
            "I1212 08:39:45.798350 140053320693632 learning.py:507] global step 1188: loss = 1.9657 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 1189: loss = 2.4803 (1.544 sec/step)\n",
            "I1212 08:39:47.379793 140053320693632 learning.py:507] global step 1189: loss = 2.4803 (1.544 sec/step)\n",
            "INFO:tensorflow:global step 1190: loss = 2.1659 (0.494 sec/step)\n",
            "I1212 08:39:48.122708 140053320693632 learning.py:507] global step 1190: loss = 2.1659 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 1191: loss = 2.2432 (0.597 sec/step)\n",
            "I1212 08:39:48.952072 140053320693632 learning.py:507] global step 1191: loss = 2.2432 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1192: loss = 2.8378 (0.838 sec/step)\n",
            "I1212 08:39:50.016570 140053320693632 learning.py:507] global step 1192: loss = 2.8378 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 1193: loss = 2.3513 (2.019 sec/step)\n",
            "I1212 08:39:52.315485 140053320693632 learning.py:507] global step 1193: loss = 2.3513 (2.019 sec/step)\n",
            "INFO:tensorflow:global step 1194: loss = 2.2194 (0.569 sec/step)\n",
            "I1212 08:39:53.236011 140053320693632 learning.py:507] global step 1194: loss = 2.2194 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1195: loss = 2.1911 (0.644 sec/step)\n",
            "I1212 08:39:53.984693 140053320693632 learning.py:507] global step 1195: loss = 2.1911 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1196: loss = 2.3944 (1.947 sec/step)\n",
            "I1212 08:39:55.933424 140053320693632 learning.py:507] global step 1196: loss = 2.3944 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 1197: loss = 2.6591 (0.587 sec/step)\n",
            "I1212 08:39:56.834443 140053320693632 learning.py:507] global step 1197: loss = 2.6591 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1198: loss = 2.6146 (0.873 sec/step)\n",
            "I1212 08:39:57.755925 140053320693632 learning.py:507] global step 1198: loss = 2.6146 (0.873 sec/step)\n",
            "INFO:tensorflow:global step 1199: loss = 2.4110 (1.794 sec/step)\n",
            "I1212 08:39:59.551518 140053320693632 learning.py:507] global step 1199: loss = 2.4110 (1.794 sec/step)\n",
            "INFO:tensorflow:global step 1200: loss = 2.5241 (0.550 sec/step)\n",
            "I1212 08:40:00.103230 140053320693632 learning.py:507] global step 1200: loss = 2.5241 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 1201: loss = 2.8057 (1.069 sec/step)\n",
            "I1212 08:40:01.655822 140053320693632 learning.py:507] global step 1201: loss = 2.8057 (1.069 sec/step)\n",
            "INFO:tensorflow:global step 1202: loss = 2.0929 (0.714 sec/step)\n",
            "I1212 08:40:02.806084 140053320693632 learning.py:507] global step 1202: loss = 2.0929 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 1203: loss = 3.2398 (1.536 sec/step)\n",
            "I1212 08:40:04.371309 140053320693632 learning.py:507] global step 1203: loss = 3.2398 (1.536 sec/step)\n",
            "INFO:tensorflow:global step 1204: loss = 2.6289 (0.658 sec/step)\n",
            "I1212 08:40:05.089066 140053320693632 learning.py:507] global step 1204: loss = 2.6289 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1205: loss = 2.8150 (0.750 sec/step)\n",
            "I1212 08:40:06.099924 140053320693632 learning.py:507] global step 1205: loss = 2.8150 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 1206: loss = 2.3508 (0.884 sec/step)\n",
            "I1212 08:40:07.284721 140053320693632 learning.py:507] global step 1206: loss = 2.3508 (0.884 sec/step)\n",
            "INFO:tensorflow:global step 1207: loss = 2.9009 (1.948 sec/step)\n",
            "I1212 08:40:09.396825 140053320693632 learning.py:507] global step 1207: loss = 2.9009 (1.948 sec/step)\n",
            "INFO:tensorflow:global step 1208: loss = 2.4795 (0.620 sec/step)\n",
            "I1212 08:40:10.173893 140053320693632 learning.py:507] global step 1208: loss = 2.4795 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1209: loss = 2.8141 (0.973 sec/step)\n",
            "I1212 08:40:11.590209 140053320693632 learning.py:507] global step 1209: loss = 2.8141 (0.973 sec/step)\n",
            "INFO:tensorflow:global step 1210: loss = 2.3712 (0.682 sec/step)\n",
            "I1212 08:40:12.460784 140053320693632 learning.py:507] global step 1210: loss = 2.3712 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1211: loss = 2.6939 (1.126 sec/step)\n",
            "I1212 08:40:13.681969 140053320693632 learning.py:507] global step 1211: loss = 2.6939 (1.126 sec/step)\n",
            "INFO:tensorflow:global step 1212: loss = 2.8989 (1.787 sec/step)\n",
            "I1212 08:40:15.498320 140053320693632 learning.py:507] global step 1212: loss = 2.8989 (1.787 sec/step)\n",
            "INFO:tensorflow:global step 1213: loss = 2.4224 (0.626 sec/step)\n",
            "I1212 08:40:16.126514 140053320693632 learning.py:507] global step 1213: loss = 2.4224 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1214: loss = 3.2728 (1.742 sec/step)\n",
            "I1212 08:40:17.894486 140053320693632 learning.py:507] global step 1214: loss = 3.2728 (1.742 sec/step)\n",
            "INFO:tensorflow:global step 1215: loss = 2.8847 (1.102 sec/step)\n",
            "I1212 08:40:19.211970 140053320693632 learning.py:507] global step 1215: loss = 2.8847 (1.102 sec/step)\n",
            "INFO:tensorflow:global step 1216: loss = 2.9168 (1.602 sec/step)\n",
            "I1212 08:40:20.827011 140053320693632 learning.py:507] global step 1216: loss = 2.9168 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 1217: loss = 2.2588 (0.641 sec/step)\n",
            "I1212 08:40:21.469746 140053320693632 learning.py:507] global step 1217: loss = 2.2588 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1218: loss = 2.8410 (1.418 sec/step)\n",
            "I1212 08:40:23.188466 140053320693632 learning.py:507] global step 1218: loss = 2.8410 (1.418 sec/step)\n",
            "INFO:tensorflow:global step 1219: loss = 2.9580 (0.678 sec/step)\n",
            "I1212 08:40:24.143326 140053320693632 learning.py:507] global step 1219: loss = 2.9580 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1220: loss = 2.7246 (1.545 sec/step)\n",
            "I1212 08:40:25.732939 140053320693632 learning.py:507] global step 1220: loss = 2.7246 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1221: loss = 2.1838 (0.688 sec/step)\n",
            "I1212 08:40:26.533524 140053320693632 learning.py:507] global step 1221: loss = 2.1838 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1222: loss = 2.6440 (0.724 sec/step)\n",
            "I1212 08:40:27.779815 140053320693632 learning.py:507] global step 1222: loss = 2.6440 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1223: loss = 2.4185 (0.681 sec/step)\n",
            "I1212 08:40:28.635501 140053320693632 learning.py:507] global step 1223: loss = 2.4185 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1224: loss = 2.2528 (0.725 sec/step)\n",
            "I1212 08:40:29.762884 140053320693632 learning.py:507] global step 1224: loss = 2.2528 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1225: loss = 2.2472 (1.474 sec/step)\n",
            "I1212 08:40:31.294692 140053320693632 learning.py:507] global step 1225: loss = 2.2472 (1.474 sec/step)\n",
            "INFO:tensorflow:global step 1226: loss = 2.4194 (0.642 sec/step)\n",
            "I1212 08:40:32.225450 140053320693632 learning.py:507] global step 1226: loss = 2.4194 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1227: loss = 2.2816 (0.750 sec/step)\n",
            "I1212 08:40:33.304687 140053320693632 learning.py:507] global step 1227: loss = 2.2816 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 1228: loss = 2.7493 (0.590 sec/step)\n",
            "I1212 08:40:34.225098 140053320693632 learning.py:507] global step 1228: loss = 2.7493 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 1229: loss = 2.3610 (0.642 sec/step)\n",
            "I1212 08:40:35.005279 140053320693632 learning.py:507] global step 1229: loss = 2.3610 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1230: loss = 2.4617 (1.034 sec/step)\n",
            "I1212 08:40:36.202102 140053320693632 learning.py:507] global step 1230: loss = 2.4617 (1.034 sec/step)\n",
            "INFO:tensorflow:global step 1231: loss = 2.6936 (2.259 sec/step)\n",
            "I1212 08:40:38.729267 140053320693632 learning.py:507] global step 1231: loss = 2.6936 (2.259 sec/step)\n",
            "INFO:tensorflow:global step 1232: loss = 2.6923 (0.783 sec/step)\n",
            "I1212 08:40:39.696582 140053320693632 learning.py:507] global step 1232: loss = 2.6923 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 1233: loss = 2.1165 (0.610 sec/step)\n",
            "I1212 08:40:40.738099 140053320693632 learning.py:507] global step 1233: loss = 2.1165 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1234: loss = 2.2593 (0.614 sec/step)\n",
            "I1212 08:40:41.625374 140053320693632 learning.py:507] global step 1234: loss = 2.2593 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1235: loss = 2.1504 (2.077 sec/step)\n",
            "I1212 08:40:43.764790 140053320693632 learning.py:507] global step 1235: loss = 2.1504 (2.077 sec/step)\n",
            "INFO:tensorflow:global step 1236: loss = 3.2512 (0.644 sec/step)\n",
            "I1212 08:40:44.794979 140053320693632 learning.py:507] global step 1236: loss = 3.2512 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1237: loss = 2.0849 (0.557 sec/step)\n",
            "I1212 08:40:45.367921 140053320693632 learning.py:507] global step 1237: loss = 2.0849 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 1238: loss = 2.5251 (0.604 sec/step)\n",
            "I1212 08:40:45.974093 140053320693632 learning.py:507] global step 1238: loss = 2.5251 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 1239: loss = 2.4050 (1.241 sec/step)\n",
            "I1212 08:40:47.455442 140053320693632 learning.py:507] global step 1239: loss = 2.4050 (1.241 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1239.\n",
            "I1212 08:40:50.399833 140049614505728 supervisor.py:1050] Recording summary at step 1239.\n",
            "INFO:tensorflow:global step 1240: loss = 2.4733 (2.902 sec/step)\n",
            "I1212 08:40:50.602209 140053320693632 learning.py:507] global step 1240: loss = 2.4733 (2.902 sec/step)\n",
            "INFO:tensorflow:global step 1241: loss = 2.6473 (0.565 sec/step)\n",
            "I1212 08:40:51.538985 140053320693632 learning.py:507] global step 1241: loss = 2.6473 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 1242: loss = 3.2322 (0.632 sec/step)\n",
            "I1212 08:40:52.501814 140053320693632 learning.py:507] global step 1242: loss = 3.2322 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1243: loss = 2.4325 (0.720 sec/step)\n",
            "I1212 08:40:53.578556 140053320693632 learning.py:507] global step 1243: loss = 2.4325 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 1244: loss = 2.5477 (0.727 sec/step)\n",
            "I1212 08:40:54.945403 140053320693632 learning.py:507] global step 1244: loss = 2.5477 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1245: loss = 2.1778 (0.629 sec/step)\n",
            "I1212 08:40:55.663009 140053320693632 learning.py:507] global step 1245: loss = 2.1778 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1246: loss = 2.1061 (1.836 sec/step)\n",
            "I1212 08:40:57.501891 140053320693632 learning.py:507] global step 1246: loss = 2.1061 (1.836 sec/step)\n",
            "INFO:tensorflow:global step 1247: loss = 3.3106 (0.667 sec/step)\n",
            "I1212 08:40:58.376930 140053320693632 learning.py:507] global step 1247: loss = 3.3106 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1248: loss = 2.4385 (0.698 sec/step)\n",
            "I1212 08:40:59.542427 140053320693632 learning.py:507] global step 1248: loss = 2.4385 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1249: loss = 1.8418 (1.535 sec/step)\n",
            "I1212 08:41:01.253970 140053320693632 learning.py:507] global step 1249: loss = 1.8418 (1.535 sec/step)\n",
            "INFO:tensorflow:global step 1250: loss = 2.5862 (0.706 sec/step)\n",
            "I1212 08:41:02.280172 140053320693632 learning.py:507] global step 1250: loss = 2.5862 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1251: loss = 2.5127 (0.549 sec/step)\n",
            "I1212 08:41:02.882486 140053320693632 learning.py:507] global step 1251: loss = 2.5127 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 1252: loss = 2.7662 (1.776 sec/step)\n",
            "I1212 08:41:04.660752 140053320693632 learning.py:507] global step 1252: loss = 2.7662 (1.776 sec/step)\n",
            "INFO:tensorflow:global step 1253: loss = 2.2383 (0.611 sec/step)\n",
            "I1212 08:41:05.584227 140053320693632 learning.py:507] global step 1253: loss = 2.2383 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1254: loss = 2.2926 (1.384 sec/step)\n",
            "I1212 08:41:07.013618 140053320693632 learning.py:507] global step 1254: loss = 2.2926 (1.384 sec/step)\n",
            "INFO:tensorflow:global step 1255: loss = 2.3625 (0.596 sec/step)\n",
            "I1212 08:41:07.919077 140053320693632 learning.py:507] global step 1255: loss = 2.3625 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1256: loss = 2.6664 (0.720 sec/step)\n",
            "I1212 08:41:09.052649 140053320693632 learning.py:507] global step 1256: loss = 2.6664 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 1257: loss = 2.5226 (0.684 sec/step)\n",
            "I1212 08:41:09.928551 140053320693632 learning.py:507] global step 1257: loss = 2.5226 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1258: loss = 1.9909 (1.432 sec/step)\n",
            "I1212 08:41:11.394974 140053320693632 learning.py:507] global step 1258: loss = 1.9909 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 1259: loss = 2.9430 (0.544 sec/step)\n",
            "I1212 08:41:11.940660 140053320693632 learning.py:507] global step 1259: loss = 2.9430 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1260: loss = 2.5160 (1.668 sec/step)\n",
            "I1212 08:41:13.609841 140053320693632 learning.py:507] global step 1260: loss = 2.5160 (1.668 sec/step)\n",
            "INFO:tensorflow:global step 1261: loss = 2.7064 (0.681 sec/step)\n",
            "I1212 08:41:14.502086 140053320693632 learning.py:507] global step 1261: loss = 2.7064 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1262: loss = 2.8800 (0.565 sec/step)\n",
            "I1212 08:41:15.450104 140053320693632 learning.py:507] global step 1262: loss = 2.8800 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 1263: loss = 3.7070 (1.807 sec/step)\n",
            "I1212 08:41:17.536847 140053320693632 learning.py:507] global step 1263: loss = 3.7070 (1.807 sec/step)\n",
            "INFO:tensorflow:global step 1264: loss = 2.3724 (0.569 sec/step)\n",
            "I1212 08:41:18.315916 140053320693632 learning.py:507] global step 1264: loss = 2.3724 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1265: loss = 2.9083 (0.678 sec/step)\n",
            "I1212 08:41:19.415741 140053320693632 learning.py:507] global step 1265: loss = 2.9083 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1266: loss = 2.4438 (0.564 sec/step)\n",
            "I1212 08:41:20.486404 140053320693632 learning.py:507] global step 1266: loss = 2.4438 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 1267: loss = 1.7986 (1.054 sec/step)\n",
            "I1212 08:41:21.689326 140053320693632 learning.py:507] global step 1267: loss = 1.7986 (1.054 sec/step)\n",
            "INFO:tensorflow:global step 1268: loss = 2.2039 (0.647 sec/step)\n",
            "I1212 08:41:22.688863 140053320693632 learning.py:507] global step 1268: loss = 2.2039 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1269: loss = 2.7008 (1.260 sec/step)\n",
            "I1212 08:41:23.960521 140053320693632 learning.py:507] global step 1269: loss = 2.7008 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1270: loss = 2.2003 (0.564 sec/step)\n",
            "I1212 08:41:24.526074 140053320693632 learning.py:507] global step 1270: loss = 2.2003 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 1271: loss = 2.5409 (0.593 sec/step)\n",
            "I1212 08:41:25.121227 140053320693632 learning.py:507] global step 1271: loss = 2.5409 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 1272: loss = 2.8286 (2.312 sec/step)\n",
            "I1212 08:41:27.434915 140053320693632 learning.py:507] global step 1272: loss = 2.8286 (2.312 sec/step)\n",
            "INFO:tensorflow:global step 1273: loss = 2.4961 (0.615 sec/step)\n",
            "I1212 08:41:28.124140 140053320693632 learning.py:507] global step 1273: loss = 2.4961 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1274: loss = 2.1369 (1.448 sec/step)\n",
            "I1212 08:41:29.597712 140053320693632 learning.py:507] global step 1274: loss = 2.1369 (1.448 sec/step)\n",
            "INFO:tensorflow:global step 1275: loss = 2.5202 (0.727 sec/step)\n",
            "I1212 08:41:30.539342 140053320693632 learning.py:507] global step 1275: loss = 2.5202 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1276: loss = 2.3074 (0.674 sec/step)\n",
            "I1212 08:41:31.617948 140053320693632 learning.py:507] global step 1276: loss = 2.3074 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 1277: loss = 2.5825 (1.651 sec/step)\n",
            "I1212 08:41:33.296840 140053320693632 learning.py:507] global step 1277: loss = 2.5825 (1.651 sec/step)\n",
            "INFO:tensorflow:global step 1278: loss = 1.9196 (0.657 sec/step)\n",
            "I1212 08:41:34.184113 140053320693632 learning.py:507] global step 1278: loss = 1.9196 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1279: loss = 1.9730 (1.213 sec/step)\n",
            "I1212 08:41:35.476644 140053320693632 learning.py:507] global step 1279: loss = 1.9730 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1280: loss = 2.4802 (0.668 sec/step)\n",
            "I1212 08:41:36.355479 140053320693632 learning.py:507] global step 1280: loss = 2.4802 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1281: loss = 2.4101 (0.601 sec/step)\n",
            "I1212 08:41:37.135645 140053320693632 learning.py:507] global step 1281: loss = 2.4101 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1282: loss = 2.3174 (0.725 sec/step)\n",
            "I1212 08:41:38.291548 140053320693632 learning.py:507] global step 1282: loss = 2.3174 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1283: loss = 2.2492 (0.734 sec/step)\n",
            "I1212 08:41:39.390434 140053320693632 learning.py:507] global step 1283: loss = 2.2492 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1284: loss = 3.0667 (1.303 sec/step)\n",
            "I1212 08:41:40.706630 140053320693632 learning.py:507] global step 1284: loss = 3.0667 (1.303 sec/step)\n",
            "INFO:tensorflow:global step 1285: loss = 2.3554 (0.548 sec/step)\n",
            "I1212 08:41:41.515678 140053320693632 learning.py:507] global step 1285: loss = 2.3554 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 1286: loss = 2.7629 (0.727 sec/step)\n",
            "I1212 08:41:42.449999 140053320693632 learning.py:507] global step 1286: loss = 2.7629 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1287: loss = 2.1010 (1.422 sec/step)\n",
            "I1212 08:41:43.891850 140053320693632 learning.py:507] global step 1287: loss = 2.1010 (1.422 sec/step)\n",
            "INFO:tensorflow:global step 1288: loss = 2.6464 (0.756 sec/step)\n",
            "I1212 08:41:44.833101 140053320693632 learning.py:507] global step 1288: loss = 2.6464 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 1289: loss = 1.8686 (0.648 sec/step)\n",
            "I1212 08:41:45.748744 140053320693632 learning.py:507] global step 1289: loss = 1.8686 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1290: loss = 2.4331 (0.678 sec/step)\n",
            "I1212 08:41:46.909602 140053320693632 learning.py:507] global step 1290: loss = 2.4331 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1291: loss = 3.1749 (0.596 sec/step)\n",
            "I1212 08:41:47.873886 140053320693632 learning.py:507] global step 1291: loss = 3.1749 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1292: loss = 2.2074 (1.215 sec/step)\n",
            "I1212 08:41:49.246610 140053320693632 learning.py:507] global step 1292: loss = 2.2074 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1293: loss = 2.2464 (0.510 sec/step)\n",
            "I1212 08:41:50.009061 140053320693632 learning.py:507] global step 1293: loss = 2.2464 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 1294: loss = 2.7562 (0.623 sec/step)\n",
            "I1212 08:41:51.112496 140053320693632 learning.py:507] global step 1294: loss = 2.7562 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1295: loss = 2.0471 (0.664 sec/step)\n",
            "I1212 08:41:52.069588 140053320693632 learning.py:507] global step 1295: loss = 2.0471 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1296: loss = 2.4034 (1.276 sec/step)\n",
            "I1212 08:41:53.485988 140053320693632 learning.py:507] global step 1296: loss = 2.4034 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 1297: loss = 2.3327 (0.604 sec/step)\n",
            "I1212 08:41:54.253274 140053320693632 learning.py:507] global step 1297: loss = 2.3327 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 1298: loss = 2.2378 (1.060 sec/step)\n",
            "I1212 08:41:55.562373 140053320693632 learning.py:507] global step 1298: loss = 2.2378 (1.060 sec/step)\n",
            "INFO:tensorflow:global step 1299: loss = 2.3931 (0.598 sec/step)\n",
            "I1212 08:41:56.522388 140053320693632 learning.py:507] global step 1299: loss = 2.3931 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1300: loss = 2.9078 (0.703 sec/step)\n",
            "I1212 08:41:57.468222 140053320693632 learning.py:507] global step 1300: loss = 2.9078 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 1301: loss = 2.2223 (0.680 sec/step)\n",
            "I1212 08:41:58.606491 140053320693632 learning.py:507] global step 1301: loss = 2.2223 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1302: loss = 2.4607 (1.219 sec/step)\n",
            "I1212 08:41:59.852401 140053320693632 learning.py:507] global step 1302: loss = 2.4607 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1303: loss = 2.0215 (0.589 sec/step)\n",
            "I1212 08:42:00.579495 140053320693632 learning.py:507] global step 1303: loss = 2.0215 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 1304: loss = 2.2844 (0.806 sec/step)\n",
            "I1212 08:42:01.743131 140053320693632 learning.py:507] global step 1304: loss = 2.2844 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 1305: loss = 2.6760 (0.694 sec/step)\n",
            "I1212 08:42:02.727979 140053320693632 learning.py:507] global step 1305: loss = 2.6760 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1306: loss = 2.1372 (1.453 sec/step)\n",
            "I1212 08:42:04.186535 140053320693632 learning.py:507] global step 1306: loss = 2.1372 (1.453 sec/step)\n",
            "INFO:tensorflow:global step 1307: loss = 2.2411 (0.562 sec/step)\n",
            "I1212 08:42:04.913151 140053320693632 learning.py:507] global step 1307: loss = 2.2411 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 1308: loss = 3.8233 (1.449 sec/step)\n",
            "I1212 08:42:06.544374 140053320693632 learning.py:507] global step 1308: loss = 3.8233 (1.449 sec/step)\n",
            "INFO:tensorflow:global step 1309: loss = 2.6404 (1.125 sec/step)\n",
            "I1212 08:42:07.670529 140053320693632 learning.py:507] global step 1309: loss = 2.6404 (1.125 sec/step)\n",
            "INFO:tensorflow:global step 1310: loss = 2.5856 (0.652 sec/step)\n",
            "I1212 08:42:08.566998 140053320693632 learning.py:507] global step 1310: loss = 2.5856 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1311: loss = 2.7070 (1.389 sec/step)\n",
            "I1212 08:42:10.014223 140053320693632 learning.py:507] global step 1311: loss = 2.7070 (1.389 sec/step)\n",
            "INFO:tensorflow:global step 1312: loss = 2.2328 (0.580 sec/step)\n",
            "I1212 08:42:10.596309 140053320693632 learning.py:507] global step 1312: loss = 2.2328 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1313: loss = 2.3174 (2.474 sec/step)\n",
            "I1212 08:42:13.072038 140053320693632 learning.py:507] global step 1313: loss = 2.3174 (2.474 sec/step)\n",
            "INFO:tensorflow:global step 1314: loss = 2.5343 (0.594 sec/step)\n",
            "I1212 08:42:13.668465 140053320693632 learning.py:507] global step 1314: loss = 2.5343 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1315: loss = 2.7259 (0.939 sec/step)\n",
            "I1212 08:42:14.784435 140053320693632 learning.py:507] global step 1315: loss = 2.7259 (0.939 sec/step)\n",
            "INFO:tensorflow:global step 1316: loss = 1.9088 (1.490 sec/step)\n",
            "I1212 08:42:16.769574 140053320693632 learning.py:507] global step 1316: loss = 1.9088 (1.490 sec/step)\n",
            "INFO:tensorflow:global step 1317: loss = 1.8915 (0.637 sec/step)\n",
            "I1212 08:42:17.788971 140053320693632 learning.py:507] global step 1317: loss = 1.8915 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1318: loss = 2.5278 (2.582 sec/step)\n",
            "I1212 08:42:20.487123 140053320693632 learning.py:507] global step 1318: loss = 2.5278 (2.582 sec/step)\n",
            "INFO:tensorflow:global step 1319: loss = 2.2775 (0.598 sec/step)\n",
            "I1212 08:42:21.086879 140053320693632 learning.py:507] global step 1319: loss = 2.2775 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1320: loss = 2.3757 (1.545 sec/step)\n",
            "I1212 08:42:22.784119 140053320693632 learning.py:507] global step 1320: loss = 2.3757 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1321: loss = 2.2129 (0.595 sec/step)\n",
            "I1212 08:42:23.381205 140053320693632 learning.py:507] global step 1321: loss = 2.2129 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 1322: loss = 1.7328 (0.944 sec/step)\n",
            "I1212 08:42:24.643567 140053320693632 learning.py:507] global step 1322: loss = 1.7328 (0.944 sec/step)\n",
            "INFO:tensorflow:global step 1323: loss = 2.0177 (1.675 sec/step)\n",
            "I1212 08:42:26.542912 140053320693632 learning.py:507] global step 1323: loss = 2.0177 (1.675 sec/step)\n",
            "INFO:tensorflow:global step 1324: loss = 2.3814 (0.609 sec/step)\n",
            "I1212 08:42:27.154030 140053320693632 learning.py:507] global step 1324: loss = 2.3814 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1325: loss = 1.7142 (0.658 sec/step)\n",
            "I1212 08:42:28.009978 140053320693632 learning.py:507] global step 1325: loss = 1.7142 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1326: loss = 2.0539 (2.243 sec/step)\n",
            "I1212 08:42:30.255212 140053320693632 learning.py:507] global step 1326: loss = 2.0539 (2.243 sec/step)\n",
            "INFO:tensorflow:global step 1327: loss = 1.9084 (0.717 sec/step)\n",
            "I1212 08:42:30.994421 140053320693632 learning.py:507] global step 1327: loss = 1.9084 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1328: loss = 1.9438 (1.518 sec/step)\n",
            "I1212 08:42:32.513916 140053320693632 learning.py:507] global step 1328: loss = 1.9438 (1.518 sec/step)\n",
            "INFO:tensorflow:global step 1329: loss = 2.7923 (0.700 sec/step)\n",
            "I1212 08:42:33.431768 140053320693632 learning.py:507] global step 1329: loss = 2.7923 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1330: loss = 2.2731 (1.977 sec/step)\n",
            "I1212 08:42:35.540187 140053320693632 learning.py:507] global step 1330: loss = 2.2731 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 1331: loss = 2.1274 (0.609 sec/step)\n",
            "I1212 08:42:36.151269 140053320693632 learning.py:507] global step 1331: loss = 2.1274 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1332: loss = 2.3835 (1.787 sec/step)\n",
            "I1212 08:42:37.940485 140053320693632 learning.py:507] global step 1332: loss = 2.3835 (1.787 sec/step)\n",
            "INFO:tensorflow:global step 1333: loss = 2.6750 (0.698 sec/step)\n",
            "I1212 08:42:38.640551 140053320693632 learning.py:507] global step 1333: loss = 2.6750 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1334: loss = 2.6370 (1.337 sec/step)\n",
            "I1212 08:42:40.228475 140053320693632 learning.py:507] global step 1334: loss = 2.6370 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 1335: loss = 2.6095 (0.706 sec/step)\n",
            "I1212 08:42:41.145091 140053320693632 learning.py:507] global step 1335: loss = 2.6095 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1336: loss = 2.4090 (1.362 sec/step)\n",
            "I1212 08:42:42.645005 140053320693632 learning.py:507] global step 1336: loss = 2.4090 (1.362 sec/step)\n",
            "INFO:tensorflow:global step 1337: loss = 2.3633 (0.639 sec/step)\n",
            "I1212 08:42:43.580920 140053320693632 learning.py:507] global step 1337: loss = 2.3633 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1338: loss = 2.2445 (0.741 sec/step)\n",
            "I1212 08:42:44.664043 140053320693632 learning.py:507] global step 1338: loss = 2.2445 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 1339: loss = 2.1625 (1.094 sec/step)\n",
            "I1212 08:42:45.903594 140053320693632 learning.py:507] global step 1339: loss = 2.1625 (1.094 sec/step)\n",
            "INFO:tensorflow:global step 1340: loss = 2.6754 (0.558 sec/step)\n",
            "I1212 08:42:46.801019 140053320693632 learning.py:507] global step 1340: loss = 2.6754 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1341: loss = 2.3696 (2.173 sec/step)\n",
            "I1212 08:42:49.122018 140053320693632 learning.py:507] global step 1341: loss = 2.3696 (2.173 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1341.\n",
            "I1212 08:42:50.067920 140049614505728 supervisor.py:1050] Recording summary at step 1341.\n",
            "INFO:tensorflow:global step 1342: loss = 2.0994 (1.151 sec/step)\n",
            "I1212 08:42:50.275881 140053320693632 learning.py:507] global step 1342: loss = 2.0994 (1.151 sec/step)\n",
            "INFO:tensorflow:global step 1343: loss = 2.9510 (0.492 sec/step)\n",
            "I1212 08:42:51.015343 140053320693632 learning.py:507] global step 1343: loss = 2.9510 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 1344: loss = 2.6427 (1.383 sec/step)\n",
            "I1212 08:42:52.494902 140053320693632 learning.py:507] global step 1344: loss = 2.6427 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 1345: loss = 2.1387 (0.544 sec/step)\n",
            "I1212 08:42:53.224814 140053320693632 learning.py:507] global step 1345: loss = 2.1387 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1346: loss = 3.2117 (1.251 sec/step)\n",
            "I1212 08:42:54.677720 140053320693632 learning.py:507] global step 1346: loss = 3.2117 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1347: loss = 3.5024 (0.610 sec/step)\n",
            "I1212 08:42:55.636453 140053320693632 learning.py:507] global step 1347: loss = 3.5024 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1348: loss = 2.4595 (1.287 sec/step)\n",
            "I1212 08:42:56.997559 140053320693632 learning.py:507] global step 1348: loss = 2.4595 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 1349: loss = 3.1514 (0.615 sec/step)\n",
            "I1212 08:42:57.614445 140053320693632 learning.py:507] global step 1349: loss = 3.1514 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1350: loss = 2.8714 (0.856 sec/step)\n",
            "I1212 08:42:58.679164 140053320693632 learning.py:507] global step 1350: loss = 2.8714 (0.856 sec/step)\n",
            "INFO:tensorflow:global step 1351: loss = 2.6404 (1.789 sec/step)\n",
            "I1212 08:43:00.622866 140053320693632 learning.py:507] global step 1351: loss = 2.6404 (1.789 sec/step)\n",
            "INFO:tensorflow:global step 1352: loss = 2.3061 (0.561 sec/step)\n",
            "I1212 08:43:01.340541 140053320693632 learning.py:507] global step 1352: loss = 2.3061 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 1353: loss = 2.7102 (0.905 sec/step)\n",
            "I1212 08:43:02.748282 140053320693632 learning.py:507] global step 1353: loss = 2.7102 (0.905 sec/step)\n",
            "INFO:tensorflow:global step 1354: loss = 1.8374 (0.683 sec/step)\n",
            "I1212 08:43:03.795736 140053320693632 learning.py:507] global step 1354: loss = 1.8374 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1355: loss = 2.2853 (0.691 sec/step)\n",
            "I1212 08:43:04.666070 140053320693632 learning.py:507] global step 1355: loss = 2.2853 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1356: loss = 2.9584 (0.562 sec/step)\n",
            "I1212 08:43:05.348843 140053320693632 learning.py:507] global step 1356: loss = 2.9584 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 1357: loss = 2.5870 (0.622 sec/step)\n",
            "I1212 08:43:05.972995 140053320693632 learning.py:507] global step 1357: loss = 2.5870 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1358: loss = 1.7171 (2.526 sec/step)\n",
            "I1212 08:43:08.500661 140053320693632 learning.py:507] global step 1358: loss = 1.7171 (2.526 sec/step)\n",
            "INFO:tensorflow:global step 1359: loss = 2.1424 (0.731 sec/step)\n",
            "I1212 08:43:09.268987 140053320693632 learning.py:507] global step 1359: loss = 2.1424 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 1360: loss = 3.2801 (1.395 sec/step)\n",
            "I1212 08:43:10.667069 140053320693632 learning.py:507] global step 1360: loss = 3.2801 (1.395 sec/step)\n",
            "INFO:tensorflow:global step 1361: loss = 2.4016 (0.694 sec/step)\n",
            "I1212 08:43:11.591568 140053320693632 learning.py:507] global step 1361: loss = 2.4016 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1362: loss = 2.2886 (1.735 sec/step)\n",
            "I1212 08:43:13.395662 140053320693632 learning.py:507] global step 1362: loss = 2.2886 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 1363: loss = 2.3691 (0.580 sec/step)\n",
            "I1212 08:43:14.394449 140053320693632 learning.py:507] global step 1363: loss = 2.3691 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1364: loss = 2.3711 (0.729 sec/step)\n",
            "I1212 08:43:15.395561 140053320693632 learning.py:507] global step 1364: loss = 2.3711 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1365: loss = 2.6859 (1.211 sec/step)\n",
            "I1212 08:43:16.635957 140053320693632 learning.py:507] global step 1365: loss = 2.6859 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1366: loss = 2.0870 (0.488 sec/step)\n",
            "I1212 08:43:17.280128 140053320693632 learning.py:507] global step 1366: loss = 2.0870 (0.488 sec/step)\n",
            "INFO:tensorflow:global step 1367: loss = 1.9695 (1.229 sec/step)\n",
            "I1212 08:43:18.716253 140053320693632 learning.py:507] global step 1367: loss = 1.9695 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 1368: loss = 2.4881 (0.650 sec/step)\n",
            "I1212 08:43:19.550476 140053320693632 learning.py:507] global step 1368: loss = 2.4881 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1369: loss = 2.6116 (1.199 sec/step)\n",
            "I1212 08:43:20.960303 140053320693632 learning.py:507] global step 1369: loss = 2.6116 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1370: loss = 2.2318 (0.546 sec/step)\n",
            "I1212 08:43:21.508218 140053320693632 learning.py:507] global step 1370: loss = 2.2318 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 1371: loss = 2.2060 (0.863 sec/step)\n",
            "I1212 08:43:22.538779 140053320693632 learning.py:507] global step 1371: loss = 2.2060 (0.863 sec/step)\n",
            "INFO:tensorflow:global step 1372: loss = 2.2413 (1.467 sec/step)\n",
            "I1212 08:43:24.322533 140053320693632 learning.py:507] global step 1372: loss = 2.2413 (1.467 sec/step)\n",
            "INFO:tensorflow:global step 1373: loss = 2.1697 (0.683 sec/step)\n",
            "I1212 08:43:25.141243 140053320693632 learning.py:507] global step 1373: loss = 2.1697 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1374: loss = 1.6294 (1.278 sec/step)\n",
            "I1212 08:43:26.544423 140053320693632 learning.py:507] global step 1374: loss = 1.6294 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 1375: loss = 1.8056 (0.649 sec/step)\n",
            "I1212 08:43:27.581849 140053320693632 learning.py:507] global step 1375: loss = 1.8056 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1376: loss = 2.4596 (0.650 sec/step)\n",
            "I1212 08:43:28.563115 140053320693632 learning.py:507] global step 1376: loss = 2.4596 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1377: loss = 2.3582 (1.242 sec/step)\n",
            "I1212 08:43:29.851947 140053320693632 learning.py:507] global step 1377: loss = 2.3582 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1378: loss = 2.3441 (0.685 sec/step)\n",
            "I1212 08:43:30.780845 140053320693632 learning.py:507] global step 1378: loss = 2.3441 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 1379: loss = 2.6028 (0.576 sec/step)\n",
            "I1212 08:43:31.496533 140053320693632 learning.py:507] global step 1379: loss = 2.6028 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1380: loss = 1.9632 (0.687 sec/step)\n",
            "I1212 08:43:32.582910 140053320693632 learning.py:507] global step 1380: loss = 1.9632 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 1381: loss = 2.7406 (0.717 sec/step)\n",
            "I1212 08:43:33.762477 140053320693632 learning.py:507] global step 1381: loss = 2.7406 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1382: loss = 1.7098 (1.394 sec/step)\n",
            "I1212 08:43:35.235381 140053320693632 learning.py:507] global step 1382: loss = 1.7098 (1.394 sec/step)\n",
            "INFO:tensorflow:global step 1383: loss = 2.1323 (0.693 sec/step)\n",
            "I1212 08:43:36.119807 140053320693632 learning.py:507] global step 1383: loss = 2.1323 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1384: loss = 2.8508 (0.678 sec/step)\n",
            "I1212 08:43:37.184873 140053320693632 learning.py:507] global step 1384: loss = 2.8508 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1385: loss = 2.0109 (0.699 sec/step)\n",
            "I1212 08:43:37.888399 140053320693632 learning.py:507] global step 1385: loss = 2.0109 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1386: loss = 2.6812 (1.316 sec/step)\n",
            "I1212 08:43:39.344706 140053320693632 learning.py:507] global step 1386: loss = 2.6812 (1.316 sec/step)\n",
            "INFO:tensorflow:global step 1387: loss = 2.4262 (0.729 sec/step)\n",
            "I1212 08:43:40.354321 140053320693632 learning.py:507] global step 1387: loss = 2.4262 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1388: loss = 2.4315 (1.266 sec/step)\n",
            "I1212 08:43:41.691795 140053320693632 learning.py:507] global step 1388: loss = 2.4315 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 1389: loss = 2.4139 (0.661 sec/step)\n",
            "I1212 08:43:42.567002 140053320693632 learning.py:507] global step 1389: loss = 2.4139 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1390: loss = 2.6255 (1.267 sec/step)\n",
            "I1212 08:43:43.971413 140053320693632 learning.py:507] global step 1390: loss = 2.6255 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 1391: loss = 2.7707 (0.507 sec/step)\n",
            "I1212 08:43:44.480117 140053320693632 learning.py:507] global step 1391: loss = 2.7707 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 1392: loss = 2.1737 (0.733 sec/step)\n",
            "I1212 08:43:45.249571 140053320693632 learning.py:507] global step 1392: loss = 2.1737 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 1393: loss = 2.2221 (1.679 sec/step)\n",
            "I1212 08:43:47.174390 140053320693632 learning.py:507] global step 1393: loss = 2.2221 (1.679 sec/step)\n",
            "INFO:tensorflow:global step 1394: loss = 2.4030 (0.511 sec/step)\n",
            "I1212 08:43:48.086562 140053320693632 learning.py:507] global step 1394: loss = 2.4030 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 1395: loss = 2.3918 (1.285 sec/step)\n",
            "I1212 08:43:49.467808 140053320693632 learning.py:507] global step 1395: loss = 2.3918 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 1396: loss = 1.9531 (0.721 sec/step)\n",
            "I1212 08:43:50.378196 140053320693632 learning.py:507] global step 1396: loss = 1.9531 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1397: loss = 3.2043 (0.716 sec/step)\n",
            "I1212 08:43:51.437005 140053320693632 learning.py:507] global step 1397: loss = 3.2043 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1398: loss = 2.2706 (1.391 sec/step)\n",
            "I1212 08:43:52.855066 140053320693632 learning.py:507] global step 1398: loss = 2.2706 (1.391 sec/step)\n",
            "INFO:tensorflow:global step 1399: loss = 1.8807 (0.548 sec/step)\n",
            "I1212 08:43:53.404878 140053320693632 learning.py:507] global step 1399: loss = 1.8807 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 1400: loss = 1.7178 (1.792 sec/step)\n",
            "I1212 08:43:55.198886 140053320693632 learning.py:507] global step 1400: loss = 1.7178 (1.792 sec/step)\n",
            "INFO:tensorflow:global step 1401: loss = 2.3792 (0.673 sec/step)\n",
            "I1212 08:43:56.202239 140053320693632 learning.py:507] global step 1401: loss = 2.3792 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1402: loss = 2.1119 (0.665 sec/step)\n",
            "I1212 08:43:57.178944 140053320693632 learning.py:507] global step 1402: loss = 2.1119 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1403: loss = 2.7102 (0.913 sec/step)\n",
            "I1212 08:43:58.252565 140053320693632 learning.py:507] global step 1403: loss = 2.7102 (0.913 sec/step)\n",
            "INFO:tensorflow:global step 1404: loss = 2.3317 (1.307 sec/step)\n",
            "I1212 08:43:59.611522 140053320693632 learning.py:507] global step 1404: loss = 2.3317 (1.307 sec/step)\n",
            "INFO:tensorflow:global step 1405: loss = 2.7640 (0.619 sec/step)\n",
            "I1212 08:44:00.477591 140053320693632 learning.py:507] global step 1405: loss = 2.7640 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1406: loss = 2.1361 (1.329 sec/step)\n",
            "I1212 08:44:01.936297 140053320693632 learning.py:507] global step 1406: loss = 2.1361 (1.329 sec/step)\n",
            "INFO:tensorflow:global step 1407: loss = 2.0448 (0.673 sec/step)\n",
            "I1212 08:44:02.898332 140053320693632 learning.py:507] global step 1407: loss = 2.0448 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1408: loss = 2.3593 (0.697 sec/step)\n",
            "I1212 08:44:04.002728 140053320693632 learning.py:507] global step 1408: loss = 2.3593 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1409: loss = 2.6563 (0.687 sec/step)\n",
            "I1212 08:44:04.800595 140053320693632 learning.py:507] global step 1409: loss = 2.6563 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 1410: loss = 1.9767 (0.751 sec/step)\n",
            "I1212 08:44:06.001000 140053320693632 learning.py:507] global step 1410: loss = 1.9767 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 1411: loss = 2.4682 (0.628 sec/step)\n",
            "I1212 08:44:06.760176 140053320693632 learning.py:507] global step 1411: loss = 2.4682 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1412: loss = 2.7361 (1.628 sec/step)\n",
            "I1212 08:44:08.399113 140053320693632 learning.py:507] global step 1412: loss = 2.7361 (1.628 sec/step)\n",
            "INFO:tensorflow:global step 1413: loss = 2.5695 (0.597 sec/step)\n",
            "I1212 08:44:09.091093 140053320693632 learning.py:507] global step 1413: loss = 2.5695 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1414: loss = 2.9697 (0.656 sec/step)\n",
            "I1212 08:44:09.748975 140053320693632 learning.py:507] global step 1414: loss = 2.9697 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1415: loss = 2.0408 (2.493 sec/step)\n",
            "I1212 08:44:12.244226 140053320693632 learning.py:507] global step 1415: loss = 2.0408 (2.493 sec/step)\n",
            "INFO:tensorflow:global step 1416: loss = 2.1390 (1.213 sec/step)\n",
            "I1212 08:44:13.458757 140053320693632 learning.py:507] global step 1416: loss = 2.1390 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1417: loss = 2.5192 (0.702 sec/step)\n",
            "I1212 08:44:14.166914 140053320693632 learning.py:507] global step 1417: loss = 2.5192 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 1418: loss = 2.0223 (0.647 sec/step)\n",
            "I1212 08:44:15.230007 140053320693632 learning.py:507] global step 1418: loss = 2.0223 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1419: loss = 2.2456 (1.482 sec/step)\n",
            "I1212 08:44:16.811982 140053320693632 learning.py:507] global step 1419: loss = 2.2456 (1.482 sec/step)\n",
            "INFO:tensorflow:global step 1420: loss = 2.6356 (0.646 sec/step)\n",
            "I1212 08:44:17.685528 140053320693632 learning.py:507] global step 1420: loss = 2.6356 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1421: loss = 2.4009 (0.644 sec/step)\n",
            "I1212 08:44:18.576926 140053320693632 learning.py:507] global step 1421: loss = 2.4009 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1422: loss = 2.9225 (1.546 sec/step)\n",
            "I1212 08:44:20.148621 140053320693632 learning.py:507] global step 1422: loss = 2.9225 (1.546 sec/step)\n",
            "INFO:tensorflow:global step 1423: loss = 2.0992 (0.640 sec/step)\n",
            "I1212 08:44:20.934007 140053320693632 learning.py:507] global step 1423: loss = 2.0992 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1424: loss = 2.4034 (1.384 sec/step)\n",
            "I1212 08:44:22.428986 140053320693632 learning.py:507] global step 1424: loss = 2.4034 (1.384 sec/step)\n",
            "INFO:tensorflow:global step 1425: loss = 2.3508 (0.503 sec/step)\n",
            "I1212 08:44:22.934139 140053320693632 learning.py:507] global step 1425: loss = 2.3508 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 1426: loss = 2.4997 (1.802 sec/step)\n",
            "I1212 08:44:24.737577 140053320693632 learning.py:507] global step 1426: loss = 2.4997 (1.802 sec/step)\n",
            "INFO:tensorflow:global step 1427: loss = 3.3526 (0.700 sec/step)\n",
            "I1212 08:44:25.566781 140053320693632 learning.py:507] global step 1427: loss = 3.3526 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1428: loss = 2.6128 (0.567 sec/step)\n",
            "I1212 08:44:26.331671 140053320693632 learning.py:507] global step 1428: loss = 2.6128 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 1429: loss = 2.4178 (0.518 sec/step)\n",
            "I1212 08:44:26.851681 140053320693632 learning.py:507] global step 1429: loss = 2.4178 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 1430: loss = 2.2167 (1.737 sec/step)\n",
            "I1212 08:44:28.929864 140053320693632 learning.py:507] global step 1430: loss = 2.2167 (1.737 sec/step)\n",
            "INFO:tensorflow:global step 1431: loss = 2.2992 (0.881 sec/step)\n",
            "I1212 08:44:30.300702 140053320693632 learning.py:507] global step 1431: loss = 2.2992 (0.881 sec/step)\n",
            "INFO:tensorflow:global step 1432: loss = 2.5974 (0.728 sec/step)\n",
            "I1212 08:44:31.100938 140053320693632 learning.py:507] global step 1432: loss = 2.5974 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 1433: loss = 2.7751 (1.171 sec/step)\n",
            "I1212 08:44:32.574016 140053320693632 learning.py:507] global step 1433: loss = 2.7751 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1434: loss = 2.6880 (1.434 sec/step)\n",
            "I1212 08:44:34.373892 140053320693632 learning.py:507] global step 1434: loss = 2.6880 (1.434 sec/step)\n",
            "INFO:tensorflow:global step 1435: loss = 2.3648 (0.802 sec/step)\n",
            "I1212 08:44:35.557506 140053320693632 learning.py:507] global step 1435: loss = 2.3648 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 1436: loss = 2.6136 (0.638 sec/step)\n",
            "I1212 08:44:36.523782 140053320693632 learning.py:507] global step 1436: loss = 2.6136 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1437: loss = 2.7007 (0.577 sec/step)\n",
            "I1212 08:44:37.348230 140053320693632 learning.py:507] global step 1437: loss = 2.7007 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 1438: loss = 1.9469 (1.745 sec/step)\n",
            "I1212 08:44:39.100491 140053320693632 learning.py:507] global step 1438: loss = 1.9469 (1.745 sec/step)\n",
            "INFO:tensorflow:global step 1439: loss = 2.0254 (0.585 sec/step)\n",
            "I1212 08:44:39.747502 140053320693632 learning.py:507] global step 1439: loss = 2.0254 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1440: loss = 2.6632 (1.711 sec/step)\n",
            "I1212 08:44:41.460250 140053320693632 learning.py:507] global step 1440: loss = 2.6632 (1.711 sec/step)\n",
            "INFO:tensorflow:global step 1441: loss = 2.1369 (1.233 sec/step)\n",
            "I1212 08:44:42.695162 140053320693632 learning.py:507] global step 1441: loss = 2.1369 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1442: loss = 3.5165 (0.654 sec/step)\n",
            "I1212 08:44:43.679557 140053320693632 learning.py:507] global step 1442: loss = 3.5165 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1443: loss = 2.8073 (1.530 sec/step)\n",
            "I1212 08:44:45.235450 140053320693632 learning.py:507] global step 1443: loss = 2.8073 (1.530 sec/step)\n",
            "INFO:tensorflow:global step 1444: loss = 2.3324 (0.761 sec/step)\n",
            "I1212 08:44:46.295562 140053320693632 learning.py:507] global step 1444: loss = 2.3324 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 1445: loss = 2.6075 (0.676 sec/step)\n",
            "I1212 08:44:47.051485 140053320693632 learning.py:507] global step 1445: loss = 2.6075 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1446: loss = 2.2609 (0.892 sec/step)\n",
            "I1212 08:44:48.830065 140053320693632 learning.py:507] global step 1446: loss = 2.2609 (0.892 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1446.\n",
            "I1212 08:44:50.917088 140049614505728 supervisor.py:1050] Recording summary at step 1446.\n",
            "INFO:tensorflow:global step 1447: loss = 2.6653 (2.265 sec/step)\n",
            "I1212 08:44:51.229960 140053320693632 learning.py:507] global step 1447: loss = 2.6653 (2.265 sec/step)\n",
            "INFO:tensorflow:global step 1448: loss = 2.4510 (0.634 sec/step)\n",
            "I1212 08:44:51.897571 140053320693632 learning.py:507] global step 1448: loss = 2.4510 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1449: loss = 2.3382 (1.478 sec/step)\n",
            "I1212 08:44:53.394814 140053320693632 learning.py:507] global step 1449: loss = 2.3382 (1.478 sec/step)\n",
            "INFO:tensorflow:global step 1450: loss = 2.3251 (0.616 sec/step)\n",
            "I1212 08:44:54.192444 140053320693632 learning.py:507] global step 1450: loss = 2.3251 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1451: loss = 3.0317 (0.548 sec/step)\n",
            "I1212 08:44:54.942892 140053320693632 learning.py:507] global step 1451: loss = 3.0317 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 1452: loss = 3.0888 (0.945 sec/step)\n",
            "I1212 08:44:56.064745 140053320693632 learning.py:507] global step 1452: loss = 3.0888 (0.945 sec/step)\n",
            "INFO:tensorflow:global step 1453: loss = 2.6119 (2.042 sec/step)\n",
            "I1212 08:44:58.268281 140053320693632 learning.py:507] global step 1453: loss = 2.6119 (2.042 sec/step)\n",
            "INFO:tensorflow:global step 1454: loss = 2.3493 (0.612 sec/step)\n",
            "I1212 08:44:59.094585 140053320693632 learning.py:507] global step 1454: loss = 2.3493 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1455: loss = 1.9421 (1.960 sec/step)\n",
            "I1212 08:45:01.200945 140053320693632 learning.py:507] global step 1455: loss = 1.9421 (1.960 sec/step)\n",
            "INFO:tensorflow:global step 1456: loss = 2.4061 (0.573 sec/step)\n",
            "I1212 08:45:01.776206 140053320693632 learning.py:507] global step 1456: loss = 2.4061 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 1457: loss = 3.3625 (1.727 sec/step)\n",
            "I1212 08:45:03.504969 140053320693632 learning.py:507] global step 1457: loss = 3.3625 (1.727 sec/step)\n",
            "INFO:tensorflow:global step 1458: loss = 2.5183 (0.666 sec/step)\n",
            "I1212 08:45:04.387938 140053320693632 learning.py:507] global step 1458: loss = 2.5183 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1459: loss = 2.4095 (0.710 sec/step)\n",
            "I1212 08:45:05.586843 140053320693632 learning.py:507] global step 1459: loss = 2.4095 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 1460: loss = 2.3238 (0.788 sec/step)\n",
            "I1212 08:45:06.675829 140053320693632 learning.py:507] global step 1460: loss = 2.3238 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 1461: loss = 2.5063 (0.554 sec/step)\n",
            "I1212 08:45:07.253760 140053320693632 learning.py:507] global step 1461: loss = 2.5063 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 1462: loss = 2.6933 (1.003 sec/step)\n",
            "I1212 08:45:08.572606 140053320693632 learning.py:507] global step 1462: loss = 2.6933 (1.003 sec/step)\n",
            "INFO:tensorflow:global step 1463: loss = 2.1186 (1.175 sec/step)\n",
            "I1212 08:45:10.157623 140053320693632 learning.py:507] global step 1463: loss = 2.1186 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1464: loss = 2.2444 (0.631 sec/step)\n",
            "I1212 08:45:11.047945 140053320693632 learning.py:507] global step 1464: loss = 2.2444 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1465: loss = 1.9756 (0.510 sec/step)\n",
            "I1212 08:45:11.719890 140053320693632 learning.py:507] global step 1465: loss = 1.9756 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 1466: loss = 2.2245 (0.950 sec/step)\n",
            "I1212 08:45:12.867201 140053320693632 learning.py:507] global step 1466: loss = 2.2245 (0.950 sec/step)\n",
            "INFO:tensorflow:global step 1467: loss = 1.7524 (2.458 sec/step)\n",
            "I1212 08:45:15.395509 140053320693632 learning.py:507] global step 1467: loss = 1.7524 (2.458 sec/step)\n",
            "INFO:tensorflow:global step 1468: loss = 2.4083 (0.647 sec/step)\n",
            "I1212 08:45:16.099923 140053320693632 learning.py:507] global step 1468: loss = 2.4083 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1469: loss = 1.9772 (1.944 sec/step)\n",
            "I1212 08:45:18.141571 140053320693632 learning.py:507] global step 1469: loss = 1.9772 (1.944 sec/step)\n",
            "INFO:tensorflow:global step 1470: loss = 2.7953 (0.528 sec/step)\n",
            "I1212 08:45:18.670974 140053320693632 learning.py:507] global step 1470: loss = 2.7953 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 1471: loss = 2.3481 (1.029 sec/step)\n",
            "I1212 08:45:20.051048 140053320693632 learning.py:507] global step 1471: loss = 2.3481 (1.029 sec/step)\n",
            "INFO:tensorflow:global step 1472: loss = 3.0160 (1.665 sec/step)\n",
            "I1212 08:45:22.019259 140053320693632 learning.py:507] global step 1472: loss = 3.0160 (1.665 sec/step)\n",
            "INFO:tensorflow:global step 1473: loss = 2.1611 (0.637 sec/step)\n",
            "I1212 08:45:22.991273 140053320693632 learning.py:507] global step 1473: loss = 2.1611 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1474: loss = 2.3725 (0.596 sec/step)\n",
            "I1212 08:45:23.981313 140053320693632 learning.py:507] global step 1474: loss = 2.3725 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1475: loss = 2.1050 (0.619 sec/step)\n",
            "I1212 08:45:24.942983 140053320693632 learning.py:507] global step 1475: loss = 2.1050 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1476: loss = 2.9157 (2.030 sec/step)\n",
            "I1212 08:45:27.237384 140053320693632 learning.py:507] global step 1476: loss = 2.9157 (2.030 sec/step)\n",
            "INFO:tensorflow:global step 1477: loss = 2.7646 (0.706 sec/step)\n",
            "I1212 08:45:27.945013 140053320693632 learning.py:507] global step 1477: loss = 2.7646 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1478: loss = 2.0957 (0.684 sec/step)\n",
            "I1212 08:45:28.775518 140053320693632 learning.py:507] global step 1478: loss = 2.0957 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1479: loss = 3.0193 (2.354 sec/step)\n",
            "I1212 08:45:31.281567 140053320693632 learning.py:507] global step 1479: loss = 3.0193 (2.354 sec/step)\n",
            "INFO:tensorflow:global step 1480: loss = 2.8093 (0.610 sec/step)\n",
            "I1212 08:45:32.186270 140053320693632 learning.py:507] global step 1480: loss = 2.8093 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1481: loss = 2.2782 (1.167 sec/step)\n",
            "I1212 08:45:33.426216 140053320693632 learning.py:507] global step 1481: loss = 2.2782 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 1482: loss = 2.5021 (0.692 sec/step)\n",
            "I1212 08:45:34.198737 140053320693632 learning.py:507] global step 1482: loss = 2.5021 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1483: loss = 2.3877 (1.413 sec/step)\n",
            "I1212 08:45:35.684950 140053320693632 learning.py:507] global step 1483: loss = 2.3877 (1.413 sec/step)\n",
            "INFO:tensorflow:global step 1484: loss = 2.3843 (0.706 sec/step)\n",
            "I1212 08:45:36.412888 140053320693632 learning.py:507] global step 1484: loss = 2.3843 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1485: loss = 2.6030 (0.662 sec/step)\n",
            "I1212 08:45:37.749928 140053320693632 learning.py:507] global step 1485: loss = 2.6030 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1486: loss = 2.9632 (0.479 sec/step)\n",
            "I1212 08:45:38.308743 140053320693632 learning.py:507] global step 1486: loss = 2.9632 (0.479 sec/step)\n",
            "INFO:tensorflow:global step 1487: loss = 2.2900 (1.619 sec/step)\n",
            "I1212 08:45:39.929750 140053320693632 learning.py:507] global step 1487: loss = 2.2900 (1.619 sec/step)\n",
            "INFO:tensorflow:global step 1488: loss = 2.9120 (0.576 sec/step)\n",
            "I1212 08:45:40.781748 140053320693632 learning.py:507] global step 1488: loss = 2.9120 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1489: loss = 2.0211 (0.636 sec/step)\n",
            "I1212 08:45:41.836101 140053320693632 learning.py:507] global step 1489: loss = 2.0211 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1490: loss = 2.8362 (0.657 sec/step)\n",
            "I1212 08:45:42.917944 140053320693632 learning.py:507] global step 1490: loss = 2.8362 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1491: loss = 1.8295 (0.640 sec/step)\n",
            "I1212 08:45:43.688298 140053320693632 learning.py:507] global step 1491: loss = 1.8295 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1492: loss = 2.9391 (1.557 sec/step)\n",
            "I1212 08:45:45.264524 140053320693632 learning.py:507] global step 1492: loss = 2.9391 (1.557 sec/step)\n",
            "INFO:tensorflow:global step 1493: loss = 2.0153 (0.578 sec/step)\n",
            "I1212 08:45:45.964448 140053320693632 learning.py:507] global step 1493: loss = 2.0153 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 1494: loss = 2.3102 (1.222 sec/step)\n",
            "I1212 08:45:47.616708 140053320693632 learning.py:507] global step 1494: loss = 2.3102 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1495: loss = 1.9990 (0.488 sec/step)\n",
            "I1212 08:45:48.376555 140053320693632 learning.py:507] global step 1495: loss = 1.9990 (0.488 sec/step)\n",
            "INFO:tensorflow:global step 1496: loss = 2.2807 (0.691 sec/step)\n",
            "I1212 08:45:49.575463 140053320693632 learning.py:507] global step 1496: loss = 2.2807 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1497: loss = 2.8519 (0.625 sec/step)\n",
            "I1212 08:45:50.226512 140053320693632 learning.py:507] global step 1497: loss = 2.8519 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1498: loss = 2.0062 (2.334 sec/step)\n",
            "I1212 08:45:52.562294 140053320693632 learning.py:507] global step 1498: loss = 2.0062 (2.334 sec/step)\n",
            "INFO:tensorflow:global step 1499: loss = 2.0599 (0.686 sec/step)\n",
            "I1212 08:45:53.472975 140053320693632 learning.py:507] global step 1499: loss = 2.0599 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1500: loss = 2.0642 (0.619 sec/step)\n",
            "I1212 08:45:54.344960 140053320693632 learning.py:507] global step 1500: loss = 2.0642 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1501: loss = 2.0895 (1.908 sec/step)\n",
            "I1212 08:45:56.254436 140053320693632 learning.py:507] global step 1501: loss = 2.0895 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 1502: loss = 2.3360 (0.641 sec/step)\n",
            "I1212 08:45:57.058541 140053320693632 learning.py:507] global step 1502: loss = 2.3360 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1503: loss = 2.0486 (1.835 sec/step)\n",
            "I1212 08:45:58.936177 140053320693632 learning.py:507] global step 1503: loss = 2.0486 (1.835 sec/step)\n",
            "INFO:tensorflow:global step 1504: loss = 2.4326 (0.558 sec/step)\n",
            "I1212 08:45:59.780744 140053320693632 learning.py:507] global step 1504: loss = 2.4326 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1505: loss = 3.1021 (1.583 sec/step)\n",
            "I1212 08:46:01.501064 140053320693632 learning.py:507] global step 1505: loss = 3.1021 (1.583 sec/step)\n",
            "INFO:tensorflow:global step 1506: loss = 1.9687 (0.557 sec/step)\n",
            "I1212 08:46:02.059926 140053320693632 learning.py:507] global step 1506: loss = 1.9687 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 1507: loss = 2.1050 (1.323 sec/step)\n",
            "I1212 08:46:03.530793 140053320693632 learning.py:507] global step 1507: loss = 2.1050 (1.323 sec/step)\n",
            "INFO:tensorflow:global step 1508: loss = 1.8600 (1.432 sec/step)\n",
            "I1212 08:46:05.209466 140053320693632 learning.py:507] global step 1508: loss = 1.8600 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 1509: loss = 2.5383 (0.660 sec/step)\n",
            "I1212 08:46:06.026522 140053320693632 learning.py:507] global step 1509: loss = 2.5383 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1510: loss = 2.5684 (1.909 sec/step)\n",
            "I1212 08:46:08.099023 140053320693632 learning.py:507] global step 1510: loss = 2.5684 (1.909 sec/step)\n",
            "INFO:tensorflow:global step 1511: loss = 1.7727 (0.559 sec/step)\n",
            "I1212 08:46:08.659427 140053320693632 learning.py:507] global step 1511: loss = 1.7727 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1512: loss = 2.1338 (1.767 sec/step)\n",
            "I1212 08:46:10.428166 140053320693632 learning.py:507] global step 1512: loss = 2.1338 (1.767 sec/step)\n",
            "INFO:tensorflow:global step 1513: loss = 3.1635 (0.719 sec/step)\n",
            "I1212 08:46:11.296152 140053320693632 learning.py:507] global step 1513: loss = 3.1635 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 1514: loss = 2.2493 (1.280 sec/step)\n",
            "I1212 08:46:12.704202 140053320693632 learning.py:507] global step 1514: loss = 2.2493 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 1515: loss = 2.0832 (0.501 sec/step)\n",
            "I1212 08:46:13.206607 140053320693632 learning.py:507] global step 1515: loss = 2.0832 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 1516: loss = 2.3937 (1.563 sec/step)\n",
            "I1212 08:46:14.787815 140053320693632 learning.py:507] global step 1516: loss = 2.3937 (1.563 sec/step)\n",
            "INFO:tensorflow:global step 1517: loss = 2.6972 (0.653 sec/step)\n",
            "I1212 08:46:15.755345 140053320693632 learning.py:507] global step 1517: loss = 2.6972 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1518: loss = 2.1477 (0.675 sec/step)\n",
            "I1212 08:46:16.873704 140053320693632 learning.py:507] global step 1518: loss = 2.1477 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1519: loss = 2.6559 (2.318 sec/step)\n",
            "I1212 08:46:19.260040 140053320693632 learning.py:507] global step 1519: loss = 2.6559 (2.318 sec/step)\n",
            "INFO:tensorflow:global step 1520: loss = 2.0906 (0.655 sec/step)\n",
            "I1212 08:46:20.173364 140053320693632 learning.py:507] global step 1520: loss = 2.0906 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1521: loss = 2.4759 (0.643 sec/step)\n",
            "I1212 08:46:20.994858 140053320693632 learning.py:507] global step 1521: loss = 2.4759 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1522: loss = 2.1810 (1.484 sec/step)\n",
            "I1212 08:46:22.482021 140053320693632 learning.py:507] global step 1522: loss = 2.1810 (1.484 sec/step)\n",
            "INFO:tensorflow:global step 1523: loss = 2.2624 (0.713 sec/step)\n",
            "I1212 08:46:23.469791 140053320693632 learning.py:507] global step 1523: loss = 2.2624 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1524: loss = 2.3903 (1.237 sec/step)\n",
            "I1212 08:46:24.714051 140053320693632 learning.py:507] global step 1524: loss = 2.3903 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1525: loss = 1.7711 (0.555 sec/step)\n",
            "I1212 08:46:25.270896 140053320693632 learning.py:507] global step 1525: loss = 1.7711 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 1526: loss = 2.7991 (1.048 sec/step)\n",
            "I1212 08:46:26.462282 140053320693632 learning.py:507] global step 1526: loss = 2.7991 (1.048 sec/step)\n",
            "INFO:tensorflow:global step 1527: loss = 2.8092 (1.766 sec/step)\n",
            "I1212 08:46:28.247426 140053320693632 learning.py:507] global step 1527: loss = 2.8092 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 1528: loss = 3.0108 (0.760 sec/step)\n",
            "I1212 08:46:29.213535 140053320693632 learning.py:507] global step 1528: loss = 3.0108 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 1529: loss = 2.4368 (1.280 sec/step)\n",
            "I1212 08:46:30.516950 140053320693632 learning.py:507] global step 1529: loss = 2.4368 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 1530: loss = 2.2700 (0.682 sec/step)\n",
            "I1212 08:46:31.528325 140053320693632 learning.py:507] global step 1530: loss = 2.2700 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1531: loss = 1.7702 (0.509 sec/step)\n",
            "I1212 08:46:32.097097 140053320693632 learning.py:507] global step 1531: loss = 1.7702 (0.509 sec/step)\n",
            "INFO:tensorflow:global step 1532: loss = 2.2894 (1.368 sec/step)\n",
            "I1212 08:46:33.629513 140053320693632 learning.py:507] global step 1532: loss = 2.2894 (1.368 sec/step)\n",
            "INFO:tensorflow:global step 1533: loss = 2.0336 (1.456 sec/step)\n",
            "I1212 08:46:35.107705 140053320693632 learning.py:507] global step 1533: loss = 2.0336 (1.456 sec/step)\n",
            "INFO:tensorflow:global step 1534: loss = 1.8883 (0.621 sec/step)\n",
            "I1212 08:46:35.967207 140053320693632 learning.py:507] global step 1534: loss = 1.8883 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1535: loss = 2.0394 (0.647 sec/step)\n",
            "I1212 08:46:37.024257 140053320693632 learning.py:507] global step 1535: loss = 2.0394 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1536: loss = 2.5915 (0.747 sec/step)\n",
            "I1212 08:46:38.191275 140053320693632 learning.py:507] global step 1536: loss = 2.5915 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1537: loss = 1.9793 (0.774 sec/step)\n",
            "I1212 08:46:39.279468 140053320693632 learning.py:507] global step 1537: loss = 1.9793 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 1538: loss = 2.7919 (0.787 sec/step)\n",
            "I1212 08:46:40.158753 140053320693632 learning.py:507] global step 1538: loss = 2.7919 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 1539: loss = 2.9950 (2.463 sec/step)\n",
            "I1212 08:46:42.770543 140053320693632 learning.py:507] global step 1539: loss = 2.9950 (2.463 sec/step)\n",
            "INFO:tensorflow:global step 1540: loss = 2.7948 (0.583 sec/step)\n",
            "I1212 08:46:43.540026 140053320693632 learning.py:507] global step 1540: loss = 2.7948 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1541: loss = 2.2428 (1.452 sec/step)\n",
            "I1212 08:46:45.023810 140053320693632 learning.py:507] global step 1541: loss = 2.2428 (1.452 sec/step)\n",
            "INFO:tensorflow:global step 1542: loss = 2.4267 (1.095 sec/step)\n",
            "I1212 08:46:46.120192 140053320693632 learning.py:507] global step 1542: loss = 2.4267 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 1543: loss = 2.6170 (0.636 sec/step)\n",
            "I1212 08:46:47.112386 140053320693632 learning.py:507] global step 1543: loss = 2.6170 (0.636 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1212 08:46:47.882014 140049589327616 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 1543.\n",
            "I1212 08:46:50.428329 140049614505728 supervisor.py:1050] Recording summary at step 1543.\n",
            "INFO:tensorflow:global step 1544: loss = 2.4791 (3.541 sec/step)\n",
            "I1212 08:46:50.842950 140053320693632 learning.py:507] global step 1544: loss = 2.4791 (3.541 sec/step)\n",
            "INFO:tensorflow:global step 1545: loss = 2.0499 (0.647 sec/step)\n",
            "I1212 08:46:51.510467 140053320693632 learning.py:507] global step 1545: loss = 2.0499 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1546: loss = 2.7680 (1.411 sec/step)\n",
            "I1212 08:46:53.489263 140053320693632 learning.py:507] global step 1546: loss = 2.7680 (1.411 sec/step)\n",
            "INFO:tensorflow:global step 1547: loss = 2.8487 (1.716 sec/step)\n",
            "I1212 08:46:55.584083 140053320693632 learning.py:507] global step 1547: loss = 2.8487 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 1548: loss = 2.1621 (0.788 sec/step)\n",
            "I1212 08:46:56.585808 140053320693632 learning.py:507] global step 1548: loss = 2.1621 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 1549: loss = 2.3628 (0.598 sec/step)\n",
            "I1212 08:46:57.326663 140053320693632 learning.py:507] global step 1549: loss = 2.3628 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1550: loss = 1.9600 (1.783 sec/step)\n",
            "I1212 08:46:59.110864 140053320693632 learning.py:507] global step 1550: loss = 1.9600 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 1551: loss = 1.8142 (0.807 sec/step)\n",
            "I1212 08:47:00.087655 140053320693632 learning.py:507] global step 1551: loss = 1.8142 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 1552: loss = 2.3137 (1.369 sec/step)\n",
            "I1212 08:47:01.544813 140053320693632 learning.py:507] global step 1552: loss = 2.3137 (1.369 sec/step)\n",
            "INFO:tensorflow:global step 1553: loss = 1.8830 (0.693 sec/step)\n",
            "I1212 08:47:02.383758 140053320693632 learning.py:507] global step 1553: loss = 1.8830 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1554: loss = 2.3437 (0.920 sec/step)\n",
            "I1212 08:47:03.576541 140053320693632 learning.py:507] global step 1554: loss = 2.3437 (0.920 sec/step)\n",
            "INFO:tensorflow:global step 1555: loss = 1.6632 (0.576 sec/step)\n",
            "I1212 08:47:04.551258 140053320693632 learning.py:507] global step 1555: loss = 1.6632 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1556: loss = 2.3665 (1.245 sec/step)\n",
            "I1212 08:47:05.842751 140053320693632 learning.py:507] global step 1556: loss = 2.3665 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1557: loss = 2.4490 (0.608 sec/step)\n",
            "I1212 08:47:06.706293 140053320693632 learning.py:507] global step 1557: loss = 2.4490 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 1558: loss = 3.1059 (1.306 sec/step)\n",
            "I1212 08:47:08.013928 140053320693632 learning.py:507] global step 1558: loss = 3.1059 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 1559: loss = 2.3353 (0.603 sec/step)\n",
            "I1212 08:47:08.916095 140053320693632 learning.py:507] global step 1559: loss = 2.3353 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1560: loss = 2.1345 (0.762 sec/step)\n",
            "I1212 08:47:10.059616 140053320693632 learning.py:507] global step 1560: loss = 2.1345 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 1561: loss = 3.0580 (1.234 sec/step)\n",
            "I1212 08:47:11.399076 140053320693632 learning.py:507] global step 1561: loss = 3.0580 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1562: loss = 2.9317 (0.737 sec/step)\n",
            "I1212 08:47:12.266103 140053320693632 learning.py:507] global step 1562: loss = 2.9317 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1563: loss = 2.5844 (1.306 sec/step)\n",
            "I1212 08:47:13.577653 140053320693632 learning.py:507] global step 1563: loss = 2.5844 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 1564: loss = 2.4678 (0.689 sec/step)\n",
            "I1212 08:47:14.611236 140053320693632 learning.py:507] global step 1564: loss = 2.4678 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1565: loss = 2.3236 (0.550 sec/step)\n",
            "I1212 08:47:15.413408 140053320693632 learning.py:507] global step 1565: loss = 2.3236 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 1566: loss = 2.1776 (1.304 sec/step)\n",
            "I1212 08:47:16.914139 140053320693632 learning.py:507] global step 1566: loss = 2.1776 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 1567: loss = 2.5321 (0.513 sec/step)\n",
            "I1212 08:47:17.588163 140053320693632 learning.py:507] global step 1567: loss = 2.5321 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1568: loss = 2.2903 (0.808 sec/step)\n",
            "I1212 08:47:18.632471 140053320693632 learning.py:507] global step 1568: loss = 2.2903 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 1569: loss = 2.4269 (1.311 sec/step)\n",
            "I1212 08:47:20.039538 140053320693632 learning.py:507] global step 1569: loss = 2.4269 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 1570: loss = 2.9593 (0.722 sec/step)\n",
            "I1212 08:47:21.015638 140053320693632 learning.py:507] global step 1570: loss = 2.9593 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1571: loss = 2.4298 (0.645 sec/step)\n",
            "I1212 08:47:22.145700 140053320693632 learning.py:507] global step 1571: loss = 2.4298 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1572: loss = 2.2222 (0.622 sec/step)\n",
            "I1212 08:47:23.022176 140053320693632 learning.py:507] global step 1572: loss = 2.2222 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1573: loss = 2.6355 (1.185 sec/step)\n",
            "I1212 08:47:24.409028 140053320693632 learning.py:507] global step 1573: loss = 2.6355 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1574: loss = 2.3167 (0.618 sec/step)\n",
            "I1212 08:47:25.167583 140053320693632 learning.py:507] global step 1574: loss = 2.3167 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1575: loss = 2.2939 (2.467 sec/step)\n",
            "I1212 08:47:27.878120 140053320693632 learning.py:507] global step 1575: loss = 2.2939 (2.467 sec/step)\n",
            "INFO:tensorflow:global step 1576: loss = 2.1033 (0.558 sec/step)\n",
            "I1212 08:47:28.437782 140053320693632 learning.py:507] global step 1576: loss = 2.1033 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1577: loss = 2.3518 (1.686 sec/step)\n",
            "I1212 08:47:30.125742 140053320693632 learning.py:507] global step 1577: loss = 2.3518 (1.686 sec/step)\n",
            "INFO:tensorflow:global step 1578: loss = 2.1774 (0.594 sec/step)\n",
            "I1212 08:47:30.990752 140053320693632 learning.py:507] global step 1578: loss = 2.1774 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1579: loss = 2.1725 (0.682 sec/step)\n",
            "I1212 08:47:32.169088 140053320693632 learning.py:507] global step 1579: loss = 2.1725 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1580: loss = 2.2958 (0.735 sec/step)\n",
            "I1212 08:47:33.370603 140053320693632 learning.py:507] global step 1580: loss = 2.2958 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 1581: loss = 3.0130 (1.149 sec/step)\n",
            "I1212 08:47:34.633898 140053320693632 learning.py:507] global step 1581: loss = 3.0130 (1.149 sec/step)\n",
            "INFO:tensorflow:global step 1582: loss = 1.8403 (0.483 sec/step)\n",
            "I1212 08:47:35.403890 140053320693632 learning.py:507] global step 1582: loss = 1.8403 (0.483 sec/step)\n",
            "INFO:tensorflow:global step 1583: loss = 2.1718 (1.300 sec/step)\n",
            "I1212 08:47:36.926433 140053320693632 learning.py:507] global step 1583: loss = 2.1718 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 1584: loss = 1.8746 (0.667 sec/step)\n",
            "I1212 08:47:37.853203 140053320693632 learning.py:507] global step 1584: loss = 1.8746 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1585: loss = 2.0464 (0.713 sec/step)\n",
            "I1212 08:47:38.726451 140053320693632 learning.py:507] global step 1585: loss = 2.0464 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1586: loss = 2.2664 (1.649 sec/step)\n",
            "I1212 08:47:40.376456 140053320693632 learning.py:507] global step 1586: loss = 2.2664 (1.649 sec/step)\n",
            "INFO:tensorflow:global step 1587: loss = 2.4660 (0.577 sec/step)\n",
            "I1212 08:47:41.228144 140053320693632 learning.py:507] global step 1587: loss = 2.4660 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 1588: loss = 2.8854 (1.245 sec/step)\n",
            "I1212 08:47:42.608507 140053320693632 learning.py:507] global step 1588: loss = 2.8854 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1589: loss = 1.6073 (0.633 sec/step)\n",
            "I1212 08:47:43.272786 140053320693632 learning.py:507] global step 1589: loss = 1.6073 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1590: loss = 2.2530 (1.749 sec/step)\n",
            "I1212 08:47:45.284703 140053320693632 learning.py:507] global step 1590: loss = 2.2530 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 1591: loss = 2.0351 (0.708 sec/step)\n",
            "I1212 08:47:46.037512 140053320693632 learning.py:507] global step 1591: loss = 2.0351 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 1592: loss = 2.1658 (0.757 sec/step)\n",
            "I1212 08:47:47.061623 140053320693632 learning.py:507] global step 1592: loss = 2.1658 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 1593: loss = 2.4813 (1.969 sec/step)\n",
            "I1212 08:47:49.032320 140053320693632 learning.py:507] global step 1593: loss = 2.4813 (1.969 sec/step)\n",
            "INFO:tensorflow:global step 1594: loss = 2.4252 (0.626 sec/step)\n",
            "I1212 08:47:49.660542 140053320693632 learning.py:507] global step 1594: loss = 2.4252 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1595: loss = 2.4112 (1.824 sec/step)\n",
            "I1212 08:47:51.486393 140053320693632 learning.py:507] global step 1595: loss = 2.4112 (1.824 sec/step)\n",
            "INFO:tensorflow:global step 1596: loss = 2.2962 (0.665 sec/step)\n",
            "I1212 08:47:52.508050 140053320693632 learning.py:507] global step 1596: loss = 2.2962 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1597: loss = 1.8198 (0.729 sec/step)\n",
            "I1212 08:47:53.644160 140053320693632 learning.py:507] global step 1597: loss = 1.8198 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1598: loss = 2.5044 (0.635 sec/step)\n",
            "I1212 08:47:54.852189 140053320693632 learning.py:507] global step 1598: loss = 2.5044 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1599: loss = 2.0862 (0.698 sec/step)\n",
            "I1212 08:47:55.668857 140053320693632 learning.py:507] global step 1599: loss = 2.0862 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1600: loss = 2.1204 (1.220 sec/step)\n",
            "I1212 08:47:57.082385 140053320693632 learning.py:507] global step 1600: loss = 2.1204 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1601: loss = 2.1604 (0.598 sec/step)\n",
            "I1212 08:47:57.966691 140053320693632 learning.py:507] global step 1601: loss = 2.1604 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1602: loss = 2.2387 (0.538 sec/step)\n",
            "I1212 08:47:58.658100 140053320693632 learning.py:507] global step 1602: loss = 2.2387 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 1603: loss = 2.2264 (1.542 sec/step)\n",
            "I1212 08:48:00.201667 140053320693632 learning.py:507] global step 1603: loss = 2.2264 (1.542 sec/step)\n",
            "INFO:tensorflow:global step 1604: loss = 1.9632 (0.515 sec/step)\n",
            "I1212 08:48:00.718861 140053320693632 learning.py:507] global step 1604: loss = 1.9632 (0.515 sec/step)\n",
            "INFO:tensorflow:global step 1605: loss = 1.9009 (1.701 sec/step)\n",
            "I1212 08:48:02.421615 140053320693632 learning.py:507] global step 1605: loss = 1.9009 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 1606: loss = 2.6391 (0.475 sec/step)\n",
            "I1212 08:48:02.898439 140053320693632 learning.py:507] global step 1606: loss = 2.6391 (0.475 sec/step)\n",
            "INFO:tensorflow:global step 1607: loss = 2.6174 (1.578 sec/step)\n",
            "I1212 08:48:04.480535 140053320693632 learning.py:507] global step 1607: loss = 2.6174 (1.578 sec/step)\n",
            "INFO:tensorflow:global step 1608: loss = 2.4650 (1.145 sec/step)\n",
            "I1212 08:48:05.676761 140053320693632 learning.py:507] global step 1608: loss = 2.4650 (1.145 sec/step)\n",
            "INFO:tensorflow:global step 1609: loss = 2.9486 (0.712 sec/step)\n",
            "I1212 08:48:06.502342 140053320693632 learning.py:507] global step 1609: loss = 2.9486 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 1610: loss = 2.1141 (1.476 sec/step)\n",
            "I1212 08:48:07.981038 140053320693632 learning.py:507] global step 1610: loss = 2.1141 (1.476 sec/step)\n",
            "INFO:tensorflow:global step 1611: loss = 1.8932 (1.103 sec/step)\n",
            "I1212 08:48:09.085758 140053320693632 learning.py:507] global step 1611: loss = 1.8932 (1.103 sec/step)\n",
            "INFO:tensorflow:global step 1612: loss = 2.1139 (0.493 sec/step)\n",
            "I1212 08:48:09.580185 140053320693632 learning.py:507] global step 1612: loss = 2.1139 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 1613: loss = 3.1382 (1.731 sec/step)\n",
            "I1212 08:48:11.312630 140053320693632 learning.py:507] global step 1613: loss = 3.1382 (1.731 sec/step)\n",
            "INFO:tensorflow:global step 1614: loss = 1.9114 (0.625 sec/step)\n",
            "I1212 08:48:12.085571 140053320693632 learning.py:507] global step 1614: loss = 1.9114 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1615: loss = 2.5034 (0.676 sec/step)\n",
            "I1212 08:48:13.295266 140053320693632 learning.py:507] global step 1615: loss = 2.5034 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1616: loss = 1.9606 (0.783 sec/step)\n",
            "I1212 08:48:14.422530 140053320693632 learning.py:507] global step 1616: loss = 1.9606 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 1617: loss = 1.7766 (0.495 sec/step)\n",
            "I1212 08:48:15.038288 140053320693632 learning.py:507] global step 1617: loss = 1.7766 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 1618: loss = 2.5710 (1.559 sec/step)\n",
            "I1212 08:48:16.598985 140053320693632 learning.py:507] global step 1618: loss = 2.5710 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 1619: loss = 1.9623 (0.607 sec/step)\n",
            "I1212 08:48:17.207624 140053320693632 learning.py:507] global step 1619: loss = 1.9623 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 1620: loss = 2.3933 (0.915 sec/step)\n",
            "I1212 08:48:18.437118 140053320693632 learning.py:507] global step 1620: loss = 2.3933 (0.915 sec/step)\n",
            "INFO:tensorflow:global step 1621: loss = 2.1353 (1.751 sec/step)\n",
            "I1212 08:48:20.347023 140053320693632 learning.py:507] global step 1621: loss = 2.1353 (1.751 sec/step)\n",
            "INFO:tensorflow:global step 1622: loss = 2.4595 (0.672 sec/step)\n",
            "I1212 08:48:21.308761 140053320693632 learning.py:507] global step 1622: loss = 2.4595 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1623: loss = 2.1143 (1.749 sec/step)\n",
            "I1212 08:48:23.234489 140053320693632 learning.py:507] global step 1623: loss = 2.1143 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 1624: loss = 2.1349 (0.623 sec/step)\n",
            "I1212 08:48:23.859012 140053320693632 learning.py:507] global step 1624: loss = 2.1349 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1625: loss = 1.9703 (1.525 sec/step)\n",
            "I1212 08:48:25.414874 140053320693632 learning.py:507] global step 1625: loss = 1.9703 (1.525 sec/step)\n",
            "INFO:tensorflow:global step 1626: loss = 1.8433 (1.236 sec/step)\n",
            "I1212 08:48:26.678832 140053320693632 learning.py:507] global step 1626: loss = 1.8433 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1627: loss = 2.2711 (0.642 sec/step)\n",
            "I1212 08:48:27.592023 140053320693632 learning.py:507] global step 1627: loss = 2.2711 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1628: loss = 2.2106 (0.524 sec/step)\n",
            "I1212 08:48:28.179107 140053320693632 learning.py:507] global step 1628: loss = 2.2106 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 1629: loss = 2.4693 (0.822 sec/step)\n",
            "I1212 08:48:29.236638 140053320693632 learning.py:507] global step 1629: loss = 2.4693 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 1630: loss = 2.0329 (1.625 sec/step)\n",
            "I1212 08:48:31.206519 140053320693632 learning.py:507] global step 1630: loss = 2.0329 (1.625 sec/step)\n",
            "INFO:tensorflow:global step 1631: loss = 2.2504 (0.625 sec/step)\n",
            "I1212 08:48:32.073138 140053320693632 learning.py:507] global step 1631: loss = 2.2504 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1632: loss = 2.3974 (0.629 sec/step)\n",
            "I1212 08:48:33.214701 140053320693632 learning.py:507] global step 1632: loss = 2.3974 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1633: loss = 2.1283 (0.664 sec/step)\n",
            "I1212 08:48:34.311489 140053320693632 learning.py:507] global step 1633: loss = 2.1283 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1634: loss = 2.4265 (0.665 sec/step)\n",
            "I1212 08:48:35.266332 140053320693632 learning.py:507] global step 1634: loss = 2.4265 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1635: loss = 1.8524 (1.420 sec/step)\n",
            "I1212 08:48:36.711439 140053320693632 learning.py:507] global step 1635: loss = 1.8524 (1.420 sec/step)\n",
            "INFO:tensorflow:global step 1636: loss = 2.1276 (0.597 sec/step)\n",
            "I1212 08:48:37.552834 140053320693632 learning.py:507] global step 1636: loss = 2.1276 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1637: loss = 2.6069 (0.623 sec/step)\n",
            "I1212 08:48:38.544799 140053320693632 learning.py:507] global step 1637: loss = 2.6069 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1638: loss = 2.1255 (0.787 sec/step)\n",
            "I1212 08:48:39.657698 140053320693632 learning.py:507] global step 1638: loss = 2.1255 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 1639: loss = 2.5170 (1.115 sec/step)\n",
            "I1212 08:48:40.935070 140053320693632 learning.py:507] global step 1639: loss = 2.5170 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 1640: loss = 1.9465 (0.721 sec/step)\n",
            "I1212 08:48:41.886842 140053320693632 learning.py:507] global step 1640: loss = 1.9465 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1641: loss = 2.1198 (1.307 sec/step)\n",
            "I1212 08:48:43.199065 140053320693632 learning.py:507] global step 1641: loss = 2.1198 (1.307 sec/step)\n",
            "INFO:tensorflow:global step 1642: loss = 2.5129 (0.747 sec/step)\n",
            "I1212 08:48:44.190075 140053320693632 learning.py:507] global step 1642: loss = 2.5129 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1643: loss = 2.5746 (0.658 sec/step)\n",
            "I1212 08:48:45.200452 140053320693632 learning.py:507] global step 1643: loss = 2.5746 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1644: loss = 2.2436 (0.688 sec/step)\n",
            "I1212 08:48:46.113167 140053320693632 learning.py:507] global step 1644: loss = 2.2436 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1645: loss = 2.2140 (0.736 sec/step)\n",
            "I1212 08:48:47.304056 140053320693632 learning.py:507] global step 1645: loss = 2.2140 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 1646: loss = 1.8568 (2.105 sec/step)\n",
            "I1212 08:48:49.552953 140053320693632 learning.py:507] global step 1646: loss = 1.8568 (2.105 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1646.\n",
            "I1212 08:48:49.811096 140049614505728 supervisor.py:1050] Recording summary at step 1646.\n",
            "INFO:tensorflow:global step 1647: loss = 1.8757 (0.704 sec/step)\n",
            "I1212 08:48:50.490860 140053320693632 learning.py:507] global step 1647: loss = 1.8757 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1648: loss = 2.1087 (1.087 sec/step)\n",
            "I1212 08:48:51.738435 140053320693632 learning.py:507] global step 1648: loss = 2.1087 (1.087 sec/step)\n",
            "INFO:tensorflow:global step 1649: loss = 2.1505 (0.555 sec/step)\n",
            "I1212 08:48:52.515912 140053320693632 learning.py:507] global step 1649: loss = 2.1505 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 1650: loss = 1.7426 (0.563 sec/step)\n",
            "I1212 08:48:53.547494 140053320693632 learning.py:507] global step 1650: loss = 1.7426 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 1651: loss = 1.8680 (2.893 sec/step)\n",
            "I1212 08:48:56.453242 140053320693632 learning.py:507] global step 1651: loss = 1.8680 (2.893 sec/step)\n",
            "INFO:tensorflow:global step 1652: loss = 2.1286 (0.631 sec/step)\n",
            "I1212 08:48:57.167787 140053320693632 learning.py:507] global step 1652: loss = 2.1286 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1653: loss = 2.3835 (0.855 sec/step)\n",
            "I1212 08:48:58.482244 140053320693632 learning.py:507] global step 1653: loss = 2.3835 (0.855 sec/step)\n",
            "INFO:tensorflow:global step 1654: loss = 2.2866 (0.682 sec/step)\n",
            "I1212 08:48:59.329798 140053320693632 learning.py:507] global step 1654: loss = 2.2866 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1655: loss = 1.6966 (0.896 sec/step)\n",
            "I1212 08:49:00.560519 140053320693632 learning.py:507] global step 1655: loss = 1.6966 (0.896 sec/step)\n",
            "INFO:tensorflow:global step 1656: loss = 1.9803 (2.126 sec/step)\n",
            "I1212 08:49:02.981512 140053320693632 learning.py:507] global step 1656: loss = 1.9803 (2.126 sec/step)\n",
            "INFO:tensorflow:global step 1657: loss = 3.1634 (0.599 sec/step)\n",
            "I1212 08:49:03.857100 140053320693632 learning.py:507] global step 1657: loss = 3.1634 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1658: loss = 2.5964 (0.586 sec/step)\n",
            "I1212 08:49:04.904557 140053320693632 learning.py:507] global step 1658: loss = 2.5964 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 1659: loss = 3.0743 (0.769 sec/step)\n",
            "I1212 08:49:06.174578 140053320693632 learning.py:507] global step 1659: loss = 3.0743 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 1660: loss = 2.3497 (1.500 sec/step)\n",
            "I1212 08:49:07.745431 140053320693632 learning.py:507] global step 1660: loss = 2.3497 (1.500 sec/step)\n",
            "INFO:tensorflow:global step 1661: loss = 2.3946 (0.618 sec/step)\n",
            "I1212 08:49:08.582718 140053320693632 learning.py:507] global step 1661: loss = 2.3946 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1662: loss = 3.2330 (1.895 sec/step)\n",
            "I1212 08:49:10.815162 140053320693632 learning.py:507] global step 1662: loss = 3.2330 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 1663: loss = 2.3414 (0.615 sec/step)\n",
            "I1212 08:49:11.658324 140053320693632 learning.py:507] global step 1663: loss = 2.3414 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1664: loss = 2.6157 (1.273 sec/step)\n",
            "I1212 08:49:13.200835 140053320693632 learning.py:507] global step 1664: loss = 2.6157 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 1665: loss = 2.5851 (1.189 sec/step)\n",
            "I1212 08:49:14.391712 140053320693632 learning.py:507] global step 1665: loss = 2.5851 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1666: loss = 2.0801 (0.623 sec/step)\n",
            "I1212 08:49:15.016608 140053320693632 learning.py:507] global step 1666: loss = 2.0801 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1667: loss = 2.3742 (1.959 sec/step)\n",
            "I1212 08:49:16.976845 140053320693632 learning.py:507] global step 1667: loss = 2.3742 (1.959 sec/step)\n",
            "INFO:tensorflow:global step 1668: loss = 2.5329 (0.679 sec/step)\n",
            "I1212 08:49:17.848021 140053320693632 learning.py:507] global step 1668: loss = 2.5329 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1669: loss = 1.7872 (1.501 sec/step)\n",
            "I1212 08:49:19.539849 140053320693632 learning.py:507] global step 1669: loss = 1.7872 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 1670: loss = 2.2795 (0.737 sec/step)\n",
            "I1212 08:49:20.516525 140053320693632 learning.py:507] global step 1670: loss = 2.2795 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1671: loss = 2.4093 (0.595 sec/step)\n",
            "I1212 08:49:21.268269 140053320693632 learning.py:507] global step 1671: loss = 2.4093 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 1672: loss = 2.0283 (1.781 sec/step)\n",
            "I1212 08:49:23.050752 140053320693632 learning.py:507] global step 1672: loss = 2.0283 (1.781 sec/step)\n",
            "INFO:tensorflow:global step 1673: loss = 2.4030 (0.631 sec/step)\n",
            "I1212 08:49:24.023438 140053320693632 learning.py:507] global step 1673: loss = 2.4030 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1674: loss = 2.1239 (0.649 sec/step)\n",
            "I1212 08:49:24.935592 140053320693632 learning.py:507] global step 1674: loss = 2.1239 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1675: loss = 2.0900 (1.287 sec/step)\n",
            "I1212 08:49:26.326281 140053320693632 learning.py:507] global step 1675: loss = 2.0900 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 1676: loss = 2.3458 (0.599 sec/step)\n",
            "I1212 08:49:27.116468 140053320693632 learning.py:507] global step 1676: loss = 2.3458 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1677: loss = 2.2703 (1.525 sec/step)\n",
            "I1212 08:49:28.661552 140053320693632 learning.py:507] global step 1677: loss = 2.2703 (1.525 sec/step)\n",
            "INFO:tensorflow:global step 1678: loss = 2.0774 (0.792 sec/step)\n",
            "I1212 08:49:29.589526 140053320693632 learning.py:507] global step 1678: loss = 2.0774 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 1679: loss = 2.4669 (2.048 sec/step)\n",
            "I1212 08:49:31.643241 140053320693632 learning.py:507] global step 1679: loss = 2.4669 (2.048 sec/step)\n",
            "INFO:tensorflow:global step 1680: loss = 2.0932 (0.669 sec/step)\n",
            "I1212 08:49:32.623941 140053320693632 learning.py:507] global step 1680: loss = 2.0932 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1681: loss = 2.8564 (1.177 sec/step)\n",
            "I1212 08:49:33.877654 140053320693632 learning.py:507] global step 1681: loss = 2.8564 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1682: loss = 3.1285 (0.629 sec/step)\n",
            "I1212 08:49:34.508709 140053320693632 learning.py:507] global step 1682: loss = 3.1285 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1683: loss = 2.0141 (1.768 sec/step)\n",
            "I1212 08:49:36.278276 140053320693632 learning.py:507] global step 1683: loss = 2.0141 (1.768 sec/step)\n",
            "INFO:tensorflow:global step 1684: loss = 2.0304 (0.550 sec/step)\n",
            "I1212 08:49:36.829593 140053320693632 learning.py:507] global step 1684: loss = 2.0304 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 1685: loss = 1.7478 (0.947 sec/step)\n",
            "I1212 08:49:38.005541 140053320693632 learning.py:507] global step 1685: loss = 1.7478 (0.947 sec/step)\n",
            "INFO:tensorflow:global step 1686: loss = 2.0987 (1.771 sec/step)\n",
            "I1212 08:49:39.938247 140053320693632 learning.py:507] global step 1686: loss = 2.0987 (1.771 sec/step)\n",
            "INFO:tensorflow:global step 1687: loss = 2.8717 (0.684 sec/step)\n",
            "I1212 08:49:40.973235 140053320693632 learning.py:507] global step 1687: loss = 2.8717 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1688: loss = 2.8029 (0.633 sec/step)\n",
            "I1212 08:49:41.970537 140053320693632 learning.py:507] global step 1688: loss = 2.8029 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1689: loss = 2.5013 (0.614 sec/step)\n",
            "I1212 08:49:42.678380 140053320693632 learning.py:507] global step 1689: loss = 2.5013 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 1690: loss = 2.1911 (0.807 sec/step)\n",
            "I1212 08:49:43.752005 140053320693632 learning.py:507] global step 1690: loss = 2.1911 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 1691: loss = 2.0740 (1.760 sec/step)\n",
            "I1212 08:49:45.624042 140053320693632 learning.py:507] global step 1691: loss = 2.0740 (1.760 sec/step)\n",
            "INFO:tensorflow:global step 1692: loss = 2.3976 (0.532 sec/step)\n",
            "I1212 08:49:46.422617 140053320693632 learning.py:507] global step 1692: loss = 2.3976 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 1693: loss = 1.8818 (1.461 sec/step)\n",
            "I1212 08:49:47.884739 140053320693632 learning.py:507] global step 1693: loss = 1.8818 (1.461 sec/step)\n",
            "INFO:tensorflow:global step 1694: loss = 1.8458 (0.620 sec/step)\n",
            "I1212 08:49:48.810455 140053320693632 learning.py:507] global step 1694: loss = 1.8458 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1695: loss = 2.0558 (1.457 sec/step)\n",
            "I1212 08:49:50.283190 140053320693632 learning.py:507] global step 1695: loss = 2.0558 (1.457 sec/step)\n",
            "INFO:tensorflow:global step 1696: loss = 2.7467 (0.669 sec/step)\n",
            "I1212 08:49:51.058758 140053320693632 learning.py:507] global step 1696: loss = 2.7467 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1697: loss = 2.3978 (1.314 sec/step)\n",
            "I1212 08:49:52.587884 140053320693632 learning.py:507] global step 1697: loss = 2.3978 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1698: loss = 2.3196 (0.647 sec/step)\n",
            "I1212 08:49:53.463804 140053320693632 learning.py:507] global step 1698: loss = 2.3196 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1699: loss = 2.4927 (1.218 sec/step)\n",
            "I1212 08:49:54.808903 140053320693632 learning.py:507] global step 1699: loss = 2.4927 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1700: loss = 2.1844 (0.598 sec/step)\n",
            "I1212 08:49:55.586275 140053320693632 learning.py:507] global step 1700: loss = 2.1844 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1701: loss = 2.3796 (1.665 sec/step)\n",
            "I1212 08:49:57.253597 140053320693632 learning.py:507] global step 1701: loss = 2.3796 (1.665 sec/step)\n",
            "INFO:tensorflow:global step 1702: loss = 2.0253 (0.498 sec/step)\n",
            "I1212 08:49:58.095532 140053320693632 learning.py:507] global step 1702: loss = 2.0253 (0.498 sec/step)\n",
            "INFO:tensorflow:global step 1703: loss = 2.3973 (1.405 sec/step)\n",
            "I1212 08:49:59.643990 140053320693632 learning.py:507] global step 1703: loss = 2.3973 (1.405 sec/step)\n",
            "INFO:tensorflow:global step 1704: loss = 2.3184 (0.732 sec/step)\n",
            "I1212 08:50:00.471528 140053320693632 learning.py:507] global step 1704: loss = 2.3184 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1705: loss = 2.8700 (1.139 sec/step)\n",
            "I1212 08:50:01.926455 140053320693632 learning.py:507] global step 1705: loss = 2.8700 (1.139 sec/step)\n",
            "INFO:tensorflow:global step 1706: loss = 2.0643 (0.764 sec/step)\n",
            "I1212 08:50:02.794909 140053320693632 learning.py:507] global step 1706: loss = 2.0643 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 1707: loss = 3.5004 (0.788 sec/step)\n",
            "I1212 08:50:04.025127 140053320693632 learning.py:507] global step 1707: loss = 3.5004 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 1708: loss = 2.9413 (1.263 sec/step)\n",
            "I1212 08:50:05.306825 140053320693632 learning.py:507] global step 1708: loss = 2.9413 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 1709: loss = 2.1640 (0.747 sec/step)\n",
            "I1212 08:50:06.211462 140053320693632 learning.py:507] global step 1709: loss = 2.1640 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1710: loss = 2.6731 (0.588 sec/step)\n",
            "I1212 08:50:07.196573 140053320693632 learning.py:507] global step 1710: loss = 2.6731 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 1711: loss = 1.9285 (0.523 sec/step)\n",
            "I1212 08:50:07.928999 140053320693632 learning.py:507] global step 1711: loss = 1.9285 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 1712: loss = 2.5088 (0.628 sec/step)\n",
            "I1212 08:50:08.558302 140053320693632 learning.py:507] global step 1712: loss = 2.5088 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1713: loss = 1.9642 (2.716 sec/step)\n",
            "I1212 08:50:11.275559 140053320693632 learning.py:507] global step 1713: loss = 1.9642 (2.716 sec/step)\n",
            "INFO:tensorflow:global step 1714: loss = 2.5448 (0.818 sec/step)\n",
            "I1212 08:50:12.348972 140053320693632 learning.py:507] global step 1714: loss = 2.5448 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1715: loss = 3.2489 (0.635 sec/step)\n",
            "I1212 08:50:13.043180 140053320693632 learning.py:507] global step 1715: loss = 3.2489 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1716: loss = 2.4643 (1.952 sec/step)\n",
            "I1212 08:50:14.997099 140053320693632 learning.py:507] global step 1716: loss = 2.4643 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 1717: loss = 1.9534 (0.605 sec/step)\n",
            "I1212 08:50:15.603456 140053320693632 learning.py:507] global step 1717: loss = 1.9534 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 1718: loss = 1.9588 (1.801 sec/step)\n",
            "I1212 08:50:17.405948 140053320693632 learning.py:507] global step 1718: loss = 1.9588 (1.801 sec/step)\n",
            "INFO:tensorflow:global step 1719: loss = 2.5102 (0.635 sec/step)\n",
            "I1212 08:50:18.372739 140053320693632 learning.py:507] global step 1719: loss = 2.5102 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1720: loss = 2.7103 (0.806 sec/step)\n",
            "I1212 08:50:19.688609 140053320693632 learning.py:507] global step 1720: loss = 2.7103 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 1721: loss = 1.9063 (0.692 sec/step)\n",
            "I1212 08:50:20.643624 140053320693632 learning.py:507] global step 1721: loss = 1.9063 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1722: loss = 2.5722 (1.430 sec/step)\n",
            "I1212 08:50:22.089323 140053320693632 learning.py:507] global step 1722: loss = 2.5722 (1.430 sec/step)\n",
            "INFO:tensorflow:global step 1723: loss = 1.9864 (1.098 sec/step)\n",
            "I1212 08:50:23.189170 140053320693632 learning.py:507] global step 1723: loss = 1.9864 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 1724: loss = 2.9572 (0.626 sec/step)\n",
            "I1212 08:50:23.998618 140053320693632 learning.py:507] global step 1724: loss = 2.9572 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1725: loss = 2.1058 (1.235 sec/step)\n",
            "I1212 08:50:25.382743 140053320693632 learning.py:507] global step 1725: loss = 2.1058 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1726: loss = 2.2522 (0.623 sec/step)\n",
            "I1212 08:50:26.235526 140053320693632 learning.py:507] global step 1726: loss = 2.2522 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1727: loss = 1.9087 (1.190 sec/step)\n",
            "I1212 08:50:27.566880 140053320693632 learning.py:507] global step 1727: loss = 1.9087 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1728: loss = 2.1318 (0.566 sec/step)\n",
            "I1212 08:50:28.268348 140053320693632 learning.py:507] global step 1728: loss = 2.1318 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 1729: loss = 2.4709 (1.256 sec/step)\n",
            "I1212 08:50:29.677073 140053320693632 learning.py:507] global step 1729: loss = 2.4709 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1730: loss = 2.3837 (0.580 sec/step)\n",
            "I1212 08:50:30.572896 140053320693632 learning.py:507] global step 1730: loss = 2.3837 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1731: loss = 2.0903 (0.607 sec/step)\n",
            "I1212 08:50:31.530247 140053320693632 learning.py:507] global step 1731: loss = 2.0903 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 1732: loss = 2.6801 (0.819 sec/step)\n",
            "I1212 08:50:32.668526 140053320693632 learning.py:507] global step 1732: loss = 2.6801 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1733: loss = 2.3210 (0.710 sec/step)\n",
            "I1212 08:50:33.707073 140053320693632 learning.py:507] global step 1733: loss = 2.3210 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 1734: loss = 2.5976 (0.700 sec/step)\n",
            "I1212 08:50:34.671061 140053320693632 learning.py:507] global step 1734: loss = 2.5976 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1735: loss = 1.7597 (1.257 sec/step)\n",
            "I1212 08:50:36.022964 140053320693632 learning.py:507] global step 1735: loss = 1.7597 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 1736: loss = 2.3730 (0.615 sec/step)\n",
            "I1212 08:50:36.896338 140053320693632 learning.py:507] global step 1736: loss = 2.3730 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1737: loss = 2.1957 (1.196 sec/step)\n",
            "I1212 08:50:38.212799 140053320693632 learning.py:507] global step 1737: loss = 2.1957 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1738: loss = 2.3731 (0.630 sec/step)\n",
            "I1212 08:50:39.118644 140053320693632 learning.py:507] global step 1738: loss = 2.3731 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1739: loss = 2.4066 (1.114 sec/step)\n",
            "I1212 08:50:40.372539 140053320693632 learning.py:507] global step 1739: loss = 2.4066 (1.114 sec/step)\n",
            "INFO:tensorflow:global step 1740: loss = 1.9176 (0.786 sec/step)\n",
            "I1212 08:50:41.311866 140053320693632 learning.py:507] global step 1740: loss = 1.9176 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 1741: loss = 2.2383 (0.635 sec/step)\n",
            "I1212 08:50:42.203051 140053320693632 learning.py:507] global step 1741: loss = 2.2383 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1742: loss = 2.1870 (0.606 sec/step)\n",
            "I1212 08:50:43.410856 140053320693632 learning.py:507] global step 1742: loss = 2.1870 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 1743: loss = 2.7623 (0.689 sec/step)\n",
            "I1212 08:50:44.291985 140053320693632 learning.py:507] global step 1743: loss = 2.7623 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1744: loss = 1.9873 (0.603 sec/step)\n",
            "I1212 08:50:45.356579 140053320693632 learning.py:507] global step 1744: loss = 1.9873 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1745: loss = 2.1916 (1.371 sec/step)\n",
            "I1212 08:50:46.766632 140053320693632 learning.py:507] global step 1745: loss = 2.1916 (1.371 sec/step)\n",
            "INFO:tensorflow:global step 1746: loss = 2.1577 (0.603 sec/step)\n",
            "I1212 08:50:47.678035 140053320693632 learning.py:507] global step 1746: loss = 2.1577 (0.603 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1746.\n",
            "I1212 08:50:49.966470 140049614505728 supervisor.py:1050] Recording summary at step 1746.\n",
            "INFO:tensorflow:global step 1747: loss = 2.2224 (2.389 sec/step)\n",
            "I1212 08:50:50.149967 140053320693632 learning.py:507] global step 1747: loss = 2.2224 (2.389 sec/step)\n",
            "INFO:tensorflow:global step 1748: loss = 1.7869 (0.609 sec/step)\n",
            "I1212 08:50:50.760301 140053320693632 learning.py:507] global step 1748: loss = 1.7869 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1749: loss = 1.9955 (1.757 sec/step)\n",
            "I1212 08:50:52.518857 140053320693632 learning.py:507] global step 1749: loss = 1.9955 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 1750: loss = 2.6375 (0.632 sec/step)\n",
            "I1212 08:50:53.379403 140053320693632 learning.py:507] global step 1750: loss = 2.6375 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1751: loss = 2.0299 (0.718 sec/step)\n",
            "I1212 08:50:54.231599 140053320693632 learning.py:507] global step 1751: loss = 2.0299 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 1752: loss = 2.6390 (1.528 sec/step)\n",
            "I1212 08:50:55.762663 140053320693632 learning.py:507] global step 1752: loss = 2.6390 (1.528 sec/step)\n",
            "INFO:tensorflow:global step 1753: loss = 2.6309 (0.487 sec/step)\n",
            "I1212 08:50:56.250973 140053320693632 learning.py:507] global step 1753: loss = 2.6309 (0.487 sec/step)\n",
            "INFO:tensorflow:global step 1754: loss = 2.6115 (1.582 sec/step)\n",
            "I1212 08:50:57.834594 140053320693632 learning.py:507] global step 1754: loss = 2.6115 (1.582 sec/step)\n",
            "INFO:tensorflow:global step 1755: loss = 2.3233 (0.609 sec/step)\n",
            "I1212 08:50:58.712229 140053320693632 learning.py:507] global step 1755: loss = 2.3233 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1756: loss = 2.0562 (0.698 sec/step)\n",
            "I1212 08:50:59.601087 140053320693632 learning.py:507] global step 1756: loss = 2.0562 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1757: loss = 2.4591 (1.298 sec/step)\n",
            "I1212 08:51:01.013636 140053320693632 learning.py:507] global step 1757: loss = 2.4591 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 1758: loss = 2.1550 (0.586 sec/step)\n",
            "I1212 08:51:01.783427 140053320693632 learning.py:507] global step 1758: loss = 2.1550 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 1759: loss = 2.7865 (0.675 sec/step)\n",
            "I1212 08:51:02.780045 140053320693632 learning.py:507] global step 1759: loss = 2.7865 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1760: loss = 2.7134 (1.458 sec/step)\n",
            "I1212 08:51:04.442308 140053320693632 learning.py:507] global step 1760: loss = 2.7134 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 1761: loss = 2.5447 (0.680 sec/step)\n",
            "I1212 08:51:05.193861 140053320693632 learning.py:507] global step 1761: loss = 2.5447 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1762: loss = 2.1144 (1.360 sec/step)\n",
            "I1212 08:51:06.745557 140053320693632 learning.py:507] global step 1762: loss = 2.1144 (1.360 sec/step)\n",
            "INFO:tensorflow:global step 1763: loss = 1.8680 (0.579 sec/step)\n",
            "I1212 08:51:07.326308 140053320693632 learning.py:507] global step 1763: loss = 1.8680 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 1764: loss = 2.2899 (0.523 sec/step)\n",
            "I1212 08:51:07.851595 140053320693632 learning.py:507] global step 1764: loss = 2.2899 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 1765: loss = 2.3188 (2.661 sec/step)\n",
            "I1212 08:51:10.516341 140053320693632 learning.py:507] global step 1765: loss = 2.3188 (2.661 sec/step)\n",
            "INFO:tensorflow:global step 1766: loss = 1.7822 (0.721 sec/step)\n",
            "I1212 08:51:11.497616 140053320693632 learning.py:507] global step 1766: loss = 1.7822 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1767: loss = 2.0135 (1.519 sec/step)\n",
            "I1212 08:51:13.134055 140053320693632 learning.py:507] global step 1767: loss = 2.0135 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 1768: loss = 2.0511 (0.665 sec/step)\n",
            "I1212 08:51:14.109416 140053320693632 learning.py:507] global step 1768: loss = 2.0511 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1769: loss = 2.7297 (0.722 sec/step)\n",
            "I1212 08:51:15.156505 140053320693632 learning.py:507] global step 1769: loss = 2.7297 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1770: loss = 2.1772 (0.604 sec/step)\n",
            "I1212 08:51:15.893483 140053320693632 learning.py:507] global step 1770: loss = 2.1772 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 1771: loss = 2.7661 (1.744 sec/step)\n",
            "I1212 08:51:17.641092 140053320693632 learning.py:507] global step 1771: loss = 2.7661 (1.744 sec/step)\n",
            "INFO:tensorflow:global step 1772: loss = 2.7820 (0.621 sec/step)\n",
            "I1212 08:51:18.264298 140053320693632 learning.py:507] global step 1772: loss = 2.7820 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1773: loss = 2.2455 (1.761 sec/step)\n",
            "I1212 08:51:20.026740 140053320693632 learning.py:507] global step 1773: loss = 2.2455 (1.761 sec/step)\n",
            "INFO:tensorflow:global step 1774: loss = 2.6756 (0.618 sec/step)\n",
            "I1212 08:51:20.646076 140053320693632 learning.py:507] global step 1774: loss = 2.6756 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1775: loss = 2.0178 (0.954 sec/step)\n",
            "I1212 08:51:21.610680 140053320693632 learning.py:507] global step 1775: loss = 2.0178 (0.954 sec/step)\n",
            "INFO:tensorflow:global step 1776: loss = 2.1692 (2.419 sec/step)\n",
            "I1212 08:51:24.091146 140053320693632 learning.py:507] global step 1776: loss = 2.1692 (2.419 sec/step)\n",
            "INFO:tensorflow:global step 1777: loss = 1.7018 (0.601 sec/step)\n",
            "I1212 08:51:24.694034 140053320693632 learning.py:507] global step 1777: loss = 1.7018 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1778: loss = 2.0299 (2.199 sec/step)\n",
            "I1212 08:51:26.894962 140053320693632 learning.py:507] global step 1778: loss = 2.0299 (2.199 sec/step)\n",
            "INFO:tensorflow:global step 1779: loss = 2.4300 (0.601 sec/step)\n",
            "I1212 08:51:27.498014 140053320693632 learning.py:507] global step 1779: loss = 2.4300 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1780: loss = 2.0650 (1.174 sec/step)\n",
            "I1212 08:51:28.678970 140053320693632 learning.py:507] global step 1780: loss = 2.0650 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1781: loss = 2.4894 (1.982 sec/step)\n",
            "I1212 08:51:30.662631 140053320693632 learning.py:507] global step 1781: loss = 2.4894 (1.982 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 417, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "f3031f53-de7f-4ae6-8628-520569863354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-1543 --output_directory inference_graph"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1212 08:51:56.335885 139711590365056 module_wrapper.py:139] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1212 08:51:56.343269 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1212 08:51:56.343530 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1212 08:51:56.380418 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1212 08:51:56.407578 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1212 08:51:56.409946 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1212 08:51:58.659924 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1212 08:51:58.670880 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:58.671058 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:58.762690 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:58.854901 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:58.946503 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:59.032438 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1212 08:51:59.119653 139711590365056 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1212 08:51:59.418380 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1212 08:51:59.729245 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W1212 08:51:59.729526 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1212 08:51:59.732531 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W1212 08:51:59.732719 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W1212 08:51:59.733704 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/2.99m params)\n",
            "  BoxPredictor_0 (--/20.75k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/69.16k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/27.68k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/13.86k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/13.86k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/6.95k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/13.71k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1212 08:52:00.777700 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1212 08:52:01.696440 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-12-12 08:52:01.712291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-12 08:52:01.757457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:01.758000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 08:52:01.761299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:52:01.786283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:52:01.883880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 08:52:01.896003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 08:52:01.922121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 08:52:01.928128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 08:52:01.998281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:52:01.998518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:01.999102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:01.999636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 08:52:02.000236: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-12 08:52:02.015601: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000185000 Hz\n",
            "2019-12-12 08:52:02.017758: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x12c5100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 08:52:02.017803: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-12 08:52:02.149710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.150382: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x12c5640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 08:52:02.150409: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-12 08:52:02.150643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.151149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 08:52:02.151226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:52:02.151250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:52:02.151274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 08:52:02.151295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 08:52:02.151316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 08:52:02.151339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 08:52:02.151382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:52:02.151451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.151983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.152476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 08:52:02.155550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:52:02.157805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 08:52:02.157845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 08:52:02.157857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 08:52:02.158131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.158711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:02.159238: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-12 08:52:02.159284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1543\n",
            "I1212 08:52:02.162418 139711590365056 saver.py:1284] Restoring parameters from training/model.ckpt-1543\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W1212 08:52:03.432337 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-12-12 08:52:04.113570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:04.114145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 08:52:04.114232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:52:04.114257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:52:04.114279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 08:52:04.114300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 08:52:04.114321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 08:52:04.114340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 08:52:04.114378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:52:04.114462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:04.114998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:04.115486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 08:52:04.115527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 08:52:04.115541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 08:52:04.115550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 08:52:04.115651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:04.116294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:04.116875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1543\n",
            "I1212 08:52:04.118066 139711590365056 saver.py:1284] Restoring parameters from training/model.ckpt-1543\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W1212 08:52:06.918248 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W1212 08:52:06.918585 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 404 variables.\n",
            "I1212 08:52:07.334770 139711590365056 graph_util_impl.py:334] Froze 404 variables.\n",
            "INFO:tensorflow:Converted 404 variables to const ops.\n",
            "I1212 08:52:07.405872 139711590365056 graph_util_impl.py:394] Converted 404 variables to const ops.\n",
            "2019-12-12 08:52:07.534681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:07.535497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 08:52:07.535644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 08:52:07.535664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 08:52:07.535678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 08:52:07.535694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 08:52:07.535711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 08:52:07.535725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 08:52:07.535740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 08:52:07.535816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:07.536339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:07.536895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 08:52:07.536937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 08:52:07.536948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 08:52:07.536956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 08:52:07.537046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:07.537574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 08:52:07.538046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W1212 08:52:07.889837 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W1212 08:52:07.892042 139711590365056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W1212 08:52:07.892559 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W1212 08:52:07.892748 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W1212 08:52:07.892962 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W1212 08:52:07.893122 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I1212 08:52:07.893413 139711590365056 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I1212 08:52:07.893522 139711590365056 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "I1212 08:52:08.123339 139711590365056 builder_impl.py:425] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1212 08:52:08.146415 139711590365056 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to inference_graph/pipeline.config\n",
            "I1212 08:52:08.146641 139711590365056 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -r /content/models/research/object_detection/inference_graph /gdrive/My\\ Drive/colabfiles/inference_graphs/ssdliete_mobilenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasenbilder190726_01/bild101.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8-djxitLZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasen.mov /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMnwzxh-MLlQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "77378a8f-3612-4cee-d865-5bd0318461fd"
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = 'bild101.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEsCAIAAABi1XKVAAEAAElEQVR4nFT923IkSZIlCDLLTVVN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\nTAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x300 at 0x7F2E800D2C88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs2cfgcr1AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "dae9d109-215d-49d6-c3eb-5f160718d02e"
      },
      "source": [
        "######## Video Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/16/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on a video.\n",
        "# It draws boxes and scores around the objects of interest in each frame\n",
        "# of the video.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import imutils\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "VIDEO_NAME = 'rasen.mov'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "print(CWD_PATH)\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to video\n",
        "PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Open video file\n",
        "video = cv2.VideoCapture(PATH_TO_VIDEO)\n",
        "print(\"ok\")\n",
        "i = 0\n",
        "while(video.isOpened()):\n",
        "\n",
        "    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]\n",
        "    # i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "    ret, frame = video.read()\n",
        "    if ret==True:\n",
        "        frame = cv2.imread(\"/gdrive/My Drive/colabfiles/lektion20/DSC01019.JPG\")\n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={image_tensor: frame_expanded})\n",
        "\n",
        "        # Draw the results of the detection (aka 'visulaize the results')\n",
        "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            frame,\n",
        "            np.squeeze(boxes),\n",
        "            np.squeeze(classes).astype(np.int32),\n",
        "            np.squeeze(scores),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.70)\n",
        "\n",
        "        # All the results have been drawn on the frame, so it's time to display it.\n",
        "        \n",
        "        frame = imutils.resize(frame, 400)\n",
        "        cv2_imshow(frame)\n",
        "        #print(boxes)\n",
        "        time.sleep(1)\n",
        "        clear_output()\n",
        "        \n",
        "\n",
        "        # Press 'q' to quit\n",
        "        #print(i)\n",
        "        #i = i + 1\n",
        "        #if cv2.waitKey(1) == ord('q'):\n",
        "        #    break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Clean up\n",
        "print(\"end1\")\n",
        "#video.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(\"end2\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-941983e13ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/My Drive/colabfiles/lektion20/DSC01019.JPG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mframe_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}