{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/191207ssd_mobilenet_objectdetectionclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "06a53fa4-befa-494f-fba9-33ffd7674c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "3118c3da-dc73-4afc-dd01-920ffd51675c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 33138, done.\u001b[K\n",
            "remote: Total 33138 (delta 0), reused 0 (delta 0), pack-reused 33138\u001b[K\n",
            "Receiving objects: 100% (33138/33138), 511.87 MiB | 39.32 MiB/s, done.\n",
            "Resolving deltas: 100% (21132/21132), done.\n",
            "Checking out files: 100% (3188/3188), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "e4b0a0e4-85c5-4318-8d6d-ee4f8469f579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "43167bcf-22a3-4337-e2c8-3500d89bcf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 23:24:40--  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.142.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.142.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76541073 (73M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v1_coco_2018_01_28.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v1_co 100%[===================>]  73.00M   108MB/s    in 0.7s    \n",
            "\n",
            "2019-12-07 23:24:41 (108 MB/s) - ‘ssd_mobilenet_v1_coco_2018_01_28.tar.gz’ saved [76541073/76541073]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf ssd_mobilenet_v1_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "1268a77e-0d9a-4a25-c01e-b36fd144cb8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "!git clone https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10'...\n",
            "remote: Enumerating objects: 1129, done.\u001b[K\n",
            "remote: Total 1129 (delta 0), reused 0 (delta 0), pack-reused 1129\u001b[K\n",
            "Receiving objects: 100% (1129/1129), 57.63 MiB | 29.95 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/doc /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/inference_graph /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/training /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/translate /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_webcam.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/resizer.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test.mov /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test1.JPG /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/README.md /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "ab1e20cd-0bb7-4690-f7c8-2e2ca84c43ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images\n",
        "!mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6n8GqZS-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98GmqrIhe3z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm /content/models/research/object_detection/images/test_labels.csv\n",
        "#!rm /content/models/research/object_detection/images/train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python3 xml_to_csv.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv99g_MdfhZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/models/research/object_detection/generate_tfrecord.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab_type": "code",
        "outputId": "6d0b20a0-c37b-441b-ddba-4cd08c0e477d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "80bd4963-cc1b-4378-cb56-5f55d95d5297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1207 23:28:12.094268 140651023808384 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1207 23:28:12.145855 140651023808384 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "33cfb1fd-ec7e-45fb-fd9d-a65da59870d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1207 23:28:19.042488 140488359745408 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1207 23:28:19.082136 140488359745408 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "75d760d5-b5d8-4480-d444-8962c4e605d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiYCKlkF2eOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat /content/models/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZVlxOO3Cyi",
        "colab_type": "code",
        "outputId": "7400a080-2053-485b-82c4-639df0fd2200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile training/ssd_mobilenet_v1_coco.config\n",
        "\n",
        "# SSD with Mobilenet v1 configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v1'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 0\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\"\n",
        "  from_detection_checkpoint: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/ssd_mobilenet_v1_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "0a3dd717-1cce-4819-bcd1-01d75e569f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "d6c39c68-2feb-4c06-9864-aba472b68aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W1207 23:29:14.991028 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "WARNING:tensorflow:From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1207 23:29:14.991252 140583164200832 module_wrapper.py:139] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1207 23:29:14.991514 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W1207 23:29:14.994497 140583164200832 module_wrapper.py:139] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W1207 23:29:15.004926 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W1207 23:29:15.008469 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W1207 23:29:15.008696 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W1207 23:29:15.020259 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W1207 23:29:15.021316 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1207 23:29:15.021432 140583164200832 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W1207 23:29:15.028124 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W1207 23:29:15.028272 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1207 23:29:15.048802 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W1207 23:29:15.572076 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1207 23:29:15.582325 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W1207 23:29:15.590623 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1207 23:29:15.647789 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1207 23:29:15.659014 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W1207 23:29:16.248506 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1207 23:29:16.251934 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1207 23:29:16.252990 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W1207 23:29:16.257300 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W1207 23:29:16.260221 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W1207 23:29:16.262761 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1207 23:29:16.263175 140583164200832 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W1207 23:29:16.263303 140583164200832 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1207 23:29:17.016567 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1207 23:29:17.273971 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1207 23:29:18.926763 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1207 23:29:18.936601 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:18.936787 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:18.965787 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:18.994210 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:19.022711 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:19.049936 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1207 23:29:19.077166 140583164200832 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W1207 23:29:19.253855 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W1207 23:29:22.759475 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W1207 23:29:22.760693 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W1207 23:29:22.761880 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W1207 23:29:23.279055 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W1207 23:29:23.279674 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W1207 23:29:23.279918 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W1207 23:29:23.287727 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "W1207 23:29:24.998958 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W1207 23:29:25.000404 140583164200832 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W1207 23:29:26.057116 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W1207 23:29:28.298671 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W1207 23:29:28.467955 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W1207 23:29:28.470050 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W1207 23:29:28.472971 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1207 23:29:28.477374 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1207 23:29:28.477610 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W1207 23:29:29.035237 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1207 23:29:29.036754 140583164200832 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1207 23:29:29.038649 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.038771 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.038872 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.038937 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.038999 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039052 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039116 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.039167 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039221 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039276 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.039332 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039383 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039436 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.039486 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039534 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039593 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.039644 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039695 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039751 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.039838 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.039894 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.039962 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040011 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040056 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040112 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040161 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040210 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040260 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040306 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040352 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040415 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040465 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040510 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040568 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040617 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040665 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040715 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040761 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040825 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.040879 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.040925 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.040971 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041026 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041077 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041125 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041174 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041220 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041265 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041314 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041361 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041424 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041516 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041572 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041623 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041675 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041724 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041788 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.041857 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.041903 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.041966 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042025 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042076 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042127 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042180 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042229 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042277 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042329 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042402 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042456 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042519 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042573 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042627 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042683 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042741 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042824 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.042886 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.042937 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.042986 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.043049 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.043105 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.124212 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.124363 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.124448 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.124528 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.124617 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.124721 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.124832 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.124924 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.125021 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.125088 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.125159 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.125228 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.125295 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.125368 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.125437 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.125503 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.125585 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.125660 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.125730 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.125836 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.125912 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.125979 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.126055 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.126125 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.126191 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.126272 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.126344 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.126411 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.126484 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.126552 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.126618 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.126689 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.126755 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.126848 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.126944 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.127011 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.127076 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.127146 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.127246 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.127329 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.127403 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.127473 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.127549 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.127626 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.127691 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.127754 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.127846 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.127937 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.128014 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.128085 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.128175 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.128243 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.128449 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.128570 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.128674 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.128760 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.128875 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.128954 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.129036 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.129114 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.129189 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.129278 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.129361 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.129435 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.129513 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.129587 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.129668 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.129754 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.129858 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.129953 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.130059 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.130136 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.130210 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.130290 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.130386 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.130464 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.130548 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.130636 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.130714 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.130822 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.130910 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.130989 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.131071 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.131160 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.131247 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.131340 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.131445 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.131515 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.131601 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.131690 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.131763 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.131857 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.131932 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.132004 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.132081 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.132155 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.132225 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.132310 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.132384 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.132456 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.132533 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.132614 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.132687 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.132767 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.132858 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.132931 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.133024 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.133094 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.133162 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.133237 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.133324 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.133403 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.133475 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.133543 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.133618 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.133702 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.133769 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.133854 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.133932 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.134002 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.134068 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.134141 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.134208 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.134275 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.134356 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.134426 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.134494 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.134568 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.134662 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.134736 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.134829 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.134907 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.134979 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.135064 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.135140 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.135210 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.135287 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.135359 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.135430 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.135508 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.135581 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.135663 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.135751 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.135839 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.135915 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.135993 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.136066 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.136137 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.136214 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.136286 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.136357 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.136443 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.136518 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.136589 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.136698 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.136786 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.136917 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.136996 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.137069 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.137142 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.137229 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.137304 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.137374 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.137451 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.137526 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.137598 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.137695 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.137763 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.137865 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.137981 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.138055 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.138126 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.138204 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.138277 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.138346 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.138423 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.138495 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.138567 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.138679 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.138771 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.138859 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.138940 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.139013 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.139084 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.139169 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.139237 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.139304 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.139386 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.139456 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.139522 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.139623 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.139698 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.139769 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.139865 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.139938 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.140019 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.140102 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.140174 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.140240 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.140311 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.140394 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.140464 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.140538 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.140614 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.140683 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.140767 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.140855 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.140923 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.140997 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.141066 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.141133 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.141208 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.141276 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.141341 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.141422 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.141493 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.141559 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.141641 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.141710 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.141777 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.141868 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.141938 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.142004 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.142086 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.142158 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.142224 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.142334 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.142409 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.142481 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.142559 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.142640 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.142733 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1207 23:29:29.142859 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1207 23:29:29.142970 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W1207 23:29:29.143052 140583164200832 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "W1207 23:29:29.646950 140583164200832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-12-07 23:29:30.586660: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-12-07 23:29:30.588582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b21dc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-07 23:29:30.588619: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-07 23:29:30.592672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-07 23:29:30.756111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:30.756920: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b21f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-07 23:29:30.756955: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-07 23:29:30.757971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:30.758586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-07 23:29:30.769293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-07 23:29:30.975154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-07 23:29:31.094756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-07 23:29:31.116579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-07 23:29:31.325340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-07 23:29:31.343633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-07 23:29:31.756521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-07 23:29:31.756721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:31.757474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:31.758044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-07 23:29:31.761749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-07 23:29:31.763414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-07 23:29:31.763451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-07 23:29:31.763466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-07 23:29:31.764314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:31.764951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-07 23:29:31.765480: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-07 23:29:31.765521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from /content/models/research/object_detection/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\n",
            "I1207 23:29:34.018923 140583164200832 saver.py:1284] Restoring parameters from /content/models/research/object_detection/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I1207 23:29:34.486914 140583164200832 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I1207 23:29:34.935035 140583164200832 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "I1207 23:29:41.757967 140583164200832 learning.py:754] Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1207 23:29:42.075707 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "I1207 23:29:42.082161 140583164200832 learning.py:768] Starting Queues.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "I1207 23:29:45.928439 140579467605760 supervisor.py:1099] global_step/sec: 0\n",
            "2019-12-07 23:29:56.784263: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-07 23:29:58.149938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-07 23:29:58.963456: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-07 23:30:01.524204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-07 23:30:02.828832: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-07 23:30:03.741920: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-07 23:30:04.907425: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "I1207 23:30:05.412285 140579459213056 supervisor.py:1050] Recording summary at step 0.\n",
            "INFO:tensorflow:global step 1: loss = 15.3230 (23.400 sec/step)\n",
            "I1207 23:30:06.134819 140583164200832 learning.py:507] global step 1: loss = 15.3230 (23.400 sec/step)\n",
            "INFO:tensorflow:global step 2: loss = 13.0897 (2.421 sec/step)\n",
            "I1207 23:30:09.681089 140583164200832 learning.py:507] global step 2: loss = 13.0897 (2.421 sec/step)\n",
            "INFO:tensorflow:global step 3: loss = 11.9091 (2.614 sec/step)\n",
            "I1207 23:30:12.639022 140583164200832 learning.py:507] global step 3: loss = 11.9091 (2.614 sec/step)\n",
            "INFO:tensorflow:global step 4: loss = 10.7497 (1.984 sec/step)\n",
            "I1207 23:30:15.221393 140583164200832 learning.py:507] global step 4: loss = 10.7497 (1.984 sec/step)\n",
            "INFO:tensorflow:global step 5: loss = 10.4342 (1.781 sec/step)\n",
            "I1207 23:30:17.165440 140583164200832 learning.py:507] global step 5: loss = 10.4342 (1.781 sec/step)\n",
            "INFO:tensorflow:global step 6: loss = 9.8513 (0.821 sec/step)\n",
            "I1207 23:30:18.171176 140583164200832 learning.py:507] global step 6: loss = 9.8513 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 7: loss = 8.8116 (0.632 sec/step)\n",
            "I1207 23:30:19.085504 140583164200832 learning.py:507] global step 7: loss = 8.8116 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 8: loss = 9.3179 (0.633 sec/step)\n",
            "I1207 23:30:19.958044 140583164200832 learning.py:507] global step 8: loss = 9.3179 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 9: loss = 8.7727 (1.796 sec/step)\n",
            "I1207 23:30:21.755670 140583164200832 learning.py:507] global step 9: loss = 8.7727 (1.796 sec/step)\n",
            "INFO:tensorflow:global step 10: loss = 8.4038 (0.672 sec/step)\n",
            "I1207 23:30:22.430080 140583164200832 learning.py:507] global step 10: loss = 8.4038 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 11: loss = 8.4942 (1.727 sec/step)\n",
            "I1207 23:30:24.158578 140583164200832 learning.py:507] global step 11: loss = 8.4942 (1.727 sec/step)\n",
            "INFO:tensorflow:global step 12: loss = 7.6324 (0.532 sec/step)\n",
            "I1207 23:30:24.692062 140583164200832 learning.py:507] global step 12: loss = 7.6324 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 13: loss = 7.3857 (1.793 sec/step)\n",
            "I1207 23:30:26.486730 140583164200832 learning.py:507] global step 13: loss = 7.3857 (1.793 sec/step)\n",
            "INFO:tensorflow:global step 14: loss = 7.5538 (0.634 sec/step)\n",
            "I1207 23:30:27.122906 140583164200832 learning.py:507] global step 14: loss = 7.5538 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 15: loss = 6.9026 (1.625 sec/step)\n",
            "I1207 23:30:28.850673 140583164200832 learning.py:507] global step 15: loss = 6.9026 (1.625 sec/step)\n",
            "INFO:tensorflow:global step 16: loss = 7.1344 (1.263 sec/step)\n",
            "I1207 23:30:30.153494 140583164200832 learning.py:507] global step 16: loss = 7.1344 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 17: loss = 6.8601 (1.111 sec/step)\n",
            "I1207 23:30:31.265870 140583164200832 learning.py:507] global step 17: loss = 6.8601 (1.111 sec/step)\n",
            "INFO:tensorflow:global step 18: loss = 7.2268 (1.088 sec/step)\n",
            "I1207 23:30:32.355354 140583164200832 learning.py:507] global step 18: loss = 7.2268 (1.088 sec/step)\n",
            "INFO:tensorflow:global step 19: loss = 6.8596 (1.105 sec/step)\n",
            "I1207 23:30:33.461712 140583164200832 learning.py:507] global step 19: loss = 6.8596 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 20: loss = 6.7755 (1.090 sec/step)\n",
            "I1207 23:30:34.552966 140583164200832 learning.py:507] global step 20: loss = 6.7755 (1.090 sec/step)\n",
            "INFO:tensorflow:global step 21: loss = 6.3932 (1.048 sec/step)\n",
            "I1207 23:30:35.602633 140583164200832 learning.py:507] global step 21: loss = 6.3932 (1.048 sec/step)\n",
            "INFO:tensorflow:global step 22: loss = 6.1279 (0.486 sec/step)\n",
            "I1207 23:30:36.090151 140583164200832 learning.py:507] global step 22: loss = 6.1279 (0.486 sec/step)\n",
            "INFO:tensorflow:global step 23: loss = 6.5151 (1.810 sec/step)\n",
            "I1207 23:30:37.902250 140583164200832 learning.py:507] global step 23: loss = 6.5151 (1.810 sec/step)\n",
            "INFO:tensorflow:global step 24: loss = 5.9527 (0.590 sec/step)\n",
            "I1207 23:30:38.494010 140583164200832 learning.py:507] global step 24: loss = 5.9527 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 25: loss = 6.7365 (1.761 sec/step)\n",
            "I1207 23:30:40.257067 140583164200832 learning.py:507] global step 25: loss = 6.7365 (1.761 sec/step)\n",
            "INFO:tensorflow:global step 26: loss = 6.2137 (0.614 sec/step)\n",
            "I1207 23:30:41.043282 140583164200832 learning.py:507] global step 26: loss = 6.2137 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 27: loss = 6.2039 (1.254 sec/step)\n",
            "I1207 23:30:42.671961 140583164200832 learning.py:507] global step 27: loss = 6.2039 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 28: loss = 6.6865 (1.100 sec/step)\n",
            "I1207 23:30:43.779828 140583164200832 learning.py:507] global step 28: loss = 6.6865 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 29: loss = 5.6566 (1.149 sec/step)\n",
            "I1207 23:30:44.930070 140583164200832 learning.py:507] global step 29: loss = 5.6566 (1.149 sec/step)\n",
            "INFO:tensorflow:global step 30: loss = 5.6378 (1.051 sec/step)\n",
            "I1207 23:30:45.982754 140583164200832 learning.py:507] global step 30: loss = 5.6378 (1.051 sec/step)\n",
            "INFO:tensorflow:global step 31: loss = 5.4081 (1.085 sec/step)\n",
            "I1207 23:30:47.069644 140583164200832 learning.py:507] global step 31: loss = 5.4081 (1.085 sec/step)\n",
            "INFO:tensorflow:global step 32: loss = 5.6314 (1.088 sec/step)\n",
            "I1207 23:30:48.158792 140583164200832 learning.py:507] global step 32: loss = 5.6314 (1.088 sec/step)\n",
            "INFO:tensorflow:global step 33: loss = 4.9440 (0.573 sec/step)\n",
            "I1207 23:30:48.974060 140583164200832 learning.py:507] global step 33: loss = 4.9440 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 34: loss = 5.2671 (0.608 sec/step)\n",
            "I1207 23:30:50.038195 140583164200832 learning.py:507] global step 34: loss = 5.2671 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 35: loss = 5.1725 (1.189 sec/step)\n",
            "I1207 23:30:51.307289 140583164200832 learning.py:507] global step 35: loss = 5.1725 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 36: loss = 5.8698 (0.713 sec/step)\n",
            "I1207 23:30:52.164605 140583164200832 learning.py:507] global step 36: loss = 5.8698 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 37: loss = 5.2567 (1.157 sec/step)\n",
            "I1207 23:30:53.423547 140583164200832 learning.py:507] global step 37: loss = 5.2567 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 38: loss = 5.1292 (1.036 sec/step)\n",
            "I1207 23:30:54.461167 140583164200832 learning.py:507] global step 38: loss = 5.1292 (1.036 sec/step)\n",
            "INFO:tensorflow:global step 39: loss = 5.2486 (0.616 sec/step)\n",
            "I1207 23:30:55.344856 140583164200832 learning.py:507] global step 39: loss = 5.2486 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 40: loss = 5.4167 (1.197 sec/step)\n",
            "I1207 23:30:56.584831 140583164200832 learning.py:507] global step 40: loss = 5.4167 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 41: loss = 4.5983 (0.577 sec/step)\n",
            "I1207 23:30:57.466563 140583164200832 learning.py:507] global step 41: loss = 4.5983 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 42: loss = 4.3639 (1.055 sec/step)\n",
            "I1207 23:30:58.621874 140583164200832 learning.py:507] global step 42: loss = 4.3639 (1.055 sec/step)\n",
            "INFO:tensorflow:global step 43: loss = 5.0322 (1.002 sec/step)\n",
            "I1207 23:30:59.625073 140583164200832 learning.py:507] global step 43: loss = 5.0322 (1.002 sec/step)\n",
            "INFO:tensorflow:global step 44: loss = 4.6389 (0.574 sec/step)\n",
            "I1207 23:31:00.396390 140583164200832 learning.py:507] global step 44: loss = 4.6389 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 45: loss = 4.4960 (1.287 sec/step)\n",
            "I1207 23:31:01.832782 140583164200832 learning.py:507] global step 45: loss = 4.4960 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 46: loss = 4.5953 (1.084 sec/step)\n",
            "I1207 23:31:02.918545 140583164200832 learning.py:507] global step 46: loss = 4.5953 (1.084 sec/step)\n",
            "INFO:tensorflow:global step 47: loss = 4.6186 (1.044 sec/step)\n",
            "I1207 23:31:03.964210 140583164200832 learning.py:507] global step 47: loss = 4.6186 (1.044 sec/step)\n",
            "INFO:tensorflow:global step 48: loss = 4.5354 (0.571 sec/step)\n",
            "I1207 23:31:04.829920 140583164200832 learning.py:507] global step 48: loss = 4.5354 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 49: loss = 5.0456 (0.668 sec/step)\n",
            "I1207 23:31:05.702614 140583164200832 learning.py:507] global step 49: loss = 5.0456 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 50: loss = 4.4422 (1.186 sec/step)\n",
            "I1207 23:31:07.086393 140583164200832 learning.py:507] global step 50: loss = 4.4422 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 51: loss = 4.8676 (1.043 sec/step)\n",
            "I1207 23:31:08.131783 140583164200832 learning.py:507] global step 51: loss = 4.8676 (1.043 sec/step)\n",
            "INFO:tensorflow:global step 52: loss = 4.2541 (0.582 sec/step)\n",
            "I1207 23:31:08.715798 140583164200832 learning.py:507] global step 52: loss = 4.2541 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 53: loss = 5.1016 (1.025 sec/step)\n",
            "I1207 23:31:09.978306 140583164200832 learning.py:507] global step 53: loss = 5.1016 (1.025 sec/step)\n",
            "INFO:tensorflow:global step 54: loss = 4.7657 (1.383 sec/step)\n",
            "I1207 23:31:11.692916 140583164200832 learning.py:507] global step 54: loss = 4.7657 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 55: loss = 4.4360 (0.659 sec/step)\n",
            "I1207 23:31:12.353898 140583164200832 learning.py:507] global step 55: loss = 4.4360 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 56: loss = 5.8552 (2.250 sec/step)\n",
            "I1207 23:31:14.607108 140583164200832 learning.py:507] global step 56: loss = 5.8552 (2.250 sec/step)\n",
            "INFO:tensorflow:global step 57: loss = 4.2620 (0.657 sec/step)\n",
            "I1207 23:31:15.510065 140583164200832 learning.py:507] global step 57: loss = 4.2620 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 58: loss = 4.6523 (0.605 sec/step)\n",
            "I1207 23:31:16.224460 140583164200832 learning.py:507] global step 58: loss = 4.6523 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 59: loss = 4.3381 (0.828 sec/step)\n",
            "I1207 23:31:17.071024 140583164200832 learning.py:507] global step 59: loss = 4.3381 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 60: loss = 4.6894 (1.937 sec/step)\n",
            "I1207 23:31:19.009406 140583164200832 learning.py:507] global step 60: loss = 4.6894 (1.937 sec/step)\n",
            "INFO:tensorflow:global step 61: loss = 4.8023 (0.516 sec/step)\n",
            "I1207 23:31:19.913140 140583164200832 learning.py:507] global step 61: loss = 4.8023 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 62: loss = 3.9218 (0.609 sec/step)\n",
            "I1207 23:31:20.676090 140583164200832 learning.py:507] global step 62: loss = 3.9218 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 63: loss = 5.3006 (1.772 sec/step)\n",
            "I1207 23:31:22.450321 140583164200832 learning.py:507] global step 63: loss = 5.3006 (1.772 sec/step)\n",
            "INFO:tensorflow:global step 64: loss = 4.2522 (0.698 sec/step)\n",
            "I1207 23:31:23.150187 140583164200832 learning.py:507] global step 64: loss = 4.2522 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 65: loss = 4.5024 (0.759 sec/step)\n",
            "I1207 23:31:24.128421 140583164200832 learning.py:507] global step 65: loss = 4.5024 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 66: loss = 5.2066 (1.916 sec/step)\n",
            "I1207 23:31:26.307947 140583164200832 learning.py:507] global step 66: loss = 5.2066 (1.916 sec/step)\n",
            "INFO:tensorflow:global step 67: loss = 4.4725 (1.176 sec/step)\n",
            "I1207 23:31:27.485744 140583164200832 learning.py:507] global step 67: loss = 4.4725 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 68: loss = 4.6603 (0.610 sec/step)\n",
            "I1207 23:31:28.303239 140583164200832 learning.py:507] global step 68: loss = 4.6603 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 69: loss = 3.9843 (0.599 sec/step)\n",
            "I1207 23:31:29.072804 140583164200832 learning.py:507] global step 69: loss = 3.9843 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 70: loss = 4.3287 (1.389 sec/step)\n",
            "I1207 23:31:30.891593 140583164200832 learning.py:507] global step 70: loss = 4.3287 (1.389 sec/step)\n",
            "INFO:tensorflow:global step 71: loss = 4.1751 (0.618 sec/step)\n",
            "I1207 23:31:31.540664 140583164200832 learning.py:507] global step 71: loss = 4.1751 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 72: loss = 4.3438 (1.734 sec/step)\n",
            "I1207 23:31:33.276730 140583164200832 learning.py:507] global step 72: loss = 4.3438 (1.734 sec/step)\n",
            "INFO:tensorflow:global step 73: loss = 3.9897 (1.221 sec/step)\n",
            "I1207 23:31:34.499071 140583164200832 learning.py:507] global step 73: loss = 3.9897 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 74: loss = 4.5781 (0.586 sec/step)\n",
            "I1207 23:31:35.385754 140583164200832 learning.py:507] global step 74: loss = 4.5781 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 75: loss = 4.7227 (0.808 sec/step)\n",
            "I1207 23:31:36.363712 140583164200832 learning.py:507] global step 75: loss = 4.7227 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 76: loss = 4.2558 (1.655 sec/step)\n",
            "I1207 23:31:38.265016 140583164200832 learning.py:507] global step 76: loss = 4.2558 (1.655 sec/step)\n",
            "INFO:tensorflow:global step 77: loss = 4.2680 (0.640 sec/step)\n",
            "I1207 23:31:38.969096 140583164200832 learning.py:507] global step 77: loss = 4.2680 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 78: loss = 4.2137 (1.415 sec/step)\n",
            "I1207 23:31:40.587892 140583164200832 learning.py:507] global step 78: loss = 4.2137 (1.415 sec/step)\n",
            "INFO:tensorflow:global step 79: loss = 4.2982 (1.020 sec/step)\n",
            "I1207 23:31:41.708319 140583164200832 learning.py:507] global step 79: loss = 4.2982 (1.020 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.680168\n",
            "I1207 23:31:42.076340 140579467605760 supervisor.py:1099] global_step/sec: 0.680168\n",
            "INFO:tensorflow:Recording summary at step 80.\n",
            "I1207 23:31:44.552581 140579459213056 supervisor.py:1050] Recording summary at step 80.\n",
            "INFO:tensorflow:global step 80: loss = 3.9070 (2.726 sec/step)\n",
            "I1207 23:31:44.558590 140583164200832 learning.py:507] global step 80: loss = 3.9070 (2.726 sec/step)\n",
            "INFO:tensorflow:global step 81: loss = 3.9255 (1.186 sec/step)\n",
            "I1207 23:31:45.746672 140583164200832 learning.py:507] global step 81: loss = 3.9255 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 82: loss = 4.2803 (0.669 sec/step)\n",
            "I1207 23:31:46.747945 140583164200832 learning.py:507] global step 82: loss = 4.2803 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 83: loss = 4.4581 (1.527 sec/step)\n",
            "I1207 23:31:48.456511 140583164200832 learning.py:507] global step 83: loss = 4.4581 (1.527 sec/step)\n",
            "INFO:tensorflow:global step 84: loss = 4.0059 (0.770 sec/step)\n",
            "I1207 23:31:49.472911 140583164200832 learning.py:507] global step 84: loss = 4.0059 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 85: loss = 4.2187 (0.767 sec/step)\n",
            "I1207 23:31:50.666123 140583164200832 learning.py:507] global step 85: loss = 4.2187 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 86: loss = 4.3937 (1.516 sec/step)\n",
            "I1207 23:31:52.388084 140583164200832 learning.py:507] global step 86: loss = 4.3937 (1.516 sec/step)\n",
            "INFO:tensorflow:global step 87: loss = 3.7236 (0.674 sec/step)\n",
            "I1207 23:31:53.207047 140583164200832 learning.py:507] global step 87: loss = 3.7236 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 88: loss = 4.4315 (1.984 sec/step)\n",
            "I1207 23:31:55.295203 140583164200832 learning.py:507] global step 88: loss = 4.4315 (1.984 sec/step)\n",
            "INFO:tensorflow:global step 89: loss = 3.9474 (0.862 sec/step)\n",
            "I1207 23:31:56.403513 140583164200832 learning.py:507] global step 89: loss = 3.9474 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 90: loss = 4.0159 (0.732 sec/step)\n",
            "I1207 23:31:57.367462 140583164200832 learning.py:507] global step 90: loss = 4.0159 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 91: loss = 3.3135 (0.832 sec/step)\n",
            "I1207 23:31:58.997958 140583164200832 learning.py:507] global step 91: loss = 3.3135 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 92: loss = 3.4869 (0.824 sec/step)\n",
            "I1207 23:32:00.079620 140583164200832 learning.py:507] global step 92: loss = 3.4869 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 93: loss = 3.8385 (1.478 sec/step)\n",
            "I1207 23:32:01.569505 140583164200832 learning.py:507] global step 93: loss = 3.8385 (1.478 sec/step)\n",
            "INFO:tensorflow:global step 94: loss = 3.7729 (1.142 sec/step)\n",
            "I1207 23:32:02.713114 140583164200832 learning.py:507] global step 94: loss = 3.7729 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 95: loss = 4.3421 (0.753 sec/step)\n",
            "I1207 23:32:03.698088 140583164200832 learning.py:507] global step 95: loss = 4.3421 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 96: loss = 3.8933 (0.689 sec/step)\n",
            "I1207 23:32:04.980049 140583164200832 learning.py:507] global step 96: loss = 3.8933 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 97: loss = 3.5601 (1.410 sec/step)\n",
            "I1207 23:32:06.398281 140583164200832 learning.py:507] global step 97: loss = 3.5601 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 98: loss = 4.0602 (0.782 sec/step)\n",
            "I1207 23:32:07.182721 140583164200832 learning.py:507] global step 98: loss = 4.0602 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 99: loss = 3.6111 (1.811 sec/step)\n",
            "I1207 23:32:08.995545 140583164200832 learning.py:507] global step 99: loss = 3.6111 (1.811 sec/step)\n",
            "INFO:tensorflow:global step 100: loss = 3.4693 (1.278 sec/step)\n",
            "I1207 23:32:10.274627 140583164200832 learning.py:507] global step 100: loss = 3.4693 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 101: loss = 4.1627 (0.687 sec/step)\n",
            "I1207 23:32:11.304203 140583164200832 learning.py:507] global step 101: loss = 4.1627 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 102: loss = 4.4828 (0.725 sec/step)\n",
            "I1207 23:32:12.381099 140583164200832 learning.py:507] global step 102: loss = 4.4828 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 103: loss = 3.4661 (1.242 sec/step)\n",
            "I1207 23:32:13.766103 140583164200832 learning.py:507] global step 103: loss = 3.4661 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 104: loss = 3.5903 (0.673 sec/step)\n",
            "I1207 23:32:14.535371 140583164200832 learning.py:507] global step 104: loss = 3.5903 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 105: loss = 3.2969 (0.778 sec/step)\n",
            "I1207 23:32:15.842216 140583164200832 learning.py:507] global step 105: loss = 3.2969 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 106: loss = 2.9890 (1.064 sec/step)\n",
            "I1207 23:32:16.924579 140583164200832 learning.py:507] global step 106: loss = 2.9890 (1.064 sec/step)\n",
            "INFO:tensorflow:global step 107: loss = 3.7315 (0.567 sec/step)\n",
            "I1207 23:32:17.493412 140583164200832 learning.py:507] global step 107: loss = 3.7315 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 108: loss = 3.8965 (1.659 sec/step)\n",
            "I1207 23:32:19.154863 140583164200832 learning.py:507] global step 108: loss = 3.8965 (1.659 sec/step)\n",
            "INFO:tensorflow:global step 109: loss = 3.2893 (0.757 sec/step)\n",
            "I1207 23:32:20.134533 140583164200832 learning.py:507] global step 109: loss = 3.2893 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 110: loss = 3.9970 (1.159 sec/step)\n",
            "I1207 23:32:21.298288 140583164200832 learning.py:507] global step 110: loss = 3.9970 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 111: loss = 3.8343 (0.560 sec/step)\n",
            "I1207 23:32:22.212513 140583164200832 learning.py:507] global step 111: loss = 3.8343 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 112: loss = 4.3968 (0.625 sec/step)\n",
            "I1207 23:32:23.138308 140583164200832 learning.py:507] global step 112: loss = 4.3968 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 113: loss = 3.8163 (0.533 sec/step)\n",
            "I1207 23:32:23.932005 140583164200832 learning.py:507] global step 113: loss = 3.8163 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 114: loss = 3.2255 (1.683 sec/step)\n",
            "I1207 23:32:25.616715 140583164200832 learning.py:507] global step 114: loss = 3.2255 (1.683 sec/step)\n",
            "INFO:tensorflow:global step 115: loss = 3.4245 (0.665 sec/step)\n",
            "I1207 23:32:26.464651 140583164200832 learning.py:507] global step 115: loss = 3.4245 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 116: loss = 3.7750 (0.636 sec/step)\n",
            "I1207 23:32:27.335407 140583164200832 learning.py:507] global step 116: loss = 3.7750 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 117: loss = 3.2120 (1.297 sec/step)\n",
            "I1207 23:32:28.831141 140583164200832 learning.py:507] global step 117: loss = 3.2120 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 118: loss = 3.8220 (0.532 sec/step)\n",
            "I1207 23:32:29.763464 140583164200832 learning.py:507] global step 118: loss = 3.8220 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 119: loss = 3.4478 (1.064 sec/step)\n",
            "I1207 23:32:30.879403 140583164200832 learning.py:507] global step 119: loss = 3.4478 (1.064 sec/step)\n",
            "INFO:tensorflow:global step 120: loss = 4.4686 (1.098 sec/step)\n",
            "I1207 23:32:31.979440 140583164200832 learning.py:507] global step 120: loss = 4.4686 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 121: loss = 4.6686 (0.579 sec/step)\n",
            "I1207 23:32:32.876628 140583164200832 learning.py:507] global step 121: loss = 4.6686 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 122: loss = 4.2426 (0.765 sec/step)\n",
            "I1207 23:32:34.116024 140583164200832 learning.py:507] global step 122: loss = 4.2426 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 123: loss = 3.1603 (0.525 sec/step)\n",
            "I1207 23:32:34.735117 140583164200832 learning.py:507] global step 123: loss = 3.1603 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 124: loss = 3.7467 (1.990 sec/step)\n",
            "I1207 23:32:36.727312 140583164200832 learning.py:507] global step 124: loss = 3.7467 (1.990 sec/step)\n",
            "INFO:tensorflow:global step 125: loss = 3.5470 (0.609 sec/step)\n",
            "I1207 23:32:37.605629 140583164200832 learning.py:507] global step 125: loss = 3.5470 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 126: loss = 3.5067 (1.218 sec/step)\n",
            "I1207 23:32:38.964258 140583164200832 learning.py:507] global step 126: loss = 3.5067 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 127: loss = 3.5175 (0.657 sec/step)\n",
            "I1207 23:32:39.916533 140583164200832 learning.py:507] global step 127: loss = 3.5175 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 128: loss = 3.9828 (0.591 sec/step)\n",
            "I1207 23:32:40.924861 140583164200832 learning.py:507] global step 128: loss = 3.9828 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 129: loss = 3.4981 (0.475 sec/step)\n",
            "I1207 23:32:41.525751 140583164200832 learning.py:507] global step 129: loss = 3.4981 (0.475 sec/step)\n",
            "INFO:tensorflow:global step 130: loss = 3.5695 (0.983 sec/step)\n",
            "I1207 23:32:42.558834 140583164200832 learning.py:507] global step 130: loss = 3.5695 (0.983 sec/step)\n",
            "INFO:tensorflow:global step 131: loss = 3.4927 (1.559 sec/step)\n",
            "I1207 23:32:44.279170 140583164200832 learning.py:507] global step 131: loss = 3.4927 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 132: loss = 3.6635 (0.739 sec/step)\n",
            "I1207 23:32:45.141989 140583164200832 learning.py:507] global step 132: loss = 3.6635 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 133: loss = 3.2246 (1.188 sec/step)\n",
            "I1207 23:32:46.369496 140583164200832 learning.py:507] global step 133: loss = 3.2246 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 134: loss = 3.4507 (0.555 sec/step)\n",
            "I1207 23:32:46.926589 140583164200832 learning.py:507] global step 134: loss = 3.4507 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 135: loss = 3.8237 (0.537 sec/step)\n",
            "I1207 23:32:47.686609 140583164200832 learning.py:507] global step 135: loss = 3.8237 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 136: loss = 3.0354 (1.502 sec/step)\n",
            "I1207 23:32:49.268229 140583164200832 learning.py:507] global step 136: loss = 3.0354 (1.502 sec/step)\n",
            "INFO:tensorflow:global step 137: loss = 3.3123 (1.461 sec/step)\n",
            "I1207 23:32:50.776569 140583164200832 learning.py:507] global step 137: loss = 3.3123 (1.461 sec/step)\n",
            "INFO:tensorflow:global step 138: loss = 4.1731 (1.067 sec/step)\n",
            "I1207 23:32:51.844967 140583164200832 learning.py:507] global step 138: loss = 4.1731 (1.067 sec/step)\n",
            "INFO:tensorflow:global step 139: loss = 4.4091 (0.512 sec/step)\n",
            "I1207 23:32:52.358571 140583164200832 learning.py:507] global step 139: loss = 4.4091 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 140: loss = 4.0619 (1.997 sec/step)\n",
            "I1207 23:32:54.357753 140583164200832 learning.py:507] global step 140: loss = 4.0619 (1.997 sec/step)\n",
            "INFO:tensorflow:global step 141: loss = 3.7600 (0.663 sec/step)\n",
            "I1207 23:32:55.373677 140583164200832 learning.py:507] global step 141: loss = 3.7600 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 142: loss = 3.5345 (0.700 sec/step)\n",
            "I1207 23:32:56.169259 140583164200832 learning.py:507] global step 142: loss = 3.5345 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 143: loss = 3.2211 (1.220 sec/step)\n",
            "I1207 23:32:57.619346 140583164200832 learning.py:507] global step 143: loss = 3.2211 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 144: loss = 4.1724 (0.753 sec/step)\n",
            "I1207 23:32:58.610518 140583164200832 learning.py:507] global step 144: loss = 4.1724 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 145: loss = 2.7705 (0.512 sec/step)\n",
            "I1207 23:32:59.255987 140583164200832 learning.py:507] global step 145: loss = 2.7705 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 146: loss = 3.6430 (1.444 sec/step)\n",
            "I1207 23:33:00.759297 140583164200832 learning.py:507] global step 146: loss = 3.6430 (1.444 sec/step)\n",
            "INFO:tensorflow:global step 147: loss = 4.7850 (1.227 sec/step)\n",
            "I1207 23:33:01.988373 140583164200832 learning.py:507] global step 147: loss = 4.7850 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 148: loss = 3.6338 (0.561 sec/step)\n",
            "I1207 23:33:02.776178 140583164200832 learning.py:507] global step 148: loss = 3.6338 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 149: loss = 3.0054 (0.712 sec/step)\n",
            "I1207 23:33:03.786891 140583164200832 learning.py:507] global step 149: loss = 3.0054 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 150: loss = 3.3315 (0.765 sec/step)\n",
            "I1207 23:33:04.900033 140583164200832 learning.py:507] global step 150: loss = 3.3315 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 151: loss = 3.8585 (0.640 sec/step)\n",
            "I1207 23:33:05.958080 140583164200832 learning.py:507] global step 151: loss = 3.8585 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 152: loss = 3.6148 (0.587 sec/step)\n",
            "I1207 23:33:06.846160 140583164200832 learning.py:507] global step 152: loss = 3.6148 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 153: loss = 3.8612 (0.524 sec/step)\n",
            "I1207 23:33:07.571720 140583164200832 learning.py:507] global step 153: loss = 3.8612 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 154: loss = 3.7862 (1.382 sec/step)\n",
            "I1207 23:33:08.995552 140583164200832 learning.py:507] global step 154: loss = 3.7862 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 155: loss = 3.6068 (1.219 sec/step)\n",
            "I1207 23:33:10.224559 140583164200832 learning.py:507] global step 155: loss = 3.6068 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 156: loss = 3.7003 (1.062 sec/step)\n",
            "I1207 23:33:11.288245 140583164200832 learning.py:507] global step 156: loss = 3.7003 (1.062 sec/step)\n",
            "INFO:tensorflow:global step 157: loss = 3.1432 (0.597 sec/step)\n",
            "I1207 23:33:12.051341 140583164200832 learning.py:507] global step 157: loss = 3.1432 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 158: loss = 3.8411 (0.934 sec/step)\n",
            "I1207 23:33:13.242587 140583164200832 learning.py:507] global step 158: loss = 3.8411 (0.934 sec/step)\n",
            "INFO:tensorflow:global step 159: loss = 3.6386 (0.624 sec/step)\n",
            "I1207 23:33:14.179599 140583164200832 learning.py:507] global step 159: loss = 3.6386 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 160: loss = 3.6624 (0.552 sec/step)\n",
            "I1207 23:33:14.823274 140583164200832 learning.py:507] global step 160: loss = 3.6624 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 161: loss = 3.5920 (0.567 sec/step)\n",
            "I1207 23:33:15.392730 140583164200832 learning.py:507] global step 161: loss = 3.5920 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 162: loss = 3.9674 (2.399 sec/step)\n",
            "I1207 23:33:17.811697 140583164200832 learning.py:507] global step 162: loss = 3.9674 (2.399 sec/step)\n",
            "INFO:tensorflow:global step 163: loss = 3.8257 (0.562 sec/step)\n",
            "I1207 23:33:18.566186 140583164200832 learning.py:507] global step 163: loss = 3.8257 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 164: loss = 3.5475 (1.228 sec/step)\n",
            "I1207 23:33:20.027728 140583164200832 learning.py:507] global step 164: loss = 3.5475 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 165: loss = 3.8078 (0.480 sec/step)\n",
            "I1207 23:33:20.747202 140583164200832 learning.py:507] global step 165: loss = 3.8078 (0.480 sec/step)\n",
            "INFO:tensorflow:global step 166: loss = 2.9004 (0.571 sec/step)\n",
            "I1207 23:33:21.549770 140583164200832 learning.py:507] global step 166: loss = 2.9004 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 167: loss = 3.7936 (1.689 sec/step)\n",
            "I1207 23:33:23.240197 140583164200832 learning.py:507] global step 167: loss = 3.7936 (1.689 sec/step)\n",
            "INFO:tensorflow:global step 168: loss = 3.4022 (0.583 sec/step)\n",
            "I1207 23:33:24.078417 140583164200832 learning.py:507] global step 168: loss = 3.4022 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 169: loss = 3.8867 (1.084 sec/step)\n",
            "I1207 23:33:25.304021 140583164200832 learning.py:507] global step 169: loss = 3.8867 (1.084 sec/step)\n",
            "INFO:tensorflow:global step 170: loss = 3.1337 (0.617 sec/step)\n",
            "I1207 23:33:25.975640 140583164200832 learning.py:507] global step 170: loss = 3.1337 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 171: loss = 2.9952 (0.666 sec/step)\n",
            "I1207 23:33:27.004262 140583164200832 learning.py:507] global step 171: loss = 2.9952 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 172: loss = 3.2591 (1.391 sec/step)\n",
            "I1207 23:33:28.507861 140583164200832 learning.py:507] global step 172: loss = 3.2591 (1.391 sec/step)\n",
            "INFO:tensorflow:global step 173: loss = 4.5195 (0.603 sec/step)\n",
            "I1207 23:33:29.283751 140583164200832 learning.py:507] global step 173: loss = 4.5195 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 174: loss = 3.3299 (1.189 sec/step)\n",
            "I1207 23:33:30.610740 140583164200832 learning.py:507] global step 174: loss = 3.3299 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 175: loss = 3.4869 (0.706 sec/step)\n",
            "I1207 23:33:31.558845 140583164200832 learning.py:507] global step 175: loss = 3.4869 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 176: loss = 4.0756 (1.240 sec/step)\n",
            "I1207 23:33:32.848732 140583164200832 learning.py:507] global step 176: loss = 4.0756 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 177: loss = 3.8346 (0.636 sec/step)\n",
            "I1207 23:33:33.740976 140583164200832 learning.py:507] global step 177: loss = 3.8346 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 178: loss = 3.3399 (0.562 sec/step)\n",
            "I1207 23:33:34.673704 140583164200832 learning.py:507] global step 178: loss = 3.3399 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 179: loss = 3.4814 (1.227 sec/step)\n",
            "I1207 23:33:36.038291 140583164200832 learning.py:507] global step 179: loss = 3.4814 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 180: loss = 3.7920 (0.756 sec/step)\n",
            "I1207 23:33:36.989989 140583164200832 learning.py:507] global step 180: loss = 3.7920 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 181: loss = 3.0966 (0.582 sec/step)\n",
            "I1207 23:33:37.610460 140583164200832 learning.py:507] global step 181: loss = 3.0966 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 182: loss = 3.5500 (0.852 sec/step)\n",
            "I1207 23:33:38.668554 140583164200832 learning.py:507] global step 182: loss = 3.5500 (0.852 sec/step)\n",
            "INFO:tensorflow:global step 183: loss = 4.2660 (1.501 sec/step)\n",
            "I1207 23:33:40.282004 140583164200832 learning.py:507] global step 183: loss = 4.2660 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 184: loss = 4.5311 (1.096 sec/step)\n",
            "I1207 23:33:41.379485 140583164200832 learning.py:507] global step 184: loss = 4.5311 (1.096 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.882529\n",
            "I1207 23:33:42.185702 140579467605760 supervisor.py:1099] global_step/sec: 0.882529\n",
            "INFO:tensorflow:global step 185: loss = 3.7875 (0.692 sec/step)\n",
            "I1207 23:33:42.244723 140583164200832 learning.py:507] global step 185: loss = 3.7875 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 186: loss = 3.5304 (2.300 sec/step)\n",
            "I1207 23:33:44.584480 140583164200832 learning.py:507] global step 186: loss = 3.5304 (2.300 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 186.\n",
            "I1207 23:33:44.737539 140579459213056 supervisor.py:1050] Recording summary at step 186.\n",
            "INFO:tensorflow:global step 187: loss = 3.2524 (0.645 sec/step)\n",
            "I1207 23:33:45.477277 140583164200832 learning.py:507] global step 187: loss = 3.2524 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 188: loss = 3.7754 (1.239 sec/step)\n",
            "I1207 23:33:46.787547 140583164200832 learning.py:507] global step 188: loss = 3.7754 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 189: loss = 2.8809 (1.049 sec/step)\n",
            "I1207 23:33:47.838082 140583164200832 learning.py:507] global step 189: loss = 2.8809 (1.049 sec/step)\n",
            "INFO:tensorflow:global step 190: loss = 3.2812 (0.591 sec/step)\n",
            "I1207 23:33:48.645350 140583164200832 learning.py:507] global step 190: loss = 3.2812 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 191: loss = 3.4493 (1.041 sec/step)\n",
            "I1207 23:33:49.901871 140583164200832 learning.py:507] global step 191: loss = 3.4493 (1.041 sec/step)\n",
            "INFO:tensorflow:global step 192: loss = 3.4498 (0.482 sec/step)\n",
            "I1207 23:33:50.385306 140583164200832 learning.py:507] global step 192: loss = 3.4498 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 193: loss = 3.6341 (0.909 sec/step)\n",
            "I1207 23:33:51.588676 140583164200832 learning.py:507] global step 193: loss = 3.6341 (0.909 sec/step)\n",
            "INFO:tensorflow:global step 194: loss = 3.6609 (0.632 sec/step)\n",
            "I1207 23:33:52.435039 140583164200832 learning.py:507] global step 194: loss = 3.6609 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 195: loss = 3.5885 (0.984 sec/step)\n",
            "I1207 23:33:53.435828 140583164200832 learning.py:507] global step 195: loss = 3.5885 (0.984 sec/step)\n",
            "INFO:tensorflow:global step 196: loss = 2.8304 (1.634 sec/step)\n",
            "I1207 23:33:55.241302 140583164200832 learning.py:507] global step 196: loss = 2.8304 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 197: loss = 3.3268 (1.102 sec/step)\n",
            "I1207 23:33:56.344666 140583164200832 learning.py:507] global step 197: loss = 3.3268 (1.102 sec/step)\n",
            "INFO:tensorflow:global step 198: loss = 2.7527 (0.557 sec/step)\n",
            "I1207 23:33:57.223528 140583164200832 learning.py:507] global step 198: loss = 2.7527 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 199: loss = 3.0159 (1.243 sec/step)\n",
            "I1207 23:33:58.529757 140583164200832 learning.py:507] global step 199: loss = 3.0159 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 3.5201 (1.013 sec/step)\n",
            "I1207 23:33:59.544510 140583164200832 learning.py:507] global step 200: loss = 3.5201 (1.013 sec/step)\n",
            "INFO:tensorflow:global step 201: loss = 3.5447 (1.067 sec/step)\n",
            "I1207 23:34:00.612781 140583164200832 learning.py:507] global step 201: loss = 3.5447 (1.067 sec/step)\n",
            "INFO:tensorflow:global step 202: loss = 3.7613 (0.712 sec/step)\n",
            "I1207 23:34:01.452464 140583164200832 learning.py:507] global step 202: loss = 3.7613 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 203: loss = 2.7628 (0.769 sec/step)\n",
            "I1207 23:34:02.425964 140583164200832 learning.py:507] global step 203: loss = 2.7628 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 204: loss = 3.1978 (1.021 sec/step)\n",
            "I1207 23:34:03.574791 140583164200832 learning.py:507] global step 204: loss = 3.1978 (1.021 sec/step)\n",
            "INFO:tensorflow:global step 205: loss = 3.8051 (0.622 sec/step)\n",
            "I1207 23:34:04.321079 140583164200832 learning.py:507] global step 205: loss = 3.8051 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 206: loss = 3.5308 (0.571 sec/step)\n",
            "I1207 23:34:05.164405 140583164200832 learning.py:507] global step 206: loss = 3.5308 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 207: loss = 3.2277 (1.775 sec/step)\n",
            "I1207 23:34:06.940836 140583164200832 learning.py:507] global step 207: loss = 3.2277 (1.775 sec/step)\n",
            "INFO:tensorflow:global step 208: loss = 3.3025 (0.638 sec/step)\n",
            "I1207 23:34:07.766041 140583164200832 learning.py:507] global step 208: loss = 3.3025 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 209: loss = 2.9194 (1.232 sec/step)\n",
            "I1207 23:34:09.122918 140583164200832 learning.py:507] global step 209: loss = 2.9194 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 210: loss = 2.9125 (0.598 sec/step)\n",
            "I1207 23:34:09.841541 140583164200832 learning.py:507] global step 210: loss = 2.9125 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 211: loss = 3.1679 (0.645 sec/step)\n",
            "I1207 23:34:10.872588 140583164200832 learning.py:507] global step 211: loss = 3.1679 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 212: loss = 3.3811 (1.592 sec/step)\n",
            "I1207 23:34:12.473162 140583164200832 learning.py:507] global step 212: loss = 3.3811 (1.592 sec/step)\n",
            "INFO:tensorflow:global step 213: loss = 2.8840 (1.121 sec/step)\n",
            "I1207 23:34:13.596047 140583164200832 learning.py:507] global step 213: loss = 2.8840 (1.121 sec/step)\n",
            "INFO:tensorflow:global step 214: loss = 3.3874 (0.583 sec/step)\n",
            "I1207 23:34:14.536017 140583164200832 learning.py:507] global step 214: loss = 3.3874 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 215: loss = 3.5722 (0.629 sec/step)\n",
            "I1207 23:34:15.662698 140583164200832 learning.py:507] global step 215: loss = 3.5722 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 216: loss = 3.6050 (0.533 sec/step)\n",
            "I1207 23:34:16.257495 140583164200832 learning.py:507] global step 216: loss = 3.6050 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 217: loss = 2.7933 (1.688 sec/step)\n",
            "I1207 23:34:17.947339 140583164200832 learning.py:507] global step 217: loss = 2.7933 (1.688 sec/step)\n",
            "INFO:tensorflow:global step 218: loss = 2.8093 (0.653 sec/step)\n",
            "I1207 23:34:18.701843 140583164200832 learning.py:507] global step 218: loss = 2.8093 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 219: loss = 3.0920 (1.230 sec/step)\n",
            "I1207 23:34:20.105317 140583164200832 learning.py:507] global step 219: loss = 3.0920 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 220: loss = 3.2066 (0.713 sec/step)\n",
            "I1207 23:34:21.078845 140583164200832 learning.py:507] global step 220: loss = 3.2066 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 221: loss = 3.4069 (0.542 sec/step)\n",
            "I1207 23:34:21.881883 140583164200832 learning.py:507] global step 221: loss = 3.4069 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 222: loss = 3.1584 (1.301 sec/step)\n",
            "I1207 23:34:23.376718 140583164200832 learning.py:507] global step 222: loss = 3.1584 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 223: loss = 3.8005 (0.575 sec/step)\n",
            "I1207 23:34:24.237042 140583164200832 learning.py:507] global step 223: loss = 3.8005 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 224: loss = 3.8016 (0.569 sec/step)\n",
            "I1207 23:34:25.066480 140583164200832 learning.py:507] global step 224: loss = 3.8016 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 225: loss = 2.9789 (1.334 sec/step)\n",
            "I1207 23:34:26.420868 140583164200832 learning.py:507] global step 225: loss = 2.9789 (1.334 sec/step)\n",
            "INFO:tensorflow:global step 226: loss = 3.0937 (0.611 sec/step)\n",
            "I1207 23:34:27.299577 140583164200832 learning.py:507] global step 226: loss = 3.0937 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 227: loss = 3.3426 (0.613 sec/step)\n",
            "I1207 23:34:28.312980 140583164200832 learning.py:507] global step 227: loss = 3.3426 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 228: loss = 4.1466 (0.486 sec/step)\n",
            "I1207 23:34:29.083441 140583164200832 learning.py:507] global step 228: loss = 4.1466 (0.486 sec/step)\n",
            "INFO:tensorflow:global step 229: loss = 3.7981 (1.375 sec/step)\n",
            "I1207 23:34:30.890995 140583164200832 learning.py:507] global step 229: loss = 3.7981 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 230: loss = 3.5677 (0.635 sec/step)\n",
            "I1207 23:34:31.794779 140583164200832 learning.py:507] global step 230: loss = 3.5677 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 231: loss = 3.4429 (0.986 sec/step)\n",
            "I1207 23:34:32.918608 140583164200832 learning.py:507] global step 231: loss = 3.4429 (0.986 sec/step)\n",
            "INFO:tensorflow:global step 232: loss = 3.4551 (0.519 sec/step)\n",
            "I1207 23:34:33.896238 140583164200832 learning.py:507] global step 232: loss = 3.4551 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 233: loss = 2.6435 (0.704 sec/step)\n",
            "I1207 23:34:34.997864 140583164200832 learning.py:507] global step 233: loss = 2.6435 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 234: loss = 3.6444 (0.764 sec/step)\n",
            "I1207 23:34:35.929229 140583164200832 learning.py:507] global step 234: loss = 3.6444 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 235: loss = 3.7069 (1.186 sec/step)\n",
            "I1207 23:34:37.377729 140583164200832 learning.py:507] global step 235: loss = 3.7069 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 236: loss = 3.2991 (0.546 sec/step)\n",
            "I1207 23:34:38.234787 140583164200832 learning.py:507] global step 236: loss = 3.2991 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 237: loss = 3.6782 (1.318 sec/step)\n",
            "I1207 23:34:39.789254 140583164200832 learning.py:507] global step 237: loss = 3.6782 (1.318 sec/step)\n",
            "INFO:tensorflow:global step 238: loss = 2.5156 (0.787 sec/step)\n",
            "I1207 23:34:40.781185 140583164200832 learning.py:507] global step 238: loss = 2.5156 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 239: loss = 2.6804 (1.337 sec/step)\n",
            "I1207 23:34:42.244828 140583164200832 learning.py:507] global step 239: loss = 2.6804 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 240: loss = 3.7342 (0.655 sec/step)\n",
            "I1207 23:34:43.111892 140583164200832 learning.py:507] global step 240: loss = 3.7342 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 241: loss = 3.3192 (1.241 sec/step)\n",
            "I1207 23:34:44.470619 140583164200832 learning.py:507] global step 241: loss = 3.3192 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 242: loss = 3.5770 (1.119 sec/step)\n",
            "I1207 23:34:45.592031 140583164200832 learning.py:507] global step 242: loss = 3.5770 (1.119 sec/step)\n",
            "INFO:tensorflow:global step 243: loss = 3.2675 (0.644 sec/step)\n",
            "I1207 23:34:46.433797 140583164200832 learning.py:507] global step 243: loss = 3.2675 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 244: loss = 3.1380 (1.251 sec/step)\n",
            "I1207 23:34:47.829326 140583164200832 learning.py:507] global step 244: loss = 3.1380 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 245: loss = 3.4139 (0.608 sec/step)\n",
            "I1207 23:34:48.707953 140583164200832 learning.py:507] global step 245: loss = 3.4139 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 246: loss = 3.7668 (0.725 sec/step)\n",
            "I1207 23:34:49.617149 140583164200832 learning.py:507] global step 246: loss = 3.7668 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 247: loss = 3.1450 (0.586 sec/step)\n",
            "I1207 23:34:50.456329 140583164200832 learning.py:507] global step 247: loss = 3.1450 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 248: loss = 3.1908 (1.538 sec/step)\n",
            "I1207 23:34:52.107399 140583164200832 learning.py:507] global step 248: loss = 3.1908 (1.538 sec/step)\n",
            "INFO:tensorflow:global step 249: loss = 4.7817 (0.708 sec/step)\n",
            "I1207 23:34:52.922376 140583164200832 learning.py:507] global step 249: loss = 4.7817 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 250: loss = 4.0007 (1.213 sec/step)\n",
            "I1207 23:34:54.294598 140583164200832 learning.py:507] global step 250: loss = 4.0007 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 251: loss = 3.2887 (0.646 sec/step)\n",
            "I1207 23:34:55.010765 140583164200832 learning.py:507] global step 251: loss = 3.2887 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 252: loss = 3.1286 (1.493 sec/step)\n",
            "I1207 23:34:56.506186 140583164200832 learning.py:507] global step 252: loss = 3.1286 (1.493 sec/step)\n",
            "INFO:tensorflow:global step 253: loss = 3.4248 (0.620 sec/step)\n",
            "I1207 23:34:57.371377 140583164200832 learning.py:507] global step 253: loss = 3.4248 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 254: loss = 3.5645 (1.313 sec/step)\n",
            "I1207 23:34:58.808731 140583164200832 learning.py:507] global step 254: loss = 3.5645 (1.313 sec/step)\n",
            "INFO:tensorflow:global step 255: loss = 3.2953 (0.649 sec/step)\n",
            "I1207 23:34:59.665795 140583164200832 learning.py:507] global step 255: loss = 3.2953 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 256: loss = 3.2624 (1.241 sec/step)\n",
            "I1207 23:35:01.121278 140583164200832 learning.py:507] global step 256: loss = 3.2624 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 257: loss = 3.0293 (0.505 sec/step)\n",
            "I1207 23:35:01.628839 140583164200832 learning.py:507] global step 257: loss = 3.0293 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 258: loss = 3.0843 (2.565 sec/step)\n",
            "I1207 23:35:04.195960 140583164200832 learning.py:507] global step 258: loss = 3.0843 (2.565 sec/step)\n",
            "INFO:tensorflow:global step 259: loss = 3.3082 (0.490 sec/step)\n",
            "I1207 23:35:04.829110 140583164200832 learning.py:507] global step 259: loss = 3.3082 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 260: loss = 3.1672 (0.727 sec/step)\n",
            "I1207 23:35:06.057459 140583164200832 learning.py:507] global step 260: loss = 3.1672 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 261: loss = 3.0931 (0.774 sec/step)\n",
            "I1207 23:35:06.946290 140583164200832 learning.py:507] global step 261: loss = 3.0931 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 262: loss = 2.7857 (1.924 sec/step)\n",
            "I1207 23:35:08.885106 140583164200832 learning.py:507] global step 262: loss = 2.7857 (1.924 sec/step)\n",
            "INFO:tensorflow:global step 263: loss = 3.1852 (0.488 sec/step)\n",
            "I1207 23:35:09.375309 140583164200832 learning.py:507] global step 263: loss = 3.1852 (0.488 sec/step)\n",
            "INFO:tensorflow:global step 264: loss = 2.9639 (1.663 sec/step)\n",
            "I1207 23:35:11.039479 140583164200832 learning.py:507] global step 264: loss = 2.9639 (1.663 sec/step)\n",
            "INFO:tensorflow:global step 265: loss = 3.4223 (0.598 sec/step)\n",
            "I1207 23:35:11.823506 140583164200832 learning.py:507] global step 265: loss = 3.4223 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 266: loss = 3.2736 (1.215 sec/step)\n",
            "I1207 23:35:13.146740 140583164200832 learning.py:507] global step 266: loss = 3.2736 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 267: loss = 3.0648 (0.561 sec/step)\n",
            "I1207 23:35:13.938086 140583164200832 learning.py:507] global step 267: loss = 3.0648 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 268: loss = 2.7460 (1.160 sec/step)\n",
            "I1207 23:35:15.190363 140583164200832 learning.py:507] global step 268: loss = 2.7460 (1.160 sec/step)\n",
            "INFO:tensorflow:global step 269: loss = 3.6166 (0.582 sec/step)\n",
            "I1207 23:35:16.021219 140583164200832 learning.py:507] global step 269: loss = 3.6166 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 270: loss = 3.7850 (1.277 sec/step)\n",
            "I1207 23:35:17.346425 140583164200832 learning.py:507] global step 270: loss = 3.7850 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 271: loss = 3.8578 (1.039 sec/step)\n",
            "I1207 23:35:18.386873 140583164200832 learning.py:507] global step 271: loss = 3.8578 (1.039 sec/step)\n",
            "INFO:tensorflow:global step 272: loss = 3.9662 (0.584 sec/step)\n",
            "I1207 23:35:19.049911 140583164200832 learning.py:507] global step 272: loss = 3.9662 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 273: loss = 3.4615 (1.456 sec/step)\n",
            "I1207 23:35:20.580658 140583164200832 learning.py:507] global step 273: loss = 3.4615 (1.456 sec/step)\n",
            "INFO:tensorflow:global step 274: loss = 4.5445 (0.529 sec/step)\n",
            "I1207 23:35:21.111638 140583164200832 learning.py:507] global step 274: loss = 4.5445 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 275: loss = 3.0799 (1.697 sec/step)\n",
            "I1207 23:35:22.810513 140583164200832 learning.py:507] global step 275: loss = 3.0799 (1.697 sec/step)\n",
            "INFO:tensorflow:global step 276: loss = 3.2881 (0.704 sec/step)\n",
            "I1207 23:35:23.840712 140583164200832 learning.py:507] global step 276: loss = 3.2881 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 277: loss = 3.0527 (0.589 sec/step)\n",
            "I1207 23:35:24.754160 140583164200832 learning.py:507] global step 277: loss = 3.0527 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 278: loss = 3.0319 (0.527 sec/step)\n",
            "I1207 23:35:25.415106 140583164200832 learning.py:507] global step 278: loss = 3.0319 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 279: loss = 3.5935 (1.278 sec/step)\n",
            "I1207 23:35:26.998371 140583164200832 learning.py:507] global step 279: loss = 3.5935 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 280: loss = 2.8655 (0.612 sec/step)\n",
            "I1207 23:35:28.008014 140583164200832 learning.py:507] global step 280: loss = 2.8655 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 281: loss = 4.1723 (0.632 sec/step)\n",
            "I1207 23:35:28.791798 140583164200832 learning.py:507] global step 281: loss = 4.1723 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 282: loss = 2.7743 (0.871 sec/step)\n",
            "I1207 23:35:29.894965 140583164200832 learning.py:507] global step 282: loss = 2.7743 (0.871 sec/step)\n",
            "INFO:tensorflow:global step 283: loss = 3.4707 (1.404 sec/step)\n",
            "I1207 23:35:31.331565 140583164200832 learning.py:507] global step 283: loss = 3.4707 (1.404 sec/step)\n",
            "INFO:tensorflow:global step 284: loss = 3.3250 (1.081 sec/step)\n",
            "I1207 23:35:32.414669 140583164200832 learning.py:507] global step 284: loss = 3.3250 (1.081 sec/step)\n",
            "INFO:tensorflow:global step 285: loss = 2.9061 (0.628 sec/step)\n",
            "I1207 23:35:33.168225 140583164200832 learning.py:507] global step 285: loss = 2.9061 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 286: loss = 2.6795 (1.360 sec/step)\n",
            "I1207 23:35:34.678178 140583164200832 learning.py:507] global step 286: loss = 2.6795 (1.360 sec/step)\n",
            "INFO:tensorflow:global step 287: loss = 3.7698 (0.768 sec/step)\n",
            "I1207 23:35:35.604608 140583164200832 learning.py:507] global step 287: loss = 3.7698 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 288: loss = 3.1065 (1.100 sec/step)\n",
            "I1207 23:35:36.885357 140583164200832 learning.py:507] global step 288: loss = 3.1065 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 289: loss = 2.8732 (0.639 sec/step)\n",
            "I1207 23:35:37.729979 140583164200832 learning.py:507] global step 289: loss = 2.8732 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 290: loss = 3.6027 (1.094 sec/step)\n",
            "I1207 23:35:39.004981 140583164200832 learning.py:507] global step 290: loss = 3.6027 (1.094 sec/step)\n",
            "INFO:tensorflow:global step 291: loss = 3.4565 (0.684 sec/step)\n",
            "I1207 23:35:39.798547 140583164200832 learning.py:507] global step 291: loss = 3.4565 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 292: loss = 3.8453 (1.193 sec/step)\n",
            "I1207 23:35:41.218622 140583164200832 learning.py:507] global step 292: loss = 3.8453 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 293: loss = 3.2963 (0.635 sec/step)\n",
            "I1207 23:35:41.871104 140583164200832 learning.py:507] global step 293: loss = 3.2963 (0.635 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.899779\n",
            "I1207 23:35:42.215348 140579467605760 supervisor.py:1099] global_step/sec: 0.899779\n",
            "INFO:tensorflow:Recording summary at step 293.\n",
            "I1207 23:35:44.690690 140579459213056 supervisor.py:1050] Recording summary at step 293.\n",
            "INFO:tensorflow:global step 294: loss = 3.3213 (2.691 sec/step)\n",
            "I1207 23:35:44.877272 140583164200832 learning.py:507] global step 294: loss = 3.3213 (2.691 sec/step)\n",
            "INFO:tensorflow:global step 295: loss = 3.2933 (0.504 sec/step)\n",
            "I1207 23:35:45.382679 140583164200832 learning.py:507] global step 295: loss = 3.2933 (0.504 sec/step)\n",
            "INFO:tensorflow:global step 296: loss = 3.0751 (1.579 sec/step)\n",
            "I1207 23:35:46.963079 140583164200832 learning.py:507] global step 296: loss = 3.0751 (1.579 sec/step)\n",
            "INFO:tensorflow:global step 297: loss = 3.2883 (0.568 sec/step)\n",
            "I1207 23:35:47.843867 140583164200832 learning.py:507] global step 297: loss = 3.2883 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 298: loss = 3.6484 (0.642 sec/step)\n",
            "I1207 23:35:48.904909 140583164200832 learning.py:507] global step 298: loss = 3.6484 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 299: loss = 3.1350 (0.656 sec/step)\n",
            "I1207 23:35:49.563278 140583164200832 learning.py:507] global step 299: loss = 3.1350 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 3.5823 (0.493 sec/step)\n",
            "I1207 23:35:50.361974 140583164200832 learning.py:507] global step 300: loss = 3.5823 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 301: loss = 2.7986 (0.799 sec/step)\n",
            "I1207 23:35:51.296974 140583164200832 learning.py:507] global step 301: loss = 2.7986 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 302: loss = 3.4153 (1.690 sec/step)\n",
            "I1207 23:35:53.323431 140583164200832 learning.py:507] global step 302: loss = 3.4153 (1.690 sec/step)\n",
            "INFO:tensorflow:global step 303: loss = 3.5667 (0.569 sec/step)\n",
            "I1207 23:35:53.894466 140583164200832 learning.py:507] global step 303: loss = 3.5667 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 304: loss = 3.6327 (1.768 sec/step)\n",
            "I1207 23:35:55.664773 140583164200832 learning.py:507] global step 304: loss = 3.6327 (1.768 sec/step)\n",
            "INFO:tensorflow:global step 305: loss = 3.3662 (1.291 sec/step)\n",
            "I1207 23:35:56.957017 140583164200832 learning.py:507] global step 305: loss = 3.3662 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 306: loss = 3.2022 (0.512 sec/step)\n",
            "I1207 23:35:57.839178 140583164200832 learning.py:507] global step 306: loss = 3.2022 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 307: loss = 3.3186 (0.746 sec/step)\n",
            "I1207 23:35:58.765744 140583164200832 learning.py:507] global step 307: loss = 3.3186 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 308: loss = 3.0788 (1.050 sec/step)\n",
            "I1207 23:35:59.890261 140583164200832 learning.py:507] global step 308: loss = 3.0788 (1.050 sec/step)\n",
            "INFO:tensorflow:global step 309: loss = 3.2636 (0.782 sec/step)\n",
            "I1207 23:36:00.964224 140583164200832 learning.py:507] global step 309: loss = 3.2636 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 310: loss = 3.6225 (1.406 sec/step)\n",
            "I1207 23:36:02.606714 140583164200832 learning.py:507] global step 310: loss = 3.6225 (1.406 sec/step)\n",
            "INFO:tensorflow:global step 311: loss = 2.6428 (0.624 sec/step)\n",
            "I1207 23:36:03.376824 140583164200832 learning.py:507] global step 311: loss = 2.6428 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 312: loss = 3.3339 (0.616 sec/step)\n",
            "I1207 23:36:04.244981 140583164200832 learning.py:507] global step 312: loss = 3.3339 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 313: loss = 3.3431 (1.271 sec/step)\n",
            "I1207 23:36:05.825964 140583164200832 learning.py:507] global step 313: loss = 3.3431 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 314: loss = 3.5554 (1.209 sec/step)\n",
            "I1207 23:36:07.036448 140583164200832 learning.py:507] global step 314: loss = 3.5554 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 315: loss = 3.1500 (0.654 sec/step)\n",
            "I1207 23:36:07.890585 140583164200832 learning.py:507] global step 315: loss = 3.1500 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 316: loss = 3.2962 (0.556 sec/step)\n",
            "I1207 23:36:08.635211 140583164200832 learning.py:507] global step 316: loss = 3.2962 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 317: loss = 2.9931 (0.507 sec/step)\n",
            "I1207 23:36:09.143798 140583164200832 learning.py:507] global step 317: loss = 2.9931 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 318: loss = 3.0600 (1.446 sec/step)\n",
            "I1207 23:36:10.750362 140583164200832 learning.py:507] global step 318: loss = 3.0600 (1.446 sec/step)\n",
            "INFO:tensorflow:global step 319: loss = 2.7211 (2.800 sec/step)\n",
            "I1207 23:36:13.744096 140583164200832 learning.py:507] global step 319: loss = 2.7211 (2.800 sec/step)\n",
            "INFO:tensorflow:global step 320: loss = 2.9941 (0.754 sec/step)\n",
            "I1207 23:36:14.717331 140583164200832 learning.py:507] global step 320: loss = 2.9941 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 321: loss = 3.1201 (1.439 sec/step)\n",
            "I1207 23:36:16.247238 140583164200832 learning.py:507] global step 321: loss = 3.1201 (1.439 sec/step)\n",
            "INFO:tensorflow:global step 322: loss = 3.2055 (0.764 sec/step)\n",
            "I1207 23:36:17.290225 140583164200832 learning.py:507] global step 322: loss = 3.2055 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 323: loss = 3.6398 (1.255 sec/step)\n",
            "I1207 23:36:18.621818 140583164200832 learning.py:507] global step 323: loss = 3.6398 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 324: loss = 2.7805 (0.694 sec/step)\n",
            "I1207 23:36:19.605663 140583164200832 learning.py:507] global step 324: loss = 2.7805 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 325: loss = 2.7698 (1.136 sec/step)\n",
            "I1207 23:36:20.880106 140583164200832 learning.py:507] global step 325: loss = 2.7698 (1.136 sec/step)\n",
            "INFO:tensorflow:global step 326: loss = 3.1589 (0.607 sec/step)\n",
            "I1207 23:36:21.489019 140583164200832 learning.py:507] global step 326: loss = 3.1589 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 327: loss = 3.1448 (0.997 sec/step)\n",
            "I1207 23:36:22.764290 140583164200832 learning.py:507] global step 327: loss = 3.1448 (0.997 sec/step)\n",
            "INFO:tensorflow:global step 328: loss = 2.7853 (1.585 sec/step)\n",
            "I1207 23:36:24.624216 140583164200832 learning.py:507] global step 328: loss = 2.7853 (1.585 sec/step)\n",
            "INFO:tensorflow:global step 329: loss = 2.9801 (0.701 sec/step)\n",
            "I1207 23:36:25.604417 140583164200832 learning.py:507] global step 329: loss = 2.9801 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 330: loss = 3.4135 (1.108 sec/step)\n",
            "I1207 23:36:26.791750 140583164200832 learning.py:507] global step 330: loss = 3.4135 (1.108 sec/step)\n",
            "INFO:tensorflow:global step 331: loss = 2.9570 (0.664 sec/step)\n",
            "I1207 23:36:27.572021 140583164200832 learning.py:507] global step 331: loss = 2.9570 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 332: loss = 3.1250 (1.300 sec/step)\n",
            "I1207 23:36:29.128542 140583164200832 learning.py:507] global step 332: loss = 3.1250 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 333: loss = 3.0963 (0.550 sec/step)\n",
            "I1207 23:36:29.679903 140583164200832 learning.py:507] global step 333: loss = 3.0963 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 334: loss = 2.7819 (1.630 sec/step)\n",
            "I1207 23:36:31.311795 140583164200832 learning.py:507] global step 334: loss = 2.7819 (1.630 sec/step)\n",
            "INFO:tensorflow:global step 335: loss = 3.1411 (1.255 sec/step)\n",
            "I1207 23:36:32.568399 140583164200832 learning.py:507] global step 335: loss = 3.1411 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 336: loss = 3.9937 (1.151 sec/step)\n",
            "I1207 23:36:33.721033 140583164200832 learning.py:507] global step 336: loss = 3.9937 (1.151 sec/step)\n",
            "INFO:tensorflow:global step 337: loss = 2.7359 (1.168 sec/step)\n",
            "I1207 23:36:34.890678 140583164200832 learning.py:507] global step 337: loss = 2.7359 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 338: loss = 2.9978 (0.588 sec/step)\n",
            "I1207 23:36:35.821830 140583164200832 learning.py:507] global step 338: loss = 2.9978 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 339: loss = 2.8387 (0.672 sec/step)\n",
            "I1207 23:36:36.700985 140583164200832 learning.py:507] global step 339: loss = 2.8387 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 340: loss = 3.3546 (1.592 sec/step)\n",
            "I1207 23:36:38.339690 140583164200832 learning.py:507] global step 340: loss = 3.3546 (1.592 sec/step)\n",
            "INFO:tensorflow:global step 341: loss = 2.6733 (0.723 sec/step)\n",
            "I1207 23:36:39.069729 140583164200832 learning.py:507] global step 341: loss = 2.6733 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 342: loss = 3.3134 (1.479 sec/step)\n",
            "I1207 23:36:40.572796 140583164200832 learning.py:507] global step 342: loss = 3.3134 (1.479 sec/step)\n",
            "INFO:tensorflow:global step 343: loss = 3.2405 (0.551 sec/step)\n",
            "I1207 23:36:41.535579 140583164200832 learning.py:507] global step 343: loss = 3.2405 (0.551 sec/step)\n",
            "INFO:tensorflow:global step 344: loss = 3.0095 (0.643 sec/step)\n",
            "I1207 23:36:42.527895 140583164200832 learning.py:507] global step 344: loss = 3.0095 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 345: loss = 4.3001 (1.263 sec/step)\n",
            "I1207 23:36:43.828284 140583164200832 learning.py:507] global step 345: loss = 4.3001 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 346: loss = 3.6362 (1.136 sec/step)\n",
            "I1207 23:36:44.966231 140583164200832 learning.py:507] global step 346: loss = 3.6362 (1.136 sec/step)\n",
            "INFO:tensorflow:global step 347: loss = 3.7316 (1.110 sec/step)\n",
            "I1207 23:36:46.077744 140583164200832 learning.py:507] global step 347: loss = 3.7316 (1.110 sec/step)\n",
            "INFO:tensorflow:global step 348: loss = 2.6559 (0.482 sec/step)\n",
            "I1207 23:36:46.561899 140583164200832 learning.py:507] global step 348: loss = 2.6559 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 349: loss = 3.1781 (1.630 sec/step)\n",
            "I1207 23:36:48.194621 140583164200832 learning.py:507] global step 349: loss = 3.1781 (1.630 sec/step)\n",
            "INFO:tensorflow:global step 350: loss = 3.1223 (0.579 sec/step)\n",
            "I1207 23:36:48.897135 140583164200832 learning.py:507] global step 350: loss = 3.1223 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 351: loss = 2.6701 (1.273 sec/step)\n",
            "I1207 23:36:50.409780 140583164200832 learning.py:507] global step 351: loss = 2.6701 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 352: loss = 2.5231 (0.548 sec/step)\n",
            "I1207 23:36:51.247428 140583164200832 learning.py:507] global step 352: loss = 2.5231 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 353: loss = 3.2228 (0.779 sec/step)\n",
            "I1207 23:36:52.411845 140583164200832 learning.py:507] global step 353: loss = 3.2228 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 354: loss = 2.7295 (1.098 sec/step)\n",
            "I1207 23:36:53.559122 140583164200832 learning.py:507] global step 354: loss = 2.7295 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 355: loss = 3.4955 (0.685 sec/step)\n",
            "I1207 23:36:54.519173 140583164200832 learning.py:507] global step 355: loss = 3.4955 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 356: loss = 3.4998 (0.717 sec/step)\n",
            "I1207 23:36:55.449427 140583164200832 learning.py:507] global step 356: loss = 3.4998 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 357: loss = 3.2810 (0.663 sec/step)\n",
            "I1207 23:36:56.220371 140583164200832 learning.py:507] global step 357: loss = 3.2810 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 358: loss = 3.3397 (1.783 sec/step)\n",
            "I1207 23:36:58.055608 140583164200832 learning.py:507] global step 358: loss = 3.3397 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 359: loss = 2.8408 (0.666 sec/step)\n",
            "I1207 23:36:58.898428 140583164200832 learning.py:507] global step 359: loss = 2.8408 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 360: loss = 3.1294 (1.458 sec/step)\n",
            "I1207 23:37:00.528638 140583164200832 learning.py:507] global step 360: loss = 3.1294 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 361: loss = 3.2394 (0.660 sec/step)\n",
            "I1207 23:37:01.327986 140583164200832 learning.py:507] global step 361: loss = 3.2394 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 362: loss = 3.3827 (0.739 sec/step)\n",
            "I1207 23:37:02.538273 140583164200832 learning.py:507] global step 362: loss = 3.3827 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 363: loss = 2.8992 (1.329 sec/step)\n",
            "I1207 23:37:03.966755 140583164200832 learning.py:507] global step 363: loss = 2.8992 (1.329 sec/step)\n",
            "INFO:tensorflow:global step 364: loss = 3.5521 (0.519 sec/step)\n",
            "I1207 23:37:04.911934 140583164200832 learning.py:507] global step 364: loss = 3.5521 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 365: loss = 3.1034 (1.481 sec/step)\n",
            "I1207 23:37:06.394581 140583164200832 learning.py:507] global step 365: loss = 3.1034 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 366: loss = 3.0078 (0.708 sec/step)\n",
            "I1207 23:37:07.295384 140583164200832 learning.py:507] global step 366: loss = 3.0078 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 367: loss = 3.2788 (0.740 sec/step)\n",
            "I1207 23:37:08.213986 140583164200832 learning.py:507] global step 367: loss = 3.2788 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 368: loss = 2.9092 (1.374 sec/step)\n",
            "I1207 23:37:09.719647 140583164200832 learning.py:507] global step 368: loss = 2.9092 (1.374 sec/step)\n",
            "INFO:tensorflow:global step 369: loss = 3.5203 (0.631 sec/step)\n",
            "I1207 23:37:10.511571 140583164200832 learning.py:507] global step 369: loss = 3.5203 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 370: loss = 2.8035 (1.191 sec/step)\n",
            "I1207 23:37:11.865430 140583164200832 learning.py:507] global step 370: loss = 2.8035 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 371: loss = 2.8460 (0.490 sec/step)\n",
            "I1207 23:37:12.647575 140583164200832 learning.py:507] global step 371: loss = 2.8460 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 372: loss = 2.7418 (0.512 sec/step)\n",
            "I1207 23:37:13.748346 140583164200832 learning.py:507] global step 372: loss = 2.7418 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 373: loss = 3.2173 (1.807 sec/step)\n",
            "I1207 23:37:15.567705 140583164200832 learning.py:507] global step 373: loss = 3.2173 (1.807 sec/step)\n",
            "INFO:tensorflow:global step 374: loss = 2.4743 (0.650 sec/step)\n",
            "I1207 23:37:16.458764 140583164200832 learning.py:507] global step 374: loss = 2.4743 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 375: loss = 3.5730 (0.675 sec/step)\n",
            "I1207 23:37:17.581861 140583164200832 learning.py:507] global step 375: loss = 3.5730 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 376: loss = 3.8362 (0.582 sec/step)\n",
            "I1207 23:37:18.390962 140583164200832 learning.py:507] global step 376: loss = 3.8362 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 377: loss = 4.1722 (1.459 sec/step)\n",
            "I1207 23:37:19.870594 140583164200832 learning.py:507] global step 377: loss = 4.1722 (1.459 sec/step)\n",
            "INFO:tensorflow:global step 378: loss = 3.2781 (0.684 sec/step)\n",
            "I1207 23:37:20.735905 140583164200832 learning.py:507] global step 378: loss = 3.2781 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 379: loss = 2.7728 (0.646 sec/step)\n",
            "I1207 23:37:21.858114 140583164200832 learning.py:507] global step 379: loss = 2.7728 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 380: loss = 2.9781 (1.058 sec/step)\n",
            "I1207 23:37:22.932427 140583164200832 learning.py:507] global step 380: loss = 2.9781 (1.058 sec/step)\n",
            "INFO:tensorflow:global step 381: loss = 2.6506 (0.680 sec/step)\n",
            "I1207 23:37:23.744332 140583164200832 learning.py:507] global step 381: loss = 2.6506 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 382: loss = 3.0868 (0.651 sec/step)\n",
            "I1207 23:37:24.768175 140583164200832 learning.py:507] global step 382: loss = 3.0868 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 383: loss = 2.6661 (1.035 sec/step)\n",
            "I1207 23:37:26.018516 140583164200832 learning.py:507] global step 383: loss = 2.6661 (1.035 sec/step)\n",
            "INFO:tensorflow:global step 384: loss = 3.0777 (0.506 sec/step)\n",
            "I1207 23:37:26.784919 140583164200832 learning.py:507] global step 384: loss = 3.0777 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 385: loss = 2.5511 (1.266 sec/step)\n",
            "I1207 23:37:28.262945 140583164200832 learning.py:507] global step 385: loss = 2.5511 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 386: loss = 3.1465 (1.118 sec/step)\n",
            "I1207 23:37:29.382342 140583164200832 learning.py:507] global step 386: loss = 3.1465 (1.118 sec/step)\n",
            "INFO:tensorflow:global step 387: loss = 3.1940 (0.573 sec/step)\n",
            "I1207 23:37:30.228085 140583164200832 learning.py:507] global step 387: loss = 3.1940 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 388: loss = 2.7718 (0.748 sec/step)\n",
            "I1207 23:37:31.286017 140583164200832 learning.py:507] global step 388: loss = 2.7718 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 389: loss = 3.2772 (0.644 sec/step)\n",
            "I1207 23:37:32.405627 140583164200832 learning.py:507] global step 389: loss = 3.2772 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 390: loss = 2.9612 (0.690 sec/step)\n",
            "I1207 23:37:33.420404 140583164200832 learning.py:507] global step 390: loss = 2.9612 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 391: loss = 2.9631 (1.165 sec/step)\n",
            "I1207 23:37:34.674289 140583164200832 learning.py:507] global step 391: loss = 2.9631 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 392: loss = 2.1927 (1.112 sec/step)\n",
            "I1207 23:37:35.787947 140583164200832 learning.py:507] global step 392: loss = 2.1927 (1.112 sec/step)\n",
            "INFO:tensorflow:global step 393: loss = 2.8618 (0.491 sec/step)\n",
            "I1207 23:37:36.281228 140583164200832 learning.py:507] global step 393: loss = 2.8618 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 394: loss = 3.1822 (1.789 sec/step)\n",
            "I1207 23:37:38.072522 140583164200832 learning.py:507] global step 394: loss = 3.1822 (1.789 sec/step)\n",
            "INFO:tensorflow:global step 395: loss = 3.0620 (0.558 sec/step)\n",
            "I1207 23:37:38.774693 140583164200832 learning.py:507] global step 395: loss = 3.0620 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 396: loss = 2.7804 (1.244 sec/step)\n",
            "I1207 23:37:40.275696 140583164200832 learning.py:507] global step 396: loss = 2.7804 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 397: loss = 3.2906 (0.495 sec/step)\n",
            "I1207 23:37:41.083739 140583164200832 learning.py:507] global step 397: loss = 3.2906 (0.495 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.867466\n",
            "I1207 23:37:42.104618 140579467605760 supervisor.py:1099] global_step/sec: 0.867466\n",
            "INFO:tensorflow:global step 398: loss = 3.0134 (1.701 sec/step)\n",
            "I1207 23:37:43.302571 140583164200832 learning.py:507] global step 398: loss = 3.0134 (1.701 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 398.\n",
            "I1207 23:37:44.591101 140579459213056 supervisor.py:1050] Recording summary at step 398.\n",
            "INFO:tensorflow:global step 399: loss = 2.6749 (1.343 sec/step)\n",
            "I1207 23:37:44.808216 140583164200832 learning.py:507] global step 399: loss = 2.6749 (1.343 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 3.2304 (1.157 sec/step)\n",
            "I1207 23:37:45.966860 140583164200832 learning.py:507] global step 400: loss = 3.2304 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 401: loss = 2.7522 (1.115 sec/step)\n",
            "I1207 23:37:47.084040 140583164200832 learning.py:507] global step 401: loss = 2.7522 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 402: loss = 3.6671 (0.641 sec/step)\n",
            "I1207 23:37:47.978986 140583164200832 learning.py:507] global step 402: loss = 3.6671 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 403: loss = 2.5247 (0.666 sec/step)\n",
            "I1207 23:37:48.981439 140583164200832 learning.py:507] global step 403: loss = 2.5247 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 404: loss = 2.4902 (1.314 sec/step)\n",
            "I1207 23:37:50.445700 140583164200832 learning.py:507] global step 404: loss = 2.4902 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 405: loss = 2.9909 (0.479 sec/step)\n",
            "I1207 23:37:50.926444 140583164200832 learning.py:507] global step 405: loss = 2.9909 (0.479 sec/step)\n",
            "INFO:tensorflow:global step 406: loss = 3.2803 (0.623 sec/step)\n",
            "I1207 23:37:51.552244 140583164200832 learning.py:507] global step 406: loss = 3.2803 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 407: loss = 3.1606 (1.884 sec/step)\n",
            "I1207 23:37:53.691220 140583164200832 learning.py:507] global step 407: loss = 3.1606 (1.884 sec/step)\n",
            "INFO:tensorflow:global step 408: loss = 2.4874 (0.686 sec/step)\n",
            "I1207 23:37:54.875910 140583164200832 learning.py:507] global step 408: loss = 2.4874 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 409: loss = 4.0295 (0.706 sec/step)\n",
            "I1207 23:37:55.966364 140583164200832 learning.py:507] global step 409: loss = 4.0295 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 410: loss = 3.0632 (1.192 sec/step)\n",
            "I1207 23:37:57.290529 140583164200832 learning.py:507] global step 410: loss = 3.0632 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 411: loss = 2.9821 (0.601 sec/step)\n",
            "I1207 23:37:58.148037 140583164200832 learning.py:507] global step 411: loss = 2.9821 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 412: loss = 3.1267 (0.535 sec/step)\n",
            "I1207 23:37:59.204847 140583164200832 learning.py:507] global step 412: loss = 3.1267 (0.535 sec/step)\n",
            "INFO:tensorflow:global step 413: loss = 3.3301 (0.687 sec/step)\n",
            "I1207 23:38:00.292005 140583164200832 learning.py:507] global step 413: loss = 3.3301 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 414: loss = 3.1964 (0.634 sec/step)\n",
            "I1207 23:38:01.328993 140583164200832 learning.py:507] global step 414: loss = 3.1964 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 415: loss = 4.3197 (0.499 sec/step)\n",
            "I1207 23:38:01.981513 140583164200832 learning.py:507] global step 415: loss = 4.3197 (0.499 sec/step)\n",
            "INFO:tensorflow:global step 416: loss = 2.7520 (1.551 sec/step)\n",
            "I1207 23:38:03.582969 140583164200832 learning.py:507] global step 416: loss = 2.7520 (1.551 sec/step)\n",
            "INFO:tensorflow:global step 417: loss = 3.1124 (1.096 sec/step)\n",
            "I1207 23:38:04.680969 140583164200832 learning.py:507] global step 417: loss = 3.1124 (1.096 sec/step)\n",
            "INFO:tensorflow:global step 418: loss = 4.0431 (0.639 sec/step)\n",
            "I1207 23:38:05.668939 140583164200832 learning.py:507] global step 418: loss = 4.0431 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 419: loss = 3.5567 (0.616 sec/step)\n",
            "I1207 23:38:06.536069 140583164200832 learning.py:507] global step 419: loss = 3.5567 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 420: loss = 2.4615 (1.258 sec/step)\n",
            "I1207 23:38:07.951893 140583164200832 learning.py:507] global step 420: loss = 2.4615 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 421: loss = 3.8778 (0.672 sec/step)\n",
            "I1207 23:38:08.868372 140583164200832 learning.py:507] global step 421: loss = 3.8778 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 422: loss = 3.3748 (0.628 sec/step)\n",
            "I1207 23:38:09.827301 140583164200832 learning.py:507] global step 422: loss = 3.3748 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 423: loss = 3.3628 (0.598 sec/step)\n",
            "I1207 23:38:10.923358 140583164200832 learning.py:507] global step 423: loss = 3.3628 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 424: loss = 3.4646 (1.224 sec/step)\n",
            "I1207 23:38:12.325128 140583164200832 learning.py:507] global step 424: loss = 3.4646 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 425: loss = 2.9131 (0.626 sec/step)\n",
            "I1207 23:38:13.024742 140583164200832 learning.py:507] global step 425: loss = 2.9131 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 426: loss = 3.0206 (1.206 sec/step)\n",
            "I1207 23:38:14.529452 140583164200832 learning.py:507] global step 426: loss = 3.0206 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 427: loss = 2.6689 (0.778 sec/step)\n",
            "I1207 23:38:15.514791 140583164200832 learning.py:507] global step 427: loss = 2.6689 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 428: loss = 3.8766 (1.793 sec/step)\n",
            "I1207 23:38:17.354675 140583164200832 learning.py:507] global step 428: loss = 3.8766 (1.793 sec/step)\n",
            "INFO:tensorflow:global step 429: loss = 2.6460 (0.565 sec/step)\n",
            "I1207 23:38:18.159864 140583164200832 learning.py:507] global step 429: loss = 2.6460 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 430: loss = 2.7819 (0.511 sec/step)\n",
            "I1207 23:38:18.891402 140583164200832 learning.py:507] global step 430: loss = 2.7819 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 431: loss = 3.0882 (1.811 sec/step)\n",
            "I1207 23:38:20.704004 140583164200832 learning.py:507] global step 431: loss = 3.0882 (1.811 sec/step)\n",
            "INFO:tensorflow:global step 432: loss = 3.1178 (0.714 sec/step)\n",
            "I1207 23:38:21.732692 140583164200832 learning.py:507] global step 432: loss = 3.1178 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 433: loss = 2.8547 (0.610 sec/step)\n",
            "I1207 23:38:22.368199 140583164200832 learning.py:507] global step 433: loss = 2.8547 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 434: loss = 2.9184 (1.571 sec/step)\n",
            "I1207 23:38:24.165823 140583164200832 learning.py:507] global step 434: loss = 2.9184 (1.571 sec/step)\n",
            "INFO:tensorflow:global step 435: loss = 3.3095 (1.178 sec/step)\n",
            "I1207 23:38:25.365287 140583164200832 learning.py:507] global step 435: loss = 3.3095 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 436: loss = 2.6833 (0.701 sec/step)\n",
            "I1207 23:38:26.268859 140583164200832 learning.py:507] global step 436: loss = 2.6833 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 437: loss = 2.6053 (0.693 sec/step)\n",
            "I1207 23:38:27.185244 140583164200832 learning.py:507] global step 437: loss = 2.6053 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 438: loss = 3.8785 (1.585 sec/step)\n",
            "I1207 23:38:28.772007 140583164200832 learning.py:507] global step 438: loss = 3.8785 (1.585 sec/step)\n",
            "INFO:tensorflow:global step 439: loss = 2.8053 (0.658 sec/step)\n",
            "I1207 23:38:29.622941 140583164200832 learning.py:507] global step 439: loss = 2.8053 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 440: loss = 3.0981 (1.859 sec/step)\n",
            "I1207 23:38:31.484126 140583164200832 learning.py:507] global step 440: loss = 3.0981 (1.859 sec/step)\n",
            "INFO:tensorflow:global step 441: loss = 2.7654 (0.547 sec/step)\n",
            "I1207 23:38:32.032922 140583164200832 learning.py:507] global step 441: loss = 2.7654 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 442: loss = 3.1554 (1.715 sec/step)\n",
            "I1207 23:38:33.750036 140583164200832 learning.py:507] global step 442: loss = 3.1554 (1.715 sec/step)\n",
            "INFO:tensorflow:global step 443: loss = 2.4869 (0.643 sec/step)\n",
            "I1207 23:38:34.395483 140583164200832 learning.py:507] global step 443: loss = 2.4869 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 444: loss = 2.7494 (1.812 sec/step)\n",
            "I1207 23:38:36.209846 140583164200832 learning.py:507] global step 444: loss = 2.7494 (1.812 sec/step)\n",
            "INFO:tensorflow:global step 445: loss = 4.1644 (0.622 sec/step)\n",
            "I1207 23:38:36.834015 140583164200832 learning.py:507] global step 445: loss = 4.1644 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 446: loss = 2.7014 (0.990 sec/step)\n",
            "I1207 23:38:37.985405 140583164200832 learning.py:507] global step 446: loss = 2.7014 (0.990 sec/step)\n",
            "INFO:tensorflow:global step 447: loss = 3.0666 (1.677 sec/step)\n",
            "I1207 23:38:39.906841 140583164200832 learning.py:507] global step 447: loss = 3.0666 (1.677 sec/step)\n",
            "INFO:tensorflow:global step 448: loss = 2.4046 (0.526 sec/step)\n",
            "I1207 23:38:40.435180 140583164200832 learning.py:507] global step 448: loss = 2.4046 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 449: loss = 3.4515 (0.605 sec/step)\n",
            "I1207 23:38:41.497236 140583164200832 learning.py:507] global step 449: loss = 3.4515 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 450: loss = 2.8694 (2.121 sec/step)\n",
            "I1207 23:38:43.620619 140583164200832 learning.py:507] global step 450: loss = 2.8694 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 451: loss = 2.8474 (0.561 sec/step)\n",
            "I1207 23:38:44.420143 140583164200832 learning.py:507] global step 451: loss = 2.8474 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 452: loss = 2.6877 (0.624 sec/step)\n",
            "I1207 23:38:45.341603 140583164200832 learning.py:507] global step 452: loss = 2.6877 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 453: loss = 2.5173 (0.862 sec/step)\n",
            "I1207 23:38:46.432635 140583164200832 learning.py:507] global step 453: loss = 2.5173 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 454: loss = 2.7969 (1.886 sec/step)\n",
            "I1207 23:38:48.483374 140583164200832 learning.py:507] global step 454: loss = 2.7969 (1.886 sec/step)\n",
            "INFO:tensorflow:global step 455: loss = 2.3350 (0.717 sec/step)\n",
            "I1207 23:38:49.544486 140583164200832 learning.py:507] global step 455: loss = 2.3350 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 456: loss = 3.1052 (0.520 sec/step)\n",
            "I1207 23:38:50.162178 140583164200832 learning.py:507] global step 456: loss = 3.1052 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 457: loss = 2.9338 (0.700 sec/step)\n",
            "I1207 23:38:50.864494 140583164200832 learning.py:507] global step 457: loss = 2.9338 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 458: loss = 3.1630 (0.888 sec/step)\n",
            "I1207 23:38:51.801825 140583164200832 learning.py:507] global step 458: loss = 3.1630 (0.888 sec/step)\n",
            "INFO:tensorflow:global step 459: loss = 2.7725 (2.538 sec/step)\n",
            "I1207 23:38:54.619048 140583164200832 learning.py:507] global step 459: loss = 2.7725 (2.538 sec/step)\n",
            "INFO:tensorflow:global step 460: loss = 2.6658 (0.637 sec/step)\n",
            "I1207 23:38:55.383992 140583164200832 learning.py:507] global step 460: loss = 2.6658 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 461: loss = 2.7486 (0.785 sec/step)\n",
            "I1207 23:38:56.636677 140583164200832 learning.py:507] global step 461: loss = 2.7486 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 462: loss = 2.8162 (1.119 sec/step)\n",
            "I1207 23:38:57.949078 140583164200832 learning.py:507] global step 462: loss = 2.8162 (1.119 sec/step)\n",
            "INFO:tensorflow:global step 463: loss = 2.9430 (0.699 sec/step)\n",
            "I1207 23:38:58.962484 140583164200832 learning.py:507] global step 463: loss = 2.9430 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 464: loss = 3.0297 (0.632 sec/step)\n",
            "I1207 23:38:59.623422 140583164200832 learning.py:507] global step 464: loss = 3.0297 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 465: loss = 2.8527 (2.210 sec/step)\n",
            "I1207 23:39:01.835136 140583164200832 learning.py:507] global step 465: loss = 2.8527 (2.210 sec/step)\n",
            "INFO:tensorflow:global step 466: loss = 2.4689 (0.583 sec/step)\n",
            "I1207 23:39:02.622967 140583164200832 learning.py:507] global step 466: loss = 2.4689 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 467: loss = 2.4640 (0.700 sec/step)\n",
            "I1207 23:39:03.775110 140583164200832 learning.py:507] global step 467: loss = 2.4640 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 468: loss = 2.5439 (0.697 sec/step)\n",
            "I1207 23:39:04.969992 140583164200832 learning.py:507] global step 468: loss = 2.5439 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 469: loss = 2.7564 (0.641 sec/step)\n",
            "I1207 23:39:05.686532 140583164200832 learning.py:507] global step 469: loss = 2.7564 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 470: loss = 3.5725 (0.981 sec/step)\n",
            "I1207 23:39:06.888942 140583164200832 learning.py:507] global step 470: loss = 3.5725 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 471: loss = 3.4021 (1.825 sec/step)\n",
            "I1207 23:39:08.920414 140583164200832 learning.py:507] global step 471: loss = 3.4021 (1.825 sec/step)\n",
            "INFO:tensorflow:global step 472: loss = 2.4516 (0.651 sec/step)\n",
            "I1207 23:39:09.898508 140583164200832 learning.py:507] global step 472: loss = 2.4516 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 473: loss = 2.9000 (0.538 sec/step)\n",
            "I1207 23:39:10.488506 140583164200832 learning.py:507] global step 473: loss = 2.9000 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 474: loss = 2.9099 (0.675 sec/step)\n",
            "I1207 23:39:11.165462 140583164200832 learning.py:507] global step 474: loss = 2.9099 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 475: loss = 3.1023 (2.542 sec/step)\n",
            "I1207 23:39:13.785199 140583164200832 learning.py:507] global step 475: loss = 3.1023 (2.542 sec/step)\n",
            "INFO:tensorflow:global step 476: loss = 2.7659 (0.622 sec/step)\n",
            "I1207 23:39:14.727281 140583164200832 learning.py:507] global step 476: loss = 2.7659 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 477: loss = 2.4434 (1.747 sec/step)\n",
            "I1207 23:39:16.521996 140583164200832 learning.py:507] global step 477: loss = 2.4434 (1.747 sec/step)\n",
            "INFO:tensorflow:global step 478: loss = 3.2883 (0.692 sec/step)\n",
            "I1207 23:39:17.249183 140583164200832 learning.py:507] global step 478: loss = 3.2883 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 479: loss = 3.4586 (1.364 sec/step)\n",
            "I1207 23:39:18.801535 140583164200832 learning.py:507] global step 479: loss = 3.4586 (1.364 sec/step)\n",
            "INFO:tensorflow:global step 480: loss = 3.0834 (0.595 sec/step)\n",
            "I1207 23:39:19.601178 140583164200832 learning.py:507] global step 480: loss = 3.0834 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 481: loss = 3.0284 (1.237 sec/step)\n",
            "I1207 23:39:21.001253 140583164200832 learning.py:507] global step 481: loss = 3.0284 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 482: loss = 3.4320 (1.075 sec/step)\n",
            "I1207 23:39:22.077760 140583164200832 learning.py:507] global step 482: loss = 3.4320 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 483: loss = 2.0692 (0.572 sec/step)\n",
            "I1207 23:39:22.929856 140583164200832 learning.py:507] global step 483: loss = 2.0692 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 484: loss = 2.8033 (0.666 sec/step)\n",
            "I1207 23:39:23.942096 140583164200832 learning.py:507] global step 484: loss = 2.8033 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 485: loss = 3.6277 (0.600 sec/step)\n",
            "I1207 23:39:25.065551 140583164200832 learning.py:507] global step 485: loss = 3.6277 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 486: loss = 3.0596 (0.613 sec/step)\n",
            "I1207 23:39:25.715878 140583164200832 learning.py:507] global step 486: loss = 3.0596 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 487: loss = 3.0233 (1.661 sec/step)\n",
            "I1207 23:39:27.455496 140583164200832 learning.py:507] global step 487: loss = 3.0233 (1.661 sec/step)\n",
            "INFO:tensorflow:global step 488: loss = 2.3327 (1.596 sec/step)\n",
            "I1207 23:39:29.053135 140583164200832 learning.py:507] global step 488: loss = 2.3327 (1.596 sec/step)\n",
            "INFO:tensorflow:global step 489: loss = 2.7419 (0.624 sec/step)\n",
            "I1207 23:39:30.001429 140583164200832 learning.py:507] global step 489: loss = 2.7419 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 490: loss = 3.1051 (0.622 sec/step)\n",
            "I1207 23:39:30.718376 140583164200832 learning.py:507] global step 490: loss = 3.1051 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 491: loss = 2.6079 (1.626 sec/step)\n",
            "I1207 23:39:32.355133 140583164200832 learning.py:507] global step 491: loss = 2.6079 (1.626 sec/step)\n",
            "INFO:tensorflow:global step 492: loss = 2.8468 (0.774 sec/step)\n",
            "I1207 23:39:33.453978 140583164200832 learning.py:507] global step 492: loss = 2.8468 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 493: loss = 3.0664 (0.652 sec/step)\n",
            "I1207 23:39:34.461589 140583164200832 learning.py:507] global step 493: loss = 3.0664 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 494: loss = 2.5844 (0.610 sec/step)\n",
            "I1207 23:39:35.487225 140583164200832 learning.py:507] global step 494: loss = 2.5844 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 495: loss = 2.8626 (0.490 sec/step)\n",
            "I1207 23:39:36.209748 140583164200832 learning.py:507] global step 495: loss = 2.8626 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 496: loss = 3.2485 (1.954 sec/step)\n",
            "I1207 23:39:38.165698 140583164200832 learning.py:507] global step 496: loss = 3.2485 (1.954 sec/step)\n",
            "INFO:tensorflow:global step 497: loss = 3.0058 (0.570 sec/step)\n",
            "I1207 23:39:38.979562 140583164200832 learning.py:507] global step 497: loss = 3.0058 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 498: loss = 3.3846 (1.173 sec/step)\n",
            "I1207 23:39:40.500895 140583164200832 learning.py:507] global step 498: loss = 3.3846 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 499: loss = 2.3429 (0.559 sec/step)\n",
            "I1207 23:39:41.061936 140583164200832 learning.py:507] global step 499: loss = 2.3429 (0.559 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1207 23:39:42.076223 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0.849406\n",
            "I1207 23:39:42.188519 140579467605760 supervisor.py:1099] global_step/sec: 0.849406\n",
            "INFO:tensorflow:global step 500: loss = 3.5650 (2.557 sec/step)\n",
            "I1207 23:39:43.934947 140583164200832 learning.py:507] global step 500: loss = 3.5650 (2.557 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 501.\n",
            "I1207 23:39:45.697188 140579459213056 supervisor.py:1050] Recording summary at step 501.\n",
            "INFO:tensorflow:global step 501: loss = 2.8965 (1.651 sec/step)\n",
            "I1207 23:39:45.957647 140583164200832 learning.py:507] global step 501: loss = 2.8965 (1.651 sec/step)\n",
            "INFO:tensorflow:global step 502: loss = 3.0239 (1.685 sec/step)\n",
            "I1207 23:39:47.925458 140583164200832 learning.py:507] global step 502: loss = 3.0239 (1.685 sec/step)\n",
            "INFO:tensorflow:global step 503: loss = 3.5049 (0.560 sec/step)\n",
            "I1207 23:39:48.487013 140583164200832 learning.py:507] global step 503: loss = 3.5049 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 504: loss = 2.9548 (1.743 sec/step)\n",
            "I1207 23:39:50.231888 140583164200832 learning.py:507] global step 504: loss = 2.9548 (1.743 sec/step)\n",
            "INFO:tensorflow:global step 505: loss = 2.8110 (0.683 sec/step)\n",
            "I1207 23:39:51.211600 140583164200832 learning.py:507] global step 505: loss = 2.8110 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 506: loss = 3.4140 (0.712 sec/step)\n",
            "I1207 23:39:52.102766 140583164200832 learning.py:507] global step 506: loss = 3.4140 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 507: loss = 2.9210 (1.304 sec/step)\n",
            "I1207 23:39:53.488929 140583164200832 learning.py:507] global step 507: loss = 2.9210 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 508: loss = 2.6218 (0.656 sec/step)\n",
            "I1207 23:39:54.167548 140583164200832 learning.py:507] global step 508: loss = 2.6218 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 509: loss = 3.5319 (1.164 sec/step)\n",
            "I1207 23:39:55.539917 140583164200832 learning.py:507] global step 509: loss = 3.5319 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 510: loss = 2.5912 (0.468 sec/step)\n",
            "I1207 23:39:56.009599 140583164200832 learning.py:507] global step 510: loss = 2.5912 (0.468 sec/step)\n",
            "INFO:tensorflow:global step 511: loss = 3.0982 (0.812 sec/step)\n",
            "I1207 23:39:56.964945 140583164200832 learning.py:507] global step 511: loss = 3.0982 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 512: loss = 3.6520 (2.007 sec/step)\n",
            "I1207 23:39:59.117503 140583164200832 learning.py:507] global step 512: loss = 3.6520 (2.007 sec/step)\n",
            "INFO:tensorflow:global step 513: loss = 2.8835 (0.536 sec/step)\n",
            "I1207 23:39:59.655659 140583164200832 learning.py:507] global step 513: loss = 2.8835 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 514: loss = 2.1256 (0.813 sec/step)\n",
            "I1207 23:40:00.731090 140583164200832 learning.py:507] global step 514: loss = 2.1256 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 515: loss = 3.0128 (1.636 sec/step)\n",
            "I1207 23:40:02.651599 140583164200832 learning.py:507] global step 515: loss = 3.0128 (1.636 sec/step)\n",
            "INFO:tensorflow:global step 516: loss = 3.2412 (0.596 sec/step)\n",
            "I1207 23:40:03.496194 140583164200832 learning.py:507] global step 516: loss = 3.2412 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 517: loss = 3.2101 (0.675 sec/step)\n",
            "I1207 23:40:04.626754 140583164200832 learning.py:507] global step 517: loss = 3.2101 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 518: loss = 2.7543 (0.612 sec/step)\n",
            "I1207 23:40:05.743272 140583164200832 learning.py:507] global step 518: loss = 2.7543 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 519: loss = 2.7891 (1.351 sec/step)\n",
            "I1207 23:40:07.314478 140583164200832 learning.py:507] global step 519: loss = 2.7891 (1.351 sec/step)\n",
            "INFO:tensorflow:global step 520: loss = 2.7763 (0.546 sec/step)\n",
            "I1207 23:40:07.863039 140583164200832 learning.py:507] global step 520: loss = 2.7763 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 521: loss = 2.9679 (2.016 sec/step)\n",
            "I1207 23:40:09.881506 140583164200832 learning.py:507] global step 521: loss = 2.9679 (2.016 sec/step)\n",
            "INFO:tensorflow:global step 522: loss = 3.0303 (0.817 sec/step)\n",
            "I1207 23:40:10.856036 140583164200832 learning.py:507] global step 522: loss = 3.0303 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 523: loss = 2.7947 (1.275 sec/step)\n",
            "I1207 23:40:12.268278 140583164200832 learning.py:507] global step 523: loss = 2.7947 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 524: loss = 2.5293 (0.580 sec/step)\n",
            "I1207 23:40:13.077971 140583164200832 learning.py:507] global step 524: loss = 2.5293 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 525: loss = 2.7374 (1.273 sec/step)\n",
            "I1207 23:40:14.573181 140583164200832 learning.py:507] global step 525: loss = 2.7374 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 526: loss = 3.5050 (0.597 sec/step)\n",
            "I1207 23:40:15.286975 140583164200832 learning.py:507] global step 526: loss = 3.5050 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 527: loss = 2.4241 (1.544 sec/step)\n",
            "I1207 23:40:16.853415 140583164200832 learning.py:507] global step 527: loss = 2.4241 (1.544 sec/step)\n",
            "INFO:tensorflow:global step 528: loss = 2.4334 (1.051 sec/step)\n",
            "I1207 23:40:17.906274 140583164200832 learning.py:507] global step 528: loss = 2.4334 (1.051 sec/step)\n",
            "INFO:tensorflow:global step 529: loss = 2.8764 (0.686 sec/step)\n",
            "I1207 23:40:18.734338 140583164200832 learning.py:507] global step 529: loss = 2.8764 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 530: loss = 3.8250 (1.266 sec/step)\n",
            "I1207 23:40:20.253938 140583164200832 learning.py:507] global step 530: loss = 3.8250 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 531: loss = 2.5471 (0.778 sec/step)\n",
            "I1207 23:40:21.172472 140583164200832 learning.py:507] global step 531: loss = 2.5471 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 532: loss = 2.8183 (1.552 sec/step)\n",
            "I1207 23:40:22.772003 140583164200832 learning.py:507] global step 532: loss = 2.8183 (1.552 sec/step)\n",
            "INFO:tensorflow:global step 533: loss = 2.7420 (0.576 sec/step)\n",
            "I1207 23:40:23.466972 140583164200832 learning.py:507] global step 533: loss = 2.7420 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 534: loss = 3.0874 (0.529 sec/step)\n",
            "I1207 23:40:24.345943 140583164200832 learning.py:507] global step 534: loss = 3.0874 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 535: loss = 3.4462 (1.614 sec/step)\n",
            "I1207 23:40:26.054426 140583164200832 learning.py:507] global step 535: loss = 3.4462 (1.614 sec/step)\n",
            "INFO:tensorflow:global step 536: loss = 3.0088 (0.621 sec/step)\n",
            "I1207 23:40:26.892452 140583164200832 learning.py:507] global step 536: loss = 3.0088 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 537: loss = 2.6994 (0.651 sec/step)\n",
            "I1207 23:40:28.033199 140583164200832 learning.py:507] global step 537: loss = 2.6994 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 538: loss = 2.5484 (0.609 sec/step)\n",
            "I1207 23:40:29.015057 140583164200832 learning.py:507] global step 538: loss = 2.5484 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 539: loss = 3.1200 (0.693 sec/step)\n",
            "I1207 23:40:30.197767 140583164200832 learning.py:507] global step 539: loss = 3.1200 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 540: loss = 2.4111 (0.656 sec/step)\n",
            "I1207 23:40:31.273609 140583164200832 learning.py:507] global step 540: loss = 2.4111 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 541: loss = 2.7216 (0.671 sec/step)\n",
            "I1207 23:40:32.280147 140583164200832 learning.py:507] global step 541: loss = 2.7216 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 542: loss = 2.2929 (0.721 sec/step)\n",
            "I1207 23:40:33.358089 140583164200832 learning.py:507] global step 542: loss = 2.2929 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 543: loss = 3.1925 (0.600 sec/step)\n",
            "I1207 23:40:34.096968 140583164200832 learning.py:507] global step 543: loss = 3.1925 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 544: loss = 2.8931 (1.361 sec/step)\n",
            "I1207 23:40:35.688656 140583164200832 learning.py:507] global step 544: loss = 2.8931 (1.361 sec/step)\n",
            "INFO:tensorflow:global step 545: loss = 3.6008 (1.011 sec/step)\n",
            "I1207 23:40:36.701057 140583164200832 learning.py:507] global step 545: loss = 3.6008 (1.011 sec/step)\n",
            "INFO:tensorflow:global step 546: loss = 2.7613 (1.115 sec/step)\n",
            "I1207 23:40:37.818088 140583164200832 learning.py:507] global step 546: loss = 2.7613 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 547: loss = 2.4947 (0.534 sec/step)\n",
            "I1207 23:40:38.355996 140583164200832 learning.py:507] global step 547: loss = 2.4947 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 548: loss = 2.3298 (1.413 sec/step)\n",
            "I1207 23:40:40.073215 140583164200832 learning.py:507] global step 548: loss = 2.3298 (1.413 sec/step)\n",
            "INFO:tensorflow:global step 549: loss = 3.1300 (1.210 sec/step)\n",
            "I1207 23:40:41.307833 140583164200832 learning.py:507] global step 549: loss = 3.1300 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 550: loss = 2.4167 (0.535 sec/step)\n",
            "I1207 23:40:42.314296 140583164200832 learning.py:507] global step 550: loss = 2.4167 (0.535 sec/step)\n",
            "INFO:tensorflow:global step 551: loss = 2.9512 (0.833 sec/step)\n",
            "I1207 23:40:43.338591 140583164200832 learning.py:507] global step 551: loss = 2.9512 (0.833 sec/step)\n",
            "INFO:tensorflow:global step 552: loss = 3.3504 (1.291 sec/step)\n",
            "I1207 23:40:44.774647 140583164200832 learning.py:507] global step 552: loss = 3.3504 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 553: loss = 3.4934 (0.748 sec/step)\n",
            "I1207 23:40:45.814923 140583164200832 learning.py:507] global step 553: loss = 3.4934 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 554: loss = 3.2746 (0.643 sec/step)\n",
            "I1207 23:40:46.778589 140583164200832 learning.py:507] global step 554: loss = 3.2746 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 555: loss = 2.8711 (1.203 sec/step)\n",
            "I1207 23:40:48.092518 140583164200832 learning.py:507] global step 555: loss = 2.8711 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 556: loss = 3.9716 (0.756 sec/step)\n",
            "I1207 23:40:48.867366 140583164200832 learning.py:507] global step 556: loss = 3.9716 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 557: loss = 3.0735 (0.683 sec/step)\n",
            "I1207 23:40:50.111466 140583164200832 learning.py:507] global step 557: loss = 3.0735 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 558: loss = 3.1406 (0.764 sec/step)\n",
            "I1207 23:40:51.160082 140583164200832 learning.py:507] global step 558: loss = 3.1406 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 559: loss = 2.4947 (1.639 sec/step)\n",
            "I1207 23:40:52.823737 140583164200832 learning.py:507] global step 559: loss = 2.4947 (1.639 sec/step)\n",
            "INFO:tensorflow:global step 560: loss = 2.7827 (1.049 sec/step)\n",
            "I1207 23:40:53.874550 140583164200832 learning.py:507] global step 560: loss = 2.7827 (1.049 sec/step)\n",
            "INFO:tensorflow:global step 561: loss = 3.0807 (0.526 sec/step)\n",
            "I1207 23:40:54.730742 140583164200832 learning.py:507] global step 561: loss = 3.0807 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 562: loss = 2.9874 (0.622 sec/step)\n",
            "I1207 23:40:55.494287 140583164200832 learning.py:507] global step 562: loss = 2.9874 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 563: loss = 4.0964 (0.638 sec/step)\n",
            "I1207 23:40:56.252207 140583164200832 learning.py:507] global step 563: loss = 4.0964 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 564: loss = 2.7794 (1.771 sec/step)\n",
            "I1207 23:40:58.339643 140583164200832 learning.py:507] global step 564: loss = 2.7794 (1.771 sec/step)\n",
            "INFO:tensorflow:global step 565: loss = 3.1126 (0.601 sec/step)\n",
            "I1207 23:40:59.480568 140583164200832 learning.py:507] global step 565: loss = 3.1126 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 566: loss = 2.3997 (0.672 sec/step)\n",
            "I1207 23:41:00.164713 140583164200832 learning.py:507] global step 566: loss = 2.3997 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 567: loss = 2.9711 (2.431 sec/step)\n",
            "I1207 23:41:02.597150 140583164200832 learning.py:507] global step 567: loss = 2.9711 (2.431 sec/step)\n",
            "INFO:tensorflow:global step 568: loss = 3.2228 (0.587 sec/step)\n",
            "I1207 23:41:03.185661 140583164200832 learning.py:507] global step 568: loss = 3.2228 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 569: loss = 2.7714 (0.773 sec/step)\n",
            "I1207 23:41:04.081478 140583164200832 learning.py:507] global step 569: loss = 2.7714 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 570: loss = 2.6919 (1.660 sec/step)\n",
            "I1207 23:41:06.004765 140583164200832 learning.py:507] global step 570: loss = 2.6919 (1.660 sec/step)\n",
            "INFO:tensorflow:global step 571: loss = 3.1923 (0.687 sec/step)\n",
            "I1207 23:41:06.939470 140583164200832 learning.py:507] global step 571: loss = 3.1923 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 572: loss = 2.6469 (1.169 sec/step)\n",
            "I1207 23:41:08.262939 140583164200832 learning.py:507] global step 572: loss = 2.6469 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 573: loss = 2.6802 (0.619 sec/step)\n",
            "I1207 23:41:08.884025 140583164200832 learning.py:507] global step 573: loss = 2.6802 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 574: loss = 2.9973 (1.094 sec/step)\n",
            "I1207 23:41:10.180353 140583164200832 learning.py:507] global step 574: loss = 2.9973 (1.094 sec/step)\n",
            "INFO:tensorflow:global step 575: loss = 2.6284 (1.765 sec/step)\n",
            "I1207 23:41:12.269317 140583164200832 learning.py:507] global step 575: loss = 2.6284 (1.765 sec/step)\n",
            "INFO:tensorflow:global step 576: loss = 3.3714 (0.571 sec/step)\n",
            "I1207 23:41:13.122500 140583164200832 learning.py:507] global step 576: loss = 3.3714 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 577: loss = 2.2908 (0.667 sec/step)\n",
            "I1207 23:41:14.088746 140583164200832 learning.py:507] global step 577: loss = 2.2908 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 578: loss = 2.7771 (0.666 sec/step)\n",
            "I1207 23:41:15.109219 140583164200832 learning.py:507] global step 578: loss = 2.7771 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 579: loss = 2.6992 (0.676 sec/step)\n",
            "I1207 23:41:16.337323 140583164200832 learning.py:507] global step 579: loss = 2.6992 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 580: loss = 2.4790 (0.918 sec/step)\n",
            "I1207 23:41:17.747584 140583164200832 learning.py:507] global step 580: loss = 2.4790 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 581: loss = 2.8626 (1.073 sec/step)\n",
            "I1207 23:41:18.952207 140583164200832 learning.py:507] global step 581: loss = 2.8626 (1.073 sec/step)\n",
            "INFO:tensorflow:global step 582: loss = 3.2302 (0.584 sec/step)\n",
            "I1207 23:41:19.773360 140583164200832 learning.py:507] global step 582: loss = 3.2302 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 583: loss = 2.6070 (0.778 sec/step)\n",
            "I1207 23:41:20.802053 140583164200832 learning.py:507] global step 583: loss = 2.6070 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 584: loss = 2.7893 (0.690 sec/step)\n",
            "I1207 23:41:22.006069 140583164200832 learning.py:507] global step 584: loss = 2.7893 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 585: loss = 2.7397 (0.565 sec/step)\n",
            "I1207 23:41:22.706827 140583164200832 learning.py:507] global step 585: loss = 2.7397 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 586: loss = 2.6045 (0.945 sec/step)\n",
            "I1207 23:41:23.735723 140583164200832 learning.py:507] global step 586: loss = 2.6045 (0.945 sec/step)\n",
            "INFO:tensorflow:global step 587: loss = 3.0526 (2.355 sec/step)\n",
            "I1207 23:41:26.111305 140583164200832 learning.py:507] global step 587: loss = 3.0526 (2.355 sec/step)\n",
            "INFO:tensorflow:global step 588: loss = 2.6720 (0.513 sec/step)\n",
            "I1207 23:41:26.954551 140583164200832 learning.py:507] global step 588: loss = 2.6720 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 589: loss = 2.8993 (0.637 sec/step)\n",
            "I1207 23:41:27.606709 140583164200832 learning.py:507] global step 589: loss = 2.8993 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 590: loss = 2.5106 (0.753 sec/step)\n",
            "I1207 23:41:28.573991 140583164200832 learning.py:507] global step 590: loss = 2.5106 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 591: loss = 3.3758 (1.455 sec/step)\n",
            "I1207 23:41:30.264738 140583164200832 learning.py:507] global step 591: loss = 3.3758 (1.455 sec/step)\n",
            "INFO:tensorflow:global step 592: loss = 2.8999 (0.650 sec/step)\n",
            "I1207 23:41:31.091369 140583164200832 learning.py:507] global step 592: loss = 2.8999 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 593: loss = 2.5403 (1.367 sec/step)\n",
            "I1207 23:41:32.532791 140583164200832 learning.py:507] global step 593: loss = 2.5403 (1.367 sec/step)\n",
            "INFO:tensorflow:global step 594: loss = 3.0676 (1.083 sec/step)\n",
            "I1207 23:41:33.617554 140583164200832 learning.py:507] global step 594: loss = 3.0676 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 595: loss = 2.8178 (0.570 sec/step)\n",
            "I1207 23:41:34.453265 140583164200832 learning.py:507] global step 595: loss = 2.8178 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 596: loss = 2.3336 (1.206 sec/step)\n",
            "I1207 23:41:35.792543 140583164200832 learning.py:507] global step 596: loss = 2.3336 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 597: loss = 2.9449 (0.574 sec/step)\n",
            "I1207 23:41:36.585702 140583164200832 learning.py:507] global step 597: loss = 2.9449 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 598: loss = 3.1559 (1.430 sec/step)\n",
            "I1207 23:41:38.039035 140583164200832 learning.py:507] global step 598: loss = 3.1559 (1.430 sec/step)\n",
            "INFO:tensorflow:global step 599: loss = 3.5443 (0.592 sec/step)\n",
            "I1207 23:41:38.633473 140583164200832 learning.py:507] global step 599: loss = 3.5443 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 3.0456 (1.709 sec/step)\n",
            "I1207 23:41:40.344782 140583164200832 learning.py:507] global step 600: loss = 3.0456 (1.709 sec/step)\n",
            "INFO:tensorflow:global step 601: loss = 3.2702 (0.589 sec/step)\n",
            "I1207 23:41:41.193234 140583164200832 learning.py:507] global step 601: loss = 3.2702 (0.589 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.846417\n",
            "I1207 23:41:42.696520 140579467605760 supervisor.py:1099] global_step/sec: 0.846417\n",
            "INFO:tensorflow:global step 602: loss = 2.4432 (2.497 sec/step)\n",
            "I1207 23:41:43.826861 140583164200832 learning.py:507] global step 602: loss = 2.4432 (2.497 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 602.\n",
            "I1207 23:41:43.834245 140579459213056 supervisor.py:1050] Recording summary at step 602.\n",
            "INFO:tensorflow:global step 603: loss = 3.6819 (0.761 sec/step)\n",
            "I1207 23:41:44.880401 140583164200832 learning.py:507] global step 603: loss = 3.6819 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 604: loss = 3.7099 (0.582 sec/step)\n",
            "I1207 23:41:45.471660 140583164200832 learning.py:507] global step 604: loss = 3.7099 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 605: loss = 3.5782 (0.969 sec/step)\n",
            "I1207 23:41:46.466659 140583164200832 learning.py:507] global step 605: loss = 3.5782 (0.969 sec/step)\n",
            "INFO:tensorflow:global step 606: loss = 2.5999 (2.087 sec/step)\n",
            "I1207 23:41:48.555171 140583164200832 learning.py:507] global step 606: loss = 2.5999 (2.087 sec/step)\n",
            "INFO:tensorflow:global step 607: loss = 2.6204 (0.687 sec/step)\n",
            "I1207 23:41:49.420150 140583164200832 learning.py:507] global step 607: loss = 2.6204 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 608: loss = 2.6082 (1.298 sec/step)\n",
            "I1207 23:41:50.830614 140583164200832 learning.py:507] global step 608: loss = 2.6082 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 609: loss = 2.6615 (0.648 sec/step)\n",
            "I1207 23:41:51.640532 140583164200832 learning.py:507] global step 609: loss = 2.6615 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 610: loss = 3.1238 (1.347 sec/step)\n",
            "I1207 23:41:53.151726 140583164200832 learning.py:507] global step 610: loss = 3.1238 (1.347 sec/step)\n",
            "INFO:tensorflow:global step 611: loss = 3.1387 (0.482 sec/step)\n",
            "I1207 23:41:53.636410 140583164200832 learning.py:507] global step 611: loss = 3.1387 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 612: loss = 3.2536 (1.192 sec/step)\n",
            "I1207 23:41:55.023571 140583164200832 learning.py:507] global step 612: loss = 3.2536 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 613: loss = 2.9594 (1.388 sec/step)\n",
            "I1207 23:41:56.617303 140583164200832 learning.py:507] global step 613: loss = 2.9594 (1.388 sec/step)\n",
            "INFO:tensorflow:global step 614: loss = 2.8049 (0.683 sec/step)\n",
            "I1207 23:41:57.313767 140583164200832 learning.py:507] global step 614: loss = 2.8049 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 615: loss = 2.7179 (1.321 sec/step)\n",
            "I1207 23:41:58.846715 140583164200832 learning.py:507] global step 615: loss = 2.7179 (1.321 sec/step)\n",
            "INFO:tensorflow:global step 616: loss = 2.2849 (0.486 sec/step)\n",
            "I1207 23:41:59.651366 140583164200832 learning.py:507] global step 616: loss = 2.2849 (0.486 sec/step)\n",
            "INFO:tensorflow:global step 617: loss = 3.2043 (0.565 sec/step)\n",
            "I1207 23:42:00.474956 140583164200832 learning.py:507] global step 617: loss = 3.2043 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 618: loss = 3.4445 (1.438 sec/step)\n",
            "I1207 23:42:02.112652 140583164200832 learning.py:507] global step 618: loss = 3.4445 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 619: loss = 2.8044 (0.560 sec/step)\n",
            "I1207 23:42:02.936557 140583164200832 learning.py:507] global step 619: loss = 2.8044 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 620: loss = 2.8628 (0.695 sec/step)\n",
            "I1207 23:42:03.752022 140583164200832 learning.py:507] global step 620: loss = 2.8628 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 621: loss = 2.5755 (0.516 sec/step)\n",
            "I1207 23:42:04.269354 140583164200832 learning.py:507] global step 621: loss = 2.5755 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 622: loss = 3.0901 (1.954 sec/step)\n",
            "I1207 23:42:06.410178 140583164200832 learning.py:507] global step 622: loss = 3.0901 (1.954 sec/step)\n",
            "INFO:tensorflow:global step 623: loss = 2.9085 (1.330 sec/step)\n",
            "I1207 23:42:08.048879 140583164200832 learning.py:507] global step 623: loss = 2.9085 (1.330 sec/step)\n",
            "INFO:tensorflow:global step 624: loss = 2.4187 (0.683 sec/step)\n",
            "I1207 23:42:08.739431 140583164200832 learning.py:507] global step 624: loss = 2.4187 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 625: loss = 2.9980 (2.324 sec/step)\n",
            "I1207 23:42:11.327774 140583164200832 learning.py:507] global step 625: loss = 2.9980 (2.324 sec/step)\n",
            "INFO:tensorflow:global step 626: loss = 3.0959 (0.505 sec/step)\n",
            "I1207 23:42:11.834854 140583164200832 learning.py:507] global step 626: loss = 3.0959 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 627: loss = 2.5958 (1.791 sec/step)\n",
            "I1207 23:42:13.627432 140583164200832 learning.py:507] global step 627: loss = 2.5958 (1.791 sec/step)\n",
            "INFO:tensorflow:global step 628: loss = 2.4388 (0.730 sec/step)\n",
            "I1207 23:42:14.582676 140583164200832 learning.py:507] global step 628: loss = 2.4388 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 629: loss = 2.9783 (0.640 sec/step)\n",
            "I1207 23:42:15.275280 140583164200832 learning.py:507] global step 629: loss = 2.9783 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 630: loss = 2.5839 (0.565 sec/step)\n",
            "I1207 23:42:15.841766 140583164200832 learning.py:507] global step 630: loss = 2.5839 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 631: loss = 2.4054 (2.464 sec/step)\n",
            "I1207 23:42:18.512063 140583164200832 learning.py:507] global step 631: loss = 2.4054 (2.464 sec/step)\n",
            "INFO:tensorflow:global step 632: loss = 2.6639 (2.094 sec/step)\n",
            "I1207 23:42:20.693273 140583164200832 learning.py:507] global step 632: loss = 2.6639 (2.094 sec/step)\n",
            "INFO:tensorflow:global step 633: loss = 2.8322 (0.595 sec/step)\n",
            "I1207 23:42:21.542217 140583164200832 learning.py:507] global step 633: loss = 2.8322 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 634: loss = 2.5254 (1.380 sec/step)\n",
            "I1207 23:42:22.983844 140583164200832 learning.py:507] global step 634: loss = 2.5254 (1.380 sec/step)\n",
            "INFO:tensorflow:global step 635: loss = 2.3506 (0.574 sec/step)\n",
            "I1207 23:42:23.935299 140583164200832 learning.py:507] global step 635: loss = 2.3506 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 636: loss = 2.4949 (0.650 sec/step)\n",
            "I1207 23:42:24.928575 140583164200832 learning.py:507] global step 636: loss = 2.4949 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 637: loss = 2.8830 (1.458 sec/step)\n",
            "I1207 23:42:26.671634 140583164200832 learning.py:507] global step 637: loss = 2.8830 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 638: loss = 3.3129 (0.626 sec/step)\n",
            "I1207 23:42:27.454869 140583164200832 learning.py:507] global step 638: loss = 3.3129 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 639: loss = 2.7721 (1.731 sec/step)\n",
            "I1207 23:42:29.228196 140583164200832 learning.py:507] global step 639: loss = 2.7721 (1.731 sec/step)\n",
            "INFO:tensorflow:global step 640: loss = 2.8038 (0.538 sec/step)\n",
            "I1207 23:42:29.918564 140583164200832 learning.py:507] global step 640: loss = 2.8038 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 641: loss = 2.6340 (1.898 sec/step)\n",
            "I1207 23:42:31.995180 140583164200832 learning.py:507] global step 641: loss = 2.6340 (1.898 sec/step)\n",
            "INFO:tensorflow:global step 642: loss = 2.4632 (0.693 sec/step)\n",
            "I1207 23:42:32.948857 140583164200832 learning.py:507] global step 642: loss = 2.4632 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 643: loss = 2.9867 (0.623 sec/step)\n",
            "I1207 23:42:33.794722 140583164200832 learning.py:507] global step 643: loss = 2.9867 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 644: loss = 2.4232 (1.944 sec/step)\n",
            "I1207 23:42:35.740877 140583164200832 learning.py:507] global step 644: loss = 2.4232 (1.944 sec/step)\n",
            "INFO:tensorflow:global step 645: loss = 2.6359 (0.690 sec/step)\n",
            "I1207 23:42:36.756464 140583164200832 learning.py:507] global step 645: loss = 2.6359 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 646: loss = 2.6423 (0.559 sec/step)\n",
            "I1207 23:42:37.817211 140583164200832 learning.py:507] global step 646: loss = 2.6423 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 647: loss = 2.8327 (0.580 sec/step)\n",
            "I1207 23:42:38.492790 140583164200832 learning.py:507] global step 647: loss = 2.8327 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 648: loss = 2.7846 (1.762 sec/step)\n",
            "I1207 23:42:40.296218 140583164200832 learning.py:507] global step 648: loss = 2.7846 (1.762 sec/step)\n",
            "INFO:tensorflow:global step 649: loss = 3.0460 (1.758 sec/step)\n",
            "I1207 23:42:42.056205 140583164200832 learning.py:507] global step 649: loss = 3.0460 (1.758 sec/step)\n",
            "INFO:tensorflow:global step 650: loss = 3.3142 (0.604 sec/step)\n",
            "I1207 23:42:42.662164 140583164200832 learning.py:507] global step 650: loss = 3.3142 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 651: loss = 2.5604 (1.744 sec/step)\n",
            "I1207 23:42:44.408583 140583164200832 learning.py:507] global step 651: loss = 2.5604 (1.744 sec/step)\n",
            "INFO:tensorflow:global step 652: loss = 2.9152 (0.807 sec/step)\n",
            "I1207 23:42:45.470541 140583164200832 learning.py:507] global step 652: loss = 2.9152 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 653: loss = 3.5221 (0.526 sec/step)\n",
            "I1207 23:42:46.015312 140583164200832 learning.py:507] global step 653: loss = 3.5221 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 654: loss = 2.7168 (1.929 sec/step)\n",
            "I1207 23:42:47.946146 140583164200832 learning.py:507] global step 654: loss = 2.7168 (1.929 sec/step)\n",
            "INFO:tensorflow:global step 655: loss = 2.7694 (0.591 sec/step)\n",
            "I1207 23:42:48.861373 140583164200832 learning.py:507] global step 655: loss = 2.7694 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 656: loss = 2.3255 (0.626 sec/step)\n",
            "I1207 23:42:49.689131 140583164200832 learning.py:507] global step 656: loss = 2.3255 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 657: loss = 2.6461 (0.930 sec/step)\n",
            "I1207 23:42:50.692602 140583164200832 learning.py:507] global step 657: loss = 2.6461 (0.930 sec/step)\n",
            "INFO:tensorflow:global step 658: loss = 2.6541 (2.474 sec/step)\n",
            "I1207 23:42:53.186937 140583164200832 learning.py:507] global step 658: loss = 2.6541 (2.474 sec/step)\n",
            "INFO:tensorflow:global step 659: loss = 2.6270 (0.631 sec/step)\n",
            "I1207 23:42:54.260203 140583164200832 learning.py:507] global step 659: loss = 2.6270 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 660: loss = 3.4845 (0.692 sec/step)\n",
            "I1207 23:42:55.074843 140583164200832 learning.py:507] global step 660: loss = 3.4845 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 661: loss = 2.5793 (0.912 sec/step)\n",
            "I1207 23:42:56.098832 140583164200832 learning.py:507] global step 661: loss = 2.5793 (0.912 sec/step)\n",
            "INFO:tensorflow:global step 662: loss = 2.7253 (1.757 sec/step)\n",
            "I1207 23:42:58.258076 140583164200832 learning.py:507] global step 662: loss = 2.7253 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 663: loss = 3.0930 (0.518 sec/step)\n",
            "I1207 23:42:58.777987 140583164200832 learning.py:507] global step 663: loss = 3.0930 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 664: loss = 3.5462 (1.733 sec/step)\n",
            "I1207 23:43:00.513115 140583164200832 learning.py:507] global step 664: loss = 3.5462 (1.733 sec/step)\n",
            "INFO:tensorflow:global step 665: loss = 2.7012 (0.642 sec/step)\n",
            "I1207 23:43:01.157474 140583164200832 learning.py:507] global step 665: loss = 2.7012 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 666: loss = 3.2256 (2.141 sec/step)\n",
            "I1207 23:43:03.300447 140583164200832 learning.py:507] global step 666: loss = 3.2256 (2.141 sec/step)\n",
            "INFO:tensorflow:global step 667: loss = 2.6012 (0.760 sec/step)\n",
            "I1207 23:43:04.288106 140583164200832 learning.py:507] global step 667: loss = 2.6012 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 668: loss = 3.2477 (1.757 sec/step)\n",
            "I1207 23:43:06.148558 140583164200832 learning.py:507] global step 668: loss = 3.2477 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 669: loss = 2.4070 (0.474 sec/step)\n",
            "I1207 23:43:06.624302 140583164200832 learning.py:507] global step 669: loss = 2.4070 (0.474 sec/step)\n",
            "INFO:tensorflow:global step 670: loss = 3.2426 (1.910 sec/step)\n",
            "I1207 23:43:08.536432 140583164200832 learning.py:507] global step 670: loss = 3.2426 (1.910 sec/step)\n",
            "INFO:tensorflow:global step 671: loss = 2.5199 (0.574 sec/step)\n",
            "I1207 23:43:09.113389 140583164200832 learning.py:507] global step 671: loss = 2.5199 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 672: loss = 2.8333 (1.091 sec/step)\n",
            "I1207 23:43:10.376050 140583164200832 learning.py:507] global step 672: loss = 2.8333 (1.091 sec/step)\n",
            "INFO:tensorflow:global step 673: loss = 2.6157 (2.307 sec/step)\n",
            "I1207 23:43:12.985259 140583164200832 learning.py:507] global step 673: loss = 2.6157 (2.307 sec/step)\n",
            "INFO:tensorflow:global step 674: loss = 2.3844 (0.516 sec/step)\n",
            "I1207 23:43:13.503678 140583164200832 learning.py:507] global step 674: loss = 2.3844 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 675: loss = 2.4952 (0.585 sec/step)\n",
            "I1207 23:43:14.745317 140583164200832 learning.py:507] global step 675: loss = 2.4952 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 676: loss = 2.2083 (1.633 sec/step)\n",
            "I1207 23:43:16.661392 140583164200832 learning.py:507] global step 676: loss = 2.2083 (1.633 sec/step)\n",
            "INFO:tensorflow:global step 677: loss = 2.9064 (0.759 sec/step)\n",
            "I1207 23:43:17.755774 140583164200832 learning.py:507] global step 677: loss = 2.9064 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 678: loss = 2.5153 (0.539 sec/step)\n",
            "I1207 23:43:18.568499 140583164200832 learning.py:507] global step 678: loss = 2.5153 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 679: loss = 2.7183 (0.754 sec/step)\n",
            "I1207 23:43:19.636572 140583164200832 learning.py:507] global step 679: loss = 2.7183 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 680: loss = 3.4117 (1.388 sec/step)\n",
            "I1207 23:43:21.077273 140583164200832 learning.py:507] global step 680: loss = 3.4117 (1.388 sec/step)\n",
            "INFO:tensorflow:global step 681: loss = 3.3073 (0.637 sec/step)\n",
            "I1207 23:43:21.962975 140583164200832 learning.py:507] global step 681: loss = 3.3073 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 682: loss = 3.2055 (0.629 sec/step)\n",
            "I1207 23:43:22.877900 140583164200832 learning.py:507] global step 682: loss = 3.2055 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 683: loss = 2.8522 (1.495 sec/step)\n",
            "I1207 23:43:24.640527 140583164200832 learning.py:507] global step 683: loss = 2.8522 (1.495 sec/step)\n",
            "INFO:tensorflow:global step 684: loss = 2.7138 (0.489 sec/step)\n",
            "I1207 23:43:25.131399 140583164200832 learning.py:507] global step 684: loss = 2.7138 (0.489 sec/step)\n",
            "INFO:tensorflow:global step 685: loss = 2.6394 (1.750 sec/step)\n",
            "I1207 23:43:26.882868 140583164200832 learning.py:507] global step 685: loss = 2.6394 (1.750 sec/step)\n",
            "INFO:tensorflow:global step 686: loss = 2.7737 (0.470 sec/step)\n",
            "I1207 23:43:27.354065 140583164200832 learning.py:507] global step 686: loss = 2.7737 (0.470 sec/step)\n",
            "INFO:tensorflow:global step 687: loss = 3.4139 (0.516 sec/step)\n",
            "I1207 23:43:27.871379 140583164200832 learning.py:507] global step 687: loss = 3.4139 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 688: loss = 2.4491 (0.686 sec/step)\n",
            "I1207 23:43:29.251905 140583164200832 learning.py:507] global step 688: loss = 2.4491 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 689: loss = 2.6361 (2.510 sec/step)\n",
            "I1207 23:43:31.937222 140583164200832 learning.py:507] global step 689: loss = 2.6361 (2.510 sec/step)\n",
            "INFO:tensorflow:global step 690: loss = 2.5345 (1.495 sec/step)\n",
            "I1207 23:43:33.461422 140583164200832 learning.py:507] global step 690: loss = 2.5345 (1.495 sec/step)\n",
            "INFO:tensorflow:global step 691: loss = 2.6110 (0.574 sec/step)\n",
            "I1207 23:43:34.037536 140583164200832 learning.py:507] global step 691: loss = 2.6110 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 692: loss = 2.9847 (1.791 sec/step)\n",
            "I1207 23:43:35.830713 140583164200832 learning.py:507] global step 692: loss = 2.9847 (1.791 sec/step)\n",
            "INFO:tensorflow:global step 693: loss = 2.4817 (0.701 sec/step)\n",
            "I1207 23:43:36.724488 140583164200832 learning.py:507] global step 693: loss = 2.4817 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 694: loss = 2.6790 (0.563 sec/step)\n",
            "I1207 23:43:37.476137 140583164200832 learning.py:507] global step 694: loss = 2.6790 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 695: loss = 2.6297 (1.706 sec/step)\n",
            "I1207 23:43:39.184386 140583164200832 learning.py:507] global step 695: loss = 2.6297 (1.706 sec/step)\n",
            "INFO:tensorflow:global step 696: loss = 2.5410 (0.637 sec/step)\n",
            "I1207 23:43:40.284279 140583164200832 learning.py:507] global step 696: loss = 2.5410 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 697: loss = 2.6165 (1.221 sec/step)\n",
            "I1207 23:43:41.597973 140583164200832 learning.py:507] global step 697: loss = 2.6165 (1.221 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.80365\n",
            "I1207 23:43:42.151547 140579467605760 supervisor.py:1099] global_step/sec: 0.80365\n",
            "INFO:tensorflow:global step 698: loss = 2.7067 (1.302 sec/step)\n",
            "I1207 23:43:43.363723 140583164200832 learning.py:507] global step 698: loss = 2.7067 (1.302 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 698.\n",
            "I1207 23:43:45.155007 140579459213056 supervisor.py:1050] Recording summary at step 698.\n",
            "INFO:tensorflow:global step 699: loss = 2.9469 (1.992 sec/step)\n",
            "I1207 23:43:45.359591 140583164200832 learning.py:507] global step 699: loss = 2.9469 (1.992 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 3.0999 (0.559 sec/step)\n",
            "I1207 23:43:45.920508 140583164200832 learning.py:507] global step 700: loss = 3.0999 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 701: loss = 2.8754 (0.916 sec/step)\n",
            "I1207 23:43:47.086782 140583164200832 learning.py:507] global step 701: loss = 2.8754 (0.916 sec/step)\n",
            "INFO:tensorflow:global step 702: loss = 2.6090 (0.660 sec/step)\n",
            "I1207 23:43:48.088430 140583164200832 learning.py:507] global step 702: loss = 2.6090 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 703: loss = 2.9650 (1.926 sec/step)\n",
            "I1207 23:43:50.183634 140583164200832 learning.py:507] global step 703: loss = 2.9650 (1.926 sec/step)\n",
            "INFO:tensorflow:global step 704: loss = 2.7764 (0.637 sec/step)\n",
            "I1207 23:43:51.100329 140583164200832 learning.py:507] global step 704: loss = 2.7764 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 705: loss = 3.1496 (1.377 sec/step)\n",
            "I1207 23:43:52.640089 140583164200832 learning.py:507] global step 705: loss = 3.1496 (1.377 sec/step)\n",
            "INFO:tensorflow:global step 706: loss = 2.9940 (0.531 sec/step)\n",
            "I1207 23:43:53.311501 140583164200832 learning.py:507] global step 706: loss = 2.9940 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 707: loss = 2.6016 (1.220 sec/step)\n",
            "I1207 23:43:54.805335 140583164200832 learning.py:507] global step 707: loss = 2.6016 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 708: loss = 2.7546 (0.598 sec/step)\n",
            "I1207 23:43:55.750639 140583164200832 learning.py:507] global step 708: loss = 2.7546 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 709: loss = 2.7329 (0.623 sec/step)\n",
            "I1207 23:43:56.855947 140583164200832 learning.py:507] global step 709: loss = 2.7329 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 710: loss = 2.4112 (0.794 sec/step)\n",
            "I1207 23:43:57.872349 140583164200832 learning.py:507] global step 710: loss = 2.4112 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 711: loss = 2.9556 (0.802 sec/step)\n",
            "I1207 23:43:59.210590 140583164200832 learning.py:507] global step 711: loss = 2.9556 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 712: loss = 2.8709 (1.277 sec/step)\n",
            "I1207 23:44:00.538715 140583164200832 learning.py:507] global step 712: loss = 2.8709 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 713: loss = 2.5135 (0.618 sec/step)\n",
            "I1207 23:44:01.216891 140583164200832 learning.py:507] global step 713: loss = 2.5135 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 714: loss = 2.0684 (1.497 sec/step)\n",
            "I1207 23:44:02.747984 140583164200832 learning.py:507] global step 714: loss = 2.0684 (1.497 sec/step)\n",
            "INFO:tensorflow:global step 715: loss = 3.4912 (0.502 sec/step)\n",
            "I1207 23:44:03.569844 140583164200832 learning.py:507] global step 715: loss = 3.4912 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 716: loss = 3.3628 (1.573 sec/step)\n",
            "I1207 23:44:05.274427 140583164200832 learning.py:507] global step 716: loss = 3.3628 (1.573 sec/step)\n",
            "INFO:tensorflow:global step 717: loss = 2.9567 (0.639 sec/step)\n",
            "I1207 23:44:06.280318 140583164200832 learning.py:507] global step 717: loss = 2.9567 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 718: loss = 2.3734 (1.242 sec/step)\n",
            "I1207 23:44:07.615989 140583164200832 learning.py:507] global step 718: loss = 2.3734 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 719: loss = 3.2554 (0.564 sec/step)\n",
            "I1207 23:44:08.182126 140583164200832 learning.py:507] global step 719: loss = 3.2554 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 720: loss = 2.3160 (0.656 sec/step)\n",
            "I1207 23:44:08.840832 140583164200832 learning.py:507] global step 720: loss = 2.3160 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 721: loss = 3.0693 (2.760 sec/step)\n",
            "I1207 23:44:11.642506 140583164200832 learning.py:507] global step 721: loss = 3.0693 (2.760 sec/step)\n",
            "INFO:tensorflow:global step 722: loss = 2.6320 (0.623 sec/step)\n",
            "I1207 23:44:12.268140 140583164200832 learning.py:507] global step 722: loss = 2.6320 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 723: loss = 3.0483 (1.056 sec/step)\n",
            "I1207 23:44:13.555428 140583164200832 learning.py:507] global step 723: loss = 3.0483 (1.056 sec/step)\n",
            "INFO:tensorflow:global step 724: loss = 2.2616 (2.113 sec/step)\n",
            "I1207 23:44:15.832951 140583164200832 learning.py:507] global step 724: loss = 2.2616 (2.113 sec/step)\n",
            "INFO:tensorflow:global step 725: loss = 2.8691 (0.797 sec/step)\n",
            "I1207 23:44:16.662973 140583164200832 learning.py:507] global step 725: loss = 2.8691 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 726: loss = 3.0654 (1.594 sec/step)\n",
            "I1207 23:44:18.346491 140583164200832 learning.py:507] global step 726: loss = 3.0654 (1.594 sec/step)\n",
            "INFO:tensorflow:global step 727: loss = 2.4901 (0.672 sec/step)\n",
            "I1207 23:44:19.353635 140583164200832 learning.py:507] global step 727: loss = 2.4901 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 728: loss = 2.5042 (0.606 sec/step)\n",
            "I1207 23:44:19.984359 140583164200832 learning.py:507] global step 728: loss = 2.5042 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 729: loss = 2.3915 (1.001 sec/step)\n",
            "I1207 23:44:21.223074 140583164200832 learning.py:507] global step 729: loss = 2.3915 (1.001 sec/step)\n",
            "INFO:tensorflow:global step 730: loss = 2.4887 (1.687 sec/step)\n",
            "I1207 23:44:23.163500 140583164200832 learning.py:507] global step 730: loss = 2.4887 (1.687 sec/step)\n",
            "INFO:tensorflow:global step 731: loss = 3.0430 (0.521 sec/step)\n",
            "I1207 23:44:23.686954 140583164200832 learning.py:507] global step 731: loss = 3.0430 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 732: loss = 3.0265 (1.186 sec/step)\n",
            "I1207 23:44:25.130979 140583164200832 learning.py:507] global step 732: loss = 3.0265 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 733: loss = 2.8029 (1.801 sec/step)\n",
            "I1207 23:44:27.239179 140583164200832 learning.py:507] global step 733: loss = 2.8029 (1.801 sec/step)\n",
            "INFO:tensorflow:global step 734: loss = 2.6838 (0.580 sec/step)\n",
            "I1207 23:44:28.300643 140583164200832 learning.py:507] global step 734: loss = 2.6838 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 735: loss = 3.0646 (1.375 sec/step)\n",
            "I1207 23:44:29.742938 140583164200832 learning.py:507] global step 735: loss = 3.0646 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 736: loss = 2.3613 (0.770 sec/step)\n",
            "I1207 23:44:30.886408 140583164200832 learning.py:507] global step 736: loss = 2.3613 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 737: loss = 2.5218 (0.607 sec/step)\n",
            "I1207 23:44:31.924998 140583164200832 learning.py:507] global step 737: loss = 2.5218 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 738: loss = 2.4671 (0.653 sec/step)\n",
            "I1207 23:44:32.881828 140583164200832 learning.py:507] global step 738: loss = 2.4671 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 739: loss = 3.3302 (0.594 sec/step)\n",
            "I1207 23:44:33.709776 140583164200832 learning.py:507] global step 739: loss = 3.3302 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 740: loss = 2.5853 (1.651 sec/step)\n",
            "I1207 23:44:35.390003 140583164200832 learning.py:507] global step 740: loss = 2.5853 (1.651 sec/step)\n",
            "INFO:tensorflow:global step 741: loss = 2.1353 (0.740 sec/step)\n",
            "I1207 23:44:36.284958 140583164200832 learning.py:507] global step 741: loss = 2.1353 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 742: loss = 2.4736 (0.680 sec/step)\n",
            "I1207 23:44:37.279175 140583164200832 learning.py:507] global step 742: loss = 2.4736 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 743: loss = 2.8075 (1.153 sec/step)\n",
            "I1207 23:44:38.621284 140583164200832 learning.py:507] global step 743: loss = 2.8075 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 744: loss = 2.1565 (0.573 sec/step)\n",
            "I1207 23:44:39.488970 140583164200832 learning.py:507] global step 744: loss = 2.1565 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 745: loss = 3.0131 (1.626 sec/step)\n",
            "I1207 23:44:41.123600 140583164200832 learning.py:507] global step 745: loss = 3.0131 (1.626 sec/step)\n",
            "INFO:tensorflow:global step 746: loss = 2.4631 (0.592 sec/step)\n",
            "I1207 23:44:42.108843 140583164200832 learning.py:507] global step 746: loss = 2.4631 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 747: loss = 3.1401 (1.246 sec/step)\n",
            "I1207 23:44:43.471853 140583164200832 learning.py:507] global step 747: loss = 3.1401 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 748: loss = 2.4867 (1.115 sec/step)\n",
            "I1207 23:44:44.588357 140583164200832 learning.py:507] global step 748: loss = 2.4867 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 749: loss = 2.4273 (0.713 sec/step)\n",
            "I1207 23:44:45.316991 140583164200832 learning.py:507] global step 749: loss = 2.4273 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 750: loss = 3.4589 (1.478 sec/step)\n",
            "I1207 23:44:46.822413 140583164200832 learning.py:507] global step 750: loss = 3.4589 (1.478 sec/step)\n",
            "INFO:tensorflow:global step 751: loss = 2.6029 (0.769 sec/step)\n",
            "I1207 23:44:47.828474 140583164200832 learning.py:507] global step 751: loss = 2.6029 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 752: loss = 2.3404 (0.630 sec/step)\n",
            "I1207 23:44:48.654064 140583164200832 learning.py:507] global step 752: loss = 2.3404 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 753: loss = 2.0899 (0.614 sec/step)\n",
            "I1207 23:44:49.879048 140583164200832 learning.py:507] global step 753: loss = 2.0899 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 754: loss = 2.3470 (0.539 sec/step)\n",
            "I1207 23:44:50.842206 140583164200832 learning.py:507] global step 754: loss = 2.3470 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 755: loss = 3.0888 (1.632 sec/step)\n",
            "I1207 23:44:52.610693 140583164200832 learning.py:507] global step 755: loss = 3.0888 (1.632 sec/step)\n",
            "INFO:tensorflow:global step 756: loss = 2.1666 (0.618 sec/step)\n",
            "I1207 23:44:53.505167 140583164200832 learning.py:507] global step 756: loss = 2.1666 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 757: loss = 2.7873 (1.245 sec/step)\n",
            "I1207 23:44:54.886887 140583164200832 learning.py:507] global step 757: loss = 2.7873 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 758: loss = 3.0486 (0.634 sec/step)\n",
            "I1207 23:44:55.724539 140583164200832 learning.py:507] global step 758: loss = 3.0486 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 759: loss = 2.9588 (0.819 sec/step)\n",
            "I1207 23:44:56.808319 140583164200832 learning.py:507] global step 759: loss = 2.9588 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 760: loss = 3.1628 (1.819 sec/step)\n",
            "I1207 23:44:58.665632 140583164200832 learning.py:507] global step 760: loss = 3.1628 (1.819 sec/step)\n",
            "INFO:tensorflow:global step 761: loss = 3.0070 (0.673 sec/step)\n",
            "I1207 23:44:59.340932 140583164200832 learning.py:507] global step 761: loss = 3.0070 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 762: loss = 2.9975 (1.879 sec/step)\n",
            "I1207 23:45:01.221920 140583164200832 learning.py:507] global step 762: loss = 2.9975 (1.879 sec/step)\n",
            "INFO:tensorflow:global step 763: loss = 2.8406 (0.492 sec/step)\n",
            "I1207 23:45:01.715253 140583164200832 learning.py:507] global step 763: loss = 2.8406 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 764: loss = 2.3321 (0.521 sec/step)\n",
            "I1207 23:45:02.237869 140583164200832 learning.py:507] global step 764: loss = 2.3321 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 765: loss = 2.6793 (2.830 sec/step)\n",
            "I1207 23:45:05.070435 140583164200832 learning.py:507] global step 765: loss = 2.6793 (2.830 sec/step)\n",
            "INFO:tensorflow:global step 766: loss = 2.5932 (0.578 sec/step)\n",
            "I1207 23:45:06.050169 140583164200832 learning.py:507] global step 766: loss = 2.5932 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 767: loss = 2.4324 (1.388 sec/step)\n",
            "I1207 23:45:07.521957 140583164200832 learning.py:507] global step 767: loss = 2.4324 (1.388 sec/step)\n",
            "INFO:tensorflow:global step 768: loss = 1.8816 (0.737 sec/step)\n",
            "I1207 23:45:08.369950 140583164200832 learning.py:507] global step 768: loss = 1.8816 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 769: loss = 2.3964 (1.300 sec/step)\n",
            "I1207 23:45:09.906754 140583164200832 learning.py:507] global step 769: loss = 2.3964 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 770: loss = 2.4070 (0.609 sec/step)\n",
            "I1207 23:45:10.807994 140583164200832 learning.py:507] global step 770: loss = 2.4070 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 771: loss = 3.2641 (0.598 sec/step)\n",
            "I1207 23:45:11.801141 140583164200832 learning.py:507] global step 771: loss = 3.2641 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 772: loss = 2.2040 (0.541 sec/step)\n",
            "I1207 23:45:12.604007 140583164200832 learning.py:507] global step 772: loss = 2.2040 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 773: loss = 2.3823 (1.566 sec/step)\n",
            "I1207 23:45:14.324377 140583164200832 learning.py:507] global step 773: loss = 2.3823 (1.566 sec/step)\n",
            "INFO:tensorflow:global step 774: loss = 2.8113 (0.592 sec/step)\n",
            "I1207 23:45:15.308982 140583164200832 learning.py:507] global step 774: loss = 2.8113 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 775: loss = 2.2760 (1.273 sec/step)\n",
            "I1207 23:45:16.655237 140583164200832 learning.py:507] global step 775: loss = 2.2760 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 776: loss = 3.0699 (0.645 sec/step)\n",
            "I1207 23:45:17.521409 140583164200832 learning.py:507] global step 776: loss = 3.0699 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 777: loss = 2.3233 (1.249 sec/step)\n",
            "I1207 23:45:18.931491 140583164200832 learning.py:507] global step 777: loss = 2.3233 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 778: loss = 3.3296 (0.565 sec/step)\n",
            "I1207 23:45:19.844139 140583164200832 learning.py:507] global step 778: loss = 3.3296 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 779: loss = 2.8650 (1.153 sec/step)\n",
            "I1207 23:45:21.108117 140583164200832 learning.py:507] global step 779: loss = 2.8650 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 780: loss = 3.2486 (0.746 sec/step)\n",
            "I1207 23:45:22.141532 140583164200832 learning.py:507] global step 780: loss = 3.2486 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 781: loss = 2.4440 (0.528 sec/step)\n",
            "I1207 23:45:22.926337 140583164200832 learning.py:507] global step 781: loss = 2.4440 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 782: loss = 2.6031 (1.248 sec/step)\n",
            "I1207 23:45:24.417323 140583164200832 learning.py:507] global step 782: loss = 2.6031 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 783: loss = 2.5740 (0.553 sec/step)\n",
            "I1207 23:45:24.972366 140583164200832 learning.py:507] global step 783: loss = 2.5740 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 784: loss = 3.1097 (1.118 sec/step)\n",
            "I1207 23:45:26.232964 140583164200832 learning.py:507] global step 784: loss = 3.1097 (1.118 sec/step)\n",
            "INFO:tensorflow:global step 785: loss = 3.1723 (1.470 sec/step)\n",
            "I1207 23:45:27.852289 140583164200832 learning.py:507] global step 785: loss = 3.1723 (1.470 sec/step)\n",
            "INFO:tensorflow:global step 786: loss = 3.2412 (0.673 sec/step)\n",
            "I1207 23:45:28.558664 140583164200832 learning.py:507] global step 786: loss = 3.2412 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 787: loss = 2.5096 (1.553 sec/step)\n",
            "I1207 23:45:30.147752 140583164200832 learning.py:507] global step 787: loss = 2.5096 (1.553 sec/step)\n",
            "INFO:tensorflow:global step 788: loss = 3.0467 (0.482 sec/step)\n",
            "I1207 23:45:30.631551 140583164200832 learning.py:507] global step 788: loss = 3.0467 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 789: loss = 2.6665 (1.517 sec/step)\n",
            "I1207 23:45:32.198665 140583164200832 learning.py:507] global step 789: loss = 2.6665 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 790: loss = 2.8658 (1.264 sec/step)\n",
            "I1207 23:45:33.472233 140583164200832 learning.py:507] global step 790: loss = 2.8658 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 791: loss = 3.0429 (0.577 sec/step)\n",
            "I1207 23:45:34.361058 140583164200832 learning.py:507] global step 791: loss = 3.0429 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 792: loss = 3.4993 (1.234 sec/step)\n",
            "I1207 23:45:35.760247 140583164200832 learning.py:507] global step 792: loss = 3.4993 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 793: loss = 2.6538 (0.550 sec/step)\n",
            "I1207 23:45:36.312291 140583164200832 learning.py:507] global step 793: loss = 2.6538 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 794: loss = 2.7747 (1.590 sec/step)\n",
            "I1207 23:45:37.903827 140583164200832 learning.py:507] global step 794: loss = 2.7747 (1.590 sec/step)\n",
            "INFO:tensorflow:global step 795: loss = 2.4757 (0.522 sec/step)\n",
            "I1207 23:45:38.427626 140583164200832 learning.py:507] global step 795: loss = 2.4757 (0.522 sec/step)\n",
            "INFO:tensorflow:global step 796: loss = 3.3630 (0.539 sec/step)\n",
            "I1207 23:45:38.969111 140583164200832 learning.py:507] global step 796: loss = 3.3630 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 797: loss = 2.6144 (1.525 sec/step)\n",
            "I1207 23:45:40.646532 140583164200832 learning.py:507] global step 797: loss = 2.6144 (1.525 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.832538\n",
            "I1207 23:45:42.266320 140579467605760 supervisor.py:1099] global_step/sec: 0.832538\n",
            "INFO:tensorflow:Recording summary at step 798.\n",
            "I1207 23:45:45.289938 140579459213056 supervisor.py:1050] Recording summary at step 798.\n",
            "INFO:tensorflow:global step 798: loss = 3.3275 (4.464 sec/step)\n",
            "I1207 23:45:45.301013 140583164200832 learning.py:507] global step 798: loss = 3.3275 (4.464 sec/step)\n",
            "INFO:tensorflow:global step 799: loss = 2.5109 (0.582 sec/step)\n",
            "I1207 23:45:45.886234 140583164200832 learning.py:507] global step 799: loss = 2.5109 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 2.5491 (1.917 sec/step)\n",
            "I1207 23:45:47.805496 140583164200832 learning.py:507] global step 800: loss = 2.5491 (1.917 sec/step)\n",
            "INFO:tensorflow:global step 801: loss = 2.9364 (0.610 sec/step)\n",
            "I1207 23:45:48.417109 140583164200832 learning.py:507] global step 801: loss = 2.9364 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 802: loss = 2.6301 (2.306 sec/step)\n",
            "I1207 23:45:50.725691 140583164200832 learning.py:507] global step 802: loss = 2.6301 (2.306 sec/step)\n",
            "INFO:tensorflow:global step 803: loss = 3.2948 (0.484 sec/step)\n",
            "I1207 23:45:51.211490 140583164200832 learning.py:507] global step 803: loss = 3.2948 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 804: loss = 2.5990 (1.118 sec/step)\n",
            "I1207 23:45:52.539206 140583164200832 learning.py:507] global step 804: loss = 2.5990 (1.118 sec/step)\n",
            "INFO:tensorflow:global step 805: loss = 2.7023 (1.649 sec/step)\n",
            "I1207 23:45:54.417939 140583164200832 learning.py:507] global step 805: loss = 2.7023 (1.649 sec/step)\n",
            "INFO:tensorflow:global step 806: loss = 2.6491 (0.601 sec/step)\n",
            "I1207 23:45:55.132100 140583164200832 learning.py:507] global step 806: loss = 2.6491 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 807: loss = 2.6341 (0.769 sec/step)\n",
            "I1207 23:45:56.172976 140583164200832 learning.py:507] global step 807: loss = 2.6341 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 808: loss = 2.5347 (1.368 sec/step)\n",
            "I1207 23:45:57.701121 140583164200832 learning.py:507] global step 808: loss = 2.5347 (1.368 sec/step)\n",
            "INFO:tensorflow:global step 809: loss = 3.4278 (0.494 sec/step)\n",
            "I1207 23:45:58.197538 140583164200832 learning.py:507] global step 809: loss = 3.4278 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 810: loss = 3.1978 (1.691 sec/step)\n",
            "I1207 23:45:59.891023 140583164200832 learning.py:507] global step 810: loss = 3.1978 (1.691 sec/step)\n",
            "INFO:tensorflow:global step 811: loss = 2.4476 (0.730 sec/step)\n",
            "I1207 23:46:00.747373 140583164200832 learning.py:507] global step 811: loss = 2.4476 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 812: loss = 2.7402 (1.223 sec/step)\n",
            "I1207 23:46:02.191654 140583164200832 learning.py:507] global step 812: loss = 2.7402 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 813: loss = 2.2446 (0.543 sec/step)\n",
            "I1207 23:46:02.736952 140583164200832 learning.py:507] global step 813: loss = 2.2446 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 814: loss = 3.2071 (0.899 sec/step)\n",
            "I1207 23:46:03.947834 140583164200832 learning.py:507] global step 814: loss = 3.2071 (0.899 sec/step)\n",
            "INFO:tensorflow:global step 815: loss = 3.1379 (1.431 sec/step)\n",
            "I1207 23:46:05.704230 140583164200832 learning.py:507] global step 815: loss = 3.1379 (1.431 sec/step)\n",
            "INFO:tensorflow:global step 816: loss = 2.4195 (0.623 sec/step)\n",
            "I1207 23:46:06.471096 140583164200832 learning.py:507] global step 816: loss = 2.4195 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 817: loss = 2.7223 (2.152 sec/step)\n",
            "I1207 23:46:08.638355 140583164200832 learning.py:507] global step 817: loss = 2.7223 (2.152 sec/step)\n",
            "INFO:tensorflow:global step 818: loss = 3.0868 (0.632 sec/step)\n",
            "I1207 23:46:09.439012 140583164200832 learning.py:507] global step 818: loss = 3.0868 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 819: loss = 2.7472 (0.575 sec/step)\n",
            "I1207 23:46:10.385647 140583164200832 learning.py:507] global step 819: loss = 2.7472 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 820: loss = 2.3438 (0.549 sec/step)\n",
            "I1207 23:46:10.936944 140583164200832 learning.py:507] global step 820: loss = 2.3438 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 821: loss = 2.6934 (1.215 sec/step)\n",
            "I1207 23:46:12.386138 140583164200832 learning.py:507] global step 821: loss = 2.6934 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 822: loss = 2.8133 (2.274 sec/step)\n",
            "I1207 23:46:14.812136 140583164200832 learning.py:507] global step 822: loss = 2.8133 (2.274 sec/step)\n",
            "INFO:tensorflow:global step 823: loss = 3.6274 (0.566 sec/step)\n",
            "I1207 23:46:15.657831 140583164200832 learning.py:507] global step 823: loss = 3.6274 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 824: loss = 2.1905 (0.680 sec/step)\n",
            "I1207 23:46:16.840062 140583164200832 learning.py:507] global step 824: loss = 2.1905 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 825: loss = 2.4368 (0.697 sec/step)\n",
            "I1207 23:46:17.832365 140583164200832 learning.py:507] global step 825: loss = 2.4368 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 826: loss = 3.0138 (1.205 sec/step)\n",
            "I1207 23:46:19.225539 140583164200832 learning.py:507] global step 826: loss = 3.0138 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 827: loss = 2.5877 (0.575 sec/step)\n",
            "I1207 23:46:20.138514 140583164200832 learning.py:507] global step 827: loss = 2.5877 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 828: loss = 2.5797 (2.698 sec/step)\n",
            "I1207 23:46:22.857102 140583164200832 learning.py:507] global step 828: loss = 2.5797 (2.698 sec/step)\n",
            "INFO:tensorflow:global step 829: loss = 2.8236 (0.570 sec/step)\n",
            "I1207 23:46:23.723458 140583164200832 learning.py:507] global step 829: loss = 2.8236 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 830: loss = 2.5484 (0.729 sec/step)\n",
            "I1207 23:46:24.890765 140583164200832 learning.py:507] global step 830: loss = 2.5484 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 831: loss = 3.0326 (0.529 sec/step)\n",
            "I1207 23:46:25.659602 140583164200832 learning.py:507] global step 831: loss = 3.0326 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 832: loss = 2.5464 (1.903 sec/step)\n",
            "I1207 23:46:27.565040 140583164200832 learning.py:507] global step 832: loss = 2.5464 (1.903 sec/step)\n",
            "INFO:tensorflow:global step 833: loss = 2.7363 (0.647 sec/step)\n",
            "I1207 23:46:28.488001 140583164200832 learning.py:507] global step 833: loss = 2.7363 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 834: loss = 2.8916 (0.787 sec/step)\n",
            "I1207 23:46:29.580032 140583164200832 learning.py:507] global step 834: loss = 2.8916 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 835: loss = 3.2694 (1.283 sec/step)\n",
            "I1207 23:46:31.074548 140583164200832 learning.py:507] global step 835: loss = 3.2694 (1.283 sec/step)\n",
            "INFO:tensorflow:global step 836: loss = 2.6296 (0.688 sec/step)\n",
            "I1207 23:46:31.988356 140583164200832 learning.py:507] global step 836: loss = 2.6296 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 837: loss = 2.1952 (1.458 sec/step)\n",
            "I1207 23:46:33.607477 140583164200832 learning.py:507] global step 837: loss = 2.1952 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 838: loss = 2.4153 (1.176 sec/step)\n",
            "I1207 23:46:34.785645 140583164200832 learning.py:507] global step 838: loss = 2.4153 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 839: loss = 2.8069 (0.602 sec/step)\n",
            "I1207 23:46:35.756971 140583164200832 learning.py:507] global step 839: loss = 2.8069 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 840: loss = 2.6486 (0.572 sec/step)\n",
            "I1207 23:46:36.658440 140583164200832 learning.py:507] global step 840: loss = 2.6486 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 841: loss = 2.8212 (1.519 sec/step)\n",
            "I1207 23:46:38.495029 140583164200832 learning.py:507] global step 841: loss = 2.8212 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 842: loss = 3.3367 (1.089 sec/step)\n",
            "I1207 23:46:39.586011 140583164200832 learning.py:507] global step 842: loss = 3.3367 (1.089 sec/step)\n",
            "INFO:tensorflow:global step 843: loss = 3.0008 (1.025 sec/step)\n",
            "I1207 23:46:40.613012 140583164200832 learning.py:507] global step 843: loss = 3.0008 (1.025 sec/step)\n",
            "INFO:tensorflow:global step 844: loss = 2.7628 (0.600 sec/step)\n",
            "I1207 23:46:41.511509 140583164200832 learning.py:507] global step 844: loss = 2.7628 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 845: loss = 2.3970 (0.719 sec/step)\n",
            "I1207 23:46:42.751128 140583164200832 learning.py:507] global step 845: loss = 2.3970 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 846: loss = 2.9209 (0.850 sec/step)\n",
            "I1207 23:46:43.893657 140583164200832 learning.py:507] global step 846: loss = 2.9209 (0.850 sec/step)\n",
            "INFO:tensorflow:global step 847: loss = 2.3724 (0.638 sec/step)\n",
            "I1207 23:46:44.917860 140583164200832 learning.py:507] global step 847: loss = 2.3724 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 848: loss = 3.5374 (0.690 sec/step)\n",
            "I1207 23:46:46.014443 140583164200832 learning.py:507] global step 848: loss = 3.5374 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 849: loss = 2.9884 (1.331 sec/step)\n",
            "I1207 23:46:47.469085 140583164200832 learning.py:507] global step 849: loss = 2.9884 (1.331 sec/step)\n",
            "INFO:tensorflow:global step 850: loss = 2.5527 (0.727 sec/step)\n",
            "I1207 23:46:48.286999 140583164200832 learning.py:507] global step 850: loss = 2.5527 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 851: loss = 2.5783 (0.671 sec/step)\n",
            "I1207 23:46:49.507743 140583164200832 learning.py:507] global step 851: loss = 2.5783 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 852: loss = 2.8715 (0.569 sec/step)\n",
            "I1207 23:46:50.522302 140583164200832 learning.py:507] global step 852: loss = 2.8715 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 853: loss = 2.6463 (1.600 sec/step)\n",
            "I1207 23:46:52.349953 140583164200832 learning.py:507] global step 853: loss = 2.6463 (1.600 sec/step)\n",
            "INFO:tensorflow:global step 854: loss = 2.7433 (1.223 sec/step)\n",
            "I1207 23:46:53.574173 140583164200832 learning.py:507] global step 854: loss = 2.7433 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 855: loss = 2.3916 (0.607 sec/step)\n",
            "I1207 23:46:54.182714 140583164200832 learning.py:507] global step 855: loss = 2.3916 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 856: loss = 2.3859 (1.784 sec/step)\n",
            "I1207 23:46:55.968821 140583164200832 learning.py:507] global step 856: loss = 2.3859 (1.784 sec/step)\n",
            "INFO:tensorflow:global step 857: loss = 2.4592 (0.665 sec/step)\n",
            "I1207 23:46:56.859988 140583164200832 learning.py:507] global step 857: loss = 2.4592 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 858: loss = 2.3804 (1.332 sec/step)\n",
            "I1207 23:46:58.395184 140583164200832 learning.py:507] global step 858: loss = 2.3804 (1.332 sec/step)\n",
            "INFO:tensorflow:global step 859: loss = 3.3801 (0.696 sec/step)\n",
            "I1207 23:46:59.137797 140583164200832 learning.py:507] global step 859: loss = 3.3801 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 860: loss = 2.7420 (0.765 sec/step)\n",
            "I1207 23:47:00.373702 140583164200832 learning.py:507] global step 860: loss = 2.7420 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 861: loss = 2.6769 (0.657 sec/step)\n",
            "I1207 23:47:01.250484 140583164200832 learning.py:507] global step 861: loss = 2.6769 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 862: loss = 2.5315 (1.438 sec/step)\n",
            "I1207 23:47:02.831149 140583164200832 learning.py:507] global step 862: loss = 2.5315 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 863: loss = 2.9616 (0.597 sec/step)\n",
            "I1207 23:47:03.793879 140583164200832 learning.py:507] global step 863: loss = 2.9616 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 864: loss = 2.7075 (1.282 sec/step)\n",
            "I1207 23:47:05.230272 140583164200832 learning.py:507] global step 864: loss = 2.7075 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 865: loss = 2.3270 (0.555 sec/step)\n",
            "I1207 23:47:05.787939 140583164200832 learning.py:507] global step 865: loss = 2.3270 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 866: loss = 2.6457 (0.618 sec/step)\n",
            "I1207 23:47:06.408402 140583164200832 learning.py:507] global step 866: loss = 2.6457 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 867: loss = 2.5710 (1.535 sec/step)\n",
            "I1207 23:47:08.062921 140583164200832 learning.py:507] global step 867: loss = 2.5710 (1.535 sec/step)\n",
            "INFO:tensorflow:global step 868: loss = 2.5243 (2.508 sec/step)\n",
            "I1207 23:47:10.606126 140583164200832 learning.py:507] global step 868: loss = 2.5243 (2.508 sec/step)\n",
            "INFO:tensorflow:global step 869: loss = 2.4178 (0.616 sec/step)\n",
            "I1207 23:47:11.224210 140583164200832 learning.py:507] global step 869: loss = 2.4178 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 870: loss = 2.4984 (1.022 sec/step)\n",
            "I1207 23:47:12.659830 140583164200832 learning.py:507] global step 870: loss = 2.4984 (1.022 sec/step)\n",
            "INFO:tensorflow:global step 871: loss = 3.1149 (0.839 sec/step)\n",
            "I1207 23:47:14.240009 140583164200832 learning.py:507] global step 871: loss = 3.1149 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 872: loss = 3.2614 (0.597 sec/step)\n",
            "I1207 23:47:15.514008 140583164200832 learning.py:507] global step 872: loss = 3.2614 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 873: loss = 2.3145 (1.636 sec/step)\n",
            "I1207 23:47:17.194429 140583164200832 learning.py:507] global step 873: loss = 2.3145 (1.636 sec/step)\n",
            "INFO:tensorflow:global step 874: loss = 2.3237 (0.671 sec/step)\n",
            "I1207 23:47:18.147000 140583164200832 learning.py:507] global step 874: loss = 2.3237 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 875: loss = 2.8453 (1.546 sec/step)\n",
            "I1207 23:47:19.918979 140583164200832 learning.py:507] global step 875: loss = 2.8453 (1.546 sec/step)\n",
            "INFO:tensorflow:global step 876: loss = 3.1080 (0.553 sec/step)\n",
            "I1207 23:47:20.473997 140583164200832 learning.py:507] global step 876: loss = 3.1080 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 877: loss = 3.0469 (2.072 sec/step)\n",
            "I1207 23:47:22.548024 140583164200832 learning.py:507] global step 877: loss = 3.0469 (2.072 sec/step)\n",
            "INFO:tensorflow:global step 878: loss = 2.1020 (0.580 sec/step)\n",
            "I1207 23:47:23.539999 140583164200832 learning.py:507] global step 878: loss = 2.1020 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 879: loss = 2.0647 (1.224 sec/step)\n",
            "I1207 23:47:24.920622 140583164200832 learning.py:507] global step 879: loss = 2.0647 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 880: loss = 2.2651 (0.661 sec/step)\n",
            "I1207 23:47:25.640192 140583164200832 learning.py:507] global step 880: loss = 2.2651 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 881: loss = 2.5194 (1.633 sec/step)\n",
            "I1207 23:47:27.312494 140583164200832 learning.py:507] global step 881: loss = 2.5194 (1.633 sec/step)\n",
            "INFO:tensorflow:global step 882: loss = 2.9388 (0.632 sec/step)\n",
            "I1207 23:47:28.144075 140583164200832 learning.py:507] global step 882: loss = 2.9388 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 883: loss = 2.4725 (0.861 sec/step)\n",
            "I1207 23:47:29.213068 140583164200832 learning.py:507] global step 883: loss = 2.4725 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 884: loss = 2.3828 (1.379 sec/step)\n",
            "I1207 23:47:30.720475 140583164200832 learning.py:507] global step 884: loss = 2.3828 (1.379 sec/step)\n",
            "INFO:tensorflow:global step 885: loss = 2.1252 (0.596 sec/step)\n",
            "I1207 23:47:31.755472 140583164200832 learning.py:507] global step 885: loss = 2.1252 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 886: loss = 2.4238 (0.533 sec/step)\n",
            "I1207 23:47:32.730285 140583164200832 learning.py:507] global step 886: loss = 2.4238 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 887: loss = 2.3104 (1.505 sec/step)\n",
            "I1207 23:47:34.238353 140583164200832 learning.py:507] global step 887: loss = 2.3104 (1.505 sec/step)\n",
            "INFO:tensorflow:global step 888: loss = 2.6327 (0.634 sec/step)\n",
            "I1207 23:47:35.175097 140583164200832 learning.py:507] global step 888: loss = 2.6327 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 889: loss = 2.5772 (0.623 sec/step)\n",
            "I1207 23:47:36.112394 140583164200832 learning.py:507] global step 889: loss = 2.5772 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 890: loss = 3.3138 (1.298 sec/step)\n",
            "I1207 23:47:37.709662 140583164200832 learning.py:507] global step 890: loss = 3.3138 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 891: loss = 2.0060 (0.586 sec/step)\n",
            "I1207 23:47:38.590207 140583164200832 learning.py:507] global step 891: loss = 2.0060 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 892: loss = 2.2773 (1.337 sec/step)\n",
            "I1207 23:47:40.055526 140583164200832 learning.py:507] global step 892: loss = 2.2773 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 893: loss = 2.4115 (0.510 sec/step)\n",
            "I1207 23:47:40.567263 140583164200832 learning.py:507] global step 893: loss = 2.4115 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 894: loss = 2.6620 (1.286 sec/step)\n",
            "I1207 23:47:42.096880 140583164200832 learning.py:507] global step 894: loss = 2.6620 (1.286 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.80798\n",
            "I1207 23:47:42.320930 140579467605760 supervisor.py:1099] global_step/sec: 0.80798\n",
            "INFO:tensorflow:global step 895: loss = 2.0911 (1.332 sec/step)\n",
            "I1207 23:47:44.237872 140583164200832 learning.py:507] global step 895: loss = 2.0911 (1.332 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 895.\n",
            "I1207 23:47:45.897712 140579459213056 supervisor.py:1050] Recording summary at step 895.\n",
            "INFO:tensorflow:global step 896: loss = 2.3822 (1.684 sec/step)\n",
            "I1207 23:47:46.167319 140583164200832 learning.py:507] global step 896: loss = 2.3822 (1.684 sec/step)\n",
            "INFO:tensorflow:global step 897: loss = 2.3091 (0.567 sec/step)\n",
            "I1207 23:47:46.736736 140583164200832 learning.py:507] global step 897: loss = 2.3091 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 898: loss = 3.3945 (0.634 sec/step)\n",
            "I1207 23:47:47.373834 140583164200832 learning.py:507] global step 898: loss = 3.3945 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 899: loss = 2.7175 (3.150 sec/step)\n",
            "I1207 23:47:50.526286 140583164200832 learning.py:507] global step 899: loss = 2.7175 (3.150 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 2.7252 (0.506 sec/step)\n",
            "I1207 23:47:51.034670 140583164200832 learning.py:507] global step 900: loss = 2.7252 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 901: loss = 2.5863 (1.654 sec/step)\n",
            "I1207 23:47:52.708596 140583164200832 learning.py:507] global step 901: loss = 2.5863 (1.654 sec/step)\n",
            "INFO:tensorflow:global step 902: loss = 2.8028 (0.582 sec/step)\n",
            "I1207 23:47:53.332200 140583164200832 learning.py:507] global step 902: loss = 2.8028 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 903: loss = 2.9241 (1.951 sec/step)\n",
            "I1207 23:47:55.285421 140583164200832 learning.py:507] global step 903: loss = 2.9241 (1.951 sec/step)\n",
            "INFO:tensorflow:global step 904: loss = 2.7158 (0.733 sec/step)\n",
            "I1207 23:47:56.050023 140583164200832 learning.py:507] global step 904: loss = 2.7158 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 905: loss = 2.2736 (1.458 sec/step)\n",
            "I1207 23:47:57.688049 140583164200832 learning.py:507] global step 905: loss = 2.2736 (1.458 sec/step)\n",
            "INFO:tensorflow:global step 906: loss = 2.8168 (0.505 sec/step)\n",
            "I1207 23:47:58.195079 140583164200832 learning.py:507] global step 906: loss = 2.8168 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 907: loss = 2.7328 (1.426 sec/step)\n",
            "I1207 23:47:59.789889 140583164200832 learning.py:507] global step 907: loss = 2.7328 (1.426 sec/step)\n",
            "INFO:tensorflow:global step 908: loss = 2.2410 (1.213 sec/step)\n",
            "I1207 23:48:01.005126 140583164200832 learning.py:507] global step 908: loss = 2.2410 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 909: loss = 3.0785 (0.498 sec/step)\n",
            "I1207 23:48:01.660081 140583164200832 learning.py:507] global step 909: loss = 3.0785 (0.498 sec/step)\n",
            "INFO:tensorflow:global step 910: loss = 2.5422 (1.233 sec/step)\n",
            "I1207 23:48:03.203605 140583164200832 learning.py:507] global step 910: loss = 2.5422 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 911: loss = 2.4151 (0.587 sec/step)\n",
            "I1207 23:48:04.055383 140583164200832 learning.py:507] global step 911: loss = 2.4151 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 912: loss = 2.9528 (0.732 sec/step)\n",
            "I1207 23:48:05.349275 140583164200832 learning.py:507] global step 912: loss = 2.9528 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 913: loss = 2.7463 (1.214 sec/step)\n",
            "I1207 23:48:06.576745 140583164200832 learning.py:507] global step 913: loss = 2.7463 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 914: loss = 2.3624 (0.518 sec/step)\n",
            "I1207 23:48:07.096964 140583164200832 learning.py:507] global step 914: loss = 2.3624 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 915: loss = 2.7451 (1.705 sec/step)\n",
            "I1207 23:48:08.803900 140583164200832 learning.py:507] global step 915: loss = 2.7451 (1.705 sec/step)\n",
            "INFO:tensorflow:global step 916: loss = 3.1546 (0.695 sec/step)\n",
            "I1207 23:48:09.667285 140583164200832 learning.py:507] global step 916: loss = 3.1546 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 917: loss = 3.1306 (0.592 sec/step)\n",
            "I1207 23:48:10.453725 140583164200832 learning.py:507] global step 917: loss = 3.1306 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 918: loss = 2.5841 (1.000 sec/step)\n",
            "I1207 23:48:11.799352 140583164200832 learning.py:507] global step 918: loss = 2.5841 (1.000 sec/step)\n",
            "INFO:tensorflow:global step 919: loss = 2.3134 (1.454 sec/step)\n",
            "I1207 23:48:13.548268 140583164200832 learning.py:507] global step 919: loss = 2.3134 (1.454 sec/step)\n",
            "INFO:tensorflow:global step 920: loss = 3.2435 (0.554 sec/step)\n",
            "I1207 23:48:14.313683 140583164200832 learning.py:507] global step 920: loss = 3.2435 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 921: loss = 2.0555 (1.199 sec/step)\n",
            "I1207 23:48:15.780179 140583164200832 learning.py:507] global step 921: loss = 2.0555 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 922: loss = 2.6955 (0.590 sec/step)\n",
            "I1207 23:48:16.588841 140583164200832 learning.py:507] global step 922: loss = 2.6955 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 923: loss = 2.7665 (1.469 sec/step)\n",
            "I1207 23:48:18.140442 140583164200832 learning.py:507] global step 923: loss = 2.7665 (1.469 sec/step)\n",
            "INFO:tensorflow:global step 924: loss = 2.4874 (0.709 sec/step)\n",
            "I1207 23:48:19.088262 140583164200832 learning.py:507] global step 924: loss = 2.4874 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 925: loss = 2.6745 (0.646 sec/step)\n",
            "I1207 23:48:20.089664 140583164200832 learning.py:507] global step 925: loss = 2.6745 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 926: loss = 2.0915 (0.652 sec/step)\n",
            "I1207 23:48:21.142435 140583164200832 learning.py:507] global step 926: loss = 2.0915 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 927: loss = 2.2406 (0.660 sec/step)\n",
            "I1207 23:48:22.028270 140583164200832 learning.py:507] global step 927: loss = 2.2406 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 928: loss = 2.5595 (1.761 sec/step)\n",
            "I1207 23:48:23.917205 140583164200832 learning.py:507] global step 928: loss = 2.5595 (1.761 sec/step)\n",
            "INFO:tensorflow:global step 929: loss = 3.5002 (0.504 sec/step)\n",
            "I1207 23:48:24.423303 140583164200832 learning.py:507] global step 929: loss = 3.5002 (0.504 sec/step)\n",
            "INFO:tensorflow:global step 930: loss = 2.5053 (0.746 sec/step)\n",
            "I1207 23:48:25.527253 140583164200832 learning.py:507] global step 930: loss = 2.5053 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 931: loss = 3.0755 (2.656 sec/step)\n",
            "I1207 23:48:28.489285 140583164200832 learning.py:507] global step 931: loss = 3.0755 (2.656 sec/step)\n",
            "INFO:tensorflow:global step 932: loss = 2.3327 (0.593 sec/step)\n",
            "I1207 23:48:29.513273 140583164200832 learning.py:507] global step 932: loss = 2.3327 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 933: loss = 2.0377 (0.668 sec/step)\n",
            "I1207 23:48:30.352232 140583164200832 learning.py:507] global step 933: loss = 2.0377 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 934: loss = 2.3597 (1.642 sec/step)\n",
            "I1207 23:48:32.031322 140583164200832 learning.py:507] global step 934: loss = 2.3597 (1.642 sec/step)\n",
            "INFO:tensorflow:global step 935: loss = 2.7590 (0.752 sec/step)\n",
            "I1207 23:48:32.966937 140583164200832 learning.py:507] global step 935: loss = 2.7590 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 936: loss = 1.9712 (1.454 sec/step)\n",
            "I1207 23:48:34.422903 140583164200832 learning.py:507] global step 936: loss = 1.9712 (1.454 sec/step)\n",
            "INFO:tensorflow:global step 937: loss = 2.7248 (0.685 sec/step)\n",
            "I1207 23:48:35.363085 140583164200832 learning.py:507] global step 937: loss = 2.7248 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 938: loss = 2.8623 (1.421 sec/step)\n",
            "I1207 23:48:36.959667 140583164200832 learning.py:507] global step 938: loss = 2.8623 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 939: loss = 3.0785 (0.574 sec/step)\n",
            "I1207 23:48:37.535599 140583164200832 learning.py:507] global step 939: loss = 3.0785 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 940: loss = 2.4641 (1.804 sec/step)\n",
            "I1207 23:48:39.341311 140583164200832 learning.py:507] global step 940: loss = 2.4641 (1.804 sec/step)\n",
            "INFO:tensorflow:global step 941: loss = 2.8084 (0.609 sec/step)\n",
            "I1207 23:48:40.090133 140583164200832 learning.py:507] global step 941: loss = 2.8084 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 942: loss = 3.1909 (0.697 sec/step)\n",
            "I1207 23:48:41.389440 140583164200832 learning.py:507] global step 942: loss = 3.1909 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 943: loss = 2.8819 (0.633 sec/step)\n",
            "I1207 23:48:42.504234 140583164200832 learning.py:507] global step 943: loss = 2.8819 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 944: loss = 2.5807 (0.491 sec/step)\n",
            "I1207 23:48:43.111012 140583164200832 learning.py:507] global step 944: loss = 2.5807 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 945: loss = 3.0515 (1.704 sec/step)\n",
            "I1207 23:48:44.817265 140583164200832 learning.py:507] global step 945: loss = 3.0515 (1.704 sec/step)\n",
            "INFO:tensorflow:global step 946: loss = 3.1934 (0.731 sec/step)\n",
            "I1207 23:48:45.745450 140583164200832 learning.py:507] global step 946: loss = 3.1934 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 947: loss = 2.3096 (0.647 sec/step)\n",
            "I1207 23:48:46.470790 140583164200832 learning.py:507] global step 947: loss = 2.3096 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 948: loss = 2.6302 (0.913 sec/step)\n",
            "I1207 23:48:47.391939 140583164200832 learning.py:507] global step 948: loss = 2.6302 (0.913 sec/step)\n",
            "INFO:tensorflow:global step 949: loss = 2.4431 (1.866 sec/step)\n",
            "I1207 23:48:49.488297 140583164200832 learning.py:507] global step 949: loss = 2.4431 (1.866 sec/step)\n",
            "INFO:tensorflow:global step 950: loss = 2.4160 (0.557 sec/step)\n",
            "I1207 23:48:50.261869 140583164200832 learning.py:507] global step 950: loss = 2.4160 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 951: loss = 2.5520 (1.803 sec/step)\n",
            "I1207 23:48:52.249760 140583164200832 learning.py:507] global step 951: loss = 2.5520 (1.803 sec/step)\n",
            "INFO:tensorflow:global step 952: loss = 2.8378 (0.536 sec/step)\n",
            "I1207 23:48:53.022956 140583164200832 learning.py:507] global step 952: loss = 2.8378 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 953: loss = 2.3247 (1.499 sec/step)\n",
            "I1207 23:48:54.706438 140583164200832 learning.py:507] global step 953: loss = 2.3247 (1.499 sec/step)\n",
            "INFO:tensorflow:global step 954: loss = 2.6451 (0.629 sec/step)\n",
            "I1207 23:48:55.531131 140583164200832 learning.py:507] global step 954: loss = 2.6451 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 955: loss = 3.1313 (1.375 sec/step)\n",
            "I1207 23:48:57.081435 140583164200832 learning.py:507] global step 955: loss = 3.1313 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 956: loss = 2.6322 (1.166 sec/step)\n",
            "I1207 23:48:58.249588 140583164200832 learning.py:507] global step 956: loss = 2.6322 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 957: loss = 2.6349 (0.755 sec/step)\n",
            "I1207 23:48:59.275553 140583164200832 learning.py:507] global step 957: loss = 2.6349 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 958: loss = 2.6400 (0.705 sec/step)\n",
            "I1207 23:49:00.280250 140583164200832 learning.py:507] global step 958: loss = 2.6400 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 959: loss = 2.4384 (1.616 sec/step)\n",
            "I1207 23:49:02.042533 140583164200832 learning.py:507] global step 959: loss = 2.4384 (1.616 sec/step)\n",
            "INFO:tensorflow:global step 960: loss = 2.5813 (0.661 sec/step)\n",
            "I1207 23:49:02.966469 140583164200832 learning.py:507] global step 960: loss = 2.5813 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 961: loss = 2.3033 (1.833 sec/step)\n",
            "I1207 23:49:05.119094 140583164200832 learning.py:507] global step 961: loss = 2.3033 (1.833 sec/step)\n",
            "INFO:tensorflow:global step 962: loss = 3.4222 (0.594 sec/step)\n",
            "I1207 23:49:06.056603 140583164200832 learning.py:507] global step 962: loss = 3.4222 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 963: loss = 2.8240 (0.744 sec/step)\n",
            "I1207 23:49:07.274966 140583164200832 learning.py:507] global step 963: loss = 2.8240 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 964: loss = 3.0382 (0.788 sec/step)\n",
            "I1207 23:49:08.307188 140583164200832 learning.py:507] global step 964: loss = 3.0382 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 965: loss = 3.0841 (1.286 sec/step)\n",
            "I1207 23:49:09.813402 140583164200832 learning.py:507] global step 965: loss = 3.0841 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 966: loss = 2.7679 (0.637 sec/step)\n",
            "I1207 23:49:10.632016 140583164200832 learning.py:507] global step 966: loss = 2.7679 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 967: loss = 2.9994 (0.740 sec/step)\n",
            "I1207 23:49:11.889936 140583164200832 learning.py:507] global step 967: loss = 2.9994 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 968: loss = 2.5450 (0.847 sec/step)\n",
            "I1207 23:49:13.157440 140583164200832 learning.py:507] global step 968: loss = 2.5450 (0.847 sec/step)\n",
            "INFO:tensorflow:global step 969: loss = 2.9258 (0.643 sec/step)\n",
            "I1207 23:49:13.957011 140583164200832 learning.py:507] global step 969: loss = 2.9258 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 970: loss = 2.9887 (1.410 sec/step)\n",
            "I1207 23:49:15.755590 140583164200832 learning.py:507] global step 970: loss = 2.9887 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 971: loss = 2.3770 (0.807 sec/step)\n",
            "I1207 23:49:16.848416 140583164200832 learning.py:507] global step 971: loss = 2.3770 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 972: loss = 2.3901 (0.623 sec/step)\n",
            "I1207 23:49:17.588998 140583164200832 learning.py:507] global step 972: loss = 2.3901 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 973: loss = 2.6669 (1.363 sec/step)\n",
            "I1207 23:49:19.131695 140583164200832 learning.py:507] global step 973: loss = 2.6669 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 974: loss = 3.3392 (0.621 sec/step)\n",
            "I1207 23:49:19.913088 140583164200832 learning.py:507] global step 974: loss = 3.3392 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 975: loss = 2.7350 (1.582 sec/step)\n",
            "I1207 23:49:21.511667 140583164200832 learning.py:507] global step 975: loss = 2.7350 (1.582 sec/step)\n",
            "INFO:tensorflow:global step 976: loss = 2.1719 (1.494 sec/step)\n",
            "I1207 23:49:23.008063 140583164200832 learning.py:507] global step 976: loss = 2.1719 (1.494 sec/step)\n",
            "INFO:tensorflow:global step 977: loss = 2.1090 (0.668 sec/step)\n",
            "I1207 23:49:24.088989 140583164200832 learning.py:507] global step 977: loss = 2.1090 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 978: loss = 2.6015 (0.810 sec/step)\n",
            "I1207 23:49:25.145782 140583164200832 learning.py:507] global step 978: loss = 2.6015 (0.810 sec/step)\n",
            "INFO:tensorflow:global step 979: loss = 2.5694 (1.481 sec/step)\n",
            "I1207 23:49:26.788895 140583164200832 learning.py:507] global step 979: loss = 2.5694 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 980: loss = 2.7324 (0.712 sec/step)\n",
            "I1207 23:49:27.787001 140583164200832 learning.py:507] global step 980: loss = 2.7324 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 981: loss = 3.2434 (1.321 sec/step)\n",
            "I1207 23:49:29.295730 140583164200832 learning.py:507] global step 981: loss = 3.2434 (1.321 sec/step)\n",
            "INFO:tensorflow:global step 982: loss = 2.5181 (0.516 sec/step)\n",
            "I1207 23:49:30.160842 140583164200832 learning.py:507] global step 982: loss = 2.5181 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 983: loss = 2.4464 (1.584 sec/step)\n",
            "I1207 23:49:31.882251 140583164200832 learning.py:507] global step 983: loss = 2.4464 (1.584 sec/step)\n",
            "INFO:tensorflow:global step 984: loss = 2.1823 (0.580 sec/step)\n",
            "I1207 23:49:32.464184 140583164200832 learning.py:507] global step 984: loss = 2.1823 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 985: loss = 2.6953 (0.989 sec/step)\n",
            "I1207 23:49:33.481844 140583164200832 learning.py:507] global step 985: loss = 2.6953 (0.989 sec/step)\n",
            "INFO:tensorflow:global step 986: loss = 2.0262 (2.842 sec/step)\n",
            "I1207 23:49:36.586474 140583164200832 learning.py:507] global step 986: loss = 2.0262 (2.842 sec/step)\n",
            "INFO:tensorflow:global step 987: loss = 2.7122 (0.563 sec/step)\n",
            "I1207 23:49:37.151283 140583164200832 learning.py:507] global step 987: loss = 2.7122 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 988: loss = 2.6037 (1.004 sec/step)\n",
            "I1207 23:49:38.272231 140583164200832 learning.py:507] global step 988: loss = 2.6037 (1.004 sec/step)\n",
            "INFO:tensorflow:global step 989: loss = 2.5879 (2.178 sec/step)\n",
            "I1207 23:49:40.766045 140583164200832 learning.py:507] global step 989: loss = 2.5879 (2.178 sec/step)\n",
            "INFO:tensorflow:global step 990: loss = 2.4797 (0.593 sec/step)\n",
            "I1207 23:49:41.361604 140583164200832 learning.py:507] global step 990: loss = 2.4797 (0.593 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1207 23:49:42.076297 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 990.\n",
            "I1207 23:49:45.114183 140579459213056 supervisor.py:1050] Recording summary at step 990.\n",
            "INFO:tensorflow:global step 991: loss = 2.8331 (4.192 sec/step)\n",
            "I1207 23:49:45.561431 140583164200832 learning.py:507] global step 991: loss = 2.8331 (4.192 sec/step)\n",
            "INFO:tensorflow:global step 992: loss = 2.8171 (0.745 sec/step)\n",
            "I1207 23:49:46.326471 140583164200832 learning.py:507] global step 992: loss = 2.8171 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 993: loss = 3.0768 (2.777 sec/step)\n",
            "I1207 23:49:49.105936 140583164200832 learning.py:507] global step 993: loss = 3.0768 (2.777 sec/step)\n",
            "INFO:tensorflow:global step 994: loss = 2.3143 (0.675 sec/step)\n",
            "I1207 23:49:49.782774 140583164200832 learning.py:507] global step 994: loss = 2.3143 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 995: loss = 2.1252 (1.851 sec/step)\n",
            "I1207 23:49:51.680031 140583164200832 learning.py:507] global step 995: loss = 2.1252 (1.851 sec/step)\n",
            "INFO:tensorflow:global step 996: loss = 2.5591 (0.604 sec/step)\n",
            "I1207 23:49:52.290720 140583164200832 learning.py:507] global step 996: loss = 2.5591 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 997: loss = 2.2342 (0.608 sec/step)\n",
            "I1207 23:49:52.900722 140583164200832 learning.py:507] global step 997: loss = 2.2342 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 998: loss = 2.0598 (2.987 sec/step)\n",
            "I1207 23:49:55.890128 140583164200832 learning.py:507] global step 998: loss = 2.0598 (2.987 sec/step)\n",
            "INFO:tensorflow:global step 999: loss = 1.9803 (0.589 sec/step)\n",
            "I1207 23:49:56.818757 140583164200832 learning.py:507] global step 999: loss = 1.9803 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 2.8509 (0.513 sec/step)\n",
            "I1207 23:49:57.549379 140583164200832 learning.py:507] global step 1000: loss = 2.8509 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1001: loss = 2.5916 (1.903 sec/step)\n",
            "I1207 23:49:59.454509 140583164200832 learning.py:507] global step 1001: loss = 2.5916 (1.903 sec/step)\n",
            "INFO:tensorflow:global step 1002: loss = 2.7710 (0.571 sec/step)\n",
            "I1207 23:50:00.027668 140583164200832 learning.py:507] global step 1002: loss = 2.7710 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1003: loss = 2.3003 (2.107 sec/step)\n",
            "I1207 23:50:02.136567 140583164200832 learning.py:507] global step 1003: loss = 2.3003 (2.107 sec/step)\n",
            "INFO:tensorflow:global step 1004: loss = 2.3214 (0.625 sec/step)\n",
            "I1207 23:50:02.948539 140583164200832 learning.py:507] global step 1004: loss = 2.3214 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1005: loss = 2.1650 (1.791 sec/step)\n",
            "I1207 23:50:05.000857 140583164200832 learning.py:507] global step 1005: loss = 2.1650 (1.791 sec/step)\n",
            "INFO:tensorflow:global step 1006: loss = 2.6474 (0.652 sec/step)\n",
            "I1207 23:50:05.655581 140583164200832 learning.py:507] global step 1006: loss = 2.6474 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1007: loss = 2.4583 (2.120 sec/step)\n",
            "I1207 23:50:07.777658 140583164200832 learning.py:507] global step 1007: loss = 2.4583 (2.120 sec/step)\n",
            "INFO:tensorflow:global step 1008: loss = 3.2128 (0.798 sec/step)\n",
            "I1207 23:50:08.941301 140583164200832 learning.py:507] global step 1008: loss = 3.2128 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 1009: loss = 2.8626 (0.587 sec/step)\n",
            "I1207 23:50:09.619915 140583164200832 learning.py:507] global step 1009: loss = 2.8626 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1010: loss = 3.1608 (2.126 sec/step)\n",
            "I1207 23:50:11.747922 140583164200832 learning.py:507] global step 1010: loss = 3.1608 (2.126 sec/step)\n",
            "INFO:tensorflow:global step 1011: loss = 2.8497 (0.679 sec/step)\n",
            "I1207 23:50:12.428364 140583164200832 learning.py:507] global step 1011: loss = 2.8497 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1012: loss = 2.6851 (1.952 sec/step)\n",
            "I1207 23:50:14.382616 140583164200832 learning.py:507] global step 1012: loss = 2.6851 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 1013: loss = 2.3956 (0.516 sec/step)\n",
            "I1207 23:50:14.900862 140583164200832 learning.py:507] global step 1013: loss = 2.3956 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 1014: loss = 2.7320 (0.849 sec/step)\n",
            "I1207 23:50:15.963114 140583164200832 learning.py:507] global step 1014: loss = 2.7320 (0.849 sec/step)\n",
            "INFO:tensorflow:global step 1015: loss = 2.7967 (2.412 sec/step)\n",
            "I1207 23:50:18.478202 140583164200832 learning.py:507] global step 1015: loss = 2.7967 (2.412 sec/step)\n",
            "INFO:tensorflow:global step 1016: loss = 1.9143 (0.597 sec/step)\n",
            "I1207 23:50:19.077702 140583164200832 learning.py:507] global step 1016: loss = 1.9143 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1017: loss = 2.2063 (2.001 sec/step)\n",
            "I1207 23:50:21.080942 140583164200832 learning.py:507] global step 1017: loss = 2.2063 (2.001 sec/step)\n",
            "INFO:tensorflow:global step 1018: loss = 2.1311 (0.714 sec/step)\n",
            "I1207 23:50:22.085278 140583164200832 learning.py:507] global step 1018: loss = 2.1311 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 1019: loss = 2.4322 (0.718 sec/step)\n",
            "I1207 23:50:23.246291 140583164200832 learning.py:507] global step 1019: loss = 2.4322 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 1020: loss = 2.4115 (0.891 sec/step)\n",
            "I1207 23:50:24.459024 140583164200832 learning.py:507] global step 1020: loss = 2.4115 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 1021: loss = 2.5890 (2.189 sec/step)\n",
            "I1207 23:50:26.690012 140583164200832 learning.py:507] global step 1021: loss = 2.5890 (2.189 sec/step)\n",
            "INFO:tensorflow:global step 1022: loss = 2.0263 (0.711 sec/step)\n",
            "I1207 23:50:27.697015 140583164200832 learning.py:507] global step 1022: loss = 2.0263 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 1023: loss = 2.5803 (0.559 sec/step)\n",
            "I1207 23:50:28.418091 140583164200832 learning.py:507] global step 1023: loss = 2.5803 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1024: loss = 2.4950 (1.786 sec/step)\n",
            "I1207 23:50:30.360514 140583164200832 learning.py:507] global step 1024: loss = 2.4950 (1.786 sec/step)\n",
            "INFO:tensorflow:global step 1025: loss = 2.5714 (0.672 sec/step)\n",
            "I1207 23:50:31.049071 140583164200832 learning.py:507] global step 1025: loss = 2.5714 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1026: loss = 2.7603 (2.043 sec/step)\n",
            "I1207 23:50:33.094850 140583164200832 learning.py:507] global step 1026: loss = 2.7603 (2.043 sec/step)\n",
            "INFO:tensorflow:global step 1027: loss = 2.8434 (0.613 sec/step)\n",
            "I1207 23:50:33.923822 140583164200832 learning.py:507] global step 1027: loss = 2.8434 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 1028: loss = 3.2672 (1.698 sec/step)\n",
            "I1207 23:50:35.932762 140583164200832 learning.py:507] global step 1028: loss = 3.2672 (1.698 sec/step)\n",
            "INFO:tensorflow:global step 1029: loss = 2.5441 (0.518 sec/step)\n",
            "I1207 23:50:36.453030 140583164200832 learning.py:507] global step 1029: loss = 2.5441 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 1030: loss = 2.4753 (1.923 sec/step)\n",
            "I1207 23:50:38.378046 140583164200832 learning.py:507] global step 1030: loss = 2.4753 (1.923 sec/step)\n",
            "INFO:tensorflow:global step 1031: loss = 3.0652 (0.537 sec/step)\n",
            "I1207 23:50:38.917039 140583164200832 learning.py:507] global step 1031: loss = 3.0652 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 1032: loss = 2.3873 (1.748 sec/step)\n",
            "I1207 23:50:40.845957 140583164200832 learning.py:507] global step 1032: loss = 2.3873 (1.748 sec/step)\n",
            "INFO:tensorflow:global step 1033: loss = 2.4674 (0.631 sec/step)\n",
            "I1207 23:50:41.484114 140583164200832 learning.py:507] global step 1033: loss = 2.4674 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1034: loss = 2.9269 (0.599 sec/step)\n",
            "I1207 23:50:42.464236 140583164200832 learning.py:507] global step 1034: loss = 2.9269 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1035: loss = 2.9867 (2.225 sec/step)\n",
            "I1207 23:50:44.711931 140583164200832 learning.py:507] global step 1035: loss = 2.9867 (2.225 sec/step)\n",
            "INFO:tensorflow:global step 1036: loss = 2.2337 (0.672 sec/step)\n",
            "I1207 23:50:45.658995 140583164200832 learning.py:507] global step 1036: loss = 2.2337 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1037: loss = 2.0232 (0.540 sec/step)\n",
            "I1207 23:50:46.499843 140583164200832 learning.py:507] global step 1037: loss = 2.0232 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 1038: loss = 2.7004 (2.127 sec/step)\n",
            "I1207 23:50:48.629250 140583164200832 learning.py:507] global step 1038: loss = 2.7004 (2.127 sec/step)\n",
            "INFO:tensorflow:global step 1039: loss = 2.3579 (0.747 sec/step)\n",
            "I1207 23:50:49.423676 140583164200832 learning.py:507] global step 1039: loss = 2.3579 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1040: loss = 3.1130 (1.721 sec/step)\n",
            "I1207 23:50:51.182025 140583164200832 learning.py:507] global step 1040: loss = 3.1130 (1.721 sec/step)\n",
            "INFO:tensorflow:global step 1041: loss = 1.9040 (0.636 sec/step)\n",
            "I1207 23:50:51.819882 140583164200832 learning.py:507] global step 1041: loss = 1.9040 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1042: loss = 2.5620 (2.024 sec/step)\n",
            "I1207 23:50:53.845778 140583164200832 learning.py:507] global step 1042: loss = 2.5620 (2.024 sec/step)\n",
            "INFO:tensorflow:global step 1043: loss = 2.9265 (0.619 sec/step)\n",
            "I1207 23:50:54.652273 140583164200832 learning.py:507] global step 1043: loss = 2.9265 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1044: loss = 2.1693 (0.754 sec/step)\n",
            "I1207 23:50:55.661078 140583164200832 learning.py:507] global step 1044: loss = 2.1693 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 1045: loss = 2.6759 (1.997 sec/step)\n",
            "I1207 23:50:57.660587 140583164200832 learning.py:507] global step 1045: loss = 2.6759 (1.997 sec/step)\n",
            "INFO:tensorflow:global step 1046: loss = 2.9494 (0.630 sec/step)\n",
            "I1207 23:50:58.292569 140583164200832 learning.py:507] global step 1046: loss = 2.9494 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1047: loss = 2.4643 (1.939 sec/step)\n",
            "I1207 23:51:00.233396 140583164200832 learning.py:507] global step 1047: loss = 2.4643 (1.939 sec/step)\n",
            "INFO:tensorflow:global step 1048: loss = 2.2582 (0.558 sec/step)\n",
            "I1207 23:51:00.793568 140583164200832 learning.py:507] global step 1048: loss = 2.2582 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1049: loss = 2.6837 (1.471 sec/step)\n",
            "I1207 23:51:02.292386 140583164200832 learning.py:507] global step 1049: loss = 2.6837 (1.471 sec/step)\n",
            "INFO:tensorflow:global step 1050: loss = 2.2843 (0.636 sec/step)\n",
            "I1207 23:51:03.133144 140583164200832 learning.py:507] global step 1050: loss = 2.2843 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1051: loss = 2.7844 (1.987 sec/step)\n",
            "I1207 23:51:05.122221 140583164200832 learning.py:507] global step 1051: loss = 2.7844 (1.987 sec/step)\n",
            "INFO:tensorflow:global step 1052: loss = 2.7520 (0.560 sec/step)\n",
            "I1207 23:51:05.683643 140583164200832 learning.py:507] global step 1052: loss = 2.7520 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 1053: loss = 3.1802 (1.611 sec/step)\n",
            "I1207 23:51:07.620002 140583164200832 learning.py:507] global step 1053: loss = 3.1802 (1.611 sec/step)\n",
            "INFO:tensorflow:global step 1054: loss = 2.3429 (0.669 sec/step)\n",
            "I1207 23:51:08.317699 140583164200832 learning.py:507] global step 1054: loss = 2.3429 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1055: loss = 2.6867 (1.920 sec/step)\n",
            "I1207 23:51:10.239881 140583164200832 learning.py:507] global step 1055: loss = 2.6867 (1.920 sec/step)\n",
            "INFO:tensorflow:global step 1056: loss = 2.4081 (0.635 sec/step)\n",
            "I1207 23:51:10.876870 140583164200832 learning.py:507] global step 1056: loss = 2.4081 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1057: loss = 2.4454 (0.937 sec/step)\n",
            "I1207 23:51:11.902704 140583164200832 learning.py:507] global step 1057: loss = 2.4454 (0.937 sec/step)\n",
            "INFO:tensorflow:global step 1058: loss = 2.3725 (2.117 sec/step)\n",
            "I1207 23:51:14.297249 140583164200832 learning.py:507] global step 1058: loss = 2.3725 (2.117 sec/step)\n",
            "INFO:tensorflow:global step 1059: loss = 2.3796 (0.611 sec/step)\n",
            "I1207 23:51:14.910065 140583164200832 learning.py:507] global step 1059: loss = 2.3796 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1060: loss = 1.8794 (1.552 sec/step)\n",
            "I1207 23:51:16.603314 140583164200832 learning.py:507] global step 1060: loss = 1.8794 (1.552 sec/step)\n",
            "INFO:tensorflow:global step 1061: loss = 2.4079 (0.648 sec/step)\n",
            "I1207 23:51:17.848369 140583164200832 learning.py:507] global step 1061: loss = 2.4079 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1062: loss = 2.3877 (0.602 sec/step)\n",
            "I1207 23:51:18.698713 140583164200832 learning.py:507] global step 1062: loss = 2.3877 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1063: loss = 2.3965 (1.812 sec/step)\n",
            "I1207 23:51:20.512354 140583164200832 learning.py:507] global step 1063: loss = 2.3965 (1.812 sec/step)\n",
            "INFO:tensorflow:global step 1064: loss = 2.8373 (0.591 sec/step)\n",
            "I1207 23:51:21.299055 140583164200832 learning.py:507] global step 1064: loss = 2.8373 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 1065: loss = 2.2906 (0.762 sec/step)\n",
            "I1207 23:51:22.644889 140583164200832 learning.py:507] global step 1065: loss = 2.2906 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 1066: loss = 2.4986 (1.804 sec/step)\n",
            "I1207 23:51:24.688245 140583164200832 learning.py:507] global step 1066: loss = 2.4986 (1.804 sec/step)\n",
            "INFO:tensorflow:global step 1067: loss = 2.6887 (0.704 sec/step)\n",
            "I1207 23:51:25.588847 140583164200832 learning.py:507] global step 1067: loss = 2.6887 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1068: loss = 2.5135 (0.822 sec/step)\n",
            "I1207 23:51:26.916962 140583164200832 learning.py:507] global step 1068: loss = 2.5135 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 1069: loss = 2.2708 (0.665 sec/step)\n",
            "I1207 23:51:27.683273 140583164200832 learning.py:507] global step 1069: loss = 2.2708 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1070: loss = 2.2475 (1.977 sec/step)\n",
            "I1207 23:51:29.661921 140583164200832 learning.py:507] global step 1070: loss = 2.2475 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 1071: loss = 2.7648 (0.776 sec/step)\n",
            "I1207 23:51:30.658004 140583164200832 learning.py:507] global step 1071: loss = 2.7648 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1072: loss = 2.9536 (1.857 sec/step)\n",
            "I1207 23:51:32.681223 140583164200832 learning.py:507] global step 1072: loss = 2.9536 (1.857 sec/step)\n",
            "INFO:tensorflow:global step 1073: loss = 2.1123 (0.623 sec/step)\n",
            "I1207 23:51:33.306091 140583164200832 learning.py:507] global step 1073: loss = 2.1123 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1074: loss = 2.5430 (1.605 sec/step)\n",
            "I1207 23:51:35.025541 140583164200832 learning.py:507] global step 1074: loss = 2.5430 (1.605 sec/step)\n",
            "INFO:tensorflow:global step 1075: loss = 1.9924 (1.115 sec/step)\n",
            "I1207 23:51:36.154793 140583164200832 learning.py:507] global step 1075: loss = 1.9924 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 1076: loss = 2.5438 (0.598 sec/step)\n",
            "I1207 23:51:37.195558 140583164200832 learning.py:507] global step 1076: loss = 2.5438 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1077: loss = 2.5075 (1.410 sec/step)\n",
            "I1207 23:51:38.657220 140583164200832 learning.py:507] global step 1077: loss = 2.5075 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 1078: loss = 2.1444 (0.650 sec/step)\n",
            "I1207 23:51:39.539944 140583164200832 learning.py:507] global step 1078: loss = 2.1444 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1079: loss = 3.0675 (0.864 sec/step)\n",
            "I1207 23:51:40.653445 140583164200832 learning.py:507] global step 1079: loss = 3.0675 (0.864 sec/step)\n",
            "INFO:tensorflow:global step 1080: loss = 2.5935 (0.752 sec/step)\n",
            "I1207 23:51:41.860978 140583164200832 learning.py:507] global step 1080: loss = 2.5935 (0.752 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1080.\n",
            "I1207 23:51:45.012591 140579459213056 supervisor.py:1050] Recording summary at step 1080.\n",
            "INFO:tensorflow:global step 1081: loss = 2.4499 (3.164 sec/step)\n",
            "I1207 23:51:45.199749 140583164200832 learning.py:507] global step 1081: loss = 2.4499 (3.164 sec/step)\n",
            "INFO:tensorflow:global step 1082: loss = 2.3680 (0.580 sec/step)\n",
            "I1207 23:51:45.781704 140583164200832 learning.py:507] global step 1082: loss = 2.3680 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1083: loss = 2.9687 (0.920 sec/step)\n",
            "I1207 23:51:46.970988 140583164200832 learning.py:507] global step 1083: loss = 2.9687 (0.920 sec/step)\n",
            "INFO:tensorflow:global step 1084: loss = 2.6569 (1.628 sec/step)\n",
            "I1207 23:51:48.835233 140583164200832 learning.py:507] global step 1084: loss = 2.6569 (1.628 sec/step)\n",
            "INFO:tensorflow:global step 1085: loss = 2.5436 (0.667 sec/step)\n",
            "I1207 23:51:49.677731 140583164200832 learning.py:507] global step 1085: loss = 2.5436 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1086: loss = 3.1829 (0.585 sec/step)\n",
            "I1207 23:51:50.689394 140583164200832 learning.py:507] global step 1086: loss = 3.1829 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1087: loss = 2.6292 (1.766 sec/step)\n",
            "I1207 23:51:52.487358 140583164200832 learning.py:507] global step 1087: loss = 2.6292 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 1088: loss = 2.7728 (0.705 sec/step)\n",
            "I1207 23:51:53.403340 140583164200832 learning.py:507] global step 1088: loss = 2.7728 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 1089: loss = 2.6048 (1.297 sec/step)\n",
            "I1207 23:51:54.822615 140583164200832 learning.py:507] global step 1089: loss = 2.6048 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 1090: loss = 2.3540 (0.569 sec/step)\n",
            "I1207 23:51:55.393866 140583164200832 learning.py:507] global step 1090: loss = 2.3540 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1091: loss = 2.0378 (1.766 sec/step)\n",
            "I1207 23:51:57.161667 140583164200832 learning.py:507] global step 1091: loss = 2.0378 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 1092: loss = 2.0801 (0.539 sec/step)\n",
            "I1207 23:51:57.702406 140583164200832 learning.py:507] global step 1092: loss = 2.0801 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 1093: loss = 2.6166 (2.123 sec/step)\n",
            "I1207 23:51:59.827525 140583164200832 learning.py:507] global step 1093: loss = 2.6166 (2.123 sec/step)\n",
            "INFO:tensorflow:global step 1094: loss = 2.7013 (0.628 sec/step)\n",
            "I1207 23:52:00.701287 140583164200832 learning.py:507] global step 1094: loss = 2.7013 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1095: loss = 2.6908 (0.650 sec/step)\n",
            "I1207 23:52:01.726651 140583164200832 learning.py:507] global step 1095: loss = 2.6908 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1096: loss = 2.5556 (1.716 sec/step)\n",
            "I1207 23:52:03.526734 140583164200832 learning.py:507] global step 1096: loss = 2.5556 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 1097: loss = 2.8378 (0.580 sec/step)\n",
            "I1207 23:52:04.227818 140583164200832 learning.py:507] global step 1097: loss = 2.8378 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1098: loss = 2.7437 (0.618 sec/step)\n",
            "I1207 23:52:05.542559 140583164200832 learning.py:507] global step 1098: loss = 2.7437 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1099: loss = 2.3303 (0.595 sec/step)\n",
            "I1207 23:52:06.315657 140583164200832 learning.py:507] global step 1099: loss = 2.3303 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 1100: loss = 2.4274 (1.860 sec/step)\n",
            "I1207 23:52:08.177683 140583164200832 learning.py:507] global step 1100: loss = 2.4274 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 1101: loss = 2.7642 (0.819 sec/step)\n",
            "I1207 23:52:09.224633 140583164200832 learning.py:507] global step 1101: loss = 2.7642 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1102: loss = 2.8712 (0.630 sec/step)\n",
            "I1207 23:52:10.279196 140583164200832 learning.py:507] global step 1102: loss = 2.8712 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1103: loss = 2.5967 (0.633 sec/step)\n",
            "I1207 23:52:11.069188 140583164200832 learning.py:507] global step 1103: loss = 2.5967 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1104: loss = 2.5420 (1.308 sec/step)\n",
            "I1207 23:52:12.682080 140583164200832 learning.py:507] global step 1104: loss = 2.5420 (1.308 sec/step)\n",
            "INFO:tensorflow:global step 1105: loss = 2.2116 (1.560 sec/step)\n",
            "I1207 23:52:14.304044 140583164200832 learning.py:507] global step 1105: loss = 2.2116 (1.560 sec/step)\n",
            "INFO:tensorflow:global step 1106: loss = 2.5111 (0.709 sec/step)\n",
            "I1207 23:52:15.307070 140583164200832 learning.py:507] global step 1106: loss = 2.5111 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 1107: loss = 2.2707 (1.305 sec/step)\n",
            "I1207 23:52:16.629286 140583164200832 learning.py:507] global step 1107: loss = 2.2707 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 1108: loss = 2.9858 (1.118 sec/step)\n",
            "I1207 23:52:17.749247 140583164200832 learning.py:507] global step 1108: loss = 2.9858 (1.118 sec/step)\n",
            "INFO:tensorflow:global step 1109: loss = 2.3142 (0.780 sec/step)\n",
            "I1207 23:52:18.704730 140583164200832 learning.py:507] global step 1109: loss = 2.3142 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 1110: loss = 2.5352 (0.569 sec/step)\n",
            "I1207 23:52:19.353945 140583164200832 learning.py:507] global step 1110: loss = 2.5352 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1111: loss = 2.4248 (1.802 sec/step)\n",
            "I1207 23:52:21.158309 140583164200832 learning.py:507] global step 1111: loss = 2.4248 (1.802 sec/step)\n",
            "INFO:tensorflow:global step 1112: loss = 2.9052 (0.632 sec/step)\n",
            "I1207 23:52:21.792465 140583164200832 learning.py:507] global step 1112: loss = 2.9052 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1113: loss = 2.3489 (1.914 sec/step)\n",
            "I1207 23:52:23.709929 140583164200832 learning.py:507] global step 1113: loss = 2.3489 (1.914 sec/step)\n",
            "INFO:tensorflow:global step 1114: loss = 2.9478 (0.503 sec/step)\n",
            "I1207 23:52:24.217036 140583164200832 learning.py:507] global step 1114: loss = 2.9478 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 1115: loss = 3.0275 (1.825 sec/step)\n",
            "I1207 23:52:26.075853 140583164200832 learning.py:507] global step 1115: loss = 3.0275 (1.825 sec/step)\n",
            "INFO:tensorflow:global step 1116: loss = 2.5865 (0.734 sec/step)\n",
            "I1207 23:52:26.863307 140583164200832 learning.py:507] global step 1116: loss = 2.5865 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1117: loss = 2.3402 (1.626 sec/step)\n",
            "I1207 23:52:28.491330 140583164200832 learning.py:507] global step 1117: loss = 2.3402 (1.626 sec/step)\n",
            "INFO:tensorflow:global step 1118: loss = 2.3428 (0.563 sec/step)\n",
            "I1207 23:52:29.285641 140583164200832 learning.py:507] global step 1118: loss = 2.3428 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 1119: loss = 2.5921 (1.311 sec/step)\n",
            "I1207 23:52:30.885786 140583164200832 learning.py:507] global step 1119: loss = 2.5921 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 1120: loss = 3.1516 (0.566 sec/step)\n",
            "I1207 23:52:31.832470 140583164200832 learning.py:507] global step 1120: loss = 3.1516 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 1121: loss = 1.8585 (0.750 sec/step)\n",
            "I1207 23:52:32.806737 140583164200832 learning.py:507] global step 1121: loss = 1.8585 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 1122: loss = 2.1995 (0.636 sec/step)\n",
            "I1207 23:52:33.938889 140583164200832 learning.py:507] global step 1122: loss = 2.1995 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1123: loss = 2.1908 (1.469 sec/step)\n",
            "I1207 23:52:35.451985 140583164200832 learning.py:507] global step 1123: loss = 2.1908 (1.469 sec/step)\n",
            "INFO:tensorflow:global step 1124: loss = 2.9294 (0.670 sec/step)\n",
            "I1207 23:52:36.358960 140583164200832 learning.py:507] global step 1124: loss = 2.9294 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1125: loss = 2.4711 (1.396 sec/step)\n",
            "I1207 23:52:37.795536 140583164200832 learning.py:507] global step 1125: loss = 2.4711 (1.396 sec/step)\n",
            "INFO:tensorflow:global step 1126: loss = 2.7644 (0.602 sec/step)\n",
            "I1207 23:52:38.462587 140583164200832 learning.py:507] global step 1126: loss = 2.7644 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1127: loss = 2.7808 (1.474 sec/step)\n",
            "I1207 23:52:40.047366 140583164200832 learning.py:507] global step 1127: loss = 2.7808 (1.474 sec/step)\n",
            "INFO:tensorflow:global step 1128: loss = 2.5777 (0.512 sec/step)\n",
            "I1207 23:52:40.561017 140583164200832 learning.py:507] global step 1128: loss = 2.5777 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 1129: loss = 2.1802 (1.699 sec/step)\n",
            "I1207 23:52:42.262121 140583164200832 learning.py:507] global step 1129: loss = 2.1802 (1.699 sec/step)\n",
            "INFO:tensorflow:global step 1130: loss = 2.7450 (0.570 sec/step)\n",
            "I1207 23:52:43.094794 140583164200832 learning.py:507] global step 1130: loss = 2.7450 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 1131: loss = 2.3547 (0.724 sec/step)\n",
            "I1207 23:52:44.284521 140583164200832 learning.py:507] global step 1131: loss = 2.3547 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1132: loss = 2.6604 (1.518 sec/step)\n",
            "I1207 23:52:45.924972 140583164200832 learning.py:507] global step 1132: loss = 2.6604 (1.518 sec/step)\n",
            "INFO:tensorflow:global step 1133: loss = 2.5578 (0.664 sec/step)\n",
            "I1207 23:52:46.945128 140583164200832 learning.py:507] global step 1133: loss = 2.5578 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1134: loss = 2.5058 (1.110 sec/step)\n",
            "I1207 23:52:48.107784 140583164200832 learning.py:507] global step 1134: loss = 2.5058 (1.110 sec/step)\n",
            "INFO:tensorflow:global step 1135: loss = 1.8528 (0.633 sec/step)\n",
            "I1207 23:52:49.080169 140583164200832 learning.py:507] global step 1135: loss = 1.8528 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1136: loss = 2.3251 (0.711 sec/step)\n",
            "I1207 23:52:50.142555 140583164200832 learning.py:507] global step 1136: loss = 2.3251 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 1137: loss = 2.7808 (1.313 sec/step)\n",
            "I1207 23:52:51.550295 140583164200832 learning.py:507] global step 1137: loss = 2.7808 (1.313 sec/step)\n",
            "INFO:tensorflow:global step 1138: loss = 2.5591 (0.565 sec/step)\n",
            "I1207 23:52:52.453681 140583164200832 learning.py:507] global step 1138: loss = 2.5591 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 1139: loss = 2.2801 (1.143 sec/step)\n",
            "I1207 23:52:53.804468 140583164200832 learning.py:507] global step 1139: loss = 2.2801 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 1140: loss = 2.3738 (1.148 sec/step)\n",
            "I1207 23:52:54.954414 140583164200832 learning.py:507] global step 1140: loss = 2.3738 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 1141: loss = 2.1407 (0.575 sec/step)\n",
            "I1207 23:52:55.854345 140583164200832 learning.py:507] global step 1141: loss = 2.1407 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 1142: loss = 2.0552 (1.337 sec/step)\n",
            "I1207 23:52:57.383634 140583164200832 learning.py:507] global step 1142: loss = 2.0552 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 1143: loss = 2.8700 (0.802 sec/step)\n",
            "I1207 23:52:58.431655 140583164200832 learning.py:507] global step 1143: loss = 2.8700 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 1144: loss = 2.4466 (0.660 sec/step)\n",
            "I1207 23:52:59.396921 140583164200832 learning.py:507] global step 1144: loss = 2.4466 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1145: loss = 2.9116 (1.400 sec/step)\n",
            "I1207 23:53:00.919003 140583164200832 learning.py:507] global step 1145: loss = 2.9116 (1.400 sec/step)\n",
            "INFO:tensorflow:global step 1146: loss = 2.8673 (0.556 sec/step)\n",
            "I1207 23:53:01.735617 140583164200832 learning.py:507] global step 1146: loss = 2.8673 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 1147: loss = 2.2071 (0.791 sec/step)\n",
            "I1207 23:53:03.081991 140583164200832 learning.py:507] global step 1147: loss = 2.2071 (0.791 sec/step)\n",
            "INFO:tensorflow:global step 1148: loss = 2.3921 (0.674 sec/step)\n",
            "I1207 23:53:03.946078 140583164200832 learning.py:507] global step 1148: loss = 2.3921 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 1149: loss = 1.9036 (1.680 sec/step)\n",
            "I1207 23:53:05.650926 140583164200832 learning.py:507] global step 1149: loss = 1.9036 (1.680 sec/step)\n",
            "INFO:tensorflow:global step 1150: loss = 2.1863 (0.647 sec/step)\n",
            "I1207 23:53:06.368103 140583164200832 learning.py:507] global step 1150: loss = 2.1863 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1151: loss = 2.9090 (1.578 sec/step)\n",
            "I1207 23:53:08.141668 140583164200832 learning.py:507] global step 1151: loss = 2.9090 (1.578 sec/step)\n",
            "INFO:tensorflow:global step 1152: loss = 2.8523 (0.686 sec/step)\n",
            "I1207 23:53:09.082851 140583164200832 learning.py:507] global step 1152: loss = 2.8523 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1153: loss = 2.4102 (0.608 sec/step)\n",
            "I1207 23:53:10.124070 140583164200832 learning.py:507] global step 1153: loss = 2.4102 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 1154: loss = 2.7717 (0.682 sec/step)\n",
            "I1207 23:53:11.243769 140583164200832 learning.py:507] global step 1154: loss = 2.7717 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1155: loss = 2.5608 (1.297 sec/step)\n",
            "I1207 23:53:12.630156 140583164200832 learning.py:507] global step 1155: loss = 2.5608 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 1156: loss = 2.5798 (0.513 sec/step)\n",
            "I1207 23:53:13.513615 140583164200832 learning.py:507] global step 1156: loss = 2.5798 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1157: loss = 2.8313 (1.378 sec/step)\n",
            "I1207 23:53:15.035943 140583164200832 learning.py:507] global step 1157: loss = 2.8313 (1.378 sec/step)\n",
            "INFO:tensorflow:global step 1158: loss = 2.6457 (0.719 sec/step)\n",
            "I1207 23:53:15.803047 140583164200832 learning.py:507] global step 1158: loss = 2.6457 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 1159: loss = 2.3390 (1.861 sec/step)\n",
            "I1207 23:53:17.975551 140583164200832 learning.py:507] global step 1159: loss = 2.3390 (1.861 sec/step)\n",
            "INFO:tensorflow:global step 1160: loss = 3.7430 (0.568 sec/step)\n",
            "I1207 23:53:18.551263 140583164200832 learning.py:507] global step 1160: loss = 3.7430 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1161: loss = 2.1410 (1.684 sec/step)\n",
            "I1207 23:53:20.240545 140583164200832 learning.py:507] global step 1161: loss = 2.1410 (1.684 sec/step)\n",
            "INFO:tensorflow:global step 1162: loss = 2.7556 (0.605 sec/step)\n",
            "I1207 23:53:21.153782 140583164200832 learning.py:507] global step 1162: loss = 2.7556 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 1163: loss = 2.8792 (1.387 sec/step)\n",
            "I1207 23:53:22.718827 140583164200832 learning.py:507] global step 1163: loss = 2.8792 (1.387 sec/step)\n",
            "INFO:tensorflow:global step 1164: loss = 2.0980 (0.594 sec/step)\n",
            "I1207 23:53:23.314697 140583164200832 learning.py:507] global step 1164: loss = 2.0980 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1165: loss = 2.2235 (1.860 sec/step)\n",
            "I1207 23:53:25.177125 140583164200832 learning.py:507] global step 1165: loss = 2.2235 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 1166: loss = 2.5866 (0.529 sec/step)\n",
            "I1207 23:53:25.708845 140583164200832 learning.py:507] global step 1166: loss = 2.5866 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 1167: loss = 2.9263 (1.843 sec/step)\n",
            "I1207 23:53:27.554340 140583164200832 learning.py:507] global step 1167: loss = 2.9263 (1.843 sec/step)\n",
            "INFO:tensorflow:global step 1168: loss = 2.4242 (0.482 sec/step)\n",
            "I1207 23:53:28.038665 140583164200832 learning.py:507] global step 1168: loss = 2.4242 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 1169: loss = 2.2934 (1.896 sec/step)\n",
            "I1207 23:53:29.936233 140583164200832 learning.py:507] global step 1169: loss = 2.2934 (1.896 sec/step)\n",
            "INFO:tensorflow:global step 1170: loss = 2.2054 (0.691 sec/step)\n",
            "I1207 23:53:30.791106 140583164200832 learning.py:507] global step 1170: loss = 2.2054 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1171: loss = 2.1732 (0.601 sec/step)\n",
            "I1207 23:53:31.895270 140583164200832 learning.py:507] global step 1171: loss = 2.1732 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1172: loss = 2.6123 (1.333 sec/step)\n",
            "I1207 23:53:33.380714 140583164200832 learning.py:507] global step 1172: loss = 2.6123 (1.333 sec/step)\n",
            "INFO:tensorflow:global step 1173: loss = 2.3834 (0.575 sec/step)\n",
            "I1207 23:53:34.086920 140583164200832 learning.py:507] global step 1173: loss = 2.3834 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 1174: loss = 2.4269 (1.752 sec/step)\n",
            "I1207 23:53:35.841435 140583164200832 learning.py:507] global step 1174: loss = 2.4269 (1.752 sec/step)\n",
            "INFO:tensorflow:global step 1175: loss = 2.0786 (0.670 sec/step)\n",
            "I1207 23:53:36.791954 140583164200832 learning.py:507] global step 1175: loss = 2.0786 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1176: loss = 3.1127 (0.690 sec/step)\n",
            "I1207 23:53:37.980686 140583164200832 learning.py:507] global step 1176: loss = 3.1127 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1177: loss = 2.3639 (0.951 sec/step)\n",
            "I1207 23:53:39.186055 140583164200832 learning.py:507] global step 1177: loss = 2.3639 (0.951 sec/step)\n",
            "INFO:tensorflow:global step 1178: loss = 2.7825 (0.579 sec/step)\n",
            "I1207 23:53:40.041002 140583164200832 learning.py:507] global step 1178: loss = 2.7825 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 1179: loss = 2.5492 (1.402 sec/step)\n",
            "I1207 23:53:41.684725 140583164200832 learning.py:507] global step 1179: loss = 2.5492 (1.402 sec/step)\n",
            "INFO:tensorflow:global step 1180: loss = 2.2245 (1.079 sec/step)\n",
            "I1207 23:53:43.287580 140583164200832 learning.py:507] global step 1180: loss = 2.2245 (1.079 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1180.\n",
            "I1207 23:53:45.119789 140579459213056 supervisor.py:1050] Recording summary at step 1180.\n",
            "INFO:tensorflow:global step 1181: loss = 2.5744 (1.795 sec/step)\n",
            "I1207 23:53:45.370106 140583164200832 learning.py:507] global step 1181: loss = 2.5744 (1.795 sec/step)\n",
            "INFO:tensorflow:global step 1182: loss = 2.7087 (0.559 sec/step)\n",
            "I1207 23:53:46.219058 140583164200832 learning.py:507] global step 1182: loss = 2.7087 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1183: loss = 2.6606 (1.347 sec/step)\n",
            "I1207 23:53:47.717574 140583164200832 learning.py:507] global step 1183: loss = 2.6606 (1.347 sec/step)\n",
            "INFO:tensorflow:global step 1184: loss = 2.3344 (0.775 sec/step)\n",
            "I1207 23:53:48.636281 140583164200832 learning.py:507] global step 1184: loss = 2.3344 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 1185: loss = 2.6447 (1.537 sec/step)\n",
            "I1207 23:53:50.341567 140583164200832 learning.py:507] global step 1185: loss = 2.6447 (1.537 sec/step)\n",
            "INFO:tensorflow:global step 1186: loss = 1.5931 (0.647 sec/step)\n",
            "I1207 23:53:51.190919 140583164200832 learning.py:507] global step 1186: loss = 1.5931 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1187: loss = 2.9893 (0.717 sec/step)\n",
            "I1207 23:53:52.152201 140583164200832 learning.py:507] global step 1187: loss = 2.9893 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1188: loss = 2.0946 (1.661 sec/step)\n",
            "I1207 23:53:54.020971 140583164200832 learning.py:507] global step 1188: loss = 2.0946 (1.661 sec/step)\n",
            "INFO:tensorflow:global step 1189: loss = 2.1615 (0.819 sec/step)\n",
            "I1207 23:53:55.016125 140583164200832 learning.py:507] global step 1189: loss = 2.1615 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1190: loss = 2.2519 (0.699 sec/step)\n",
            "I1207 23:53:56.017729 140583164200832 learning.py:507] global step 1190: loss = 2.2519 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1191: loss = 2.3050 (0.713 sec/step)\n",
            "I1207 23:53:57.181774 140583164200832 learning.py:507] global step 1191: loss = 2.3050 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1192: loss = 2.8508 (0.650 sec/step)\n",
            "I1207 23:53:58.267174 140583164200832 learning.py:507] global step 1192: loss = 2.8508 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1193: loss = 2.5612 (0.563 sec/step)\n",
            "I1207 23:53:58.943068 140583164200832 learning.py:507] global step 1193: loss = 2.5612 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 1194: loss = 3.1934 (0.727 sec/step)\n",
            "I1207 23:53:59.874006 140583164200832 learning.py:507] global step 1194: loss = 3.1934 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1195: loss = 3.0742 (1.715 sec/step)\n",
            "I1207 23:54:01.764312 140583164200832 learning.py:507] global step 1195: loss = 3.0742 (1.715 sec/step)\n",
            "INFO:tensorflow:global step 1196: loss = 2.7092 (0.518 sec/step)\n",
            "I1207 23:54:02.284531 140583164200832 learning.py:507] global step 1196: loss = 2.7092 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 1197: loss = 2.6657 (0.812 sec/step)\n",
            "I1207 23:54:03.290726 140583164200832 learning.py:507] global step 1197: loss = 2.6657 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 1198: loss = 2.9810 (0.891 sec/step)\n",
            "I1207 23:54:04.344546 140583164200832 learning.py:507] global step 1198: loss = 2.9810 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 1199: loss = 2.5442 (0.842 sec/step)\n",
            "I1207 23:54:05.318774 140583164200832 learning.py:507] global step 1199: loss = 2.5442 (0.842 sec/step)\n",
            "INFO:tensorflow:global step 1200: loss = 2.6220 (1.849 sec/step)\n",
            "I1207 23:54:07.170603 140583164200832 learning.py:507] global step 1200: loss = 2.6220 (1.849 sec/step)\n",
            "INFO:tensorflow:global step 1201: loss = 3.5419 (0.695 sec/step)\n",
            "I1207 23:54:08.082368 140583164200832 learning.py:507] global step 1201: loss = 3.5419 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1202: loss = 2.9521 (0.758 sec/step)\n",
            "I1207 23:54:08.946569 140583164200832 learning.py:507] global step 1202: loss = 2.9521 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1203: loss = 2.2921 (1.383 sec/step)\n",
            "I1207 23:54:10.331460 140583164200832 learning.py:507] global step 1203: loss = 2.2921 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 1204: loss = 2.0991 (0.618 sec/step)\n",
            "I1207 23:54:11.047127 140583164200832 learning.py:507] global step 1204: loss = 2.0991 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1205: loss = 2.6487 (0.518 sec/step)\n",
            "I1207 23:54:11.875293 140583164200832 learning.py:507] global step 1205: loss = 2.6487 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 1206: loss = 2.7311 (0.544 sec/step)\n",
            "I1207 23:54:13.025390 140583164200832 learning.py:507] global step 1206: loss = 2.7311 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1207: loss = 2.3448 (1.711 sec/step)\n",
            "I1207 23:54:14.996622 140583164200832 learning.py:507] global step 1207: loss = 2.3448 (1.711 sec/step)\n",
            "INFO:tensorflow:global step 1208: loss = 2.4855 (0.557 sec/step)\n",
            "I1207 23:54:15.897013 140583164200832 learning.py:507] global step 1208: loss = 2.4855 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 1209: loss = 3.4167 (0.674 sec/step)\n",
            "I1207 23:54:17.074236 140583164200832 learning.py:507] global step 1209: loss = 3.4167 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 1210: loss = 2.1287 (0.658 sec/step)\n",
            "I1207 23:54:18.190597 140583164200832 learning.py:507] global step 1210: loss = 2.1287 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1211: loss = 2.3708 (1.375 sec/step)\n",
            "I1207 23:54:19.749346 140583164200832 learning.py:507] global step 1211: loss = 2.3708 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 1212: loss = 2.2754 (0.481 sec/step)\n",
            "I1207 23:54:20.231897 140583164200832 learning.py:507] global step 1212: loss = 2.2754 (0.481 sec/step)\n",
            "INFO:tensorflow:global step 1213: loss = 2.4051 (0.921 sec/step)\n",
            "I1207 23:54:21.509408 140583164200832 learning.py:507] global step 1213: loss = 2.4051 (0.921 sec/step)\n",
            "INFO:tensorflow:global step 1214: loss = 2.1623 (1.639 sec/step)\n",
            "I1207 23:54:23.227450 140583164200832 learning.py:507] global step 1214: loss = 2.1623 (1.639 sec/step)\n",
            "INFO:tensorflow:global step 1215: loss = 1.8563 (0.546 sec/step)\n",
            "I1207 23:54:23.775455 140583164200832 learning.py:507] global step 1215: loss = 1.8563 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 1216: loss = 2.6658 (1.736 sec/step)\n",
            "I1207 23:54:25.513761 140583164200832 learning.py:507] global step 1216: loss = 2.6658 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 1217: loss = 2.5427 (0.740 sec/step)\n",
            "I1207 23:54:26.454256 140583164200832 learning.py:507] global step 1217: loss = 2.5427 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 1218: loss = 2.8129 (0.809 sec/step)\n",
            "I1207 23:54:27.487031 140583164200832 learning.py:507] global step 1218: loss = 2.8129 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 1219: loss = 2.5166 (1.302 sec/step)\n",
            "I1207 23:54:29.015999 140583164200832 learning.py:507] global step 1219: loss = 2.5166 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 1220: loss = 2.4842 (0.632 sec/step)\n",
            "I1207 23:54:29.903411 140583164200832 learning.py:507] global step 1220: loss = 2.4842 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1221: loss = 2.3016 (0.757 sec/step)\n",
            "I1207 23:54:30.943103 140583164200832 learning.py:507] global step 1221: loss = 2.3016 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 1222: loss = 2.1275 (1.239 sec/step)\n",
            "I1207 23:54:32.209345 140583164200832 learning.py:507] global step 1222: loss = 2.1275 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1223: loss = 2.9384 (0.652 sec/step)\n",
            "I1207 23:54:33.107419 140583164200832 learning.py:507] global step 1223: loss = 2.9384 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1224: loss = 2.3491 (1.305 sec/step)\n",
            "I1207 23:54:34.563168 140583164200832 learning.py:507] global step 1224: loss = 2.3491 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 1225: loss = 2.5964 (0.660 sec/step)\n",
            "I1207 23:54:35.484573 140583164200832 learning.py:507] global step 1225: loss = 2.5964 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1226: loss = 1.8895 (0.493 sec/step)\n",
            "I1207 23:54:36.429099 140583164200832 learning.py:507] global step 1226: loss = 1.8895 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 1227: loss = 2.4292 (1.393 sec/step)\n",
            "I1207 23:54:37.991430 140583164200832 learning.py:507] global step 1227: loss = 2.4292 (1.393 sec/step)\n",
            "INFO:tensorflow:global step 1228: loss = 2.4442 (0.538 sec/step)\n",
            "I1207 23:54:38.834742 140583164200832 learning.py:507] global step 1228: loss = 2.4442 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 1229: loss = 2.3785 (1.422 sec/step)\n",
            "I1207 23:54:40.424140 140583164200832 learning.py:507] global step 1229: loss = 2.3785 (1.422 sec/step)\n",
            "INFO:tensorflow:global step 1230: loss = 2.1461 (1.144 sec/step)\n",
            "I1207 23:54:41.570234 140583164200832 learning.py:507] global step 1230: loss = 2.1461 (1.144 sec/step)\n",
            "INFO:tensorflow:global step 1231: loss = 2.4292 (0.502 sec/step)\n",
            "I1207 23:54:42.073909 140583164200832 learning.py:507] global step 1231: loss = 2.4292 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 1232: loss = 2.3233 (0.749 sec/step)\n",
            "I1207 23:54:42.957831 140583164200832 learning.py:507] global step 1232: loss = 2.3233 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1233: loss = 1.9491 (1.892 sec/step)\n",
            "I1207 23:54:44.987919 140583164200832 learning.py:507] global step 1233: loss = 1.9491 (1.892 sec/step)\n",
            "INFO:tensorflow:global step 1234: loss = 2.0750 (0.556 sec/step)\n",
            "I1207 23:54:45.545467 140583164200832 learning.py:507] global step 1234: loss = 2.0750 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 1235: loss = 2.5756 (1.561 sec/step)\n",
            "I1207 23:54:47.127629 140583164200832 learning.py:507] global step 1235: loss = 2.5756 (1.561 sec/step)\n",
            "INFO:tensorflow:global step 1236: loss = 2.3847 (1.131 sec/step)\n",
            "I1207 23:54:48.260409 140583164200832 learning.py:507] global step 1236: loss = 2.3847 (1.131 sec/step)\n",
            "INFO:tensorflow:global step 1237: loss = 2.8525 (1.135 sec/step)\n",
            "I1207 23:54:49.396557 140583164200832 learning.py:507] global step 1237: loss = 2.8525 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 1238: loss = 2.3608 (0.683 sec/step)\n",
            "I1207 23:54:50.108283 140583164200832 learning.py:507] global step 1238: loss = 2.3608 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1239: loss = 2.7011 (0.733 sec/step)\n",
            "I1207 23:54:51.452478 140583164200832 learning.py:507] global step 1239: loss = 2.7011 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 1240: loss = 3.2038 (0.693 sec/step)\n",
            "I1207 23:54:52.417302 140583164200832 learning.py:507] global step 1240: loss = 3.2038 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1241: loss = 2.3977 (1.293 sec/step)\n",
            "I1207 23:54:53.800890 140583164200832 learning.py:507] global step 1241: loss = 2.3977 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 1242: loss = 2.7535 (0.792 sec/step)\n",
            "I1207 23:54:54.862293 140583164200832 learning.py:507] global step 1242: loss = 2.7535 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 1243: loss = 2.2092 (0.569 sec/step)\n",
            "I1207 23:54:55.638127 140583164200832 learning.py:507] global step 1243: loss = 2.2092 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1244: loss = 2.5053 (0.632 sec/step)\n",
            "I1207 23:54:56.795214 140583164200832 learning.py:507] global step 1244: loss = 2.5053 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1245: loss = 2.5422 (0.679 sec/step)\n",
            "I1207 23:54:57.929537 140583164200832 learning.py:507] global step 1245: loss = 2.5422 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1246: loss = 2.6983 (0.693 sec/step)\n",
            "I1207 23:54:58.935892 140583164200832 learning.py:507] global step 1246: loss = 2.6983 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1247: loss = 2.5054 (1.241 sec/step)\n",
            "I1207 23:55:00.198612 140583164200832 learning.py:507] global step 1247: loss = 2.5054 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1248: loss = 2.5263 (0.562 sec/step)\n",
            "I1207 23:55:01.009155 140583164200832 learning.py:507] global step 1248: loss = 2.5263 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 1249: loss = 2.1129 (0.533 sec/step)\n",
            "I1207 23:55:02.083306 140583164200832 learning.py:507] global step 1249: loss = 2.1129 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 1250: loss = 1.9989 (0.567 sec/step)\n",
            "I1207 23:55:03.060293 140583164200832 learning.py:507] global step 1250: loss = 1.9989 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 1251: loss = 2.1595 (0.688 sec/step)\n",
            "I1207 23:55:04.163050 140583164200832 learning.py:507] global step 1251: loss = 2.1595 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1252: loss = 2.0075 (1.178 sec/step)\n",
            "I1207 23:55:05.468731 140583164200832 learning.py:507] global step 1252: loss = 2.0075 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1253: loss = 2.1176 (0.714 sec/step)\n",
            "I1207 23:55:06.442625 140583164200832 learning.py:507] global step 1253: loss = 2.1176 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 1254: loss = 1.9392 (1.134 sec/step)\n",
            "I1207 23:55:07.647330 140583164200832 learning.py:507] global step 1254: loss = 1.9392 (1.134 sec/step)\n",
            "INFO:tensorflow:global step 1255: loss = 2.1184 (0.515 sec/step)\n",
            "I1207 23:55:08.461502 140583164200832 learning.py:507] global step 1255: loss = 2.1184 (0.515 sec/step)\n",
            "INFO:tensorflow:global step 1256: loss = 2.9034 (0.601 sec/step)\n",
            "I1207 23:55:09.501770 140583164200832 learning.py:507] global step 1256: loss = 2.9034 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 1257: loss = 2.6696 (1.305 sec/step)\n",
            "I1207 23:55:10.936872 140583164200832 learning.py:507] global step 1257: loss = 2.6696 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 1258: loss = 3.4945 (0.680 sec/step)\n",
            "I1207 23:55:11.888620 140583164200832 learning.py:507] global step 1258: loss = 3.4945 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1259: loss = 2.2074 (0.678 sec/step)\n",
            "I1207 23:55:12.824598 140583164200832 learning.py:507] global step 1259: loss = 2.2074 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1260: loss = 2.5029 (1.284 sec/step)\n",
            "I1207 23:55:14.140063 140583164200832 learning.py:507] global step 1260: loss = 2.5029 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 1261: loss = 2.3342 (0.668 sec/step)\n",
            "I1207 23:55:14.975488 140583164200832 learning.py:507] global step 1261: loss = 2.3342 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1262: loss = 2.1283 (1.220 sec/step)\n",
            "I1207 23:55:16.385656 140583164200832 learning.py:507] global step 1262: loss = 2.1283 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1263: loss = 2.5558 (0.648 sec/step)\n",
            "I1207 23:55:17.222204 140583164200832 learning.py:507] global step 1263: loss = 2.5558 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1264: loss = 2.7585 (0.611 sec/step)\n",
            "I1207 23:55:18.288105 140583164200832 learning.py:507] global step 1264: loss = 2.7585 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1265: loss = 2.1759 (1.192 sec/step)\n",
            "I1207 23:55:19.653215 140583164200832 learning.py:507] global step 1265: loss = 2.1759 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1266: loss = 2.6599 (0.611 sec/step)\n",
            "I1207 23:55:20.599125 140583164200832 learning.py:507] global step 1266: loss = 2.6599 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1267: loss = 2.1575 (0.527 sec/step)\n",
            "I1207 23:55:21.442847 140583164200832 learning.py:507] global step 1267: loss = 2.1575 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 1268: loss = 3.0044 (0.566 sec/step)\n",
            "I1207 23:55:22.287613 140583164200832 learning.py:507] global step 1268: loss = 3.0044 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 1269: loss = 2.4848 (1.063 sec/step)\n",
            "I1207 23:55:23.625028 140583164200832 learning.py:507] global step 1269: loss = 2.4848 (1.063 sec/step)\n",
            "INFO:tensorflow:global step 1270: loss = 1.7714 (2.435 sec/step)\n",
            "I1207 23:55:26.437001 140583164200832 learning.py:507] global step 1270: loss = 1.7714 (2.435 sec/step)\n",
            "INFO:tensorflow:global step 1271: loss = 2.5355 (0.635 sec/step)\n",
            "I1207 23:55:27.406001 140583164200832 learning.py:507] global step 1271: loss = 2.5355 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1272: loss = 3.1395 (0.568 sec/step)\n",
            "I1207 23:55:28.174754 140583164200832 learning.py:507] global step 1272: loss = 3.1395 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1273: loss = 2.7253 (2.172 sec/step)\n",
            "I1207 23:55:30.406008 140583164200832 learning.py:507] global step 1273: loss = 2.7253 (2.172 sec/step)\n",
            "INFO:tensorflow:global step 1274: loss = 3.3252 (0.569 sec/step)\n",
            "I1207 23:55:30.980362 140583164200832 learning.py:507] global step 1274: loss = 3.3252 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1275: loss = 1.9940 (1.862 sec/step)\n",
            "I1207 23:55:32.845375 140583164200832 learning.py:507] global step 1275: loss = 1.9940 (1.862 sec/step)\n",
            "INFO:tensorflow:global step 1276: loss = 2.0975 (0.661 sec/step)\n",
            "I1207 23:55:33.885188 140583164200832 learning.py:507] global step 1276: loss = 2.0975 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1277: loss = 2.0517 (1.314 sec/step)\n",
            "I1207 23:55:35.363967 140583164200832 learning.py:507] global step 1277: loss = 2.0517 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1278: loss = 2.5419 (0.586 sec/step)\n",
            "I1207 23:55:35.951371 140583164200832 learning.py:507] global step 1278: loss = 2.5419 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 1279: loss = 2.2658 (1.853 sec/step)\n",
            "I1207 23:55:37.805800 140583164200832 learning.py:507] global step 1279: loss = 2.2658 (1.853 sec/step)\n",
            "INFO:tensorflow:global step 1280: loss = 2.5747 (0.733 sec/step)\n",
            "I1207 23:55:38.710620 140583164200832 learning.py:507] global step 1280: loss = 2.5747 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 1281: loss = 2.3490 (0.701 sec/step)\n",
            "I1207 23:55:39.677070 140583164200832 learning.py:507] global step 1281: loss = 2.3490 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1282: loss = 1.7706 (2.060 sec/step)\n",
            "I1207 23:55:41.949750 140583164200832 learning.py:507] global step 1282: loss = 1.7706 (2.060 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1282.\n",
            "I1207 23:55:44.645237 140579459213056 supervisor.py:1050] Recording summary at step 1282.\n",
            "INFO:tensorflow:global step 1283: loss = 3.0994 (2.863 sec/step)\n",
            "I1207 23:55:44.814892 140583164200832 learning.py:507] global step 1283: loss = 3.0994 (2.863 sec/step)\n",
            "INFO:tensorflow:global step 1284: loss = 2.4786 (0.651 sec/step)\n",
            "I1207 23:55:45.709053 140583164200832 learning.py:507] global step 1284: loss = 2.4786 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1285: loss = 2.6247 (1.965 sec/step)\n",
            "I1207 23:55:47.791023 140583164200832 learning.py:507] global step 1285: loss = 2.6247 (1.965 sec/step)\n",
            "INFO:tensorflow:global step 1286: loss = 2.3769 (0.600 sec/step)\n",
            "I1207 23:55:48.393472 140583164200832 learning.py:507] global step 1286: loss = 2.3769 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 1287: loss = 2.7419 (1.694 sec/step)\n",
            "I1207 23:55:50.298227 140583164200832 learning.py:507] global step 1287: loss = 2.7419 (1.694 sec/step)\n",
            "INFO:tensorflow:global step 1288: loss = 2.2864 (0.529 sec/step)\n",
            "I1207 23:55:50.829933 140583164200832 learning.py:507] global step 1288: loss = 2.2864 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 1289: loss = 2.6675 (1.834 sec/step)\n",
            "I1207 23:55:52.665491 140583164200832 learning.py:507] global step 1289: loss = 2.6675 (1.834 sec/step)\n",
            "INFO:tensorflow:global step 1290: loss = 1.8410 (0.735 sec/step)\n",
            "I1207 23:55:53.648141 140583164200832 learning.py:507] global step 1290: loss = 1.8410 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 1291: loss = 2.4055 (0.670 sec/step)\n",
            "I1207 23:55:54.516061 140583164200832 learning.py:507] global step 1291: loss = 2.4055 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1292: loss = 1.9657 (1.874 sec/step)\n",
            "I1207 23:55:56.392094 140583164200832 learning.py:507] global step 1292: loss = 1.9657 (1.874 sec/step)\n",
            "INFO:tensorflow:global step 1293: loss = 2.6278 (0.602 sec/step)\n",
            "I1207 23:55:57.358556 140583164200832 learning.py:507] global step 1293: loss = 2.6278 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1294: loss = 2.4592 (0.731 sec/step)\n",
            "I1207 23:55:58.472480 140583164200832 learning.py:507] global step 1294: loss = 2.4592 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 1295: loss = 2.6557 (0.525 sec/step)\n",
            "I1207 23:55:59.196886 140583164200832 learning.py:507] global step 1295: loss = 2.6557 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 1296: loss = 1.9535 (0.807 sec/step)\n",
            "I1207 23:56:00.295677 140583164200832 learning.py:507] global step 1296: loss = 1.9535 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 1297: loss = 2.2652 (1.672 sec/step)\n",
            "I1207 23:56:02.294242 140583164200832 learning.py:507] global step 1297: loss = 2.2652 (1.672 sec/step)\n",
            "INFO:tensorflow:global step 1298: loss = 2.5992 (0.571 sec/step)\n",
            "I1207 23:56:02.867364 140583164200832 learning.py:507] global step 1298: loss = 2.5992 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1299: loss = 2.3335 (0.576 sec/step)\n",
            "I1207 23:56:03.445609 140583164200832 learning.py:507] global step 1299: loss = 2.3335 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1300: loss = 2.4403 (3.185 sec/step)\n",
            "I1207 23:56:06.632686 140583164200832 learning.py:507] global step 1300: loss = 2.4403 (3.185 sec/step)\n",
            "INFO:tensorflow:global step 1301: loss = 2.2665 (0.526 sec/step)\n",
            "I1207 23:56:07.160436 140583164200832 learning.py:507] global step 1301: loss = 2.2665 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 1302: loss = 2.7032 (0.678 sec/step)\n",
            "I1207 23:56:07.865112 140583164200832 learning.py:507] global step 1302: loss = 2.7032 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1303: loss = 2.1727 (2.073 sec/step)\n",
            "I1207 23:56:10.264914 140583164200832 learning.py:507] global step 1303: loss = 2.1727 (2.073 sec/step)\n",
            "INFO:tensorflow:global step 1304: loss = 2.0546 (0.490 sec/step)\n",
            "I1207 23:56:10.756782 140583164200832 learning.py:507] global step 1304: loss = 2.0546 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 1305: loss = 2.6722 (1.691 sec/step)\n",
            "I1207 23:56:12.456345 140583164200832 learning.py:507] global step 1305: loss = 2.6722 (1.691 sec/step)\n",
            "INFO:tensorflow:global step 1306: loss = 1.9988 (0.619 sec/step)\n",
            "I1207 23:56:13.219722 140583164200832 learning.py:507] global step 1306: loss = 1.9988 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1307: loss = 2.1411 (1.508 sec/step)\n",
            "I1207 23:56:15.118558 140583164200832 learning.py:507] global step 1307: loss = 2.1411 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 1308: loss = 1.8591 (0.669 sec/step)\n",
            "I1207 23:56:16.132244 140583164200832 learning.py:507] global step 1308: loss = 1.8591 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1309: loss = 2.8747 (1.831 sec/step)\n",
            "I1207 23:56:18.004975 140583164200832 learning.py:507] global step 1309: loss = 2.8747 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 1310: loss = 2.5862 (0.696 sec/step)\n",
            "I1207 23:56:19.043640 140583164200832 learning.py:507] global step 1310: loss = 2.5862 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 1311: loss = 2.3221 (1.275 sec/step)\n",
            "I1207 23:56:20.420613 140583164200832 learning.py:507] global step 1311: loss = 2.3221 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 1312: loss = 2.4363 (0.675 sec/step)\n",
            "I1207 23:56:21.122043 140583164200832 learning.py:507] global step 1312: loss = 2.4363 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1313: loss = 2.3375 (1.762 sec/step)\n",
            "I1207 23:56:22.886481 140583164200832 learning.py:507] global step 1313: loss = 2.3375 (1.762 sec/step)\n",
            "INFO:tensorflow:global step 1314: loss = 2.3900 (0.505 sec/step)\n",
            "I1207 23:56:23.393632 140583164200832 learning.py:507] global step 1314: loss = 2.3900 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 1315: loss = 3.1353 (0.819 sec/step)\n",
            "I1207 23:56:24.446180 140583164200832 learning.py:507] global step 1315: loss = 3.1353 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1316: loss = 2.8179 (2.269 sec/step)\n",
            "I1207 23:56:26.723145 140583164200832 learning.py:507] global step 1316: loss = 2.8179 (2.269 sec/step)\n",
            "INFO:tensorflow:global step 1317: loss = 2.5653 (0.506 sec/step)\n",
            "I1207 23:56:27.230712 140583164200832 learning.py:507] global step 1317: loss = 2.5653 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 1318: loss = 2.1252 (1.809 sec/step)\n",
            "I1207 23:56:29.041603 140583164200832 learning.py:507] global step 1318: loss = 2.1252 (1.809 sec/step)\n",
            "INFO:tensorflow:global step 1319: loss = 2.3336 (0.587 sec/step)\n",
            "I1207 23:56:29.630486 140583164200832 learning.py:507] global step 1319: loss = 2.3336 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1320: loss = 2.7805 (1.703 sec/step)\n",
            "I1207 23:56:31.336763 140583164200832 learning.py:507] global step 1320: loss = 2.7805 (1.703 sec/step)\n",
            "INFO:tensorflow:global step 1321: loss = 1.8652 (0.628 sec/step)\n",
            "I1207 23:56:31.967277 140583164200832 learning.py:507] global step 1321: loss = 1.8652 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1322: loss = 2.6374 (1.739 sec/step)\n",
            "I1207 23:56:33.708360 140583164200832 learning.py:507] global step 1322: loss = 2.6374 (1.739 sec/step)\n",
            "INFO:tensorflow:global step 1323: loss = 2.6242 (1.293 sec/step)\n",
            "I1207 23:56:35.003370 140583164200832 learning.py:507] global step 1323: loss = 2.6242 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 1324: loss = 2.7452 (0.624 sec/step)\n",
            "I1207 23:56:35.697559 140583164200832 learning.py:507] global step 1324: loss = 2.7452 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1325: loss = 2.1831 (1.545 sec/step)\n",
            "I1207 23:56:37.618525 140583164200832 learning.py:507] global step 1325: loss = 2.1831 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1326: loss = 2.4984 (0.568 sec/step)\n",
            "I1207 23:56:38.188663 140583164200832 learning.py:507] global step 1326: loss = 2.4984 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1327: loss = 2.0297 (1.947 sec/step)\n",
            "I1207 23:56:40.137405 140583164200832 learning.py:507] global step 1327: loss = 2.0297 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 1328: loss = 2.4075 (0.698 sec/step)\n",
            "I1207 23:56:40.838333 140583164200832 learning.py:507] global step 1328: loss = 2.4075 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1329: loss = 3.0613 (1.570 sec/step)\n",
            "I1207 23:56:42.427454 140583164200832 learning.py:507] global step 1329: loss = 3.0613 (1.570 sec/step)\n",
            "INFO:tensorflow:global step 1330: loss = 2.9441 (0.635 sec/step)\n",
            "I1207 23:56:43.064191 140583164200832 learning.py:507] global step 1330: loss = 2.9441 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1331: loss = 2.1766 (1.912 sec/step)\n",
            "I1207 23:56:44.980262 140583164200832 learning.py:507] global step 1331: loss = 2.1766 (1.912 sec/step)\n",
            "INFO:tensorflow:global step 1332: loss = 2.0824 (0.495 sec/step)\n",
            "I1207 23:56:45.477176 140583164200832 learning.py:507] global step 1332: loss = 2.0824 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 1333: loss = 2.6539 (1.809 sec/step)\n",
            "I1207 23:56:47.438397 140583164200832 learning.py:507] global step 1333: loss = 2.6539 (1.809 sec/step)\n",
            "INFO:tensorflow:global step 1334: loss = 2.0879 (0.517 sec/step)\n",
            "I1207 23:56:48.065268 140583164200832 learning.py:507] global step 1334: loss = 2.0879 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 1335: loss = 2.6392 (1.883 sec/step)\n",
            "I1207 23:56:49.949963 140583164200832 learning.py:507] global step 1335: loss = 2.6392 (1.883 sec/step)\n",
            "INFO:tensorflow:global step 1336: loss = 3.3937 (0.495 sec/step)\n",
            "I1207 23:56:50.446612 140583164200832 learning.py:507] global step 1336: loss = 3.3937 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 1337: loss = 2.1614 (1.817 sec/step)\n",
            "I1207 23:56:52.264924 140583164200832 learning.py:507] global step 1337: loss = 2.1614 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 1338: loss = 2.0219 (0.616 sec/step)\n",
            "I1207 23:56:53.245458 140583164200832 learning.py:507] global step 1338: loss = 2.0219 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1339: loss = 2.3829 (1.504 sec/step)\n",
            "I1207 23:56:54.758910 140583164200832 learning.py:507] global step 1339: loss = 2.3829 (1.504 sec/step)\n",
            "INFO:tensorflow:global step 1340: loss = 2.2008 (0.595 sec/step)\n",
            "I1207 23:56:55.356198 140583164200832 learning.py:507] global step 1340: loss = 2.2008 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 1341: loss = 2.8907 (1.055 sec/step)\n",
            "I1207 23:56:56.737583 140583164200832 learning.py:507] global step 1341: loss = 2.8907 (1.055 sec/step)\n",
            "INFO:tensorflow:global step 1342: loss = 2.3066 (1.702 sec/step)\n",
            "I1207 23:56:58.662162 140583164200832 learning.py:507] global step 1342: loss = 2.3066 (1.702 sec/step)\n",
            "INFO:tensorflow:global step 1343: loss = 2.1948 (0.626 sec/step)\n",
            "I1207 23:56:59.290257 140583164200832 learning.py:507] global step 1343: loss = 2.1948 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1344: loss = 2.5292 (0.993 sec/step)\n",
            "I1207 23:57:00.362234 140583164200832 learning.py:507] global step 1344: loss = 2.5292 (0.993 sec/step)\n",
            "INFO:tensorflow:global step 1345: loss = 1.8250 (1.913 sec/step)\n",
            "I1207 23:57:02.565633 140583164200832 learning.py:507] global step 1345: loss = 1.8250 (1.913 sec/step)\n",
            "INFO:tensorflow:global step 1346: loss = 2.1581 (0.596 sec/step)\n",
            "I1207 23:57:03.582007 140583164200832 learning.py:507] global step 1346: loss = 2.1581 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1347: loss = 1.9669 (1.749 sec/step)\n",
            "I1207 23:57:05.344855 140583164200832 learning.py:507] global step 1347: loss = 1.9669 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 1348: loss = 2.3889 (0.678 sec/step)\n",
            "I1207 23:57:06.457720 140583164200832 learning.py:507] global step 1348: loss = 2.3889 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1349: loss = 2.2335 (0.680 sec/step)\n",
            "I1207 23:57:07.255397 140583164200832 learning.py:507] global step 1349: loss = 2.2335 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1350: loss = 2.7385 (1.545 sec/step)\n",
            "I1207 23:57:08.812279 140583164200832 learning.py:507] global step 1350: loss = 2.7385 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1351: loss = 2.2124 (0.820 sec/step)\n",
            "I1207 23:57:10.104695 140583164200832 learning.py:507] global step 1351: loss = 2.2124 (0.820 sec/step)\n",
            "INFO:tensorflow:global step 1352: loss = 2.8617 (0.594 sec/step)\n",
            "I1207 23:57:10.712671 140583164200832 learning.py:507] global step 1352: loss = 2.8617 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1353: loss = 2.8677 (1.851 sec/step)\n",
            "I1207 23:57:12.565779 140583164200832 learning.py:507] global step 1353: loss = 2.8677 (1.851 sec/step)\n",
            "INFO:tensorflow:global step 1354: loss = 2.6078 (0.749 sec/step)\n",
            "I1207 23:57:13.524528 140583164200832 learning.py:507] global step 1354: loss = 2.6078 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1355: loss = 2.4725 (1.426 sec/step)\n",
            "I1207 23:57:15.182872 140583164200832 learning.py:507] global step 1355: loss = 2.4725 (1.426 sec/step)\n",
            "INFO:tensorflow:global step 1356: loss = 2.6826 (0.669 sec/step)\n",
            "I1207 23:57:15.855044 140583164200832 learning.py:507] global step 1356: loss = 2.6826 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1357: loss = 3.0077 (1.805 sec/step)\n",
            "I1207 23:57:17.662351 140583164200832 learning.py:507] global step 1357: loss = 3.0077 (1.805 sec/step)\n",
            "INFO:tensorflow:global step 1358: loss = 2.6524 (0.534 sec/step)\n",
            "I1207 23:57:18.198683 140583164200832 learning.py:507] global step 1358: loss = 2.6524 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 1359: loss = 2.2588 (1.872 sec/step)\n",
            "I1207 23:57:20.073034 140583164200832 learning.py:507] global step 1359: loss = 2.2588 (1.872 sec/step)\n",
            "INFO:tensorflow:global step 1360: loss = 2.0357 (0.800 sec/step)\n",
            "I1207 23:57:21.087725 140583164200832 learning.py:507] global step 1360: loss = 2.0357 (0.800 sec/step)\n",
            "INFO:tensorflow:global step 1361: loss = 2.6583 (1.361 sec/step)\n",
            "I1207 23:57:22.535242 140583164200832 learning.py:507] global step 1361: loss = 2.6583 (1.361 sec/step)\n",
            "INFO:tensorflow:global step 1362: loss = 2.3957 (0.548 sec/step)\n",
            "I1207 23:57:23.085059 140583164200832 learning.py:507] global step 1362: loss = 2.3957 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 1363: loss = 3.0140 (1.869 sec/step)\n",
            "I1207 23:57:24.956429 140583164200832 learning.py:507] global step 1363: loss = 3.0140 (1.869 sec/step)\n",
            "INFO:tensorflow:global step 1364: loss = 2.4776 (0.571 sec/step)\n",
            "I1207 23:57:25.596356 140583164200832 learning.py:507] global step 1364: loss = 2.4776 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1365: loss = 2.9870 (1.860 sec/step)\n",
            "I1207 23:57:27.458224 140583164200832 learning.py:507] global step 1365: loss = 2.9870 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 1366: loss = 1.8982 (0.642 sec/step)\n",
            "I1207 23:57:28.102696 140583164200832 learning.py:507] global step 1366: loss = 1.8982 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1367: loss = 2.7683 (2.331 sec/step)\n",
            "I1207 23:57:30.435954 140583164200832 learning.py:507] global step 1367: loss = 2.7683 (2.331 sec/step)\n",
            "INFO:tensorflow:global step 1368: loss = 1.9360 (0.616 sec/step)\n",
            "I1207 23:57:31.233966 140583164200832 learning.py:507] global step 1368: loss = 1.9360 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1369: loss = 2.3090 (1.279 sec/step)\n",
            "I1207 23:57:32.826132 140583164200832 learning.py:507] global step 1369: loss = 2.3090 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 1370: loss = 2.8017 (0.572 sec/step)\n",
            "I1207 23:57:33.600033 140583164200832 learning.py:507] global step 1370: loss = 2.8017 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 1371: loss = 2.2840 (0.677 sec/step)\n",
            "I1207 23:57:34.781558 140583164200832 learning.py:507] global step 1371: loss = 2.2840 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1372: loss = 2.2414 (2.710 sec/step)\n",
            "I1207 23:57:37.754222 140583164200832 learning.py:507] global step 1372: loss = 2.2414 (2.710 sec/step)\n",
            "INFO:tensorflow:global step 1373: loss = 2.2665 (0.571 sec/step)\n",
            "I1207 23:57:38.327621 140583164200832 learning.py:507] global step 1373: loss = 2.2665 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1374: loss = 2.2891 (1.271 sec/step)\n",
            "I1207 23:57:39.826403 140583164200832 learning.py:507] global step 1374: loss = 2.2891 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 1375: loss = 2.7420 (1.143 sec/step)\n",
            "I1207 23:57:41.279680 140583164200832 learning.py:507] global step 1375: loss = 2.7420 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 1376: loss = 2.7509 (0.593 sec/step)\n",
            "I1207 23:57:41.901944 140583164200832 learning.py:507] global step 1376: loss = 2.7509 (0.593 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1377.\n",
            "I1207 23:57:44.552562 140579459213056 supervisor.py:1050] Recording summary at step 1377.\n",
            "INFO:tensorflow:global step 1377: loss = 2.3363 (2.646 sec/step)\n",
            "I1207 23:57:44.558576 140583164200832 learning.py:507] global step 1377: loss = 2.3363 (2.646 sec/step)\n",
            "INFO:tensorflow:global step 1378: loss = 2.3860 (0.577 sec/step)\n",
            "I1207 23:57:45.137579 140583164200832 learning.py:507] global step 1378: loss = 2.3860 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 1379: loss = 2.3606 (0.660 sec/step)\n",
            "I1207 23:57:45.799973 140583164200832 learning.py:507] global step 1379: loss = 2.3606 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1380: loss = 2.0295 (3.005 sec/step)\n",
            "I1207 23:57:48.807214 140583164200832 learning.py:507] global step 1380: loss = 2.0295 (3.005 sec/step)\n",
            "INFO:tensorflow:global step 1381: loss = 2.1114 (0.766 sec/step)\n",
            "I1207 23:57:49.830359 140583164200832 learning.py:507] global step 1381: loss = 2.1114 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 1382: loss = 3.2375 (1.213 sec/step)\n",
            "I1207 23:57:51.140228 140583164200832 learning.py:507] global step 1382: loss = 3.2375 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1383: loss = 1.8588 (0.570 sec/step)\n",
            "I1207 23:57:51.711830 140583164200832 learning.py:507] global step 1383: loss = 1.8588 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 1384: loss = 2.4931 (1.917 sec/step)\n",
            "I1207 23:57:53.630424 140583164200832 learning.py:507] global step 1384: loss = 2.4931 (1.917 sec/step)\n",
            "INFO:tensorflow:global step 1385: loss = 2.2129 (0.608 sec/step)\n",
            "I1207 23:57:54.240612 140583164200832 learning.py:507] global step 1385: loss = 2.2129 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 1386: loss = 2.5572 (1.053 sec/step)\n",
            "I1207 23:57:55.487800 140583164200832 learning.py:507] global step 1386: loss = 2.5572 (1.053 sec/step)\n",
            "INFO:tensorflow:global step 1387: loss = 2.2281 (1.683 sec/step)\n",
            "I1207 23:57:57.544919 140583164200832 learning.py:507] global step 1387: loss = 2.2281 (1.683 sec/step)\n",
            "INFO:tensorflow:global step 1388: loss = 2.2756 (0.629 sec/step)\n",
            "I1207 23:57:58.466254 140583164200832 learning.py:507] global step 1388: loss = 2.2756 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1389: loss = 2.1816 (0.682 sec/step)\n",
            "I1207 23:57:59.277454 140583164200832 learning.py:507] global step 1389: loss = 2.1816 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1390: loss = 2.3328 (1.094 sec/step)\n",
            "I1207 23:58:00.556459 140583164200832 learning.py:507] global step 1390: loss = 2.3328 (1.094 sec/step)\n",
            "INFO:tensorflow:global step 1391: loss = 1.9472 (1.829 sec/step)\n",
            "I1207 23:58:02.855694 140583164200832 learning.py:507] global step 1391: loss = 1.9472 (1.829 sec/step)\n",
            "INFO:tensorflow:global step 1392: loss = 3.0030 (0.718 sec/step)\n",
            "I1207 23:58:03.868369 140583164200832 learning.py:507] global step 1392: loss = 3.0030 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 1393: loss = 1.7274 (0.581 sec/step)\n",
            "I1207 23:58:04.720511 140583164200832 learning.py:507] global step 1393: loss = 1.7274 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 1394: loss = 2.1388 (0.558 sec/step)\n",
            "I1207 23:58:05.825596 140583164200832 learning.py:507] global step 1394: loss = 2.1388 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1395: loss = 1.9882 (0.647 sec/step)\n",
            "I1207 23:58:06.732427 140583164200832 learning.py:507] global step 1395: loss = 1.9882 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1396: loss = 2.0018 (1.519 sec/step)\n",
            "I1207 23:58:08.253151 140583164200832 learning.py:507] global step 1396: loss = 2.0018 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 1397: loss = 2.0307 (1.098 sec/step)\n",
            "I1207 23:58:09.352442 140583164200832 learning.py:507] global step 1397: loss = 2.0307 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 1398: loss = 1.7093 (0.605 sec/step)\n",
            "I1207 23:58:10.037689 140583164200832 learning.py:507] global step 1398: loss = 1.7093 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 1399: loss = 2.4977 (1.328 sec/step)\n",
            "I1207 23:58:11.527650 140583164200832 learning.py:507] global step 1399: loss = 2.4977 (1.328 sec/step)\n",
            "INFO:tensorflow:global step 1400: loss = 2.4560 (0.583 sec/step)\n",
            "I1207 23:58:12.273970 140583164200832 learning.py:507] global step 1400: loss = 2.4560 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1401: loss = 2.3616 (1.130 sec/step)\n",
            "I1207 23:58:13.603266 140583164200832 learning.py:507] global step 1401: loss = 2.3616 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 1402: loss = 2.6897 (0.737 sec/step)\n",
            "I1207 23:58:14.559179 140583164200832 learning.py:507] global step 1402: loss = 2.6897 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1403: loss = 2.3145 (1.325 sec/step)\n",
            "I1207 23:58:15.953963 140583164200832 learning.py:507] global step 1403: loss = 2.3145 (1.325 sec/step)\n",
            "INFO:tensorflow:global step 1404: loss = 2.8234 (0.594 sec/step)\n",
            "I1207 23:58:16.821227 140583164200832 learning.py:507] global step 1404: loss = 2.8234 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1405: loss = 2.1326 (0.706 sec/step)\n",
            "I1207 23:58:17.857192 140583164200832 learning.py:507] global step 1405: loss = 2.1326 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1406: loss = 2.4614 (0.517 sec/step)\n",
            "I1207 23:58:18.505753 140583164200832 learning.py:507] global step 1406: loss = 2.4614 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 1407: loss = 2.3608 (1.140 sec/step)\n",
            "I1207 23:58:19.902899 140583164200832 learning.py:507] global step 1407: loss = 2.3608 (1.140 sec/step)\n",
            "INFO:tensorflow:global step 1408: loss = 2.2902 (0.576 sec/step)\n",
            "I1207 23:58:20.973969 140583164200832 learning.py:507] global step 1408: loss = 2.2902 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1409: loss = 2.4290 (0.656 sec/step)\n",
            "I1207 23:58:21.859014 140583164200832 learning.py:507] global step 1409: loss = 2.4290 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1410: loss = 2.1855 (1.262 sec/step)\n",
            "I1207 23:58:23.423402 140583164200832 learning.py:507] global step 1410: loss = 2.1855 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 1411: loss = 2.0507 (0.727 sec/step)\n",
            "I1207 23:58:24.302635 140583164200832 learning.py:507] global step 1411: loss = 2.0507 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1412: loss = 2.7561 (0.695 sec/step)\n",
            "I1207 23:58:25.346089 140583164200832 learning.py:507] global step 1412: loss = 2.7561 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1413: loss = 2.4667 (1.206 sec/step)\n",
            "I1207 23:58:26.627926 140583164200832 learning.py:507] global step 1413: loss = 2.4667 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1414: loss = 2.2543 (0.597 sec/step)\n",
            "I1207 23:58:27.521344 140583164200832 learning.py:507] global step 1414: loss = 2.2543 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1415: loss = 2.4285 (1.216 sec/step)\n",
            "I1207 23:58:28.953864 140583164200832 learning.py:507] global step 1415: loss = 2.4285 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1416: loss = 2.5681 (0.701 sec/step)\n",
            "I1207 23:58:29.932742 140583164200832 learning.py:507] global step 1416: loss = 2.5681 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1417: loss = 1.9181 (1.184 sec/step)\n",
            "I1207 23:58:31.261370 140583164200832 learning.py:507] global step 1417: loss = 1.9181 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1418: loss = 2.4311 (0.538 sec/step)\n",
            "I1207 23:58:31.801824 140583164200832 learning.py:507] global step 1418: loss = 2.4311 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 1419: loss = 2.1684 (1.642 sec/step)\n",
            "I1207 23:58:33.445628 140583164200832 learning.py:507] global step 1419: loss = 2.1684 (1.642 sec/step)\n",
            "INFO:tensorflow:global step 1420: loss = 2.2528 (0.526 sec/step)\n",
            "I1207 23:58:33.974230 140583164200832 learning.py:507] global step 1420: loss = 2.2528 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 1421: loss = 1.9608 (0.918 sec/step)\n",
            "I1207 23:58:34.976706 140583164200832 learning.py:507] global step 1421: loss = 1.9608 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 1422: loss = 1.9787 (1.472 sec/step)\n",
            "I1207 23:58:36.836114 140583164200832 learning.py:507] global step 1422: loss = 1.9787 (1.472 sec/step)\n",
            "INFO:tensorflow:global step 1423: loss = 2.3183 (0.533 sec/step)\n",
            "I1207 23:58:37.370688 140583164200832 learning.py:507] global step 1423: loss = 2.3183 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 1424: loss = 2.5270 (0.624 sec/step)\n",
            "I1207 23:58:37.996985 140583164200832 learning.py:507] global step 1424: loss = 2.5270 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1425: loss = 2.5957 (3.095 sec/step)\n",
            "I1207 23:58:41.093989 140583164200832 learning.py:507] global step 1425: loss = 2.5957 (3.095 sec/step)\n",
            "INFO:tensorflow:global step 1426: loss = 2.0558 (0.587 sec/step)\n",
            "I1207 23:58:41.682948 140583164200832 learning.py:507] global step 1426: loss = 2.0558 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1427: loss = 1.6611 (1.671 sec/step)\n",
            "I1207 23:58:43.356004 140583164200832 learning.py:507] global step 1427: loss = 1.6611 (1.671 sec/step)\n",
            "INFO:tensorflow:global step 1428: loss = 2.4078 (0.566 sec/step)\n",
            "I1207 23:58:43.924574 140583164200832 learning.py:507] global step 1428: loss = 2.4078 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 1429: loss = 2.1487 (1.148 sec/step)\n",
            "I1207 23:58:45.333424 140583164200832 learning.py:507] global step 1429: loss = 2.1487 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 1430: loss = 2.3554 (1.653 sec/step)\n",
            "I1207 23:58:47.051232 140583164200832 learning.py:507] global step 1430: loss = 2.3554 (1.653 sec/step)\n",
            "INFO:tensorflow:global step 1431: loss = 2.0970 (0.538 sec/step)\n",
            "I1207 23:58:47.591376 140583164200832 learning.py:507] global step 1431: loss = 2.0970 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 1432: loss = 2.8175 (0.866 sec/step)\n",
            "I1207 23:58:48.700314 140583164200832 learning.py:507] global step 1432: loss = 2.8175 (0.866 sec/step)\n",
            "INFO:tensorflow:global step 1433: loss = 1.9963 (1.319 sec/step)\n",
            "I1207 23:58:50.235988 140583164200832 learning.py:507] global step 1433: loss = 1.9963 (1.319 sec/step)\n",
            "INFO:tensorflow:global step 1434: loss = 2.4819 (0.511 sec/step)\n",
            "I1207 23:58:50.749492 140583164200832 learning.py:507] global step 1434: loss = 2.4819 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 1435: loss = 2.5921 (1.624 sec/step)\n",
            "I1207 23:58:52.375799 140583164200832 learning.py:507] global step 1435: loss = 2.5921 (1.624 sec/step)\n",
            "INFO:tensorflow:global step 1436: loss = 2.1153 (0.558 sec/step)\n",
            "I1207 23:58:52.935468 140583164200832 learning.py:507] global step 1436: loss = 2.1153 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1437: loss = 2.6191 (1.710 sec/step)\n",
            "I1207 23:58:54.706374 140583164200832 learning.py:507] global step 1437: loss = 2.6191 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 1438: loss = 2.5358 (1.591 sec/step)\n",
            "I1207 23:58:56.338931 140583164200832 learning.py:507] global step 1438: loss = 2.5358 (1.591 sec/step)\n",
            "INFO:tensorflow:global step 1439: loss = 2.2096 (0.616 sec/step)\n",
            "I1207 23:58:57.336092 140583164200832 learning.py:507] global step 1439: loss = 2.2096 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1440: loss = 2.2792 (1.384 sec/step)\n",
            "I1207 23:58:58.752958 140583164200832 learning.py:507] global step 1440: loss = 2.2792 (1.384 sec/step)\n",
            "INFO:tensorflow:global step 1441: loss = 2.1608 (0.640 sec/step)\n",
            "I1207 23:58:59.394871 140583164200832 learning.py:507] global step 1441: loss = 2.1608 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1442: loss = 2.1044 (0.984 sec/step)\n",
            "I1207 23:59:00.566351 140583164200832 learning.py:507] global step 1442: loss = 2.1044 (0.984 sec/step)\n",
            "INFO:tensorflow:global step 1443: loss = 2.2176 (1.881 sec/step)\n",
            "I1207 23:59:02.697057 140583164200832 learning.py:507] global step 1443: loss = 2.2176 (1.881 sec/step)\n",
            "INFO:tensorflow:global step 1444: loss = 2.1605 (0.504 sec/step)\n",
            "I1207 23:59:03.202898 140583164200832 learning.py:507] global step 1444: loss = 2.1605 (0.504 sec/step)\n",
            "INFO:tensorflow:global step 1445: loss = 2.0329 (1.896 sec/step)\n",
            "I1207 23:59:05.100867 140583164200832 learning.py:507] global step 1445: loss = 2.0329 (1.896 sec/step)\n",
            "INFO:tensorflow:global step 1446: loss = 2.0064 (0.749 sec/step)\n",
            "I1207 23:59:06.184881 140583164200832 learning.py:507] global step 1446: loss = 2.0064 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1447: loss = 2.5407 (0.663 sec/step)\n",
            "I1207 23:59:07.482990 140583164200832 learning.py:507] global step 1447: loss = 2.5407 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1448: loss = 2.0194 (0.629 sec/step)\n",
            "I1207 23:59:08.472225 140583164200832 learning.py:507] global step 1448: loss = 2.0194 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1449: loss = 2.5873 (0.558 sec/step)\n",
            "I1207 23:59:09.250270 140583164200832 learning.py:507] global step 1449: loss = 2.5873 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1450: loss = 1.7737 (1.771 sec/step)\n",
            "I1207 23:59:11.567453 140583164200832 learning.py:507] global step 1450: loss = 1.7737 (1.771 sec/step)\n",
            "INFO:tensorflow:global step 1451: loss = 2.0376 (0.676 sec/step)\n",
            "I1207 23:59:12.545983 140583164200832 learning.py:507] global step 1451: loss = 2.0376 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1452: loss = 1.8693 (1.363 sec/step)\n",
            "I1207 23:59:13.948399 140583164200832 learning.py:507] global step 1452: loss = 1.8693 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 1453: loss = 2.3217 (0.513 sec/step)\n",
            "I1207 23:59:14.463432 140583164200832 learning.py:507] global step 1453: loss = 2.3217 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1454: loss = 2.5472 (0.866 sec/step)\n",
            "I1207 23:59:15.593978 140583164200832 learning.py:507] global step 1454: loss = 2.5472 (0.866 sec/step)\n",
            "INFO:tensorflow:global step 1455: loss = 1.9196 (2.012 sec/step)\n",
            "I1207 23:59:17.995760 140583164200832 learning.py:507] global step 1455: loss = 1.9196 (2.012 sec/step)\n",
            "INFO:tensorflow:global step 1456: loss = 1.7387 (0.655 sec/step)\n",
            "I1207 23:59:18.881802 140583164200832 learning.py:507] global step 1456: loss = 1.7387 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1457: loss = 2.0118 (1.398 sec/step)\n",
            "I1207 23:59:20.397793 140583164200832 learning.py:507] global step 1457: loss = 2.0118 (1.398 sec/step)\n",
            "INFO:tensorflow:global step 1458: loss = 1.9520 (0.568 sec/step)\n",
            "I1207 23:59:20.968224 140583164200832 learning.py:507] global step 1458: loss = 1.9520 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1459: loss = 1.9631 (1.045 sec/step)\n",
            "I1207 23:59:22.205909 140583164200832 learning.py:507] global step 1459: loss = 1.9631 (1.045 sec/step)\n",
            "INFO:tensorflow:global step 1460: loss = 2.1818 (1.800 sec/step)\n",
            "I1207 23:59:24.062458 140583164200832 learning.py:507] global step 1460: loss = 2.1818 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 1461: loss = 2.0742 (0.537 sec/step)\n",
            "I1207 23:59:24.601287 140583164200832 learning.py:507] global step 1461: loss = 2.0742 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 1462: loss = 2.7414 (0.860 sec/step)\n",
            "I1207 23:59:25.795149 140583164200832 learning.py:507] global step 1462: loss = 2.7414 (0.860 sec/step)\n",
            "INFO:tensorflow:global step 1463: loss = 2.6663 (1.971 sec/step)\n",
            "I1207 23:59:28.123010 140583164200832 learning.py:507] global step 1463: loss = 2.6663 (1.971 sec/step)\n",
            "INFO:tensorflow:global step 1464: loss = 2.5070 (0.701 sec/step)\n",
            "I1207 23:59:28.986925 140583164200832 learning.py:507] global step 1464: loss = 2.5070 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1465: loss = 2.8352 (1.148 sec/step)\n",
            "I1207 23:59:30.500602 140583164200832 learning.py:507] global step 1465: loss = 2.8352 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 1466: loss = 2.6346 (1.492 sec/step)\n",
            "I1207 23:59:32.060356 140583164200832 learning.py:507] global step 1466: loss = 2.6346 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 1467: loss = 2.0417 (0.578 sec/step)\n",
            "I1207 23:59:33.050368 140583164200832 learning.py:507] global step 1467: loss = 2.0417 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 1468: loss = 2.0836 (1.810 sec/step)\n",
            "I1207 23:59:34.990630 140583164200832 learning.py:507] global step 1468: loss = 2.0836 (1.810 sec/step)\n",
            "INFO:tensorflow:global step 1469: loss = 2.2448 (0.664 sec/step)\n",
            "I1207 23:59:36.041632 140583164200832 learning.py:507] global step 1469: loss = 2.2448 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1470: loss = 3.1300 (0.692 sec/step)\n",
            "I1207 23:59:36.787229 140583164200832 learning.py:507] global step 1470: loss = 3.1300 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1471: loss = 1.8055 (0.875 sec/step)\n",
            "I1207 23:59:37.848943 140583164200832 learning.py:507] global step 1471: loss = 1.8055 (0.875 sec/step)\n",
            "INFO:tensorflow:global step 1472: loss = 2.5265 (1.748 sec/step)\n",
            "I1207 23:59:39.974509 140583164200832 learning.py:507] global step 1472: loss = 2.5265 (1.748 sec/step)\n",
            "INFO:tensorflow:global step 1473: loss = 2.4662 (0.727 sec/step)\n",
            "I1207 23:59:40.890916 140583164200832 learning.py:507] global step 1473: loss = 2.4662 (0.727 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1207 23:59:42.077863 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 1474: loss = 2.9754 (2.734 sec/step)\n",
            "I1207 23:59:43.930741 140583164200832 learning.py:507] global step 1474: loss = 2.9754 (2.734 sec/step)\n",
            "INFO:tensorflow:global step 1475: loss = 2.1930 (0.950 sec/step)\n",
            "I1207 23:59:44.906964 140583164200832 learning.py:507] global step 1475: loss = 2.1930 (0.950 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1475.\n",
            "I1207 23:59:44.912842 140579459213056 supervisor.py:1050] Recording summary at step 1475.\n",
            "INFO:tensorflow:global step 1476: loss = 2.5857 (1.810 sec/step)\n",
            "I1207 23:59:47.450853 140583164200832 learning.py:507] global step 1476: loss = 2.5857 (1.810 sec/step)\n",
            "INFO:tensorflow:global step 1477: loss = 2.3940 (0.699 sec/step)\n",
            "I1207 23:59:48.459230 140583164200832 learning.py:507] global step 1477: loss = 2.3940 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1478: loss = 2.7488 (0.568 sec/step)\n",
            "I1207 23:59:49.561496 140583164200832 learning.py:507] global step 1478: loss = 2.7488 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1479: loss = 2.7162 (1.316 sec/step)\n",
            "I1207 23:59:51.042650 140583164200832 learning.py:507] global step 1479: loss = 2.7162 (1.316 sec/step)\n",
            "INFO:tensorflow:global step 1480: loss = 3.1236 (0.576 sec/step)\n",
            "I1207 23:59:51.620391 140583164200832 learning.py:507] global step 1480: loss = 3.1236 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1481: loss = 2.3053 (2.027 sec/step)\n",
            "I1207 23:59:53.649539 140583164200832 learning.py:507] global step 1481: loss = 2.3053 (2.027 sec/step)\n",
            "INFO:tensorflow:global step 1482: loss = 2.4281 (0.481 sec/step)\n",
            "I1207 23:59:54.132933 140583164200832 learning.py:507] global step 1482: loss = 2.4281 (0.481 sec/step)\n",
            "INFO:tensorflow:global step 1483: loss = 1.9549 (1.746 sec/step)\n",
            "I1207 23:59:55.881201 140583164200832 learning.py:507] global step 1483: loss = 1.9549 (1.746 sec/step)\n",
            "INFO:tensorflow:global step 1484: loss = 2.6705 (0.640 sec/step)\n",
            "I1207 23:59:56.523342 140583164200832 learning.py:507] global step 1484: loss = 2.6705 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1485: loss = 1.8553 (1.634 sec/step)\n",
            "I1207 23:59:58.193217 140583164200832 learning.py:507] global step 1485: loss = 1.8553 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 1486: loss = 2.2513 (0.775 sec/step)\n",
            "I1207 23:59:59.376925 140583164200832 learning.py:507] global step 1486: loss = 2.2513 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 1487: loss = 2.3573 (0.544 sec/step)\n",
            "I1208 00:00:00.216560 140583164200832 learning.py:507] global step 1487: loss = 2.3573 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1488: loss = 1.6731 (1.970 sec/step)\n",
            "I1208 00:00:02.189316 140583164200832 learning.py:507] global step 1488: loss = 1.6731 (1.970 sec/step)\n",
            "INFO:tensorflow:global step 1489: loss = 3.0783 (0.822 sec/step)\n",
            "I1208 00:00:03.050031 140583164200832 learning.py:507] global step 1489: loss = 3.0783 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 1490: loss = 2.2159 (1.571 sec/step)\n",
            "I1208 00:00:04.859875 140583164200832 learning.py:507] global step 1490: loss = 2.2159 (1.571 sec/step)\n",
            "INFO:tensorflow:global step 1491: loss = 2.7711 (1.254 sec/step)\n",
            "I1208 00:00:06.115879 140583164200832 learning.py:507] global step 1491: loss = 2.7711 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 1492: loss = 2.4037 (0.544 sec/step)\n",
            "I1208 00:00:06.662081 140583164200832 learning.py:507] global step 1492: loss = 2.4037 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1493: loss = 2.2336 (1.984 sec/step)\n",
            "I1208 00:00:08.647722 140583164200832 learning.py:507] global step 1493: loss = 2.2336 (1.984 sec/step)\n",
            "INFO:tensorflow:global step 1494: loss = 2.8033 (0.494 sec/step)\n",
            "I1208 00:00:09.144239 140583164200832 learning.py:507] global step 1494: loss = 2.8033 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 1495: loss = 2.3461 (1.708 sec/step)\n",
            "I1208 00:00:10.854189 140583164200832 learning.py:507] global step 1495: loss = 2.3461 (1.708 sec/step)\n",
            "INFO:tensorflow:global step 1496: loss = 2.1612 (0.611 sec/step)\n",
            "I1208 00:00:11.727219 140583164200832 learning.py:507] global step 1496: loss = 2.1612 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1497: loss = 2.4095 (1.796 sec/step)\n",
            "I1208 00:00:13.735608 140583164200832 learning.py:507] global step 1497: loss = 2.4095 (1.796 sec/step)\n",
            "INFO:tensorflow:global step 1498: loss = 2.6947 (0.621 sec/step)\n",
            "I1208 00:00:14.471296 140583164200832 learning.py:507] global step 1498: loss = 2.6947 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1499: loss = 1.9184 (1.031 sec/step)\n",
            "I1208 00:00:15.987295 140583164200832 learning.py:507] global step 1499: loss = 1.9184 (1.031 sec/step)\n",
            "INFO:tensorflow:global step 1500: loss = 2.6189 (1.401 sec/step)\n",
            "I1208 00:00:17.439168 140583164200832 learning.py:507] global step 1500: loss = 2.6189 (1.401 sec/step)\n",
            "INFO:tensorflow:global step 1501: loss = 2.3327 (0.611 sec/step)\n",
            "I1208 00:00:18.435034 140583164200832 learning.py:507] global step 1501: loss = 2.3327 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1502: loss = 2.0116 (1.455 sec/step)\n",
            "I1208 00:00:19.995376 140583164200832 learning.py:507] global step 1502: loss = 2.0116 (1.455 sec/step)\n",
            "INFO:tensorflow:global step 1503: loss = 2.6391 (0.555 sec/step)\n",
            "I1208 00:00:20.585743 140583164200832 learning.py:507] global step 1503: loss = 2.6391 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 1504: loss = 2.1316 (1.667 sec/step)\n",
            "I1208 00:00:22.426699 140583164200832 learning.py:507] global step 1504: loss = 2.1316 (1.667 sec/step)\n",
            "INFO:tensorflow:global step 1505: loss = 2.2294 (0.709 sec/step)\n",
            "I1208 00:00:23.194612 140583164200832 learning.py:507] global step 1505: loss = 2.2294 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 1506: loss = 2.5245 (1.916 sec/step)\n",
            "I1208 00:00:25.249652 140583164200832 learning.py:507] global step 1506: loss = 2.5245 (1.916 sec/step)\n",
            "INFO:tensorflow:global step 1507: loss = 3.0799 (0.587 sec/step)\n",
            "I1208 00:00:25.839145 140583164200832 learning.py:507] global step 1507: loss = 3.0799 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1508: loss = 2.4817 (0.717 sec/step)\n",
            "I1208 00:00:26.920939 140583164200832 learning.py:507] global step 1508: loss = 2.4817 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1509: loss = 2.2612 (2.318 sec/step)\n",
            "I1208 00:00:29.475734 140583164200832 learning.py:507] global step 1509: loss = 2.2612 (2.318 sec/step)\n",
            "INFO:tensorflow:global step 1510: loss = 2.3826 (0.663 sec/step)\n",
            "I1208 00:00:30.142089 140583164200832 learning.py:507] global step 1510: loss = 2.3826 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 1511: loss = 1.8920 (2.219 sec/step)\n",
            "I1208 00:00:32.362782 140583164200832 learning.py:507] global step 1511: loss = 1.8920 (2.219 sec/step)\n",
            "INFO:tensorflow:global step 1512: loss = 3.1490 (0.501 sec/step)\n",
            "I1208 00:00:32.865692 140583164200832 learning.py:507] global step 1512: loss = 3.1490 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 1513: loss = 2.3647 (2.342 sec/step)\n",
            "I1208 00:00:35.209682 140583164200832 learning.py:507] global step 1513: loss = 2.3647 (2.342 sec/step)\n",
            "INFO:tensorflow:global step 1514: loss = 2.2841 (0.585 sec/step)\n",
            "I1208 00:00:35.796503 140583164200832 learning.py:507] global step 1514: loss = 2.2841 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1515: loss = 2.8552 (1.921 sec/step)\n",
            "I1208 00:00:37.743377 140583164200832 learning.py:507] global step 1515: loss = 2.8552 (1.921 sec/step)\n",
            "INFO:tensorflow:global step 1516: loss = 2.2117 (0.733 sec/step)\n",
            "I1208 00:00:38.478781 140583164200832 learning.py:507] global step 1516: loss = 2.2117 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 1517: loss = 1.9442 (1.834 sec/step)\n",
            "I1208 00:00:40.314674 140583164200832 learning.py:507] global step 1517: loss = 1.9442 (1.834 sec/step)\n",
            "INFO:tensorflow:global step 1518: loss = 2.2310 (0.587 sec/step)\n",
            "I1208 00:00:40.903321 140583164200832 learning.py:507] global step 1518: loss = 2.2310 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1519: loss = 2.0403 (2.061 sec/step)\n",
            "I1208 00:00:42.965930 140583164200832 learning.py:507] global step 1519: loss = 2.0403 (2.061 sec/step)\n",
            "INFO:tensorflow:global step 1520: loss = 2.1111 (0.574 sec/step)\n",
            "I1208 00:00:43.542010 140583164200832 learning.py:507] global step 1520: loss = 2.1111 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 1521: loss = 2.3548 (1.977 sec/step)\n",
            "I1208 00:00:45.610880 140583164200832 learning.py:507] global step 1521: loss = 2.3548 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 1522: loss = 1.9039 (0.569 sec/step)\n",
            "I1208 00:00:46.182141 140583164200832 learning.py:507] global step 1522: loss = 1.9039 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1523: loss = 2.1700 (1.018 sec/step)\n",
            "I1208 00:00:47.371053 140583164200832 learning.py:507] global step 1523: loss = 2.1700 (1.018 sec/step)\n",
            "INFO:tensorflow:global step 1524: loss = 1.8466 (1.972 sec/step)\n",
            "I1208 00:00:49.554452 140583164200832 learning.py:507] global step 1524: loss = 1.8466 (1.972 sec/step)\n",
            "INFO:tensorflow:global step 1525: loss = 2.3314 (0.650 sec/step)\n",
            "I1208 00:00:50.206614 140583164200832 learning.py:507] global step 1525: loss = 2.3314 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1526: loss = 1.9037 (2.200 sec/step)\n",
            "I1208 00:00:52.408741 140583164200832 learning.py:507] global step 1526: loss = 1.9037 (2.200 sec/step)\n",
            "INFO:tensorflow:global step 1527: loss = 2.5578 (0.737 sec/step)\n",
            "I1208 00:00:53.462620 140583164200832 learning.py:507] global step 1527: loss = 2.5578 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1528: loss = 2.3883 (0.520 sec/step)\n",
            "I1208 00:00:54.073277 140583164200832 learning.py:507] global step 1528: loss = 2.3883 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 1529: loss = 2.2385 (1.933 sec/step)\n",
            "I1208 00:00:56.007998 140583164200832 learning.py:507] global step 1529: loss = 2.2385 (1.933 sec/step)\n",
            "INFO:tensorflow:global step 1530: loss = 1.9305 (0.616 sec/step)\n",
            "I1208 00:00:56.988364 140583164200832 learning.py:507] global step 1530: loss = 1.9305 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1531: loss = 1.9995 (2.177 sec/step)\n",
            "I1208 00:00:59.315743 140583164200832 learning.py:507] global step 1531: loss = 1.9995 (2.177 sec/step)\n",
            "INFO:tensorflow:global step 1532: loss = 2.0461 (0.659 sec/step)\n",
            "I1208 00:01:00.169508 140583164200832 learning.py:507] global step 1532: loss = 2.0461 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1533: loss = 2.6417 (1.581 sec/step)\n",
            "I1208 00:01:01.891693 140583164200832 learning.py:507] global step 1533: loss = 2.6417 (1.581 sec/step)\n",
            "INFO:tensorflow:global step 1534: loss = 2.0072 (0.782 sec/step)\n",
            "I1208 00:01:02.982748 140583164200832 learning.py:507] global step 1534: loss = 2.0072 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 1535: loss = 2.8163 (0.682 sec/step)\n",
            "I1208 00:01:04.037589 140583164200832 learning.py:507] global step 1535: loss = 2.8163 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1536: loss = 3.1925 (1.926 sec/step)\n",
            "I1208 00:01:06.101991 140583164200832 learning.py:507] global step 1536: loss = 3.1925 (1.926 sec/step)\n",
            "INFO:tensorflow:global step 1537: loss = 2.6078 (0.747 sec/step)\n",
            "I1208 00:01:06.851190 140583164200832 learning.py:507] global step 1537: loss = 2.6078 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1538: loss = 2.1486 (1.659 sec/step)\n",
            "I1208 00:01:08.698601 140583164200832 learning.py:507] global step 1538: loss = 2.1486 (1.659 sec/step)\n",
            "INFO:tensorflow:global step 1539: loss = 2.5071 (0.597 sec/step)\n",
            "I1208 00:01:09.614943 140583164200832 learning.py:507] global step 1539: loss = 2.5071 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1540: loss = 1.9702 (1.833 sec/step)\n",
            "I1208 00:01:11.680040 140583164200832 learning.py:507] global step 1540: loss = 1.9702 (1.833 sec/step)\n",
            "INFO:tensorflow:global step 1541: loss = 2.5946 (0.563 sec/step)\n",
            "I1208 00:01:12.244972 140583164200832 learning.py:507] global step 1541: loss = 2.5946 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 1542: loss = 2.2506 (1.908 sec/step)\n",
            "I1208 00:01:14.411493 140583164200832 learning.py:507] global step 1542: loss = 2.2506 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 1543: loss = 2.0778 (2.147 sec/step)\n",
            "I1208 00:01:16.560304 140583164200832 learning.py:507] global step 1543: loss = 2.0778 (2.147 sec/step)\n",
            "INFO:tensorflow:global step 1544: loss = 3.3548 (0.664 sec/step)\n",
            "I1208 00:01:17.547928 140583164200832 learning.py:507] global step 1544: loss = 3.3548 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1545: loss = 1.8611 (0.615 sec/step)\n",
            "I1208 00:01:18.482575 140583164200832 learning.py:507] global step 1545: loss = 1.8611 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1546: loss = 2.0904 (1.100 sec/step)\n",
            "I1208 00:01:19.725870 140583164200832 learning.py:507] global step 1546: loss = 2.0904 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 1547: loss = 2.0032 (2.284 sec/step)\n",
            "I1208 00:01:22.215910 140583164200832 learning.py:507] global step 1547: loss = 2.0032 (2.284 sec/step)\n",
            "INFO:tensorflow:global step 1548: loss = 2.4058 (0.671 sec/step)\n",
            "I1208 00:01:22.889063 140583164200832 learning.py:507] global step 1548: loss = 2.4058 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 1549: loss = 2.4267 (0.801 sec/step)\n",
            "I1208 00:01:23.842449 140583164200832 learning.py:507] global step 1549: loss = 2.4267 (0.801 sec/step)\n",
            "INFO:tensorflow:global step 1550: loss = 2.4512 (2.478 sec/step)\n",
            "I1208 00:01:26.383637 140583164200832 learning.py:507] global step 1550: loss = 2.4512 (2.478 sec/step)\n",
            "INFO:tensorflow:global step 1551: loss = 2.4016 (0.522 sec/step)\n",
            "I1208 00:01:26.907319 140583164200832 learning.py:507] global step 1551: loss = 2.4016 (0.522 sec/step)\n",
            "INFO:tensorflow:global step 1552: loss = 2.8472 (0.885 sec/step)\n",
            "I1208 00:01:28.314171 140583164200832 learning.py:507] global step 1552: loss = 2.8472 (0.885 sec/step)\n",
            "INFO:tensorflow:global step 1553: loss = 2.0677 (2.313 sec/step)\n",
            "I1208 00:01:30.850599 140583164200832 learning.py:507] global step 1553: loss = 2.0677 (2.313 sec/step)\n",
            "INFO:tensorflow:global step 1554: loss = 2.2213 (0.596 sec/step)\n",
            "I1208 00:01:31.449443 140583164200832 learning.py:507] global step 1554: loss = 2.2213 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1555: loss = 2.2690 (1.895 sec/step)\n",
            "I1208 00:01:33.347108 140583164200832 learning.py:507] global step 1555: loss = 2.2690 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 1556: loss = 2.6030 (0.582 sec/step)\n",
            "I1208 00:01:33.931203 140583164200832 learning.py:507] global step 1556: loss = 2.6030 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 1557: loss = 2.8604 (2.202 sec/step)\n",
            "I1208 00:01:36.134799 140583164200832 learning.py:507] global step 1557: loss = 2.8604 (2.202 sec/step)\n",
            "INFO:tensorflow:global step 1558: loss = 2.7386 (0.545 sec/step)\n",
            "I1208 00:01:36.681226 140583164200832 learning.py:507] global step 1558: loss = 2.7386 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 1559: loss = 1.8819 (0.612 sec/step)\n",
            "I1208 00:01:37.294889 140583164200832 learning.py:507] global step 1559: loss = 1.8819 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1560: loss = 2.8084 (3.240 sec/step)\n",
            "I1208 00:01:40.536870 140583164200832 learning.py:507] global step 1560: loss = 2.8084 (3.240 sec/step)\n",
            "INFO:tensorflow:global step 1561: loss = 2.8356 (0.722 sec/step)\n",
            "I1208 00:01:41.261338 140583164200832 learning.py:507] global step 1561: loss = 2.8356 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1562: loss = 2.2513 (0.940 sec/step)\n",
            "I1208 00:01:43.155018 140583164200832 learning.py:507] global step 1562: loss = 2.2513 (0.940 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1562.\n",
            "I1208 00:01:45.881891 140579459213056 supervisor.py:1050] Recording summary at step 1562.\n",
            "INFO:tensorflow:global step 1563: loss = 2.6625 (2.982 sec/step)\n",
            "I1208 00:01:46.156970 140583164200832 learning.py:507] global step 1563: loss = 2.6625 (2.982 sec/step)\n",
            "INFO:tensorflow:global step 1564: loss = 2.1137 (0.638 sec/step)\n",
            "I1208 00:01:47.158492 140583164200832 learning.py:507] global step 1564: loss = 2.1137 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1565: loss = 3.3569 (0.530 sec/step)\n",
            "I1208 00:01:47.853476 140583164200832 learning.py:507] global step 1565: loss = 3.3569 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 1566: loss = 2.1984 (2.519 sec/step)\n",
            "I1208 00:01:50.374598 140583164200832 learning.py:507] global step 1566: loss = 2.1984 (2.519 sec/step)\n",
            "INFO:tensorflow:global step 1567: loss = 2.9374 (0.651 sec/step)\n",
            "I1208 00:01:51.027297 140583164200832 learning.py:507] global step 1567: loss = 2.9374 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1568: loss = 2.5916 (1.954 sec/step)\n",
            "I1208 00:01:52.983046 140583164200832 learning.py:507] global step 1568: loss = 2.5916 (1.954 sec/step)\n",
            "INFO:tensorflow:global step 1569: loss = 2.1508 (0.636 sec/step)\n",
            "I1208 00:01:53.620624 140583164200832 learning.py:507] global step 1569: loss = 2.1508 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1570: loss = 2.4146 (0.946 sec/step)\n",
            "I1208 00:01:54.762032 140583164200832 learning.py:507] global step 1570: loss = 2.4146 (0.946 sec/step)\n",
            "INFO:tensorflow:global step 1571: loss = 2.9921 (2.232 sec/step)\n",
            "I1208 00:01:57.198728 140583164200832 learning.py:507] global step 1571: loss = 2.9921 (2.232 sec/step)\n",
            "INFO:tensorflow:global step 1572: loss = 2.2040 (0.598 sec/step)\n",
            "I1208 00:01:57.799354 140583164200832 learning.py:507] global step 1572: loss = 2.2040 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1573: loss = 2.1568 (1.942 sec/step)\n",
            "I1208 00:01:59.744144 140583164200832 learning.py:507] global step 1573: loss = 2.1568 (1.942 sec/step)\n",
            "INFO:tensorflow:global step 1574: loss = 1.7009 (0.765 sec/step)\n",
            "I1208 00:02:00.677935 140583164200832 learning.py:507] global step 1574: loss = 1.7009 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 1575: loss = 1.7140 (2.045 sec/step)\n",
            "I1208 00:02:02.725208 140583164200832 learning.py:507] global step 1575: loss = 1.7140 (2.045 sec/step)\n",
            "INFO:tensorflow:global step 1576: loss = 2.1366 (0.717 sec/step)\n",
            "I1208 00:02:03.696700 140583164200832 learning.py:507] global step 1576: loss = 2.1366 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1577: loss = 1.9161 (1.739 sec/step)\n",
            "I1208 00:02:05.487789 140583164200832 learning.py:507] global step 1577: loss = 1.9161 (1.739 sec/step)\n",
            "INFO:tensorflow:global step 1578: loss = 1.8405 (0.556 sec/step)\n",
            "I1208 00:02:06.045620 140583164200832 learning.py:507] global step 1578: loss = 1.8405 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 1579: loss = 2.3279 (0.830 sec/step)\n",
            "I1208 00:02:07.129544 140583164200832 learning.py:507] global step 1579: loss = 2.3279 (0.830 sec/step)\n",
            "INFO:tensorflow:global step 1580: loss = 2.8125 (1.827 sec/step)\n",
            "I1208 00:02:09.235485 140583164200832 learning.py:507] global step 1580: loss = 2.8125 (1.827 sec/step)\n",
            "INFO:tensorflow:global step 1581: loss = 1.9750 (0.577 sec/step)\n",
            "I1208 00:02:10.137575 140583164200832 learning.py:507] global step 1581: loss = 1.9750 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 1582: loss = 2.0277 (1.314 sec/step)\n",
            "I1208 00:02:11.646162 140583164200832 learning.py:507] global step 1582: loss = 2.0277 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1583: loss = 2.8958 (0.580 sec/step)\n",
            "I1208 00:02:12.455084 140583164200832 learning.py:507] global step 1583: loss = 2.8958 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1584: loss = 2.3533 (1.347 sec/step)\n",
            "I1208 00:02:14.044200 140583164200832 learning.py:507] global step 1584: loss = 2.3533 (1.347 sec/step)\n",
            "INFO:tensorflow:global step 1585: loss = 2.5566 (1.182 sec/step)\n",
            "I1208 00:02:15.227667 140583164200832 learning.py:507] global step 1585: loss = 2.5566 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1586: loss = 1.8806 (0.547 sec/step)\n",
            "I1208 00:02:16.140406 140583164200832 learning.py:507] global step 1586: loss = 1.8806 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 1587: loss = 2.0400 (1.164 sec/step)\n",
            "I1208 00:02:17.408418 140583164200832 learning.py:507] global step 1587: loss = 2.0400 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 1588: loss = 2.1958 (0.650 sec/step)\n",
            "I1208 00:02:18.420574 140583164200832 learning.py:507] global step 1588: loss = 2.1958 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1589: loss = 1.8576 (0.675 sec/step)\n",
            "I1208 00:02:19.188054 140583164200832 learning.py:507] global step 1589: loss = 1.8576 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 1590: loss = 2.1192 (1.337 sec/step)\n",
            "I1208 00:02:20.784307 140583164200832 learning.py:507] global step 1590: loss = 2.1192 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 1591: loss = 1.8820 (0.575 sec/step)\n",
            "I1208 00:02:21.596851 140583164200832 learning.py:507] global step 1591: loss = 1.8820 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 1592: loss = 2.6297 (1.314 sec/step)\n",
            "I1208 00:02:23.087850 140583164200832 learning.py:507] global step 1592: loss = 2.6297 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1593: loss = 2.2686 (0.520 sec/step)\n",
            "I1208 00:02:23.609676 140583164200832 learning.py:507] global step 1593: loss = 2.2686 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 1594: loss = 2.2258 (1.371 sec/step)\n",
            "I1208 00:02:25.180982 140583164200832 learning.py:507] global step 1594: loss = 2.2258 (1.371 sec/step)\n",
            "INFO:tensorflow:global step 1595: loss = 2.3596 (0.542 sec/step)\n",
            "I1208 00:02:25.777847 140583164200832 learning.py:507] global step 1595: loss = 2.3596 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 1596: loss = 2.0854 (1.688 sec/step)\n",
            "I1208 00:02:27.468088 140583164200832 learning.py:507] global step 1596: loss = 2.0854 (1.688 sec/step)\n",
            "INFO:tensorflow:global step 1597: loss = 2.2979 (0.693 sec/step)\n",
            "I1208 00:02:28.345520 140583164200832 learning.py:507] global step 1597: loss = 2.2979 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1598: loss = 2.7080 (1.300 sec/step)\n",
            "I1208 00:02:29.730091 140583164200832 learning.py:507] global step 1598: loss = 2.7080 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 1599: loss = 1.9314 (0.544 sec/step)\n",
            "I1208 00:02:30.576064 140583164200832 learning.py:507] global step 1599: loss = 1.9314 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 1600: loss = 1.9024 (1.306 sec/step)\n",
            "I1208 00:02:32.037205 140583164200832 learning.py:507] global step 1600: loss = 1.9024 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 1601: loss = 2.6890 (0.492 sec/step)\n",
            "I1208 00:02:32.916318 140583164200832 learning.py:507] global step 1601: loss = 2.6890 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 1602: loss = 2.4767 (0.660 sec/step)\n",
            "I1208 00:02:33.968713 140583164200832 learning.py:507] global step 1602: loss = 2.4767 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1603: loss = 2.4749 (1.223 sec/step)\n",
            "I1208 00:02:35.214739 140583164200832 learning.py:507] global step 1603: loss = 2.4749 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1604: loss = 2.0630 (0.713 sec/step)\n",
            "I1208 00:02:36.262166 140583164200832 learning.py:507] global step 1604: loss = 2.0630 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1605: loss = 1.7599 (0.626 sec/step)\n",
            "I1208 00:02:37.207717 140583164200832 learning.py:507] global step 1605: loss = 1.7599 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1606: loss = 2.1097 (1.902 sec/step)\n",
            "I1208 00:02:39.283551 140583164200832 learning.py:507] global step 1606: loss = 2.1097 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 1607: loss = 2.5062 (0.662 sec/step)\n",
            "I1208 00:02:39.947314 140583164200832 learning.py:507] global step 1607: loss = 2.5062 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1608: loss = 2.4948 (1.928 sec/step)\n",
            "I1208 00:02:41.877749 140583164200832 learning.py:507] global step 1608: loss = 2.4948 (1.928 sec/step)\n",
            "INFO:tensorflow:global step 1609: loss = 2.2159 (0.598 sec/step)\n",
            "I1208 00:02:42.876906 140583164200832 learning.py:507] global step 1609: loss = 2.2159 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1610: loss = 2.9551 (0.678 sec/step)\n",
            "I1208 00:02:43.954876 140583164200832 learning.py:507] global step 1610: loss = 2.9551 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1611: loss = 3.3676 (0.609 sec/step)\n",
            "I1208 00:02:44.713006 140583164200832 learning.py:507] global step 1611: loss = 3.3676 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1612: loss = 2.0242 (1.336 sec/step)\n",
            "I1208 00:02:46.108911 140583164200832 learning.py:507] global step 1612: loss = 2.0242 (1.336 sec/step)\n",
            "INFO:tensorflow:global step 1613: loss = 2.2094 (1.679 sec/step)\n",
            "I1208 00:02:48.112067 140583164200832 learning.py:507] global step 1613: loss = 2.2094 (1.679 sec/step)\n",
            "INFO:tensorflow:global step 1614: loss = 1.6816 (0.658 sec/step)\n",
            "I1208 00:02:48.912345 140583164200832 learning.py:507] global step 1614: loss = 1.6816 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1615: loss = 2.0991 (1.831 sec/step)\n",
            "I1208 00:02:50.974823 140583164200832 learning.py:507] global step 1615: loss = 2.0991 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 1616: loss = 2.1137 (0.580 sec/step)\n",
            "I1208 00:02:51.556776 140583164200832 learning.py:507] global step 1616: loss = 2.1137 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1617: loss = 2.1130 (0.618 sec/step)\n",
            "I1208 00:02:52.176938 140583164200832 learning.py:507] global step 1617: loss = 2.1130 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1618: loss = 2.2362 (2.926 sec/step)\n",
            "I1208 00:02:55.113740 140583164200832 learning.py:507] global step 1618: loss = 2.2362 (2.926 sec/step)\n",
            "INFO:tensorflow:global step 1619: loss = 2.2863 (1.438 sec/step)\n",
            "I1208 00:02:56.553426 140583164200832 learning.py:507] global step 1619: loss = 2.2863 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 1620: loss = 2.2027 (0.826 sec/step)\n",
            "I1208 00:02:57.393666 140583164200832 learning.py:507] global step 1620: loss = 2.2027 (0.826 sec/step)\n",
            "INFO:tensorflow:global step 1621: loss = 2.7175 (1.935 sec/step)\n",
            "I1208 00:02:59.330546 140583164200832 learning.py:507] global step 1621: loss = 2.7175 (1.935 sec/step)\n",
            "INFO:tensorflow:global step 1622: loss = 2.0591 (0.686 sec/step)\n",
            "I1208 00:03:00.189670 140583164200832 learning.py:507] global step 1622: loss = 2.0591 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1623: loss = 2.4615 (1.538 sec/step)\n",
            "I1208 00:03:01.920548 140583164200832 learning.py:507] global step 1623: loss = 2.4615 (1.538 sec/step)\n",
            "INFO:tensorflow:global step 1624: loss = 2.0436 (0.594 sec/step)\n",
            "I1208 00:03:02.515995 140583164200832 learning.py:507] global step 1624: loss = 2.0436 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1625: loss = 1.9644 (1.513 sec/step)\n",
            "I1208 00:03:04.197886 140583164200832 learning.py:507] global step 1625: loss = 1.9644 (1.513 sec/step)\n",
            "INFO:tensorflow:global step 1626: loss = 1.9444 (0.746 sec/step)\n",
            "I1208 00:03:05.301517 140583164200832 learning.py:507] global step 1626: loss = 1.9444 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1627: loss = 2.8396 (0.528 sec/step)\n",
            "I1208 00:03:05.872519 140583164200832 learning.py:507] global step 1627: loss = 2.8396 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 1628: loss = 2.0247 (1.987 sec/step)\n",
            "I1208 00:03:07.862172 140583164200832 learning.py:507] global step 1628: loss = 2.0247 (1.987 sec/step)\n",
            "INFO:tensorflow:global step 1629: loss = 2.8931 (1.216 sec/step)\n",
            "I1208 00:03:09.079780 140583164200832 learning.py:507] global step 1629: loss = 2.8931 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1630: loss = 2.4974 (0.591 sec/step)\n",
            "I1208 00:03:09.672949 140583164200832 learning.py:507] global step 1630: loss = 2.4974 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 1631: loss = 2.3638 (2.058 sec/step)\n",
            "I1208 00:03:11.733357 140583164200832 learning.py:507] global step 1631: loss = 2.3638 (2.058 sec/step)\n",
            "INFO:tensorflow:global step 1632: loss = 2.2833 (0.732 sec/step)\n",
            "I1208 00:03:12.595968 140583164200832 learning.py:507] global step 1632: loss = 2.2833 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1633: loss = 2.4282 (0.652 sec/step)\n",
            "I1208 00:03:13.514883 140583164200832 learning.py:507] global step 1633: loss = 2.4282 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1634: loss = 2.9312 (2.061 sec/step)\n",
            "I1208 00:03:15.578214 140583164200832 learning.py:507] global step 1634: loss = 2.9312 (2.061 sec/step)\n",
            "INFO:tensorflow:global step 1635: loss = 2.5403 (0.748 sec/step)\n",
            "I1208 00:03:16.486526 140583164200832 learning.py:507] global step 1635: loss = 2.5403 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 1636: loss = 1.7610 (0.648 sec/step)\n",
            "I1208 00:03:17.570579 140583164200832 learning.py:507] global step 1636: loss = 1.7610 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1637: loss = 2.8188 (0.681 sec/step)\n",
            "I1208 00:03:18.842602 140583164200832 learning.py:507] global step 1637: loss = 2.8188 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1638: loss = 2.0160 (0.694 sec/step)\n",
            "I1208 00:03:19.722650 140583164200832 learning.py:507] global step 1638: loss = 2.0160 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1639: loss = 2.4998 (1.796 sec/step)\n",
            "I1208 00:03:21.520129 140583164200832 learning.py:507] global step 1639: loss = 2.4998 (1.796 sec/step)\n",
            "INFO:tensorflow:global step 1640: loss = 2.0318 (0.592 sec/step)\n",
            "I1208 00:03:22.113587 140583164200832 learning.py:507] global step 1640: loss = 2.0318 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 1641: loss = 2.0189 (1.772 sec/step)\n",
            "I1208 00:03:23.886898 140583164200832 learning.py:507] global step 1641: loss = 2.0189 (1.772 sec/step)\n",
            "INFO:tensorflow:global step 1642: loss = 2.6644 (0.513 sec/step)\n",
            "I1208 00:03:24.401348 140583164200832 learning.py:507] global step 1642: loss = 2.6644 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1643: loss = 2.0969 (0.785 sec/step)\n",
            "I1208 00:03:25.465560 140583164200832 learning.py:507] global step 1643: loss = 2.0969 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 1644: loss = 2.6829 (1.514 sec/step)\n",
            "I1208 00:03:27.144843 140583164200832 learning.py:507] global step 1644: loss = 2.6829 (1.514 sec/step)\n",
            "INFO:tensorflow:global step 1645: loss = 2.6842 (0.557 sec/step)\n",
            "I1208 00:03:27.922280 140583164200832 learning.py:507] global step 1645: loss = 2.6842 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 1646: loss = 2.5001 (0.723 sec/step)\n",
            "I1208 00:03:29.015722 140583164200832 learning.py:507] global step 1646: loss = 2.5001 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 1647: loss = 1.8429 (1.521 sec/step)\n",
            "I1208 00:03:30.561295 140583164200832 learning.py:507] global step 1647: loss = 1.8429 (1.521 sec/step)\n",
            "INFO:tensorflow:global step 1648: loss = 2.3485 (1.162 sec/step)\n",
            "I1208 00:03:31.724467 140583164200832 learning.py:507] global step 1648: loss = 2.3485 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 1649: loss = 2.2343 (0.574 sec/step)\n",
            "I1208 00:03:32.300389 140583164200832 learning.py:507] global step 1649: loss = 2.2343 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 1650: loss = 2.5566 (2.167 sec/step)\n",
            "I1208 00:03:34.480418 140583164200832 learning.py:507] global step 1650: loss = 2.5566 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 1651: loss = 2.1508 (0.819 sec/step)\n",
            "I1208 00:03:35.556049 140583164200832 learning.py:507] global step 1651: loss = 2.1508 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1652: loss = 1.9339 (1.417 sec/step)\n",
            "I1208 00:03:37.040659 140583164200832 learning.py:507] global step 1652: loss = 1.9339 (1.417 sec/step)\n",
            "INFO:tensorflow:global step 1653: loss = 2.7615 (0.616 sec/step)\n",
            "I1208 00:03:37.970938 140583164200832 learning.py:507] global step 1653: loss = 2.7615 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1654: loss = 1.7643 (0.699 sec/step)\n",
            "I1208 00:03:38.954225 140583164200832 learning.py:507] global step 1654: loss = 1.7643 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1655: loss = 1.7345 (0.730 sec/step)\n",
            "I1208 00:03:39.982401 140583164200832 learning.py:507] global step 1655: loss = 1.7345 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 1656: loss = 2.0336 (1.454 sec/step)\n",
            "I1208 00:03:41.533228 140583164200832 learning.py:507] global step 1656: loss = 2.0336 (1.454 sec/step)\n",
            "INFO:tensorflow:global step 1657: loss = 2.8344 (0.572 sec/step)\n",
            "I1208 00:03:43.141470 140583164200832 learning.py:507] global step 1657: loss = 2.8344 (0.572 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1657.\n",
            "I1208 00:03:45.374245 140579459213056 supervisor.py:1050] Recording summary at step 1657.\n",
            "INFO:tensorflow:global step 1658: loss = 2.3995 (2.174 sec/step)\n",
            "I1208 00:03:45.570879 140583164200832 learning.py:507] global step 1658: loss = 2.3995 (2.174 sec/step)\n",
            "INFO:tensorflow:global step 1659: loss = 1.6841 (0.623 sec/step)\n",
            "I1208 00:03:46.517085 140583164200832 learning.py:507] global step 1659: loss = 1.6841 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1660: loss = 2.5928 (1.990 sec/step)\n",
            "I1208 00:03:48.606536 140583164200832 learning.py:507] global step 1660: loss = 2.5928 (1.990 sec/step)\n",
            "INFO:tensorflow:global step 1661: loss = 2.6288 (0.610 sec/step)\n",
            "I1208 00:03:49.493069 140583164200832 learning.py:507] global step 1661: loss = 2.6288 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1662: loss = 2.2317 (1.537 sec/step)\n",
            "I1208 00:03:51.130954 140583164200832 learning.py:507] global step 1662: loss = 2.2317 (1.537 sec/step)\n",
            "INFO:tensorflow:global step 1663: loss = 1.8966 (0.727 sec/step)\n",
            "I1208 00:03:52.028993 140583164200832 learning.py:507] global step 1663: loss = 1.8966 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1664: loss = 2.2080 (1.100 sec/step)\n",
            "I1208 00:03:53.333343 140583164200832 learning.py:507] global step 1664: loss = 2.2080 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 1665: loss = 2.3431 (0.598 sec/step)\n",
            "I1208 00:03:53.989842 140583164200832 learning.py:507] global step 1665: loss = 2.3431 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1666: loss = 1.9251 (0.606 sec/step)\n",
            "I1208 00:03:54.985210 140583164200832 learning.py:507] global step 1666: loss = 1.9251 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 1667: loss = 2.7120 (1.440 sec/step)\n",
            "I1208 00:03:56.687992 140583164200832 learning.py:507] global step 1667: loss = 2.7120 (1.440 sec/step)\n",
            "INFO:tensorflow:global step 1668: loss = 2.0047 (1.713 sec/step)\n",
            "I1208 00:03:58.472108 140583164200832 learning.py:507] global step 1668: loss = 2.0047 (1.713 sec/step)\n",
            "INFO:tensorflow:global step 1669: loss = 1.9078 (0.743 sec/step)\n",
            "I1208 00:03:59.341933 140583164200832 learning.py:507] global step 1669: loss = 1.9078 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1670: loss = 2.5333 (1.268 sec/step)\n",
            "I1208 00:04:00.706295 140583164200832 learning.py:507] global step 1670: loss = 2.5333 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 1671: loss = 2.0832 (0.552 sec/step)\n",
            "I1208 00:04:01.532611 140583164200832 learning.py:507] global step 1671: loss = 2.0832 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 1672: loss = 2.2459 (1.428 sec/step)\n",
            "I1208 00:04:03.181455 140583164200832 learning.py:507] global step 1672: loss = 2.2459 (1.428 sec/step)\n",
            "INFO:tensorflow:global step 1673: loss = 2.5628 (0.533 sec/step)\n",
            "I1208 00:04:03.716644 140583164200832 learning.py:507] global step 1673: loss = 2.5628 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 1674: loss = 2.0602 (1.568 sec/step)\n",
            "I1208 00:04:05.330918 140583164200832 learning.py:507] global step 1674: loss = 2.0602 (1.568 sec/step)\n",
            "INFO:tensorflow:global step 1675: loss = 2.0229 (0.558 sec/step)\n",
            "I1208 00:04:05.891399 140583164200832 learning.py:507] global step 1675: loss = 2.0229 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1676: loss = 2.8380 (0.550 sec/step)\n",
            "I1208 00:04:06.444606 140583164200832 learning.py:507] global step 1676: loss = 2.8380 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 1677: loss = 1.9624 (1.913 sec/step)\n",
            "I1208 00:04:08.889825 140583164200832 learning.py:507] global step 1677: loss = 1.9624 (1.913 sec/step)\n",
            "INFO:tensorflow:global step 1678: loss = 2.5228 (0.516 sec/step)\n",
            "I1208 00:04:09.699105 140583164200832 learning.py:507] global step 1678: loss = 2.5228 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 1679: loss = 2.1406 (1.487 sec/step)\n",
            "I1208 00:04:11.188015 140583164200832 learning.py:507] global step 1679: loss = 2.1406 (1.487 sec/step)\n",
            "INFO:tensorflow:global step 1680: loss = 2.8473 (0.691 sec/step)\n",
            "I1208 00:04:12.032930 140583164200832 learning.py:507] global step 1680: loss = 2.8473 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1681: loss = 1.8363 (0.671 sec/step)\n",
            "I1208 00:04:13.140383 140583164200832 learning.py:507] global step 1681: loss = 1.8363 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 1682: loss = 1.9493 (0.652 sec/step)\n",
            "I1208 00:04:14.198150 140583164200832 learning.py:507] global step 1682: loss = 1.9493 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1683: loss = 2.5405 (0.548 sec/step)\n",
            "I1208 00:04:14.987078 140583164200832 learning.py:507] global step 1683: loss = 2.5405 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 1684: loss = 2.0510 (1.558 sec/step)\n",
            "I1208 00:04:16.546538 140583164200832 learning.py:507] global step 1684: loss = 2.0510 (1.558 sec/step)\n",
            "INFO:tensorflow:global step 1685: loss = 2.0948 (0.571 sec/step)\n",
            "I1208 00:04:17.306024 140583164200832 learning.py:507] global step 1685: loss = 2.0948 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1686: loss = 2.0274 (1.273 sec/step)\n",
            "I1208 00:04:18.877310 140583164200832 learning.py:507] global step 1686: loss = 2.0274 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 1687: loss = 2.0258 (0.651 sec/step)\n",
            "I1208 00:04:19.548594 140583164200832 learning.py:507] global step 1687: loss = 2.0258 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1688: loss = 2.0687 (1.207 sec/step)\n",
            "I1208 00:04:21.043401 140583164200832 learning.py:507] global step 1688: loss = 2.0687 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1689: loss = 2.5870 (0.628 sec/step)\n",
            "I1208 00:04:21.719538 140583164200832 learning.py:507] global step 1689: loss = 2.5870 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1690: loss = 2.2126 (0.684 sec/step)\n",
            "I1208 00:04:22.880471 140583164200832 learning.py:507] global step 1690: loss = 2.2126 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1691: loss = 1.8488 (0.641 sec/step)\n",
            "I1208 00:04:23.947718 140583164200832 learning.py:507] global step 1691: loss = 1.8488 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1692: loss = 2.1387 (0.565 sec/step)\n",
            "I1208 00:04:24.634370 140583164200832 learning.py:507] global step 1692: loss = 2.1387 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 1693: loss = 2.1166 (1.422 sec/step)\n",
            "I1208 00:04:26.242275 140583164200832 learning.py:507] global step 1693: loss = 2.1166 (1.422 sec/step)\n",
            "INFO:tensorflow:global step 1694: loss = 2.1514 (0.670 sec/step)\n",
            "I1208 00:04:27.132044 140583164200832 learning.py:507] global step 1694: loss = 2.1514 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1695: loss = 2.0159 (1.501 sec/step)\n",
            "I1208 00:04:28.690922 140583164200832 learning.py:507] global step 1695: loss = 2.0159 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 1696: loss = 2.2710 (1.109 sec/step)\n",
            "I1208 00:04:29.801756 140583164200832 learning.py:507] global step 1696: loss = 2.2710 (1.109 sec/step)\n",
            "INFO:tensorflow:global step 1697: loss = 2.5805 (0.502 sec/step)\n",
            "I1208 00:04:30.305903 140583164200832 learning.py:507] global step 1697: loss = 2.5805 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 1698: loss = 2.0215 (0.745 sec/step)\n",
            "I1208 00:04:31.142567 140583164200832 learning.py:507] global step 1698: loss = 2.0215 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 1699: loss = 2.0030 (1.964 sec/step)\n",
            "I1208 00:04:33.151498 140583164200832 learning.py:507] global step 1699: loss = 2.0030 (1.964 sec/step)\n",
            "INFO:tensorflow:global step 1700: loss = 2.1104 (0.578 sec/step)\n",
            "I1208 00:04:33.975709 140583164200832 learning.py:507] global step 1700: loss = 2.1104 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 1701: loss = 2.0820 (1.083 sec/step)\n",
            "I1208 00:04:35.282408 140583164200832 learning.py:507] global step 1701: loss = 2.0820 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 1702: loss = 1.9716 (1.059 sec/step)\n",
            "I1208 00:04:36.343110 140583164200832 learning.py:507] global step 1702: loss = 1.9716 (1.059 sec/step)\n",
            "INFO:tensorflow:global step 1703: loss = 2.1834 (0.631 sec/step)\n",
            "I1208 00:04:37.329381 140583164200832 learning.py:507] global step 1703: loss = 2.1834 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1704: loss = 2.3737 (1.093 sec/step)\n",
            "I1208 00:04:38.602168 140583164200832 learning.py:507] global step 1704: loss = 2.3737 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 1705: loss = 2.3406 (0.700 sec/step)\n",
            "I1208 00:04:39.659429 140583164200832 learning.py:507] global step 1705: loss = 2.3406 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1706: loss = 2.2268 (0.830 sec/step)\n",
            "I1208 00:04:40.647064 140583164200832 learning.py:507] global step 1706: loss = 2.2268 (0.830 sec/step)\n",
            "INFO:tensorflow:global step 1707: loss = 1.9214 (1.386 sec/step)\n",
            "I1208 00:04:42.181713 140583164200832 learning.py:507] global step 1707: loss = 1.9214 (1.386 sec/step)\n",
            "INFO:tensorflow:global step 1708: loss = 1.9925 (0.607 sec/step)\n",
            "I1208 00:04:42.847990 140583164200832 learning.py:507] global step 1708: loss = 1.9925 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 1709: loss = 2.0512 (0.586 sec/step)\n",
            "I1208 00:04:43.901269 140583164200832 learning.py:507] global step 1709: loss = 2.0512 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 1710: loss = 2.0901 (1.747 sec/step)\n",
            "I1208 00:04:45.650330 140583164200832 learning.py:507] global step 1710: loss = 2.0901 (1.747 sec/step)\n",
            "INFO:tensorflow:global step 1711: loss = 2.1070 (0.559 sec/step)\n",
            "I1208 00:04:46.210982 140583164200832 learning.py:507] global step 1711: loss = 2.1070 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1712: loss = 1.8889 (0.948 sec/step)\n",
            "I1208 00:04:47.463642 140583164200832 learning.py:507] global step 1712: loss = 1.8889 (0.948 sec/step)\n",
            "INFO:tensorflow:global step 1713: loss = 2.3384 (1.427 sec/step)\n",
            "I1208 00:04:49.116301 140583164200832 learning.py:507] global step 1713: loss = 2.3384 (1.427 sec/step)\n",
            "INFO:tensorflow:global step 1714: loss = 2.2390 (0.723 sec/step)\n",
            "I1208 00:04:49.892524 140583164200832 learning.py:507] global step 1714: loss = 2.2390 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 1715: loss = 2.2313 (0.538 sec/step)\n",
            "I1208 00:04:51.044473 140583164200832 learning.py:507] global step 1715: loss = 2.2313 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 1716: loss = 1.8778 (0.843 sec/step)\n",
            "I1208 00:04:52.175366 140583164200832 learning.py:507] global step 1716: loss = 1.8778 (0.843 sec/step)\n",
            "INFO:tensorflow:global step 1717: loss = 1.9678 (0.590 sec/step)\n",
            "I1208 00:04:52.994191 140583164200832 learning.py:507] global step 1717: loss = 1.9678 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 1718: loss = 1.7844 (0.581 sec/step)\n",
            "I1208 00:04:53.813502 140583164200832 learning.py:507] global step 1718: loss = 1.7844 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 1719: loss = 2.0013 (1.612 sec/step)\n",
            "I1208 00:04:55.427598 140583164200832 learning.py:507] global step 1719: loss = 2.0013 (1.612 sec/step)\n",
            "INFO:tensorflow:global step 1720: loss = 1.7456 (1.085 sec/step)\n",
            "I1208 00:04:56.514025 140583164200832 learning.py:507] global step 1720: loss = 1.7456 (1.085 sec/step)\n",
            "INFO:tensorflow:global step 1721: loss = 2.1905 (0.572 sec/step)\n",
            "I1208 00:04:57.464355 140583164200832 learning.py:507] global step 1721: loss = 2.1905 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 1722: loss = 1.9590 (1.288 sec/step)\n",
            "I1208 00:04:58.929312 140583164200832 learning.py:507] global step 1722: loss = 1.9590 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 1723: loss = 2.2807 (0.657 sec/step)\n",
            "I1208 00:05:00.088608 140583164200832 learning.py:507] global step 1723: loss = 2.2807 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1724: loss = 2.5163 (0.724 sec/step)\n",
            "I1208 00:05:01.063498 140583164200832 learning.py:507] global step 1724: loss = 2.5163 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1725: loss = 2.6935 (0.673 sec/step)\n",
            "I1208 00:05:01.932486 140583164200832 learning.py:507] global step 1725: loss = 2.6935 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1726: loss = 2.1536 (1.648 sec/step)\n",
            "I1208 00:05:03.913590 140583164200832 learning.py:507] global step 1726: loss = 2.1536 (1.648 sec/step)\n",
            "INFO:tensorflow:global step 1727: loss = 2.1189 (1.074 sec/step)\n",
            "I1208 00:05:04.989491 140583164200832 learning.py:507] global step 1727: loss = 2.1189 (1.074 sec/step)\n",
            "INFO:tensorflow:global step 1728: loss = 1.6238 (0.551 sec/step)\n",
            "I1208 00:05:05.542383 140583164200832 learning.py:507] global step 1728: loss = 1.6238 (0.551 sec/step)\n",
            "INFO:tensorflow:global step 1729: loss = 2.1404 (1.604 sec/step)\n",
            "I1208 00:05:07.148011 140583164200832 learning.py:507] global step 1729: loss = 2.1404 (1.604 sec/step)\n",
            "INFO:tensorflow:global step 1730: loss = 2.9029 (0.639 sec/step)\n",
            "I1208 00:05:07.884945 140583164200832 learning.py:507] global step 1730: loss = 2.9029 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1731: loss = 2.3575 (1.330 sec/step)\n",
            "I1208 00:05:09.403023 140583164200832 learning.py:507] global step 1731: loss = 2.3575 (1.330 sec/step)\n",
            "INFO:tensorflow:global step 1732: loss = 1.8407 (0.476 sec/step)\n",
            "I1208 00:05:09.881130 140583164200832 learning.py:507] global step 1732: loss = 1.8407 (0.476 sec/step)\n",
            "INFO:tensorflow:global step 1733: loss = 2.0508 (1.634 sec/step)\n",
            "I1208 00:05:11.516731 140583164200832 learning.py:507] global step 1733: loss = 2.0508 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 1734: loss = 1.9836 (0.637 sec/step)\n",
            "I1208 00:05:12.221789 140583164200832 learning.py:507] global step 1734: loss = 1.9836 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1735: loss = 2.4704 (1.502 sec/step)\n",
            "I1208 00:05:13.920035 140583164200832 learning.py:507] global step 1735: loss = 2.4704 (1.502 sec/step)\n",
            "INFO:tensorflow:global step 1736: loss = 2.0146 (1.137 sec/step)\n",
            "I1208 00:05:15.058584 140583164200832 learning.py:507] global step 1736: loss = 2.0146 (1.137 sec/step)\n",
            "INFO:tensorflow:global step 1737: loss = 1.9129 (1.130 sec/step)\n",
            "I1208 00:05:16.189581 140583164200832 learning.py:507] global step 1737: loss = 1.9129 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 1738: loss = 2.3610 (0.553 sec/step)\n",
            "I1208 00:05:16.744574 140583164200832 learning.py:507] global step 1738: loss = 2.3610 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 1739: loss = 2.1189 (0.733 sec/step)\n",
            "I1208 00:05:17.766924 140583164200832 learning.py:507] global step 1739: loss = 2.1189 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 1740: loss = 1.9993 (1.760 sec/step)\n",
            "I1208 00:05:19.631962 140583164200832 learning.py:507] global step 1740: loss = 1.9993 (1.760 sec/step)\n",
            "INFO:tensorflow:global step 1741: loss = 2.9391 (0.491 sec/step)\n",
            "I1208 00:05:20.124505 140583164200832 learning.py:507] global step 1741: loss = 2.9391 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 1742: loss = 2.1465 (0.757 sec/step)\n",
            "I1208 00:05:21.159883 140583164200832 learning.py:507] global step 1742: loss = 2.1465 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 1743: loss = 1.9888 (2.615 sec/step)\n",
            "I1208 00:05:24.090740 140583164200832 learning.py:507] global step 1743: loss = 1.9888 (2.615 sec/step)\n",
            "INFO:tensorflow:global step 1744: loss = 2.0950 (0.815 sec/step)\n",
            "I1208 00:05:24.913437 140583164200832 learning.py:507] global step 1744: loss = 2.0950 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1745: loss = 2.2828 (2.299 sec/step)\n",
            "I1208 00:05:27.233278 140583164200832 learning.py:507] global step 1745: loss = 2.2828 (2.299 sec/step)\n",
            "INFO:tensorflow:global step 1746: loss = 2.3021 (0.690 sec/step)\n",
            "I1208 00:05:27.925853 140583164200832 learning.py:507] global step 1746: loss = 2.3021 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1747: loss = 1.6392 (2.212 sec/step)\n",
            "I1208 00:05:30.139889 140583164200832 learning.py:507] global step 1747: loss = 1.6392 (2.212 sec/step)\n",
            "INFO:tensorflow:global step 1748: loss = 2.9738 (0.727 sec/step)\n",
            "I1208 00:05:30.869318 140583164200832 learning.py:507] global step 1748: loss = 2.9738 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1749: loss = 1.9542 (1.866 sec/step)\n",
            "I1208 00:05:32.737251 140583164200832 learning.py:507] global step 1749: loss = 1.9542 (1.866 sec/step)\n",
            "INFO:tensorflow:global step 1750: loss = 1.9378 (0.625 sec/step)\n",
            "I1208 00:05:33.364723 140583164200832 learning.py:507] global step 1750: loss = 1.9378 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1751: loss = 2.1806 (1.826 sec/step)\n",
            "I1208 00:05:35.322241 140583164200832 learning.py:507] global step 1751: loss = 2.1806 (1.826 sec/step)\n",
            "INFO:tensorflow:global step 1752: loss = 2.1757 (0.576 sec/step)\n",
            "I1208 00:05:35.907398 140583164200832 learning.py:507] global step 1752: loss = 2.1757 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1753: loss = 2.2886 (1.098 sec/step)\n",
            "I1208 00:05:37.351686 140583164200832 learning.py:507] global step 1753: loss = 2.2886 (1.098 sec/step)\n",
            "INFO:tensorflow:global step 1754: loss = 2.2307 (1.674 sec/step)\n",
            "I1208 00:05:39.040478 140583164200832 learning.py:507] global step 1754: loss = 2.2307 (1.674 sec/step)\n",
            "INFO:tensorflow:global step 1755: loss = 2.6542 (0.676 sec/step)\n",
            "I1208 00:05:39.718678 140583164200832 learning.py:507] global step 1755: loss = 2.6542 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1756: loss = 2.0746 (1.879 sec/step)\n",
            "I1208 00:05:41.599904 140583164200832 learning.py:507] global step 1756: loss = 2.0746 (1.879 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1756.\n",
            "I1208 00:05:44.667225 140579459213056 supervisor.py:1050] Recording summary at step 1756.\n",
            "INFO:tensorflow:global step 1757: loss = 2.1004 (3.229 sec/step)\n",
            "I1208 00:05:44.830484 140583164200832 learning.py:507] global step 1757: loss = 2.1004 (3.229 sec/step)\n",
            "INFO:tensorflow:global step 1758: loss = 2.7140 (0.632 sec/step)\n",
            "I1208 00:05:45.702965 140583164200832 learning.py:507] global step 1758: loss = 2.7140 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1759: loss = 2.0964 (2.045 sec/step)\n",
            "I1208 00:05:47.937934 140583164200832 learning.py:507] global step 1759: loss = 2.0964 (2.045 sec/step)\n",
            "INFO:tensorflow:global step 1760: loss = 2.1612 (0.583 sec/step)\n",
            "I1208 00:05:48.523459 140583164200832 learning.py:507] global step 1760: loss = 2.1612 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1761: loss = 1.9885 (1.619 sec/step)\n",
            "I1208 00:05:50.331327 140583164200832 learning.py:507] global step 1761: loss = 1.9885 (1.619 sec/step)\n",
            "INFO:tensorflow:global step 1762: loss = 2.5589 (0.672 sec/step)\n",
            "I1208 00:05:51.144896 140583164200832 learning.py:507] global step 1762: loss = 2.5589 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1763: loss = 2.3933 (0.645 sec/step)\n",
            "I1208 00:05:52.212053 140583164200832 learning.py:507] global step 1763: loss = 2.3933 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1764: loss = 2.9835 (2.557 sec/step)\n",
            "I1208 00:05:54.973900 140583164200832 learning.py:507] global step 1764: loss = 2.9835 (2.557 sec/step)\n",
            "INFO:tensorflow:global step 1765: loss = 2.4436 (0.593 sec/step)\n",
            "I1208 00:05:55.821384 140583164200832 learning.py:507] global step 1765: loss = 2.4436 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 1766: loss = 2.5993 (0.735 sec/step)\n",
            "I1208 00:05:57.177762 140583164200832 learning.py:507] global step 1766: loss = 2.5993 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 1767: loss = 2.0525 (1.875 sec/step)\n",
            "I1208 00:05:59.236480 140583164200832 learning.py:507] global step 1767: loss = 2.0525 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 1768: loss = 2.1281 (0.534 sec/step)\n",
            "I1208 00:05:59.772396 140583164200832 learning.py:507] global step 1768: loss = 2.1281 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 1769: loss = 1.9373 (0.976 sec/step)\n",
            "I1208 00:06:01.054224 140583164200832 learning.py:507] global step 1769: loss = 1.9373 (0.976 sec/step)\n",
            "INFO:tensorflow:global step 1770: loss = 2.5102 (1.743 sec/step)\n",
            "I1208 00:06:02.963704 140583164200832 learning.py:507] global step 1770: loss = 2.5102 (1.743 sec/step)\n",
            "INFO:tensorflow:global step 1771: loss = 2.2813 (0.593 sec/step)\n",
            "I1208 00:06:03.836058 140583164200832 learning.py:507] global step 1771: loss = 2.2813 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 1772: loss = 1.9370 (2.005 sec/step)\n",
            "I1208 00:06:05.874748 140583164200832 learning.py:507] global step 1772: loss = 1.9370 (2.005 sec/step)\n",
            "INFO:tensorflow:global step 1773: loss = 2.0229 (0.662 sec/step)\n",
            "I1208 00:06:06.790236 140583164200832 learning.py:507] global step 1773: loss = 2.0229 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1774: loss = 1.9687 (2.278 sec/step)\n",
            "I1208 00:06:09.251087 140583164200832 learning.py:507] global step 1774: loss = 1.9687 (2.278 sec/step)\n",
            "INFO:tensorflow:global step 1775: loss = 2.7880 (0.811 sec/step)\n",
            "I1208 00:06:10.224931 140583164200832 learning.py:507] global step 1775: loss = 2.7880 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 1776: loss = 2.3546 (0.812 sec/step)\n",
            "I1208 00:06:11.415574 140583164200832 learning.py:507] global step 1776: loss = 2.3546 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 1777: loss = 2.0979 (1.301 sec/step)\n",
            "I1208 00:06:13.021099 140583164200832 learning.py:507] global step 1777: loss = 2.0979 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 1778: loss = 2.3255 (0.662 sec/step)\n",
            "I1208 00:06:13.932092 140583164200832 learning.py:507] global step 1778: loss = 2.3255 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1779: loss = 2.1591 (0.737 sec/step)\n",
            "I1208 00:06:15.101435 140583164200832 learning.py:507] global step 1779: loss = 2.1591 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1780: loss = 2.0825 (1.935 sec/step)\n",
            "I1208 00:06:17.185239 140583164200832 learning.py:507] global step 1780: loss = 2.0825 (1.935 sec/step)\n",
            "INFO:tensorflow:global step 1781: loss = 1.7601 (0.708 sec/step)\n",
            "I1208 00:06:17.969556 140583164200832 learning.py:507] global step 1781: loss = 1.7601 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 1782: loss = 2.7309 (1.381 sec/step)\n",
            "I1208 00:06:19.544271 140583164200832 learning.py:507] global step 1782: loss = 2.7309 (1.381 sec/step)\n",
            "INFO:tensorflow:global step 1783: loss = 2.1938 (1.017 sec/step)\n",
            "I1208 00:06:20.562776 140583164200832 learning.py:507] global step 1783: loss = 2.1938 (1.017 sec/step)\n",
            "INFO:tensorflow:global step 1784: loss = 2.3427 (0.552 sec/step)\n",
            "I1208 00:06:21.433843 140583164200832 learning.py:507] global step 1784: loss = 2.3427 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 1785: loss = 2.8612 (0.549 sec/step)\n",
            "I1208 00:06:22.171690 140583164200832 learning.py:507] global step 1785: loss = 2.8612 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 1786: loss = 2.1111 (1.761 sec/step)\n",
            "I1208 00:06:23.934272 140583164200832 learning.py:507] global step 1786: loss = 2.1111 (1.761 sec/step)\n",
            "INFO:tensorflow:global step 1787: loss = 2.2317 (0.568 sec/step)\n",
            "I1208 00:06:24.504437 140583164200832 learning.py:507] global step 1787: loss = 2.2317 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1788: loss = 1.9857 (1.999 sec/step)\n",
            "I1208 00:06:26.691848 140583164200832 learning.py:507] global step 1788: loss = 1.9857 (1.999 sec/step)\n",
            "INFO:tensorflow:global step 1789: loss = 1.8346 (0.574 sec/step)\n",
            "I1208 00:06:27.275236 140583164200832 learning.py:507] global step 1789: loss = 1.8346 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 1790: loss = 2.6400 (1.075 sec/step)\n",
            "I1208 00:06:28.508351 140583164200832 learning.py:507] global step 1790: loss = 2.6400 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 1791: loss = 2.7227 (1.558 sec/step)\n",
            "I1208 00:06:30.323296 140583164200832 learning.py:507] global step 1791: loss = 2.7227 (1.558 sec/step)\n",
            "INFO:tensorflow:global step 1792: loss = 2.1589 (0.529 sec/step)\n",
            "I1208 00:06:30.853449 140583164200832 learning.py:507] global step 1792: loss = 2.1589 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 1793: loss = 2.0436 (0.554 sec/step)\n",
            "I1208 00:06:31.409066 140583164200832 learning.py:507] global step 1793: loss = 2.0436 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 1794: loss = 2.7812 (2.804 sec/step)\n",
            "I1208 00:06:34.215016 140583164200832 learning.py:507] global step 1794: loss = 2.7812 (2.804 sec/step)\n",
            "INFO:tensorflow:global step 1795: loss = 2.5455 (0.482 sec/step)\n",
            "I1208 00:06:34.699146 140583164200832 learning.py:507] global step 1795: loss = 2.5455 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 1796: loss = 2.2367 (1.712 sec/step)\n",
            "I1208 00:06:36.412901 140583164200832 learning.py:507] global step 1796: loss = 2.2367 (1.712 sec/step)\n",
            "INFO:tensorflow:global step 1797: loss = 2.2978 (0.589 sec/step)\n",
            "I1208 00:06:37.004122 140583164200832 learning.py:507] global step 1797: loss = 2.2978 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 1798: loss = 2.3804 (1.099 sec/step)\n",
            "I1208 00:06:38.274201 140583164200832 learning.py:507] global step 1798: loss = 2.3804 (1.099 sec/step)\n",
            "INFO:tensorflow:global step 1799: loss = 2.1935 (0.791 sec/step)\n",
            "I1208 00:06:39.427712 140583164200832 learning.py:507] global step 1799: loss = 2.1935 (0.791 sec/step)\n",
            "INFO:tensorflow:global step 1800: loss = 1.6951 (1.677 sec/step)\n",
            "I1208 00:06:41.430247 140583164200832 learning.py:507] global step 1800: loss = 1.6951 (1.677 sec/step)\n",
            "INFO:tensorflow:global step 1801: loss = 1.8996 (0.654 sec/step)\n",
            "I1208 00:06:42.360575 140583164200832 learning.py:507] global step 1801: loss = 1.8996 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1802: loss = 2.0901 (2.424 sec/step)\n",
            "I1208 00:06:44.915555 140583164200832 learning.py:507] global step 1802: loss = 2.0901 (2.424 sec/step)\n",
            "INFO:tensorflow:global step 1803: loss = 1.7215 (0.631 sec/step)\n",
            "I1208 00:06:45.928759 140583164200832 learning.py:507] global step 1803: loss = 1.7215 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1804: loss = 2.5319 (0.619 sec/step)\n",
            "I1208 00:06:46.782346 140583164200832 learning.py:507] global step 1804: loss = 2.5319 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1805: loss = 1.8379 (2.014 sec/step)\n",
            "I1208 00:06:48.891245 140583164200832 learning.py:507] global step 1805: loss = 1.8379 (2.014 sec/step)\n",
            "INFO:tensorflow:global step 1806: loss = 2.2250 (0.657 sec/step)\n",
            "I1208 00:06:49.751655 140583164200832 learning.py:507] global step 1806: loss = 2.2250 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 1807: loss = 1.8373 (0.746 sec/step)\n",
            "I1208 00:06:50.795417 140583164200832 learning.py:507] global step 1807: loss = 1.8373 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1808: loss = 2.0244 (1.522 sec/step)\n",
            "I1208 00:06:52.416978 140583164200832 learning.py:507] global step 1808: loss = 2.0244 (1.522 sec/step)\n",
            "INFO:tensorflow:global step 1809: loss = 2.4845 (0.738 sec/step)\n",
            "I1208 00:06:53.411725 140583164200832 learning.py:507] global step 1809: loss = 2.4845 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 1810: loss = 2.3915 (0.519 sec/step)\n",
            "I1208 00:06:53.964222 140583164200832 learning.py:507] global step 1810: loss = 2.3915 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 1811: loss = 2.2204 (0.758 sec/step)\n",
            "I1208 00:06:55.084887 140583164200832 learning.py:507] global step 1811: loss = 2.2204 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1812: loss = 2.6183 (2.149 sec/step)\n",
            "I1208 00:06:57.419540 140583164200832 learning.py:507] global step 1812: loss = 2.6183 (2.149 sec/step)\n",
            "INFO:tensorflow:global step 1813: loss = 2.4639 (0.621 sec/step)\n",
            "I1208 00:06:58.128966 140583164200832 learning.py:507] global step 1813: loss = 2.4639 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1814: loss = 2.1040 (1.709 sec/step)\n",
            "I1208 00:07:00.197172 140583164200832 learning.py:507] global step 1814: loss = 2.1040 (1.709 sec/step)\n",
            "INFO:tensorflow:global step 1815: loss = 2.7460 (0.530 sec/step)\n",
            "I1208 00:07:00.728788 140583164200832 learning.py:507] global step 1815: loss = 2.7460 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 1816: loss = 2.1861 (0.647 sec/step)\n",
            "I1208 00:07:01.524067 140583164200832 learning.py:507] global step 1816: loss = 2.1861 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1817: loss = 2.3244 (2.419 sec/step)\n",
            "I1208 00:07:03.964003 140583164200832 learning.py:507] global step 1817: loss = 2.3244 (2.419 sec/step)\n",
            "INFO:tensorflow:global step 1818: loss = 2.6124 (0.550 sec/step)\n",
            "I1208 00:07:04.681500 140583164200832 learning.py:507] global step 1818: loss = 2.6124 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 1819: loss = 2.1254 (1.839 sec/step)\n",
            "I1208 00:07:06.809473 140583164200832 learning.py:507] global step 1819: loss = 2.1254 (1.839 sec/step)\n",
            "INFO:tensorflow:global step 1820: loss = 2.0277 (0.512 sec/step)\n",
            "I1208 00:07:07.323023 140583164200832 learning.py:507] global step 1820: loss = 2.0277 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 1821: loss = 2.4358 (1.929 sec/step)\n",
            "I1208 00:07:09.253665 140583164200832 learning.py:507] global step 1821: loss = 2.4358 (1.929 sec/step)\n",
            "INFO:tensorflow:global step 1822: loss = 2.1160 (0.608 sec/step)\n",
            "I1208 00:07:10.123358 140583164200832 learning.py:507] global step 1822: loss = 2.1160 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 1823: loss = 2.2027 (1.812 sec/step)\n",
            "I1208 00:07:12.144924 140583164200832 learning.py:507] global step 1823: loss = 2.2027 (1.812 sec/step)\n",
            "INFO:tensorflow:global step 1824: loss = 2.6684 (0.677 sec/step)\n",
            "I1208 00:07:13.075868 140583164200832 learning.py:507] global step 1824: loss = 2.6684 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1825: loss = 2.1409 (0.686 sec/step)\n",
            "I1208 00:07:14.110084 140583164200832 learning.py:507] global step 1825: loss = 2.1409 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1826: loss = 1.9856 (1.852 sec/step)\n",
            "I1208 00:07:16.279753 140583164200832 learning.py:507] global step 1826: loss = 1.9856 (1.852 sec/step)\n",
            "INFO:tensorflow:global step 1827: loss = 2.2457 (0.665 sec/step)\n",
            "I1208 00:07:17.377111 140583164200832 learning.py:507] global step 1827: loss = 2.2457 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1828: loss = 1.9124 (1.511 sec/step)\n",
            "I1208 00:07:18.913644 140583164200832 learning.py:507] global step 1828: loss = 1.9124 (1.511 sec/step)\n",
            "INFO:tensorflow:global step 1829: loss = 2.2077 (0.500 sec/step)\n",
            "I1208 00:07:19.415481 140583164200832 learning.py:507] global step 1829: loss = 2.2077 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 1830: loss = 2.5773 (0.563 sec/step)\n",
            "I1208 00:07:19.980723 140583164200832 learning.py:507] global step 1830: loss = 2.5773 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 1831: loss = 2.3209 (1.788 sec/step)\n",
            "I1208 00:07:21.931298 140583164200832 learning.py:507] global step 1831: loss = 2.3209 (1.788 sec/step)\n",
            "INFO:tensorflow:global step 1832: loss = 2.4199 (2.238 sec/step)\n",
            "I1208 00:07:24.465283 140583164200832 learning.py:507] global step 1832: loss = 2.4199 (2.238 sec/step)\n",
            "INFO:tensorflow:global step 1833: loss = 1.9920 (0.602 sec/step)\n",
            "I1208 00:07:25.375022 140583164200832 learning.py:507] global step 1833: loss = 1.9920 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1834: loss = 1.9728 (0.694 sec/step)\n",
            "I1208 00:07:26.453040 140583164200832 learning.py:507] global step 1834: loss = 1.9728 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1835: loss = 2.4171 (0.624 sec/step)\n",
            "I1208 00:07:27.458496 140583164200832 learning.py:507] global step 1835: loss = 2.4171 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1836: loss = 2.6439 (1.217 sec/step)\n",
            "I1208 00:07:28.866384 140583164200832 learning.py:507] global step 1836: loss = 2.6439 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1837: loss = 2.6776 (0.743 sec/step)\n",
            "I1208 00:07:29.745519 140583164200832 learning.py:507] global step 1837: loss = 2.6776 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1838: loss = 2.5806 (0.749 sec/step)\n",
            "I1208 00:07:30.752134 140583164200832 learning.py:507] global step 1838: loss = 2.5806 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1839: loss = 2.5376 (1.820 sec/step)\n",
            "I1208 00:07:32.789109 140583164200832 learning.py:507] global step 1839: loss = 2.5376 (1.820 sec/step)\n",
            "INFO:tensorflow:global step 1840: loss = 2.0948 (0.693 sec/step)\n",
            "I1208 00:07:33.484768 140583164200832 learning.py:507] global step 1840: loss = 2.0948 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1841: loss = 2.0513 (1.910 sec/step)\n",
            "I1208 00:07:35.396880 140583164200832 learning.py:507] global step 1841: loss = 2.0513 (1.910 sec/step)\n",
            "INFO:tensorflow:global step 1842: loss = 2.0872 (0.649 sec/step)\n",
            "I1208 00:07:36.287391 140583164200832 learning.py:507] global step 1842: loss = 2.0872 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1843: loss = 2.1015 (0.628 sec/step)\n",
            "I1208 00:07:37.063233 140583164200832 learning.py:507] global step 1843: loss = 2.1015 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1844: loss = 2.3730 (1.969 sec/step)\n",
            "I1208 00:07:39.034314 140583164200832 learning.py:507] global step 1844: loss = 2.3730 (1.969 sec/step)\n",
            "INFO:tensorflow:global step 1845: loss = 1.8710 (0.585 sec/step)\n",
            "I1208 00:07:39.945335 140583164200832 learning.py:507] global step 1845: loss = 1.8710 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1846: loss = 2.3666 (0.667 sec/step)\n",
            "I1208 00:07:40.732471 140583164200832 learning.py:507] global step 1846: loss = 2.3666 (0.667 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1847.\n",
            "I1208 00:07:44.251983 140579459213056 supervisor.py:1050] Recording summary at step 1847.\n",
            "INFO:tensorflow:global step 1847: loss = 2.4447 (3.525 sec/step)\n",
            "I1208 00:07:44.260979 140583164200832 learning.py:507] global step 1847: loss = 2.4447 (3.525 sec/step)\n",
            "INFO:tensorflow:global step 1848: loss = 1.8596 (0.566 sec/step)\n",
            "I1208 00:07:45.316214 140583164200832 learning.py:507] global step 1848: loss = 1.8596 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 1849: loss = 2.1135 (0.704 sec/step)\n",
            "I1208 00:07:46.351618 140583164200832 learning.py:507] global step 1849: loss = 2.1135 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1850: loss = 2.6211 (0.626 sec/step)\n",
            "I1208 00:07:47.235207 140583164200832 learning.py:507] global step 1850: loss = 2.6211 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1851: loss = 2.0636 (1.841 sec/step)\n",
            "I1208 00:07:49.077866 140583164200832 learning.py:507] global step 1851: loss = 2.0636 (1.841 sec/step)\n",
            "INFO:tensorflow:global step 1852: loss = 2.5604 (0.758 sec/step)\n",
            "I1208 00:07:50.089014 140583164200832 learning.py:507] global step 1852: loss = 2.5604 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1853: loss = 2.3449 (0.679 sec/step)\n",
            "I1208 00:07:51.025984 140583164200832 learning.py:507] global step 1853: loss = 2.3449 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1854: loss = 2.4023 (0.513 sec/step)\n",
            "I1208 00:07:51.667610 140583164200832 learning.py:507] global step 1854: loss = 2.4023 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 1855: loss = 2.5999 (0.999 sec/step)\n",
            "I1208 00:07:52.710180 140583164200832 learning.py:507] global step 1855: loss = 2.5999 (0.999 sec/step)\n",
            "INFO:tensorflow:global step 1856: loss = 1.9064 (1.802 sec/step)\n",
            "I1208 00:07:55.056380 140583164200832 learning.py:507] global step 1856: loss = 1.9064 (1.802 sec/step)\n",
            "INFO:tensorflow:global step 1857: loss = 2.0295 (1.138 sec/step)\n",
            "I1208 00:07:56.195685 140583164200832 learning.py:507] global step 1857: loss = 2.0295 (1.138 sec/step)\n",
            "INFO:tensorflow:global step 1858: loss = 2.2275 (0.630 sec/step)\n",
            "I1208 00:07:57.050924 140583164200832 learning.py:507] global step 1858: loss = 2.2275 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1859: loss = 2.1433 (1.814 sec/step)\n",
            "I1208 00:07:59.155854 140583164200832 learning.py:507] global step 1859: loss = 2.1433 (1.814 sec/step)\n",
            "INFO:tensorflow:global step 1860: loss = 1.9467 (0.669 sec/step)\n",
            "I1208 00:08:00.089551 140583164200832 learning.py:507] global step 1860: loss = 1.9467 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1861: loss = 2.0868 (0.855 sec/step)\n",
            "I1208 00:08:01.380585 140583164200832 learning.py:507] global step 1861: loss = 2.0868 (0.855 sec/step)\n",
            "INFO:tensorflow:global step 1862: loss = 1.7172 (0.776 sec/step)\n",
            "I1208 00:08:02.377065 140583164200832 learning.py:507] global step 1862: loss = 1.7172 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1863: loss = 2.7927 (0.670 sec/step)\n",
            "I1208 00:08:03.334366 140583164200832 learning.py:507] global step 1863: loss = 2.7927 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1864: loss = 2.3167 (1.738 sec/step)\n",
            "I1208 00:08:05.133986 140583164200832 learning.py:507] global step 1864: loss = 2.3167 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 1865: loss = 2.1132 (0.570 sec/step)\n",
            "I1208 00:08:05.910643 140583164200832 learning.py:507] global step 1865: loss = 2.1132 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 1866: loss = 2.1740 (1.561 sec/step)\n",
            "I1208 00:08:07.832609 140583164200832 learning.py:507] global step 1866: loss = 2.1740 (1.561 sec/step)\n",
            "INFO:tensorflow:global step 1867: loss = 2.5323 (0.690 sec/step)\n",
            "I1208 00:08:08.777977 140583164200832 learning.py:507] global step 1867: loss = 2.5323 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1868: loss = 2.5559 (1.854 sec/step)\n",
            "I1208 00:08:10.699070 140583164200832 learning.py:507] global step 1868: loss = 2.5559 (1.854 sec/step)\n",
            "INFO:tensorflow:global step 1869: loss = 2.4479 (0.685 sec/step)\n",
            "I1208 00:08:11.386802 140583164200832 learning.py:507] global step 1869: loss = 2.4479 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 1870: loss = 2.5185 (1.613 sec/step)\n",
            "I1208 00:08:13.125285 140583164200832 learning.py:507] global step 1870: loss = 2.5185 (1.613 sec/step)\n",
            "INFO:tensorflow:global step 1871: loss = 1.8704 (0.697 sec/step)\n",
            "I1208 00:08:14.106032 140583164200832 learning.py:507] global step 1871: loss = 1.8704 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1872: loss = 2.0604 (1.512 sec/step)\n",
            "I1208 00:08:15.690824 140583164200832 learning.py:507] global step 1872: loss = 2.0604 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 1873: loss = 2.0076 (0.646 sec/step)\n",
            "I1208 00:08:16.376001 140583164200832 learning.py:507] global step 1873: loss = 2.0076 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1874: loss = 2.0420 (1.684 sec/step)\n",
            "I1208 00:08:18.259732 140583164200832 learning.py:507] global step 1874: loss = 2.0420 (1.684 sec/step)\n",
            "INFO:tensorflow:global step 1875: loss = 1.9774 (0.664 sec/step)\n",
            "I1208 00:08:19.156007 140583164200832 learning.py:507] global step 1875: loss = 1.9774 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 1876: loss = 2.2409 (1.281 sec/step)\n",
            "I1208 00:08:20.560234 140583164200832 learning.py:507] global step 1876: loss = 2.2409 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 1877: loss = 1.9724 (0.619 sec/step)\n",
            "I1208 00:08:21.563588 140583164200832 learning.py:507] global step 1877: loss = 1.9724 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1878: loss = 1.8662 (1.956 sec/step)\n",
            "I1208 00:08:23.546568 140583164200832 learning.py:507] global step 1878: loss = 1.8662 (1.956 sec/step)\n",
            "INFO:tensorflow:global step 1879: loss = 2.2634 (0.561 sec/step)\n",
            "I1208 00:08:24.108775 140583164200832 learning.py:507] global step 1879: loss = 2.2634 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 1880: loss = 2.0364 (1.285 sec/step)\n",
            "I1208 00:08:25.551043 140583164200832 learning.py:507] global step 1880: loss = 2.0364 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 1881: loss = 1.7671 (1.942 sec/step)\n",
            "I1208 00:08:27.792510 140583164200832 learning.py:507] global step 1881: loss = 1.7671 (1.942 sec/step)\n",
            "INFO:tensorflow:global step 1882: loss = 2.2003 (0.532 sec/step)\n",
            "I1208 00:08:28.328583 140583164200832 learning.py:507] global step 1882: loss = 2.2003 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 1883: loss = 1.4444 (0.560 sec/step)\n",
            "I1208 00:08:28.890318 140583164200832 learning.py:507] global step 1883: loss = 1.4444 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 1884: loss = 2.4889 (1.369 sec/step)\n",
            "I1208 00:08:30.412734 140583164200832 learning.py:507] global step 1884: loss = 2.4889 (1.369 sec/step)\n",
            "INFO:tensorflow:global step 1885: loss = 2.2831 (2.149 sec/step)\n",
            "I1208 00:08:32.927803 140583164200832 learning.py:507] global step 1885: loss = 2.2831 (2.149 sec/step)\n",
            "INFO:tensorflow:global step 1886: loss = 2.1350 (0.514 sec/step)\n",
            "I1208 00:08:33.443997 140583164200832 learning.py:507] global step 1886: loss = 2.1350 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 1887: loss = 2.0047 (1.328 sec/step)\n",
            "I1208 00:08:35.046040 140583164200832 learning.py:507] global step 1887: loss = 2.0047 (1.328 sec/step)\n",
            "INFO:tensorflow:global step 1888: loss = 2.4720 (0.713 sec/step)\n",
            "I1208 00:08:36.517544 140583164200832 learning.py:507] global step 1888: loss = 2.4720 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1889: loss = 2.0854 (1.807 sec/step)\n",
            "I1208 00:08:38.556854 140583164200832 learning.py:507] global step 1889: loss = 2.0854 (1.807 sec/step)\n",
            "INFO:tensorflow:global step 1890: loss = 1.9673 (0.562 sec/step)\n",
            "I1208 00:08:39.121109 140583164200832 learning.py:507] global step 1890: loss = 1.9673 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 1891: loss = 1.9205 (0.650 sec/step)\n",
            "I1208 00:08:39.773382 140583164200832 learning.py:507] global step 1891: loss = 1.9205 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1892: loss = 2.1862 (2.243 sec/step)\n",
            "I1208 00:08:42.273433 140583164200832 learning.py:507] global step 1892: loss = 2.1862 (2.243 sec/step)\n",
            "INFO:tensorflow:global step 1893: loss = 2.3491 (1.434 sec/step)\n",
            "I1208 00:08:43.754248 140583164200832 learning.py:507] global step 1893: loss = 2.3491 (1.434 sec/step)\n",
            "INFO:tensorflow:global step 1894: loss = 1.7103 (1.133 sec/step)\n",
            "I1208 00:08:44.888638 140583164200832 learning.py:507] global step 1894: loss = 1.7103 (1.133 sec/step)\n",
            "INFO:tensorflow:global step 1895: loss = 2.0218 (0.679 sec/step)\n",
            "I1208 00:08:45.887882 140583164200832 learning.py:507] global step 1895: loss = 2.0218 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1896: loss = 2.2093 (0.771 sec/step)\n",
            "I1208 00:08:46.884290 140583164200832 learning.py:507] global step 1896: loss = 2.2093 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 1897: loss = 2.5029 (2.005 sec/step)\n",
            "I1208 00:08:48.914864 140583164200832 learning.py:507] global step 1897: loss = 2.5029 (2.005 sec/step)\n",
            "INFO:tensorflow:global step 1898: loss = 1.9697 (0.726 sec/step)\n",
            "I1208 00:08:49.831737 140583164200832 learning.py:507] global step 1898: loss = 1.9697 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 1899: loss = 1.7313 (0.588 sec/step)\n",
            "I1208 00:08:50.600649 140583164200832 learning.py:507] global step 1899: loss = 1.7313 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 1900: loss = 2.1825 (0.848 sec/step)\n",
            "I1208 00:08:51.744987 140583164200832 learning.py:507] global step 1900: loss = 2.1825 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 1901: loss = 2.5403 (1.714 sec/step)\n",
            "I1208 00:08:53.551984 140583164200832 learning.py:507] global step 1901: loss = 2.5403 (1.714 sec/step)\n",
            "INFO:tensorflow:global step 1902: loss = 2.1525 (0.784 sec/step)\n",
            "I1208 00:08:54.468061 140583164200832 learning.py:507] global step 1902: loss = 2.1525 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 1903: loss = 2.3255 (0.573 sec/step)\n",
            "I1208 00:08:55.246095 140583164200832 learning.py:507] global step 1903: loss = 2.3255 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 1904: loss = 2.6739 (1.017 sec/step)\n",
            "I1208 00:08:56.552433 140583164200832 learning.py:507] global step 1904: loss = 2.6739 (1.017 sec/step)\n",
            "INFO:tensorflow:global step 1905: loss = 2.1724 (1.829 sec/step)\n",
            "I1208 00:08:58.383560 140583164200832 learning.py:507] global step 1905: loss = 2.1724 (1.829 sec/step)\n",
            "INFO:tensorflow:global step 1906: loss = 2.1482 (0.616 sec/step)\n",
            "I1208 00:08:59.220079 140583164200832 learning.py:507] global step 1906: loss = 2.1482 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 1907: loss = 1.7315 (0.841 sec/step)\n",
            "I1208 00:09:00.484745 140583164200832 learning.py:507] global step 1907: loss = 1.7315 (0.841 sec/step)\n",
            "INFO:tensorflow:global step 1908: loss = 1.6662 (0.633 sec/step)\n",
            "I1208 00:09:01.254801 140583164200832 learning.py:507] global step 1908: loss = 1.6662 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1909: loss = 2.1385 (1.771 sec/step)\n",
            "I1208 00:09:03.027938 140583164200832 learning.py:507] global step 1909: loss = 2.1385 (1.771 sec/step)\n",
            "INFO:tensorflow:global step 1910: loss = 1.8984 (0.848 sec/step)\n",
            "I1208 00:09:04.155007 140583164200832 learning.py:507] global step 1910: loss = 1.8984 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 1911: loss = 2.3133 (0.647 sec/step)\n",
            "I1208 00:09:05.259619 140583164200832 learning.py:507] global step 1911: loss = 2.3133 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1912: loss = 2.1791 (0.524 sec/step)\n",
            "I1208 00:09:05.891521 140583164200832 learning.py:507] global step 1912: loss = 2.1791 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 1913: loss = 2.7087 (1.645 sec/step)\n",
            "I1208 00:09:07.588790 140583164200832 learning.py:507] global step 1913: loss = 2.7087 (1.645 sec/step)\n",
            "INFO:tensorflow:global step 1914: loss = 2.6568 (0.654 sec/step)\n",
            "I1208 00:09:08.245512 140583164200832 learning.py:507] global step 1914: loss = 2.6568 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1915: loss = 2.0676 (1.888 sec/step)\n",
            "I1208 00:09:10.136180 140583164200832 learning.py:507] global step 1915: loss = 2.0676 (1.888 sec/step)\n",
            "INFO:tensorflow:global step 1916: loss = 2.5581 (0.600 sec/step)\n",
            "I1208 00:09:10.738841 140583164200832 learning.py:507] global step 1916: loss = 2.5581 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 1917: loss = 1.9936 (1.811 sec/step)\n",
            "I1208 00:09:12.552228 140583164200832 learning.py:507] global step 1917: loss = 1.9936 (1.811 sec/step)\n",
            "INFO:tensorflow:global step 1918: loss = 1.9253 (0.577 sec/step)\n",
            "I1208 00:09:13.253551 140583164200832 learning.py:507] global step 1918: loss = 1.9253 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 1919: loss = 1.5478 (1.501 sec/step)\n",
            "I1208 00:09:14.924824 140583164200832 learning.py:507] global step 1919: loss = 1.5478 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 1920: loss = 2.0699 (0.568 sec/step)\n",
            "I1208 00:09:15.817603 140583164200832 learning.py:507] global step 1920: loss = 2.0699 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1921: loss = 2.3899 (0.636 sec/step)\n",
            "I1208 00:09:17.081966 140583164200832 learning.py:507] global step 1921: loss = 2.3899 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 1922: loss = 2.7333 (1.352 sec/step)\n",
            "I1208 00:09:18.574922 140583164200832 learning.py:507] global step 1922: loss = 2.7333 (1.352 sec/step)\n",
            "INFO:tensorflow:global step 1923: loss = 1.8178 (0.703 sec/step)\n",
            "I1208 00:09:19.345037 140583164200832 learning.py:507] global step 1923: loss = 1.8178 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 1924: loss = 2.1264 (1.668 sec/step)\n",
            "I1208 00:09:21.091373 140583164200832 learning.py:507] global step 1924: loss = 2.1264 (1.668 sec/step)\n",
            "INFO:tensorflow:global step 1925: loss = 2.1433 (0.630 sec/step)\n",
            "I1208 00:09:21.966776 140583164200832 learning.py:507] global step 1925: loss = 2.1433 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1926: loss = 2.3433 (1.545 sec/step)\n",
            "I1208 00:09:23.531451 140583164200832 learning.py:507] global step 1926: loss = 2.3433 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1927: loss = 2.6575 (0.760 sec/step)\n",
            "I1208 00:09:24.515375 140583164200832 learning.py:507] global step 1927: loss = 2.6575 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 1928: loss = 1.3590 (0.569 sec/step)\n",
            "I1208 00:09:25.586176 140583164200832 learning.py:507] global step 1928: loss = 1.3590 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 1929: loss = 2.0536 (1.474 sec/step)\n",
            "I1208 00:09:27.105246 140583164200832 learning.py:507] global step 1929: loss = 2.0536 (1.474 sec/step)\n",
            "INFO:tensorflow:global step 1930: loss = 2.1887 (0.623 sec/step)\n",
            "I1208 00:09:27.914898 140583164200832 learning.py:507] global step 1930: loss = 2.1887 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1931: loss = 2.6970 (1.318 sec/step)\n",
            "I1208 00:09:29.559774 140583164200832 learning.py:507] global step 1931: loss = 2.6970 (1.318 sec/step)\n",
            "INFO:tensorflow:global step 1932: loss = 2.0294 (0.603 sec/step)\n",
            "I1208 00:09:30.557529 140583164200832 learning.py:507] global step 1932: loss = 2.0294 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1933: loss = 1.8681 (0.615 sec/step)\n",
            "I1208 00:09:31.395260 140583164200832 learning.py:507] global step 1933: loss = 1.8681 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 1934: loss = 2.2501 (1.545 sec/step)\n",
            "I1208 00:09:33.283696 140583164200832 learning.py:507] global step 1934: loss = 2.2501 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 1935: loss = 1.8370 (0.571 sec/step)\n",
            "I1208 00:09:34.131111 140583164200832 learning.py:507] global step 1935: loss = 1.8370 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 1936: loss = 2.3253 (1.570 sec/step)\n",
            "I1208 00:09:35.702742 140583164200832 learning.py:507] global step 1936: loss = 2.3253 (1.570 sec/step)\n",
            "INFO:tensorflow:global step 1937: loss = 2.7170 (0.602 sec/step)\n",
            "I1208 00:09:36.635548 140583164200832 learning.py:507] global step 1937: loss = 2.7170 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 1938: loss = 1.6849 (1.332 sec/step)\n",
            "I1208 00:09:38.182667 140583164200832 learning.py:507] global step 1938: loss = 1.6849 (1.332 sec/step)\n",
            "INFO:tensorflow:global step 1939: loss = 2.2371 (0.585 sec/step)\n",
            "I1208 00:09:38.942834 140583164200832 learning.py:507] global step 1939: loss = 2.2371 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1940: loss = 2.0821 (1.623 sec/step)\n",
            "I1208 00:09:40.700345 140583164200832 learning.py:507] global step 1940: loss = 2.0821 (1.623 sec/step)\n",
            "INFO:tensorflow:global step 1941: loss = 2.2440 (0.559 sec/step)\n",
            "I1208 00:09:41.260626 140583164200832 learning.py:507] global step 1941: loss = 2.2440 (0.559 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:09:42.076586 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 1942: loss = 2.1114 (0.767 sec/step)\n",
            "I1208 00:09:42.123924 140583164200832 learning.py:507] global step 1942: loss = 2.1114 (0.767 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1942.\n",
            "I1208 00:09:44.588735 140579459213056 supervisor.py:1050] Recording summary at step 1942.\n",
            "INFO:tensorflow:global step 1943: loss = 2.3547 (2.297 sec/step)\n",
            "I1208 00:09:45.674367 140583164200832 learning.py:507] global step 1943: loss = 2.3547 (2.297 sec/step)\n",
            "INFO:tensorflow:global step 1944: loss = 1.9907 (2.110 sec/step)\n",
            "I1208 00:09:48.036032 140583164200832 learning.py:507] global step 1944: loss = 1.9907 (2.110 sec/step)\n",
            "INFO:tensorflow:global step 1945: loss = 2.0492 (0.691 sec/step)\n",
            "I1208 00:09:48.909105 140583164200832 learning.py:507] global step 1945: loss = 2.0492 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1946: loss = 1.8234 (1.558 sec/step)\n",
            "I1208 00:09:50.651163 140583164200832 learning.py:507] global step 1946: loss = 1.8234 (1.558 sec/step)\n",
            "INFO:tensorflow:global step 1947: loss = 1.7224 (0.727 sec/step)\n",
            "I1208 00:09:51.622093 140583164200832 learning.py:507] global step 1947: loss = 1.7224 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1948: loss = 2.2717 (0.725 sec/step)\n",
            "I1208 00:09:52.800121 140583164200832 learning.py:507] global step 1948: loss = 2.2717 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1949: loss = 2.1923 (0.815 sec/step)\n",
            "I1208 00:09:53.846894 140583164200832 learning.py:507] global step 1949: loss = 2.1923 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1950: loss = 2.6948 (1.489 sec/step)\n",
            "I1208 00:09:55.457659 140583164200832 learning.py:507] global step 1950: loss = 2.6948 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 1951: loss = 2.2695 (0.521 sec/step)\n",
            "I1208 00:09:55.981316 140583164200832 learning.py:507] global step 1951: loss = 2.2695 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 1952: loss = 2.1303 (1.924 sec/step)\n",
            "I1208 00:09:57.907648 140583164200832 learning.py:507] global step 1952: loss = 2.1303 (1.924 sec/step)\n",
            "INFO:tensorflow:global step 1953: loss = 1.7902 (0.750 sec/step)\n",
            "I1208 00:09:58.830774 140583164200832 learning.py:507] global step 1953: loss = 1.7902 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 1954: loss = 2.1360 (1.565 sec/step)\n",
            "I1208 00:10:00.397675 140583164200832 learning.py:507] global step 1954: loss = 2.1360 (1.565 sec/step)\n",
            "INFO:tensorflow:global step 1955: loss = 2.2468 (0.611 sec/step)\n",
            "I1208 00:10:01.010512 140583164200832 learning.py:507] global step 1955: loss = 2.2468 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1956: loss = 2.7720 (1.220 sec/step)\n",
            "I1208 00:10:02.494096 140583164200832 learning.py:507] global step 1956: loss = 2.7720 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1957: loss = 2.0194 (0.583 sec/step)\n",
            "I1208 00:10:03.747272 140583164200832 learning.py:507] global step 1957: loss = 2.0194 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1958: loss = 2.0185 (1.357 sec/step)\n",
            "I1208 00:10:05.142314 140583164200832 learning.py:507] global step 1958: loss = 2.0185 (1.357 sec/step)\n",
            "INFO:tensorflow:global step 1959: loss = 2.4630 (0.734 sec/step)\n",
            "I1208 00:10:06.101529 140583164200832 learning.py:507] global step 1959: loss = 2.4630 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1960: loss = 1.6658 (1.199 sec/step)\n",
            "I1208 00:10:07.507476 140583164200832 learning.py:507] global step 1960: loss = 1.6658 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1961: loss = 1.9978 (0.481 sec/step)\n",
            "I1208 00:10:07.990596 140583164200832 learning.py:507] global step 1961: loss = 1.9978 (0.481 sec/step)\n",
            "INFO:tensorflow:global step 1962: loss = 2.0368 (1.533 sec/step)\n",
            "I1208 00:10:09.531589 140583164200832 learning.py:507] global step 1962: loss = 2.0368 (1.533 sec/step)\n",
            "INFO:tensorflow:global step 1963: loss = 2.2342 (1.914 sec/step)\n",
            "I1208 00:10:11.446874 140583164200832 learning.py:507] global step 1963: loss = 2.2342 (1.914 sec/step)\n",
            "INFO:tensorflow:global step 1964: loss = 2.9239 (0.518 sec/step)\n",
            "I1208 00:10:11.966770 140583164200832 learning.py:507] global step 1964: loss = 2.9239 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 1965: loss = 2.2954 (1.977 sec/step)\n",
            "I1208 00:10:13.945030 140583164200832 learning.py:507] global step 1965: loss = 2.2954 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 1966: loss = 2.1983 (0.624 sec/step)\n",
            "I1208 00:10:14.570223 140583164200832 learning.py:507] global step 1966: loss = 2.1983 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 1967: loss = 1.6795 (1.907 sec/step)\n",
            "I1208 00:10:16.566247 140583164200832 learning.py:507] global step 1967: loss = 1.6795 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 1968: loss = 1.5096 (0.597 sec/step)\n",
            "I1208 00:10:17.169217 140583164200832 learning.py:507] global step 1968: loss = 1.5096 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 1969: loss = 2.2204 (1.905 sec/step)\n",
            "I1208 00:10:19.076976 140583164200832 learning.py:507] global step 1969: loss = 2.2204 (1.905 sec/step)\n",
            "INFO:tensorflow:global step 1970: loss = 1.8372 (0.644 sec/step)\n",
            "I1208 00:10:19.999142 140583164200832 learning.py:507] global step 1970: loss = 1.8372 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1971: loss = 1.9513 (1.576 sec/step)\n",
            "I1208 00:10:21.692963 140583164200832 learning.py:507] global step 1971: loss = 1.9513 (1.576 sec/step)\n",
            "INFO:tensorflow:global step 1972: loss = 2.0898 (0.608 sec/step)\n",
            "I1208 00:10:22.538222 140583164200832 learning.py:507] global step 1972: loss = 2.0898 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 1973: loss = 2.1311 (1.515 sec/step)\n",
            "I1208 00:10:24.245590 140583164200832 learning.py:507] global step 1973: loss = 2.1311 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 1974: loss = 1.6902 (0.630 sec/step)\n",
            "I1208 00:10:25.152390 140583164200832 learning.py:507] global step 1974: loss = 1.6902 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1975: loss = 2.4942 (1.381 sec/step)\n",
            "I1208 00:10:26.651662 140583164200832 learning.py:507] global step 1975: loss = 2.4942 (1.381 sec/step)\n",
            "INFO:tensorflow:global step 1976: loss = 2.3289 (0.587 sec/step)\n",
            "I1208 00:10:27.430146 140583164200832 learning.py:507] global step 1976: loss = 2.3289 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 1977: loss = 1.8543 (1.431 sec/step)\n",
            "I1208 00:10:29.188263 140583164200832 learning.py:507] global step 1977: loss = 1.8543 (1.431 sec/step)\n",
            "INFO:tensorflow:global step 1978: loss = 1.9475 (0.621 sec/step)\n",
            "I1208 00:10:30.054014 140583164200832 learning.py:507] global step 1978: loss = 1.9475 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1979: loss = 2.4220 (0.691 sec/step)\n",
            "I1208 00:10:31.252411 140583164200832 learning.py:507] global step 1979: loss = 2.4220 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 1980: loss = 2.5347 (1.502 sec/step)\n",
            "I1208 00:10:32.835208 140583164200832 learning.py:507] global step 1980: loss = 2.5347 (1.502 sec/step)\n",
            "INFO:tensorflow:global step 1981: loss = 1.8036 (1.198 sec/step)\n",
            "I1208 00:10:34.034341 140583164200832 learning.py:507] global step 1981: loss = 1.8036 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1982: loss = 1.8506 (0.698 sec/step)\n",
            "I1208 00:10:34.899167 140583164200832 learning.py:507] global step 1982: loss = 1.8506 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1983: loss = 2.2457 (1.305 sec/step)\n",
            "I1208 00:10:36.468264 140583164200832 learning.py:507] global step 1983: loss = 2.2457 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 1984: loss = 2.5824 (0.714 sec/step)\n",
            "I1208 00:10:37.481021 140583164200832 learning.py:507] global step 1984: loss = 2.5824 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 1985: loss = 2.2162 (1.370 sec/step)\n",
            "I1208 00:10:38.880746 140583164200832 learning.py:507] global step 1985: loss = 2.2162 (1.370 sec/step)\n",
            "INFO:tensorflow:global step 1986: loss = 1.8680 (0.618 sec/step)\n",
            "I1208 00:10:39.768520 140583164200832 learning.py:507] global step 1986: loss = 1.8680 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1987: loss = 1.8390 (1.604 sec/step)\n",
            "I1208 00:10:41.403073 140583164200832 learning.py:507] global step 1987: loss = 1.8390 (1.604 sec/step)\n",
            "INFO:tensorflow:global step 1988: loss = 1.5590 (0.568 sec/step)\n",
            "I1208 00:10:41.972471 140583164200832 learning.py:507] global step 1988: loss = 1.5590 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 1989: loss = 1.8304 (1.846 sec/step)\n",
            "I1208 00:10:43.820388 140583164200832 learning.py:507] global step 1989: loss = 1.8304 (1.846 sec/step)\n",
            "INFO:tensorflow:global step 1990: loss = 1.6946 (0.697 sec/step)\n",
            "I1208 00:10:44.742912 140583164200832 learning.py:507] global step 1990: loss = 1.6946 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1991: loss = 2.2393 (0.645 sec/step)\n",
            "I1208 00:10:45.554656 140583164200832 learning.py:507] global step 1991: loss = 2.2393 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1992: loss = 1.7573 (1.867 sec/step)\n",
            "I1208 00:10:47.423921 140583164200832 learning.py:507] global step 1992: loss = 1.7573 (1.867 sec/step)\n",
            "INFO:tensorflow:global step 1993: loss = 1.6767 (0.576 sec/step)\n",
            "I1208 00:10:48.111989 140583164200832 learning.py:507] global step 1993: loss = 1.6767 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 1994: loss = 2.3566 (1.572 sec/step)\n",
            "I1208 00:10:49.864612 140583164200832 learning.py:507] global step 1994: loss = 2.3566 (1.572 sec/step)\n",
            "INFO:tensorflow:global step 1995: loss = 2.1492 (0.585 sec/step)\n",
            "I1208 00:10:50.628707 140583164200832 learning.py:507] global step 1995: loss = 2.1492 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 1996: loss = 2.4542 (0.758 sec/step)\n",
            "I1208 00:10:51.716237 140583164200832 learning.py:507] global step 1996: loss = 2.4542 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1997: loss = 1.6685 (1.991 sec/step)\n",
            "I1208 00:10:53.709637 140583164200832 learning.py:507] global step 1997: loss = 1.6685 (1.991 sec/step)\n",
            "INFO:tensorflow:global step 1998: loss = 2.4286 (0.570 sec/step)\n",
            "I1208 00:10:54.573734 140583164200832 learning.py:507] global step 1998: loss = 2.4286 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 1999: loss = 1.6393 (0.632 sec/step)\n",
            "I1208 00:10:55.369408 140583164200832 learning.py:507] global step 1999: loss = 1.6393 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2000: loss = 2.3744 (0.990 sec/step)\n",
            "I1208 00:10:56.528172 140583164200832 learning.py:507] global step 2000: loss = 2.3744 (0.990 sec/step)\n",
            "INFO:tensorflow:global step 2001: loss = 1.9738 (1.955 sec/step)\n",
            "I1208 00:10:58.803957 140583164200832 learning.py:507] global step 2001: loss = 1.9738 (1.955 sec/step)\n",
            "INFO:tensorflow:global step 2002: loss = 2.0675 (0.653 sec/step)\n",
            "I1208 00:10:59.597011 140583164200832 learning.py:507] global step 2002: loss = 2.0675 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2003: loss = 1.9599 (1.519 sec/step)\n",
            "I1208 00:11:01.193175 140583164200832 learning.py:507] global step 2003: loss = 1.9599 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 2004: loss = 2.0391 (0.566 sec/step)\n",
            "I1208 00:11:01.761499 140583164200832 learning.py:507] global step 2004: loss = 2.0391 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2005: loss = 1.7091 (1.860 sec/step)\n",
            "I1208 00:11:03.623524 140583164200832 learning.py:507] global step 2005: loss = 1.7091 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 2006: loss = 2.8241 (0.706 sec/step)\n",
            "I1208 00:11:04.414335 140583164200832 learning.py:507] global step 2006: loss = 2.8241 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 2007: loss = 1.8557 (1.220 sec/step)\n",
            "I1208 00:11:05.927522 140583164200832 learning.py:507] global step 2007: loss = 1.8557 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2008: loss = 2.2585 (0.531 sec/step)\n",
            "I1208 00:11:06.460379 140583164200832 learning.py:507] global step 2008: loss = 2.2585 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 2009: loss = 2.5362 (1.721 sec/step)\n",
            "I1208 00:11:08.185883 140583164200832 learning.py:507] global step 2009: loss = 2.5362 (1.721 sec/step)\n",
            "INFO:tensorflow:global step 2010: loss = 1.9658 (0.719 sec/step)\n",
            "I1208 00:11:09.143049 140583164200832 learning.py:507] global step 2010: loss = 1.9658 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 2011: loss = 2.4648 (0.506 sec/step)\n",
            "I1208 00:11:09.886687 140583164200832 learning.py:507] global step 2011: loss = 2.4648 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 2012: loss = 1.8994 (0.861 sec/step)\n",
            "I1208 00:11:10.867451 140583164200832 learning.py:507] global step 2012: loss = 1.8994 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 2013: loss = 1.6845 (1.630 sec/step)\n",
            "I1208 00:11:12.715764 140583164200832 learning.py:507] global step 2013: loss = 1.6845 (1.630 sec/step)\n",
            "INFO:tensorflow:global step 2014: loss = 1.9639 (0.700 sec/step)\n",
            "I1208 00:11:13.811138 140583164200832 learning.py:507] global step 2014: loss = 1.9639 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 2015: loss = 1.9538 (0.762 sec/step)\n",
            "I1208 00:11:14.959353 140583164200832 learning.py:507] global step 2015: loss = 1.9538 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 2016: loss = 2.0493 (1.516 sec/step)\n",
            "I1208 00:11:16.552679 140583164200832 learning.py:507] global step 2016: loss = 2.0493 (1.516 sec/step)\n",
            "INFO:tensorflow:global step 2017: loss = 1.8636 (0.616 sec/step)\n",
            "I1208 00:11:17.455046 140583164200832 learning.py:507] global step 2017: loss = 1.8636 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2018: loss = 2.2276 (1.579 sec/step)\n",
            "I1208 00:11:19.155247 140583164200832 learning.py:507] global step 2018: loss = 2.2276 (1.579 sec/step)\n",
            "INFO:tensorflow:global step 2019: loss = 2.4593 (1.165 sec/step)\n",
            "I1208 00:11:20.322166 140583164200832 learning.py:507] global step 2019: loss = 2.4593 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 2020: loss = 1.8300 (0.709 sec/step)\n",
            "I1208 00:11:21.186578 140583164200832 learning.py:507] global step 2020: loss = 1.8300 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 2021: loss = 2.0094 (0.556 sec/step)\n",
            "I1208 00:11:21.875263 140583164200832 learning.py:507] global step 2021: loss = 2.0094 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 2022: loss = 2.6321 (1.723 sec/step)\n",
            "I1208 00:11:23.600003 140583164200832 learning.py:507] global step 2022: loss = 2.6321 (1.723 sec/step)\n",
            "INFO:tensorflow:global step 2023: loss = 2.1010 (0.546 sec/step)\n",
            "I1208 00:11:24.147902 140583164200832 learning.py:507] global step 2023: loss = 2.1010 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 2024: loss = 1.5997 (1.787 sec/step)\n",
            "I1208 00:11:25.937195 140583164200832 learning.py:507] global step 2024: loss = 1.5997 (1.787 sec/step)\n",
            "INFO:tensorflow:global step 2025: loss = 2.0593 (0.662 sec/step)\n",
            "I1208 00:11:26.925597 140583164200832 learning.py:507] global step 2025: loss = 2.0593 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2026: loss = 2.3511 (1.552 sec/step)\n",
            "I1208 00:11:28.510196 140583164200832 learning.py:507] global step 2026: loss = 2.3511 (1.552 sec/step)\n",
            "INFO:tensorflow:global step 2027: loss = 1.5787 (1.165 sec/step)\n",
            "I1208 00:11:29.676500 140583164200832 learning.py:507] global step 2027: loss = 1.5787 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 2028: loss = 1.9532 (0.563 sec/step)\n",
            "I1208 00:11:30.241640 140583164200832 learning.py:507] global step 2028: loss = 1.9532 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2029: loss = 1.5881 (0.837 sec/step)\n",
            "I1208 00:11:31.322787 140583164200832 learning.py:507] global step 2029: loss = 1.5881 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 2030: loss = 1.7690 (1.794 sec/step)\n",
            "I1208 00:11:33.260652 140583164200832 learning.py:507] global step 2030: loss = 1.7690 (1.794 sec/step)\n",
            "INFO:tensorflow:global step 2031: loss = 2.0467 (0.577 sec/step)\n",
            "I1208 00:11:34.172991 140583164200832 learning.py:507] global step 2031: loss = 2.0467 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2032: loss = 1.7180 (1.516 sec/step)\n",
            "I1208 00:11:35.724570 140583164200832 learning.py:507] global step 2032: loss = 1.7180 (1.516 sec/step)\n",
            "INFO:tensorflow:global step 2033: loss = 1.8794 (0.665 sec/step)\n",
            "I1208 00:11:36.473074 140583164200832 learning.py:507] global step 2033: loss = 1.8794 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2034: loss = 2.3042 (1.346 sec/step)\n",
            "I1208 00:11:38.121304 140583164200832 learning.py:507] global step 2034: loss = 2.3042 (1.346 sec/step)\n",
            "INFO:tensorflow:global step 2035: loss = 1.8124 (0.576 sec/step)\n",
            "I1208 00:11:39.063971 140583164200832 learning.py:507] global step 2035: loss = 1.8124 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 2036: loss = 1.9344 (0.604 sec/step)\n",
            "I1208 00:11:39.784691 140583164200832 learning.py:507] global step 2036: loss = 1.9344 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2037: loss = 2.0563 (1.726 sec/step)\n",
            "I1208 00:11:41.564394 140583164200832 learning.py:507] global step 2037: loss = 2.0563 (1.726 sec/step)\n",
            "INFO:tensorflow:global step 2038: loss = 2.4432 (1.180 sec/step)\n",
            "I1208 00:11:43.293483 140583164200832 learning.py:507] global step 2038: loss = 2.4432 (1.180 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2038.\n",
            "I1208 00:11:45.094401 140579459213056 supervisor.py:1050] Recording summary at step 2038.\n",
            "INFO:tensorflow:global step 2039: loss = 2.5178 (1.813 sec/step)\n",
            "I1208 00:11:45.348292 140583164200832 learning.py:507] global step 2039: loss = 2.5178 (1.813 sec/step)\n",
            "INFO:tensorflow:global step 2040: loss = 1.9712 (0.652 sec/step)\n",
            "I1208 00:11:46.210427 140583164200832 learning.py:507] global step 2040: loss = 1.9712 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2041: loss = 2.1461 (1.497 sec/step)\n",
            "I1208 00:11:47.898158 140583164200832 learning.py:507] global step 2041: loss = 2.1461 (1.497 sec/step)\n",
            "INFO:tensorflow:global step 2042: loss = 1.8224 (0.541 sec/step)\n",
            "I1208 00:11:48.771005 140583164200832 learning.py:507] global step 2042: loss = 1.8224 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 2043: loss = 2.5423 (0.604 sec/step)\n",
            "I1208 00:11:49.633633 140583164200832 learning.py:507] global step 2043: loss = 2.5423 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2044: loss = 1.8204 (1.829 sec/step)\n",
            "I1208 00:11:51.464967 140583164200832 learning.py:507] global step 2044: loss = 1.8204 (1.829 sec/step)\n",
            "INFO:tensorflow:global step 2045: loss = 2.3222 (0.639 sec/step)\n",
            "I1208 00:11:52.492522 140583164200832 learning.py:507] global step 2045: loss = 2.3222 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2046: loss = 2.6811 (1.356 sec/step)\n",
            "I1208 00:11:53.885626 140583164200832 learning.py:507] global step 2046: loss = 2.6811 (1.356 sec/step)\n",
            "INFO:tensorflow:global step 2047: loss = 2.4510 (0.764 sec/step)\n",
            "I1208 00:11:54.861205 140583164200832 learning.py:507] global step 2047: loss = 2.4510 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 2048: loss = 2.8513 (0.849 sec/step)\n",
            "I1208 00:11:56.007836 140583164200832 learning.py:507] global step 2048: loss = 2.8513 (0.849 sec/step)\n",
            "INFO:tensorflow:global step 2049: loss = 1.7282 (0.587 sec/step)\n",
            "I1208 00:11:56.622356 140583164200832 learning.py:507] global step 2049: loss = 1.7282 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 2050: loss = 2.9500 (1.895 sec/step)\n",
            "I1208 00:11:58.520399 140583164200832 learning.py:507] global step 2050: loss = 2.9500 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 2051: loss = 2.8311 (1.158 sec/step)\n",
            "I1208 00:11:59.680577 140583164200832 learning.py:507] global step 2051: loss = 2.8311 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 2052: loss = 1.8800 (0.609 sec/step)\n",
            "I1208 00:12:00.559970 140583164200832 learning.py:507] global step 2052: loss = 1.8800 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2053: loss = 2.1808 (0.637 sec/step)\n",
            "I1208 00:12:01.673552 140583164200832 learning.py:507] global step 2053: loss = 2.1808 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2054: loss = 2.8296 (1.515 sec/step)\n",
            "I1208 00:12:03.238682 140583164200832 learning.py:507] global step 2054: loss = 2.8296 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 2055: loss = 2.0388 (0.552 sec/step)\n",
            "I1208 00:12:03.792501 140583164200832 learning.py:507] global step 2055: loss = 2.0388 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 2056: loss = 2.4212 (1.244 sec/step)\n",
            "I1208 00:12:05.118024 140583164200832 learning.py:507] global step 2056: loss = 2.4212 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2057: loss = 2.0401 (1.543 sec/step)\n",
            "I1208 00:12:06.714625 140583164200832 learning.py:507] global step 2057: loss = 2.0401 (1.543 sec/step)\n",
            "INFO:tensorflow:global step 2058: loss = 2.6942 (0.631 sec/step)\n",
            "I1208 00:12:07.565474 140583164200832 learning.py:507] global step 2058: loss = 2.6942 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2059: loss = 2.2171 (0.726 sec/step)\n",
            "I1208 00:12:08.810446 140583164200832 learning.py:507] global step 2059: loss = 2.2171 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 2060: loss = 3.3790 (1.459 sec/step)\n",
            "I1208 00:12:10.417763 140583164200832 learning.py:507] global step 2060: loss = 3.3790 (1.459 sec/step)\n",
            "INFO:tensorflow:global step 2061: loss = 2.0218 (0.651 sec/step)\n",
            "I1208 00:12:11.262298 140583164200832 learning.py:507] global step 2061: loss = 2.0218 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2062: loss = 2.3954 (0.743 sec/step)\n",
            "I1208 00:12:12.332691 140583164200832 learning.py:507] global step 2062: loss = 2.3954 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2063: loss = 2.1464 (0.800 sec/step)\n",
            "I1208 00:12:13.591394 140583164200832 learning.py:507] global step 2063: loss = 2.1464 (0.800 sec/step)\n",
            "INFO:tensorflow:global step 2064: loss = 2.0125 (1.412 sec/step)\n",
            "I1208 00:12:15.270328 140583164200832 learning.py:507] global step 2064: loss = 2.0125 (1.412 sec/step)\n",
            "INFO:tensorflow:global step 2065: loss = 1.9569 (0.597 sec/step)\n",
            "I1208 00:12:15.870113 140583164200832 learning.py:507] global step 2065: loss = 1.9569 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 2066: loss = 1.9640 (1.075 sec/step)\n",
            "I1208 00:12:17.020394 140583164200832 learning.py:507] global step 2066: loss = 1.9640 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 2067: loss = 1.8550 (1.783 sec/step)\n",
            "I1208 00:12:18.805406 140583164200832 learning.py:507] global step 2067: loss = 1.8550 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 2068: loss = 2.4714 (0.644 sec/step)\n",
            "I1208 00:12:19.766342 140583164200832 learning.py:507] global step 2068: loss = 2.4714 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2069: loss = 1.7392 (0.622 sec/step)\n",
            "I1208 00:12:20.602266 140583164200832 learning.py:507] global step 2069: loss = 1.7392 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2070: loss = 1.8146 (1.281 sec/step)\n",
            "I1208 00:12:22.122863 140583164200832 learning.py:507] global step 2070: loss = 1.8146 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 2071: loss = 1.7679 (0.555 sec/step)\n",
            "I1208 00:12:22.679527 140583164200832 learning.py:507] global step 2071: loss = 1.7679 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2072: loss = 2.4371 (0.852 sec/step)\n",
            "I1208 00:12:23.902838 140583164200832 learning.py:507] global step 2072: loss = 2.4371 (0.852 sec/step)\n",
            "INFO:tensorflow:global step 2073: loss = 1.9361 (1.375 sec/step)\n",
            "I1208 00:12:25.608429 140583164200832 learning.py:507] global step 2073: loss = 1.9361 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 2074: loss = 2.0311 (0.554 sec/step)\n",
            "I1208 00:12:26.163655 140583164200832 learning.py:507] global step 2074: loss = 2.0311 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 2075: loss = 2.7425 (1.712 sec/step)\n",
            "I1208 00:12:27.877484 140583164200832 learning.py:507] global step 2075: loss = 2.7425 (1.712 sec/step)\n",
            "INFO:tensorflow:global step 2076: loss = 1.8744 (0.552 sec/step)\n",
            "I1208 00:12:28.431185 140583164200832 learning.py:507] global step 2076: loss = 1.8744 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 2077: loss = 1.8384 (1.789 sec/step)\n",
            "I1208 00:12:30.222199 140583164200832 learning.py:507] global step 2077: loss = 1.8384 (1.789 sec/step)\n",
            "INFO:tensorflow:global step 2078: loss = 1.7322 (0.483 sec/step)\n",
            "I1208 00:12:30.707138 140583164200832 learning.py:507] global step 2078: loss = 1.7322 (0.483 sec/step)\n",
            "INFO:tensorflow:global step 2079: loss = 2.3463 (1.701 sec/step)\n",
            "I1208 00:12:32.643067 140583164200832 learning.py:507] global step 2079: loss = 2.3463 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 2080: loss = 2.4809 (0.540 sec/step)\n",
            "I1208 00:12:33.278522 140583164200832 learning.py:507] global step 2080: loss = 2.4809 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 2081: loss = 1.9697 (0.644 sec/step)\n",
            "I1208 00:12:33.925448 140583164200832 learning.py:507] global step 2081: loss = 1.9697 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2082: loss = 2.4958 (2.394 sec/step)\n",
            "I1208 00:12:36.389976 140583164200832 learning.py:507] global step 2082: loss = 2.4958 (2.394 sec/step)\n",
            "INFO:tensorflow:global step 2083: loss = 2.3373 (0.702 sec/step)\n",
            "I1208 00:12:37.163365 140583164200832 learning.py:507] global step 2083: loss = 2.3373 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 2084: loss = 1.6833 (2.535 sec/step)\n",
            "I1208 00:12:40.160383 140583164200832 learning.py:507] global step 2084: loss = 1.6833 (2.535 sec/step)\n",
            "INFO:tensorflow:global step 2085: loss = 1.9941 (0.643 sec/step)\n",
            "I1208 00:12:40.959667 140583164200832 learning.py:507] global step 2085: loss = 1.9941 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2086: loss = 1.6791 (2.561 sec/step)\n",
            "I1208 00:12:43.827579 140583164200832 learning.py:507] global step 2086: loss = 1.6791 (2.561 sec/step)\n",
            "INFO:tensorflow:global step 2087: loss = 2.2058 (0.608 sec/step)\n",
            "I1208 00:12:44.639994 140583164200832 learning.py:507] global step 2087: loss = 2.2058 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2088: loss = 1.8609 (0.809 sec/step)\n",
            "I1208 00:12:46.087072 140583164200832 learning.py:507] global step 2088: loss = 1.8609 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 2089: loss = 2.5545 (0.660 sec/step)\n",
            "I1208 00:12:46.833092 140583164200832 learning.py:507] global step 2089: loss = 2.5545 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2090: loss = 1.6233 (1.356 sec/step)\n",
            "I1208 00:12:48.355933 140583164200832 learning.py:507] global step 2090: loss = 1.6233 (1.356 sec/step)\n",
            "INFO:tensorflow:global step 2091: loss = 1.9506 (2.469 sec/step)\n",
            "I1208 00:12:50.849304 140583164200832 learning.py:507] global step 2091: loss = 1.9506 (2.469 sec/step)\n",
            "INFO:tensorflow:global step 2092: loss = 2.1721 (0.574 sec/step)\n",
            "I1208 00:12:51.649988 140583164200832 learning.py:507] global step 2092: loss = 2.1721 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 2093: loss = 1.9417 (1.861 sec/step)\n",
            "I1208 00:12:53.695903 140583164200832 learning.py:507] global step 2093: loss = 1.9417 (1.861 sec/step)\n",
            "INFO:tensorflow:global step 2094: loss = 2.1806 (0.719 sec/step)\n",
            "I1208 00:12:54.746083 140583164200832 learning.py:507] global step 2094: loss = 2.1806 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 2095: loss = 2.1686 (0.672 sec/step)\n",
            "I1208 00:12:55.693441 140583164200832 learning.py:507] global step 2095: loss = 2.1686 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2096: loss = 2.4098 (1.759 sec/step)\n",
            "I1208 00:12:57.454243 140583164200832 learning.py:507] global step 2096: loss = 2.4098 (1.759 sec/step)\n",
            "INFO:tensorflow:global step 2097: loss = 2.2339 (0.502 sec/step)\n",
            "I1208 00:12:57.958228 140583164200832 learning.py:507] global step 2097: loss = 2.2339 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 2098: loss = 2.1527 (2.370 sec/step)\n",
            "I1208 00:13:00.329371 140583164200832 learning.py:507] global step 2098: loss = 2.1527 (2.370 sec/step)\n",
            "INFO:tensorflow:global step 2099: loss = 2.1842 (0.688 sec/step)\n",
            "I1208 00:13:01.265619 140583164200832 learning.py:507] global step 2099: loss = 2.1842 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 2100: loss = 2.1511 (0.754 sec/step)\n",
            "I1208 00:13:02.500994 140583164200832 learning.py:507] global step 2100: loss = 2.1511 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 2101: loss = 2.1926 (2.249 sec/step)\n",
            "I1208 00:13:04.829214 140583164200832 learning.py:507] global step 2101: loss = 2.1926 (2.249 sec/step)\n",
            "INFO:tensorflow:global step 2102: loss = 1.9898 (0.594 sec/step)\n",
            "I1208 00:13:05.425215 140583164200832 learning.py:507] global step 2102: loss = 1.9898 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2103: loss = 1.7754 (1.940 sec/step)\n",
            "I1208 00:13:07.367079 140583164200832 learning.py:507] global step 2103: loss = 1.7754 (1.940 sec/step)\n",
            "INFO:tensorflow:global step 2104: loss = 1.8083 (0.609 sec/step)\n",
            "I1208 00:13:08.354120 140583164200832 learning.py:507] global step 2104: loss = 1.8083 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2105: loss = 2.3673 (0.721 sec/step)\n",
            "I1208 00:13:09.598973 140583164200832 learning.py:507] global step 2105: loss = 2.3673 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 2106: loss = 2.2323 (1.782 sec/step)\n",
            "I1208 00:13:11.466734 140583164200832 learning.py:507] global step 2106: loss = 2.2323 (1.782 sec/step)\n",
            "INFO:tensorflow:global step 2107: loss = 2.2911 (0.644 sec/step)\n",
            "I1208 00:13:12.112797 140583164200832 learning.py:507] global step 2107: loss = 2.2911 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2108: loss = 1.5323 (1.041 sec/step)\n",
            "I1208 00:13:13.196385 140583164200832 learning.py:507] global step 2108: loss = 1.5323 (1.041 sec/step)\n",
            "INFO:tensorflow:global step 2109: loss = 1.9460 (2.199 sec/step)\n",
            "I1208 00:13:15.424833 140583164200832 learning.py:507] global step 2109: loss = 1.9460 (2.199 sec/step)\n",
            "INFO:tensorflow:global step 2110: loss = 2.0688 (0.609 sec/step)\n",
            "I1208 00:13:16.035917 140583164200832 learning.py:507] global step 2110: loss = 2.0688 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2111: loss = 1.8303 (1.676 sec/step)\n",
            "I1208 00:13:18.019158 140583164200832 learning.py:507] global step 2111: loss = 1.8303 (1.676 sec/step)\n",
            "INFO:tensorflow:global step 2112: loss = 2.5262 (1.334 sec/step)\n",
            "I1208 00:13:19.399278 140583164200832 learning.py:507] global step 2112: loss = 2.5262 (1.334 sec/step)\n",
            "INFO:tensorflow:global step 2113: loss = 2.1236 (0.596 sec/step)\n",
            "I1208 00:13:20.358852 140583164200832 learning.py:507] global step 2113: loss = 2.1236 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2114: loss = 2.1642 (0.681 sec/step)\n",
            "I1208 00:13:21.424876 140583164200832 learning.py:507] global step 2114: loss = 2.1642 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2115: loss = 2.1566 (1.192 sec/step)\n",
            "I1208 00:13:22.845516 140583164200832 learning.py:507] global step 2115: loss = 2.1566 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 2116: loss = 1.8909 (0.651 sec/step)\n",
            "I1208 00:13:23.691565 140583164200832 learning.py:507] global step 2116: loss = 1.8909 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2117: loss = 2.5989 (0.624 sec/step)\n",
            "I1208 00:13:24.969787 140583164200832 learning.py:507] global step 2117: loss = 2.5989 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2118: loss = 2.4091 (2.276 sec/step)\n",
            "I1208 00:13:27.262216 140583164200832 learning.py:507] global step 2118: loss = 2.4091 (2.276 sec/step)\n",
            "INFO:tensorflow:global step 2119: loss = 2.9629 (0.733 sec/step)\n",
            "I1208 00:13:28.170238 140583164200832 learning.py:507] global step 2119: loss = 2.9629 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 2120: loss = 1.6723 (0.747 sec/step)\n",
            "I1208 00:13:29.482911 140583164200832 learning.py:507] global step 2120: loss = 1.6723 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 2121: loss = 2.0030 (1.519 sec/step)\n",
            "I1208 00:13:31.235743 140583164200832 learning.py:507] global step 2121: loss = 2.0030 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 2122: loss = 3.0504 (0.647 sec/step)\n",
            "I1208 00:13:31.884928 140583164200832 learning.py:507] global step 2122: loss = 3.0504 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2123: loss = 2.4415 (2.053 sec/step)\n",
            "I1208 00:13:33.940037 140583164200832 learning.py:507] global step 2123: loss = 2.4415 (2.053 sec/step)\n",
            "INFO:tensorflow:global step 2124: loss = 1.8403 (0.789 sec/step)\n",
            "I1208 00:13:35.197971 140583164200832 learning.py:507] global step 2124: loss = 1.8403 (0.789 sec/step)\n",
            "INFO:tensorflow:global step 2125: loss = 1.8430 (0.601 sec/step)\n",
            "I1208 00:13:36.058300 140583164200832 learning.py:507] global step 2125: loss = 1.8430 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 2126: loss = 2.2103 (0.589 sec/step)\n",
            "I1208 00:13:36.974160 140583164200832 learning.py:507] global step 2126: loss = 2.2103 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 2127: loss = 2.1765 (2.086 sec/step)\n",
            "I1208 00:13:39.061789 140583164200832 learning.py:507] global step 2127: loss = 2.1765 (2.086 sec/step)\n",
            "INFO:tensorflow:global step 2128: loss = 2.0114 (0.809 sec/step)\n",
            "I1208 00:13:39.885831 140583164200832 learning.py:507] global step 2128: loss = 2.0114 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 2129: loss = 2.0326 (0.864 sec/step)\n",
            "I1208 00:13:41.061291 140583164200832 learning.py:507] global step 2129: loss = 2.0326 (0.864 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2130.\n",
            "I1208 00:13:44.428620 140579459213056 supervisor.py:1050] Recording summary at step 2130.\n",
            "INFO:tensorflow:global step 2130: loss = 2.0508 (3.325 sec/step)\n",
            "I1208 00:13:44.438467 140583164200832 learning.py:507] global step 2130: loss = 2.0508 (3.325 sec/step)\n",
            "INFO:tensorflow:global step 2131: loss = 2.1117 (0.594 sec/step)\n",
            "I1208 00:13:45.034775 140583164200832 learning.py:507] global step 2131: loss = 2.1117 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2132: loss = 1.5394 (1.859 sec/step)\n",
            "I1208 00:13:46.896229 140583164200832 learning.py:507] global step 2132: loss = 1.5394 (1.859 sec/step)\n",
            "INFO:tensorflow:global step 2133: loss = 1.6995 (0.663 sec/step)\n",
            "I1208 00:13:47.561537 140583164200832 learning.py:507] global step 2133: loss = 1.6995 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2134: loss = 2.1481 (1.947 sec/step)\n",
            "I1208 00:13:49.510350 140583164200832 learning.py:507] global step 2134: loss = 2.1481 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 2135: loss = 2.5350 (0.575 sec/step)\n",
            "I1208 00:13:50.296633 140583164200832 learning.py:507] global step 2135: loss = 2.5350 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 2136: loss = 2.3297 (1.584 sec/step)\n",
            "I1208 00:13:52.270132 140583164200832 learning.py:507] global step 2136: loss = 2.3297 (1.584 sec/step)\n",
            "INFO:tensorflow:global step 2137: loss = 2.4145 (0.611 sec/step)\n",
            "I1208 00:13:52.926336 140583164200832 learning.py:507] global step 2137: loss = 2.4145 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2138: loss = 2.2795 (1.360 sec/step)\n",
            "I1208 00:13:54.596903 140583164200832 learning.py:507] global step 2138: loss = 2.2795 (1.360 sec/step)\n",
            "INFO:tensorflow:global step 2139: loss = 1.9781 (0.551 sec/step)\n",
            "I1208 00:13:55.150203 140583164200832 learning.py:507] global step 2139: loss = 1.9781 (0.551 sec/step)\n",
            "INFO:tensorflow:global step 2140: loss = 2.2568 (1.824 sec/step)\n",
            "I1208 00:13:56.976484 140583164200832 learning.py:507] global step 2140: loss = 2.2568 (1.824 sec/step)\n",
            "INFO:tensorflow:global step 2141: loss = 1.7600 (0.644 sec/step)\n",
            "I1208 00:13:57.622593 140583164200832 learning.py:507] global step 2141: loss = 1.7600 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2142: loss = 2.2076 (1.040 sec/step)\n",
            "I1208 00:13:58.846173 140583164200832 learning.py:507] global step 2142: loss = 2.2076 (1.040 sec/step)\n",
            "INFO:tensorflow:global step 2143: loss = 2.1170 (1.976 sec/step)\n",
            "I1208 00:14:00.824210 140583164200832 learning.py:507] global step 2143: loss = 2.1170 (1.976 sec/step)\n",
            "INFO:tensorflow:global step 2144: loss = 1.9525 (0.603 sec/step)\n",
            "I1208 00:14:01.429112 140583164200832 learning.py:507] global step 2144: loss = 1.9525 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 2145: loss = 2.5887 (2.178 sec/step)\n",
            "I1208 00:14:03.609532 140583164200832 learning.py:507] global step 2145: loss = 2.5887 (2.178 sec/step)\n",
            "INFO:tensorflow:global step 2146: loss = 2.3647 (0.577 sec/step)\n",
            "I1208 00:14:04.188175 140583164200832 learning.py:507] global step 2146: loss = 2.3647 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2147: loss = 2.2736 (1.847 sec/step)\n",
            "I1208 00:14:06.036445 140583164200832 learning.py:507] global step 2147: loss = 2.2736 (1.847 sec/step)\n",
            "INFO:tensorflow:global step 2148: loss = 1.9066 (0.607 sec/step)\n",
            "I1208 00:14:07.000254 140583164200832 learning.py:507] global step 2148: loss = 1.9066 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2149: loss = 2.6496 (2.248 sec/step)\n",
            "I1208 00:14:09.349176 140583164200832 learning.py:507] global step 2149: loss = 2.6496 (2.248 sec/step)\n",
            "INFO:tensorflow:global step 2150: loss = 2.1639 (1.179 sec/step)\n",
            "I1208 00:14:10.530274 140583164200832 learning.py:507] global step 2150: loss = 2.1639 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 2151: loss = 1.9027 (0.596 sec/step)\n",
            "I1208 00:14:11.128628 140583164200832 learning.py:507] global step 2151: loss = 1.9027 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2152: loss = 1.8363 (1.953 sec/step)\n",
            "I1208 00:14:13.083933 140583164200832 learning.py:507] global step 2152: loss = 1.8363 (1.953 sec/step)\n",
            "INFO:tensorflow:global step 2153: loss = 2.1471 (1.271 sec/step)\n",
            "I1208 00:14:14.356393 140583164200832 learning.py:507] global step 2153: loss = 2.1471 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2154: loss = 1.8540 (0.604 sec/step)\n",
            "I1208 00:14:14.962558 140583164200832 learning.py:507] global step 2154: loss = 1.8540 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2155: loss = 2.0703 (2.023 sec/step)\n",
            "I1208 00:14:17.035304 140583164200832 learning.py:507] global step 2155: loss = 2.0703 (2.023 sec/step)\n",
            "INFO:tensorflow:global step 2156: loss = 2.5898 (0.588 sec/step)\n",
            "I1208 00:14:17.625077 140583164200832 learning.py:507] global step 2156: loss = 2.5898 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 2157: loss = 1.8203 (0.722 sec/step)\n",
            "I1208 00:14:18.366752 140583164200832 learning.py:507] global step 2157: loss = 1.8203 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2158: loss = 3.5018 (2.299 sec/step)\n",
            "I1208 00:14:20.957994 140583164200832 learning.py:507] global step 2158: loss = 3.5018 (2.299 sec/step)\n",
            "INFO:tensorflow:global step 2159: loss = 2.4340 (0.624 sec/step)\n",
            "I1208 00:14:21.583606 140583164200832 learning.py:507] global step 2159: loss = 2.4340 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2160: loss = 2.0745 (1.866 sec/step)\n",
            "I1208 00:14:23.451767 140583164200832 learning.py:507] global step 2160: loss = 2.0745 (1.866 sec/step)\n",
            "INFO:tensorflow:global step 2161: loss = 1.9543 (0.529 sec/step)\n",
            "I1208 00:14:23.982746 140583164200832 learning.py:507] global step 2161: loss = 1.9543 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 2162: loss = 2.2413 (2.010 sec/step)\n",
            "I1208 00:14:25.995041 140583164200832 learning.py:507] global step 2162: loss = 2.2413 (2.010 sec/step)\n",
            "INFO:tensorflow:global step 2163: loss = 2.2868 (0.604 sec/step)\n",
            "I1208 00:14:26.753573 140583164200832 learning.py:507] global step 2163: loss = 2.2868 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2164: loss = 1.7821 (1.939 sec/step)\n",
            "I1208 00:14:28.726262 140583164200832 learning.py:507] global step 2164: loss = 1.7821 (1.939 sec/step)\n",
            "INFO:tensorflow:global step 2165: loss = 3.4000 (0.619 sec/step)\n",
            "I1208 00:14:29.346844 140583164200832 learning.py:507] global step 2165: loss = 3.4000 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 2166: loss = 2.2754 (1.653 sec/step)\n",
            "I1208 00:14:31.110243 140583164200832 learning.py:507] global step 2166: loss = 2.2754 (1.653 sec/step)\n",
            "INFO:tensorflow:global step 2167: loss = 2.5490 (0.664 sec/step)\n",
            "I1208 00:14:31.776341 140583164200832 learning.py:507] global step 2167: loss = 2.5490 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 2168: loss = 2.6340 (1.936 sec/step)\n",
            "I1208 00:14:33.714113 140583164200832 learning.py:507] global step 2168: loss = 2.6340 (1.936 sec/step)\n",
            "INFO:tensorflow:global step 2169: loss = 2.7381 (0.594 sec/step)\n",
            "I1208 00:14:34.704082 140583164200832 learning.py:507] global step 2169: loss = 2.7381 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2170: loss = 1.8841 (0.708 sec/step)\n",
            "I1208 00:14:35.697126 140583164200832 learning.py:507] global step 2170: loss = 1.8841 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 2171: loss = 3.2145 (1.837 sec/step)\n",
            "I1208 00:14:37.557195 140583164200832 learning.py:507] global step 2171: loss = 3.2145 (1.837 sec/step)\n",
            "INFO:tensorflow:global step 2172: loss = 2.3256 (0.623 sec/step)\n",
            "I1208 00:14:38.181988 140583164200832 learning.py:507] global step 2172: loss = 2.3256 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2173: loss = 2.1689 (1.992 sec/step)\n",
            "I1208 00:14:40.176369 140583164200832 learning.py:507] global step 2173: loss = 2.1689 (1.992 sec/step)\n",
            "INFO:tensorflow:global step 2174: loss = 1.8952 (0.616 sec/step)\n",
            "I1208 00:14:41.010828 140583164200832 learning.py:507] global step 2174: loss = 1.8952 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2175: loss = 2.3603 (1.304 sec/step)\n",
            "I1208 00:14:42.670940 140583164200832 learning.py:507] global step 2175: loss = 2.3603 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 2176: loss = 2.1390 (0.608 sec/step)\n",
            "I1208 00:14:43.520836 140583164200832 learning.py:507] global step 2176: loss = 2.1390 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2177: loss = 1.7681 (0.518 sec/step)\n",
            "I1208 00:14:44.339834 140583164200832 learning.py:507] global step 2177: loss = 1.7681 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 2178: loss = 2.2182 (1.776 sec/step)\n",
            "I1208 00:14:46.117650 140583164200832 learning.py:507] global step 2178: loss = 2.2182 (1.776 sec/step)\n",
            "INFO:tensorflow:global step 2179: loss = 2.1085 (0.721 sec/step)\n",
            "I1208 00:14:47.067837 140583164200832 learning.py:507] global step 2179: loss = 2.1085 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 2180: loss = 2.4073 (1.481 sec/step)\n",
            "I1208 00:14:48.585358 140583164200832 learning.py:507] global step 2180: loss = 2.4073 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 2181: loss = 2.1106 (0.605 sec/step)\n",
            "I1208 00:14:49.192317 140583164200832 learning.py:507] global step 2181: loss = 2.1106 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2182: loss = 1.9277 (1.743 sec/step)\n",
            "I1208 00:14:50.937421 140583164200832 learning.py:507] global step 2182: loss = 1.9277 (1.743 sec/step)\n",
            "INFO:tensorflow:global step 2183: loss = 1.6148 (0.579 sec/step)\n",
            "I1208 00:14:51.518210 140583164200832 learning.py:507] global step 2183: loss = 1.6148 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 2184: loss = 2.3957 (0.886 sec/step)\n",
            "I1208 00:14:52.616542 140583164200832 learning.py:507] global step 2184: loss = 2.3957 (0.886 sec/step)\n",
            "INFO:tensorflow:global step 2185: loss = 2.1839 (2.062 sec/step)\n",
            "I1208 00:14:54.926985 140583164200832 learning.py:507] global step 2185: loss = 2.1839 (2.062 sec/step)\n",
            "INFO:tensorflow:global step 2186: loss = 1.9065 (0.563 sec/step)\n",
            "I1208 00:14:55.491913 140583164200832 learning.py:507] global step 2186: loss = 1.9065 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2187: loss = 2.0643 (1.831 sec/step)\n",
            "I1208 00:14:57.325026 140583164200832 learning.py:507] global step 2187: loss = 2.0643 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 2188: loss = 2.2180 (0.700 sec/step)\n",
            "I1208 00:14:58.214514 140583164200832 learning.py:507] global step 2188: loss = 2.2180 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 2189: loss = 1.7055 (1.781 sec/step)\n",
            "I1208 00:15:00.238579 140583164200832 learning.py:507] global step 2189: loss = 1.7055 (1.781 sec/step)\n",
            "INFO:tensorflow:global step 2190: loss = 2.1517 (0.622 sec/step)\n",
            "I1208 00:15:00.863116 140583164200832 learning.py:507] global step 2190: loss = 2.1517 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2191: loss = 2.1958 (1.687 sec/step)\n",
            "I1208 00:15:02.606065 140583164200832 learning.py:507] global step 2191: loss = 2.1958 (1.687 sec/step)\n",
            "INFO:tensorflow:global step 2192: loss = 2.1655 (0.680 sec/step)\n",
            "I1208 00:15:03.628314 140583164200832 learning.py:507] global step 2192: loss = 2.1655 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2193: loss = 2.3203 (0.626 sec/step)\n",
            "I1208 00:15:04.348893 140583164200832 learning.py:507] global step 2193: loss = 2.3203 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2194: loss = 2.1544 (2.064 sec/step)\n",
            "I1208 00:15:06.414860 140583164200832 learning.py:507] global step 2194: loss = 2.1544 (2.064 sec/step)\n",
            "INFO:tensorflow:global step 2195: loss = 1.8703 (0.667 sec/step)\n",
            "I1208 00:15:07.364529 140583164200832 learning.py:507] global step 2195: loss = 1.8703 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 2196: loss = 2.2142 (0.596 sec/step)\n",
            "I1208 00:15:08.126127 140583164200832 learning.py:507] global step 2196: loss = 2.2142 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2197: loss = 2.0696 (0.924 sec/step)\n",
            "I1208 00:15:09.239236 140583164200832 learning.py:507] global step 2197: loss = 2.0696 (0.924 sec/step)\n",
            "INFO:tensorflow:global step 2198: loss = 2.4745 (1.617 sec/step)\n",
            "I1208 00:15:11.045857 140583164200832 learning.py:507] global step 2198: loss = 2.4745 (1.617 sec/step)\n",
            "INFO:tensorflow:global step 2199: loss = 1.4296 (0.593 sec/step)\n",
            "I1208 00:15:11.641104 140583164200832 learning.py:507] global step 2199: loss = 1.4296 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 2200: loss = 2.6175 (1.741 sec/step)\n",
            "I1208 00:15:13.441344 140583164200832 learning.py:507] global step 2200: loss = 2.6175 (1.741 sec/step)\n",
            "INFO:tensorflow:global step 2201: loss = 1.9324 (2.018 sec/step)\n",
            "I1208 00:15:15.468189 140583164200832 learning.py:507] global step 2201: loss = 1.9324 (2.018 sec/step)\n",
            "INFO:tensorflow:global step 2202: loss = 2.1200 (0.782 sec/step)\n",
            "I1208 00:15:16.559358 140583164200832 learning.py:507] global step 2202: loss = 2.1200 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 2203: loss = 1.6367 (0.607 sec/step)\n",
            "I1208 00:15:17.245741 140583164200832 learning.py:507] global step 2203: loss = 1.6367 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2204: loss = 2.0365 (2.120 sec/step)\n",
            "I1208 00:15:19.367934 140583164200832 learning.py:507] global step 2204: loss = 2.0365 (2.120 sec/step)\n",
            "INFO:tensorflow:global step 2205: loss = 2.3907 (0.652 sec/step)\n",
            "I1208 00:15:20.350931 140583164200832 learning.py:507] global step 2205: loss = 2.3907 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2206: loss = 1.9116 (0.568 sec/step)\n",
            "I1208 00:15:21.102973 140583164200832 learning.py:507] global step 2206: loss = 1.9116 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 2207: loss = 2.2002 (1.778 sec/step)\n",
            "I1208 00:15:22.883420 140583164200832 learning.py:507] global step 2207: loss = 2.2002 (1.778 sec/step)\n",
            "INFO:tensorflow:global step 2208: loss = 2.1618 (0.816 sec/step)\n",
            "I1208 00:15:23.961960 140583164200832 learning.py:507] global step 2208: loss = 2.1618 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 2209: loss = 1.6887 (0.629 sec/step)\n",
            "I1208 00:15:24.732258 140583164200832 learning.py:507] global step 2209: loss = 1.6887 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2210: loss = 1.6800 (2.191 sec/step)\n",
            "I1208 00:15:27.133888 140583164200832 learning.py:507] global step 2210: loss = 1.6800 (2.191 sec/step)\n",
            "INFO:tensorflow:global step 2211: loss = 1.9063 (0.878 sec/step)\n",
            "I1208 00:15:28.039608 140583164200832 learning.py:507] global step 2211: loss = 1.9063 (0.878 sec/step)\n",
            "INFO:tensorflow:global step 2212: loss = 2.2441 (1.298 sec/step)\n",
            "I1208 00:15:29.504472 140583164200832 learning.py:507] global step 2212: loss = 2.2441 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 2213: loss = 1.5760 (0.555 sec/step)\n",
            "I1208 00:15:30.061132 140583164200832 learning.py:507] global step 2213: loss = 1.5760 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2214: loss = 2.5848 (1.839 sec/step)\n",
            "I1208 00:15:31.903414 140583164200832 learning.py:507] global step 2214: loss = 2.5848 (1.839 sec/step)\n",
            "INFO:tensorflow:global step 2215: loss = 2.2542 (0.597 sec/step)\n",
            "I1208 00:15:32.502066 140583164200832 learning.py:507] global step 2215: loss = 2.2542 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 2216: loss = 1.8490 (1.407 sec/step)\n",
            "I1208 00:15:33.983953 140583164200832 learning.py:507] global step 2216: loss = 1.8490 (1.407 sec/step)\n",
            "INFO:tensorflow:global step 2217: loss = 1.9540 (1.441 sec/step)\n",
            "I1208 00:15:35.552232 140583164200832 learning.py:507] global step 2217: loss = 1.9540 (1.441 sec/step)\n",
            "INFO:tensorflow:global step 2218: loss = 1.6469 (0.615 sec/step)\n",
            "I1208 00:15:36.560647 140583164200832 learning.py:507] global step 2218: loss = 1.6469 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2219: loss = 2.4802 (0.546 sec/step)\n",
            "I1208 00:15:37.178675 140583164200832 learning.py:507] global step 2219: loss = 2.4802 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 2220: loss = 1.9998 (1.877 sec/step)\n",
            "I1208 00:15:39.058458 140583164200832 learning.py:507] global step 2220: loss = 1.9998 (1.877 sec/step)\n",
            "INFO:tensorflow:global step 2221: loss = 1.9701 (0.521 sec/step)\n",
            "I1208 00:15:39.580845 140583164200832 learning.py:507] global step 2221: loss = 1.9701 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 2222: loss = 2.3013 (0.600 sec/step)\n",
            "I1208 00:15:40.183434 140583164200832 learning.py:507] global step 2222: loss = 2.3013 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 2223: loss = 2.6795 (1.918 sec/step)\n",
            "I1208 00:15:42.203878 140583164200832 learning.py:507] global step 2223: loss = 2.6795 (1.918 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2223.\n",
            "I1208 00:15:45.233628 140579459213056 supervisor.py:1050] Recording summary at step 2223.\n",
            "INFO:tensorflow:global step 2224: loss = 1.9684 (3.063 sec/step)\n",
            "I1208 00:15:45.268399 140583164200832 learning.py:507] global step 2224: loss = 1.9684 (3.063 sec/step)\n",
            "INFO:tensorflow:global step 2225: loss = 2.3820 (0.577 sec/step)\n",
            "I1208 00:15:45.847846 140583164200832 learning.py:507] global step 2225: loss = 2.3820 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2226: loss = 2.7605 (0.924 sec/step)\n",
            "I1208 00:15:47.151919 140583164200832 learning.py:507] global step 2226: loss = 2.7605 (0.924 sec/step)\n",
            "INFO:tensorflow:global step 2227: loss = 2.0846 (1.730 sec/step)\n",
            "I1208 00:15:49.233840 140583164200832 learning.py:507] global step 2227: loss = 2.0846 (1.730 sec/step)\n",
            "INFO:tensorflow:global step 2228: loss = 1.7852 (0.517 sec/step)\n",
            "I1208 00:15:49.752960 140583164200832 learning.py:507] global step 2228: loss = 1.7852 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 2229: loss = 2.0582 (0.564 sec/step)\n",
            "I1208 00:15:50.319615 140583164200832 learning.py:507] global step 2229: loss = 2.0582 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 2230: loss = 2.1548 (2.675 sec/step)\n",
            "I1208 00:15:52.996316 140583164200832 learning.py:507] global step 2230: loss = 2.1548 (2.675 sec/step)\n",
            "INFO:tensorflow:global step 2231: loss = 1.8592 (0.719 sec/step)\n",
            "I1208 00:15:53.734860 140583164200832 learning.py:507] global step 2231: loss = 1.8592 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 2232: loss = 2.4161 (1.805 sec/step)\n",
            "I1208 00:15:55.541784 140583164200832 learning.py:507] global step 2232: loss = 2.4161 (1.805 sec/step)\n",
            "INFO:tensorflow:global step 2233: loss = 2.0785 (0.552 sec/step)\n",
            "I1208 00:15:56.095886 140583164200832 learning.py:507] global step 2233: loss = 2.0785 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 2234: loss = 2.1996 (1.900 sec/step)\n",
            "I1208 00:15:57.997586 140583164200832 learning.py:507] global step 2234: loss = 2.1996 (1.900 sec/step)\n",
            "INFO:tensorflow:global step 2235: loss = 1.8769 (1.064 sec/step)\n",
            "I1208 00:15:59.071876 140583164200832 learning.py:507] global step 2235: loss = 1.8769 (1.064 sec/step)\n",
            "INFO:tensorflow:global step 2236: loss = 1.8208 (0.585 sec/step)\n",
            "I1208 00:15:59.658603 140583164200832 learning.py:507] global step 2236: loss = 1.8208 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2237: loss = 1.7425 (1.653 sec/step)\n",
            "I1208 00:16:01.312846 140583164200832 learning.py:507] global step 2237: loss = 1.7425 (1.653 sec/step)\n",
            "INFO:tensorflow:global step 2238: loss = 2.4478 (0.546 sec/step)\n",
            "I1208 00:16:01.859966 140583164200832 learning.py:507] global step 2238: loss = 2.4478 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 2239: loss = 1.7101 (1.637 sec/step)\n",
            "I1208 00:16:03.536541 140583164200832 learning.py:507] global step 2239: loss = 1.7101 (1.637 sec/step)\n",
            "INFO:tensorflow:global step 2240: loss = 1.6937 (1.741 sec/step)\n",
            "I1208 00:16:05.608733 140583164200832 learning.py:507] global step 2240: loss = 1.6937 (1.741 sec/step)\n",
            "INFO:tensorflow:global step 2241: loss = 2.1856 (0.747 sec/step)\n",
            "I1208 00:16:06.499709 140583164200832 learning.py:507] global step 2241: loss = 2.1856 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 2242: loss = 1.9597 (1.461 sec/step)\n",
            "I1208 00:16:08.039131 140583164200832 learning.py:507] global step 2242: loss = 1.9597 (1.461 sec/step)\n",
            "INFO:tensorflow:global step 2243: loss = 1.8388 (0.723 sec/step)\n",
            "I1208 00:16:08.868346 140583164200832 learning.py:507] global step 2243: loss = 1.8388 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 2244: loss = 1.7622 (0.577 sec/step)\n",
            "I1208 00:16:09.599784 140583164200832 learning.py:507] global step 2244: loss = 1.7622 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2245: loss = 1.7417 (1.800 sec/step)\n",
            "I1208 00:16:11.401870 140583164200832 learning.py:507] global step 2245: loss = 1.7417 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 2246: loss = 1.8112 (0.573 sec/step)\n",
            "I1208 00:16:12.143997 140583164200832 learning.py:507] global step 2246: loss = 1.8112 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 2247: loss = 1.8969 (1.881 sec/step)\n",
            "I1208 00:16:14.154944 140583164200832 learning.py:507] global step 2247: loss = 1.8969 (1.881 sec/step)\n",
            "INFO:tensorflow:global step 2248: loss = 2.2904 (0.685 sec/step)\n",
            "I1208 00:16:15.159112 140583164200832 learning.py:507] global step 2248: loss = 2.2904 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 2249: loss = 1.8493 (0.525 sec/step)\n",
            "I1208 00:16:15.855916 140583164200832 learning.py:507] global step 2249: loss = 1.8493 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 2250: loss = 1.6910 (1.819 sec/step)\n",
            "I1208 00:16:17.677186 140583164200832 learning.py:507] global step 2250: loss = 1.6910 (1.819 sec/step)\n",
            "INFO:tensorflow:global step 2251: loss = 1.9913 (0.637 sec/step)\n",
            "I1208 00:16:18.316188 140583164200832 learning.py:507] global step 2251: loss = 1.9913 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2252: loss = 1.9287 (1.919 sec/step)\n",
            "I1208 00:16:20.237050 140583164200832 learning.py:507] global step 2252: loss = 1.9287 (1.919 sec/step)\n",
            "INFO:tensorflow:global step 2253: loss = 1.6341 (0.506 sec/step)\n",
            "I1208 00:16:20.745460 140583164200832 learning.py:507] global step 2253: loss = 1.6341 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 2254: loss = 2.3911 (1.009 sec/step)\n",
            "I1208 00:16:21.983040 140583164200832 learning.py:507] global step 2254: loss = 2.3911 (1.009 sec/step)\n",
            "INFO:tensorflow:global step 2255: loss = 2.8666 (2.012 sec/step)\n",
            "I1208 00:16:24.057028 140583164200832 learning.py:507] global step 2255: loss = 2.8666 (2.012 sec/step)\n",
            "INFO:tensorflow:global step 2256: loss = 2.2792 (0.508 sec/step)\n",
            "I1208 00:16:24.567644 140583164200832 learning.py:507] global step 2256: loss = 2.2792 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 2257: loss = 1.7286 (1.664 sec/step)\n",
            "I1208 00:16:26.454426 140583164200832 learning.py:507] global step 2257: loss = 1.7286 (1.664 sec/step)\n",
            "INFO:tensorflow:global step 2258: loss = 1.7254 (0.751 sec/step)\n",
            "I1208 00:16:27.782918 140583164200832 learning.py:507] global step 2258: loss = 1.7254 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 2259: loss = 2.3801 (1.914 sec/step)\n",
            "I1208 00:16:29.756193 140583164200832 learning.py:507] global step 2259: loss = 2.3801 (1.914 sec/step)\n",
            "INFO:tensorflow:global step 2260: loss = 1.9728 (0.499 sec/step)\n",
            "I1208 00:16:30.257189 140583164200832 learning.py:507] global step 2260: loss = 1.9728 (0.499 sec/step)\n",
            "INFO:tensorflow:global step 2261: loss = 2.3130 (1.747 sec/step)\n",
            "I1208 00:16:32.005948 140583164200832 learning.py:507] global step 2261: loss = 2.3130 (1.747 sec/step)\n",
            "INFO:tensorflow:global step 2262: loss = 1.6189 (0.639 sec/step)\n",
            "I1208 00:16:32.719138 140583164200832 learning.py:507] global step 2262: loss = 1.6189 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2263: loss = 2.5157 (1.942 sec/step)\n",
            "I1208 00:16:34.888422 140583164200832 learning.py:507] global step 2263: loss = 2.5157 (1.942 sec/step)\n",
            "INFO:tensorflow:global step 2264: loss = 1.9110 (0.711 sec/step)\n",
            "I1208 00:16:35.836231 140583164200832 learning.py:507] global step 2264: loss = 1.9110 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 2265: loss = 1.8529 (0.617 sec/step)\n",
            "I1208 00:16:36.617593 140583164200832 learning.py:507] global step 2265: loss = 1.8529 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2266: loss = 1.8379 (0.947 sec/step)\n",
            "I1208 00:16:37.745983 140583164200832 learning.py:507] global step 2266: loss = 1.8379 (0.947 sec/step)\n",
            "INFO:tensorflow:global step 2267: loss = 1.7681 (1.958 sec/step)\n",
            "I1208 00:16:39.751090 140583164200832 learning.py:507] global step 2267: loss = 1.7681 (1.958 sec/step)\n",
            "INFO:tensorflow:global step 2268: loss = 2.0343 (0.576 sec/step)\n",
            "I1208 00:16:40.329343 140583164200832 learning.py:507] global step 2268: loss = 2.0343 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 2269: loss = 3.3672 (1.766 sec/step)\n",
            "I1208 00:16:42.123259 140583164200832 learning.py:507] global step 2269: loss = 3.3672 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 2270: loss = 2.7035 (1.602 sec/step)\n",
            "I1208 00:16:43.727828 140583164200832 learning.py:507] global step 2270: loss = 2.7035 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 2271: loss = 2.3778 (0.597 sec/step)\n",
            "I1208 00:16:44.695759 140583164200832 learning.py:507] global step 2271: loss = 2.3778 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 2272: loss = 2.3227 (1.535 sec/step)\n",
            "I1208 00:16:46.374284 140583164200832 learning.py:507] global step 2272: loss = 2.3227 (1.535 sec/step)\n",
            "INFO:tensorflow:global step 2273: loss = 1.8313 (0.680 sec/step)\n",
            "I1208 00:16:47.264971 140583164200832 learning.py:507] global step 2273: loss = 1.8313 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2274: loss = 1.9053 (0.533 sec/step)\n",
            "I1208 00:16:48.018239 140583164200832 learning.py:507] global step 2274: loss = 1.9053 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 2275: loss = 1.4946 (0.799 sec/step)\n",
            "I1208 00:16:49.237654 140583164200832 learning.py:507] global step 2275: loss = 1.4946 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2276: loss = 2.5659 (1.702 sec/step)\n",
            "I1208 00:16:51.148010 140583164200832 learning.py:507] global step 2276: loss = 2.5659 (1.702 sec/step)\n",
            "INFO:tensorflow:global step 2277: loss = 2.6657 (0.611 sec/step)\n",
            "I1208 00:16:51.760881 140583164200832 learning.py:507] global step 2277: loss = 2.6657 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2278: loss = 1.8773 (2.018 sec/step)\n",
            "I1208 00:16:53.780750 140583164200832 learning.py:507] global step 2278: loss = 1.8773 (2.018 sec/step)\n",
            "INFO:tensorflow:global step 2279: loss = 1.8839 (0.658 sec/step)\n",
            "I1208 00:16:54.754878 140583164200832 learning.py:507] global step 2279: loss = 1.8839 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2280: loss = 1.8775 (1.450 sec/step)\n",
            "I1208 00:16:56.395612 140583164200832 learning.py:507] global step 2280: loss = 1.8775 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 2281: loss = 2.1983 (0.548 sec/step)\n",
            "I1208 00:16:56.945397 140583164200832 learning.py:507] global step 2281: loss = 2.1983 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 2282: loss = 1.8401 (1.959 sec/step)\n",
            "I1208 00:16:58.906155 140583164200832 learning.py:507] global step 2282: loss = 1.8401 (1.959 sec/step)\n",
            "INFO:tensorflow:global step 2283: loss = 1.4390 (0.686 sec/step)\n",
            "I1208 00:16:59.950659 140583164200832 learning.py:507] global step 2283: loss = 1.4390 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 2284: loss = 1.8098 (1.336 sec/step)\n",
            "I1208 00:17:01.350049 140583164200832 learning.py:507] global step 2284: loss = 1.8098 (1.336 sec/step)\n",
            "INFO:tensorflow:global step 2285: loss = 1.8460 (0.670 sec/step)\n",
            "I1208 00:17:02.329382 140583164200832 learning.py:507] global step 2285: loss = 1.8460 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 2286: loss = 3.0760 (0.644 sec/step)\n",
            "I1208 00:17:03.205641 140583164200832 learning.py:507] global step 2286: loss = 3.0760 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2287: loss = 2.0753 (0.678 sec/step)\n",
            "I1208 00:17:04.413151 140583164200832 learning.py:507] global step 2287: loss = 2.0753 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2288: loss = 1.6666 (0.624 sec/step)\n",
            "I1208 00:17:05.192475 140583164200832 learning.py:507] global step 2288: loss = 1.6666 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2289: loss = 1.7567 (1.713 sec/step)\n",
            "I1208 00:17:06.907308 140583164200832 learning.py:507] global step 2289: loss = 1.7567 (1.713 sec/step)\n",
            "INFO:tensorflow:global step 2290: loss = 1.9805 (0.671 sec/step)\n",
            "I1208 00:17:07.854921 140583164200832 learning.py:507] global step 2290: loss = 1.9805 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 2291: loss = 2.0271 (1.380 sec/step)\n",
            "I1208 00:17:09.245707 140583164200832 learning.py:507] global step 2291: loss = 2.0271 (1.380 sec/step)\n",
            "INFO:tensorflow:global step 2292: loss = 1.8741 (0.512 sec/step)\n",
            "I1208 00:17:09.760006 140583164200832 learning.py:507] global step 2292: loss = 1.8741 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 2293: loss = 2.2599 (0.981 sec/step)\n",
            "I1208 00:17:11.067164 140583164200832 learning.py:507] global step 2293: loss = 2.2599 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 2294: loss = 2.2024 (1.636 sec/step)\n",
            "I1208 00:17:12.715499 140583164200832 learning.py:507] global step 2294: loss = 2.2024 (1.636 sec/step)\n",
            "INFO:tensorflow:global step 2295: loss = 2.2489 (0.721 sec/step)\n",
            "I1208 00:17:13.716996 140583164200832 learning.py:507] global step 2295: loss = 2.2489 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 2296: loss = 2.0365 (0.614 sec/step)\n",
            "I1208 00:17:14.395839 140583164200832 learning.py:507] global step 2296: loss = 2.0365 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2297: loss = 2.0256 (1.801 sec/step)\n",
            "I1208 00:17:16.198797 140583164200832 learning.py:507] global step 2297: loss = 2.0256 (1.801 sec/step)\n",
            "INFO:tensorflow:global step 2298: loss = 1.8148 (0.484 sec/step)\n",
            "I1208 00:17:16.684356 140583164200832 learning.py:507] global step 2298: loss = 1.8148 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 2299: loss = 2.0933 (1.747 sec/step)\n",
            "I1208 00:17:18.434373 140583164200832 learning.py:507] global step 2299: loss = 2.0933 (1.747 sec/step)\n",
            "INFO:tensorflow:global step 2300: loss = 1.7398 (0.569 sec/step)\n",
            "I1208 00:17:19.005321 140583164200832 learning.py:507] global step 2300: loss = 1.7398 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 2301: loss = 1.7554 (1.784 sec/step)\n",
            "I1208 00:17:20.791247 140583164200832 learning.py:507] global step 2301: loss = 1.7554 (1.784 sec/step)\n",
            "INFO:tensorflow:global step 2302: loss = 2.0327 (0.584 sec/step)\n",
            "I1208 00:17:21.739198 140583164200832 learning.py:507] global step 2302: loss = 2.0327 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 2303: loss = 1.5223 (1.234 sec/step)\n",
            "I1208 00:17:23.148400 140583164200832 learning.py:507] global step 2303: loss = 1.5223 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2304: loss = 2.3485 (0.674 sec/step)\n",
            "I1208 00:17:24.088001 140583164200832 learning.py:507] global step 2304: loss = 2.3485 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2305: loss = 2.5113 (1.257 sec/step)\n",
            "I1208 00:17:25.399700 140583164200832 learning.py:507] global step 2305: loss = 2.5113 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2306: loss = 2.3112 (0.585 sec/step)\n",
            "I1208 00:17:26.075605 140583164200832 learning.py:507] global step 2306: loss = 2.3112 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2307: loss = 1.6826 (1.599 sec/step)\n",
            "I1208 00:17:27.757205 140583164200832 learning.py:507] global step 2307: loss = 1.6826 (1.599 sec/step)\n",
            "INFO:tensorflow:global step 2308: loss = 2.2261 (1.183 sec/step)\n",
            "I1208 00:17:28.941679 140583164200832 learning.py:507] global step 2308: loss = 2.2261 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 2309: loss = 1.6010 (0.496 sec/step)\n",
            "I1208 00:17:29.439349 140583164200832 learning.py:507] global step 2309: loss = 1.6010 (0.496 sec/step)\n",
            "INFO:tensorflow:global step 2310: loss = 2.0648 (1.712 sec/step)\n",
            "I1208 00:17:31.153532 140583164200832 learning.py:507] global step 2310: loss = 2.0648 (1.712 sec/step)\n",
            "INFO:tensorflow:global step 2311: loss = 1.7269 (0.681 sec/step)\n",
            "I1208 00:17:31.960121 140583164200832 learning.py:507] global step 2311: loss = 1.7269 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2312: loss = 1.7118 (1.243 sec/step)\n",
            "I1208 00:17:33.394600 140583164200832 learning.py:507] global step 2312: loss = 1.7118 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2313: loss = 2.1260 (0.646 sec/step)\n",
            "I1208 00:17:34.176128 140583164200832 learning.py:507] global step 2313: loss = 2.1260 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2314: loss = 2.1713 (1.463 sec/step)\n",
            "I1208 00:17:35.673873 140583164200832 learning.py:507] global step 2314: loss = 2.1713 (1.463 sec/step)\n",
            "INFO:tensorflow:global step 2315: loss = 2.2296 (0.675 sec/step)\n",
            "I1208 00:17:36.579622 140583164200832 learning.py:507] global step 2315: loss = 2.2296 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 2316: loss = 1.9914 (1.166 sec/step)\n",
            "I1208 00:17:37.904494 140583164200832 learning.py:507] global step 2316: loss = 1.9914 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 2317: loss = 1.7300 (0.614 sec/step)\n",
            "I1208 00:17:38.721233 140583164200832 learning.py:507] global step 2317: loss = 1.7300 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2318: loss = 2.5829 (1.280 sec/step)\n",
            "I1208 00:17:40.048359 140583164200832 learning.py:507] global step 2318: loss = 2.5829 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2319: loss = 2.1805 (0.585 sec/step)\n",
            "I1208 00:17:40.636319 140583164200832 learning.py:507] global step 2319: loss = 2.1805 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2320: loss = 2.2847 (1.779 sec/step)\n",
            "I1208 00:17:42.944040 140583164200832 learning.py:507] global step 2320: loss = 2.2847 (1.779 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2320.\n",
            "I1208 00:17:44.691199 140579459213056 supervisor.py:1050] Recording summary at step 2320.\n",
            "INFO:tensorflow:global step 2321: loss = 2.7420 (1.619 sec/step)\n",
            "I1208 00:17:44.921989 140583164200832 learning.py:507] global step 2321: loss = 2.7420 (1.619 sec/step)\n",
            "INFO:tensorflow:global step 2322: loss = 2.2721 (0.731 sec/step)\n",
            "I1208 00:17:45.683011 140583164200832 learning.py:507] global step 2322: loss = 2.2721 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 2323: loss = 1.8920 (1.423 sec/step)\n",
            "I1208 00:17:47.277719 140583164200832 learning.py:507] global step 2323: loss = 1.8920 (1.423 sec/step)\n",
            "INFO:tensorflow:global step 2324: loss = 2.3408 (0.468 sec/step)\n",
            "I1208 00:17:47.746946 140583164200832 learning.py:507] global step 2324: loss = 2.3408 (0.468 sec/step)\n",
            "INFO:tensorflow:global step 2325: loss = 2.6957 (1.737 sec/step)\n",
            "I1208 00:17:49.485687 140583164200832 learning.py:507] global step 2325: loss = 2.6957 (1.737 sec/step)\n",
            "INFO:tensorflow:global step 2326: loss = 2.1074 (0.728 sec/step)\n",
            "I1208 00:17:50.470278 140583164200832 learning.py:507] global step 2326: loss = 2.1074 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 2327: loss = 1.8200 (0.598 sec/step)\n",
            "I1208 00:17:51.288010 140583164200832 learning.py:507] global step 2327: loss = 1.8200 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2328: loss = 1.8553 (1.504 sec/step)\n",
            "I1208 00:17:52.878394 140583164200832 learning.py:507] global step 2328: loss = 1.8553 (1.504 sec/step)\n",
            "INFO:tensorflow:global step 2329: loss = 1.7439 (1.106 sec/step)\n",
            "I1208 00:17:53.985769 140583164200832 learning.py:507] global step 2329: loss = 1.7439 (1.106 sec/step)\n",
            "INFO:tensorflow:global step 2330: loss = 1.8443 (0.557 sec/step)\n",
            "I1208 00:17:54.663646 140583164200832 learning.py:507] global step 2330: loss = 1.8443 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 2331: loss = 2.2193 (1.448 sec/step)\n",
            "I1208 00:17:56.308445 140583164200832 learning.py:507] global step 2331: loss = 2.2193 (1.448 sec/step)\n",
            "INFO:tensorflow:global step 2332: loss = 1.9119 (0.549 sec/step)\n",
            "I1208 00:17:57.103873 140583164200832 learning.py:507] global step 2332: loss = 1.9119 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 2333: loss = 2.0996 (0.525 sec/step)\n",
            "I1208 00:17:57.907604 140583164200832 learning.py:507] global step 2333: loss = 2.0996 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 2334: loss = 1.9149 (1.691 sec/step)\n",
            "I1208 00:17:59.600728 140583164200832 learning.py:507] global step 2334: loss = 1.9149 (1.691 sec/step)\n",
            "INFO:tensorflow:global step 2335: loss = 2.3456 (0.568 sec/step)\n",
            "I1208 00:18:00.385423 140583164200832 learning.py:507] global step 2335: loss = 2.3456 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 2336: loss = 2.1616 (1.326 sec/step)\n",
            "I1208 00:18:01.911328 140583164200832 learning.py:507] global step 2336: loss = 2.1616 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 2337: loss = 2.3774 (0.648 sec/step)\n",
            "I1208 00:18:02.699732 140583164200832 learning.py:507] global step 2337: loss = 2.3774 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2338: loss = 1.8060 (1.288 sec/step)\n",
            "I1208 00:18:04.218068 140583164200832 learning.py:507] global step 2338: loss = 1.8060 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 2339: loss = 1.9991 (0.634 sec/step)\n",
            "I1208 00:18:05.104558 140583164200832 learning.py:507] global step 2339: loss = 1.9991 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2340: loss = 2.1936 (1.116 sec/step)\n",
            "I1208 00:18:06.405711 140583164200832 learning.py:507] global step 2340: loss = 2.1936 (1.116 sec/step)\n",
            "INFO:tensorflow:global step 2341: loss = 2.2128 (0.910 sec/step)\n",
            "I1208 00:18:07.336498 140583164200832 learning.py:507] global step 2341: loss = 2.2128 (0.910 sec/step)\n",
            "INFO:tensorflow:global step 2342: loss = 2.5705 (1.404 sec/step)\n",
            "I1208 00:18:08.742280 140583164200832 learning.py:507] global step 2342: loss = 2.5705 (1.404 sec/step)\n",
            "INFO:tensorflow:global step 2343: loss = 2.1700 (0.555 sec/step)\n",
            "I1208 00:18:09.634172 140583164200832 learning.py:507] global step 2343: loss = 2.1700 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2344: loss = 2.2117 (0.553 sec/step)\n",
            "I1208 00:18:10.341748 140583164200832 learning.py:507] global step 2344: loss = 2.2117 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 2345: loss = 2.6391 (0.916 sec/step)\n",
            "I1208 00:18:11.360102 140583164200832 learning.py:507] global step 2345: loss = 2.6391 (0.916 sec/step)\n",
            "INFO:tensorflow:global step 2346: loss = 2.2238 (1.698 sec/step)\n",
            "I1208 00:18:13.158162 140583164200832 learning.py:507] global step 2346: loss = 2.2238 (1.698 sec/step)\n",
            "INFO:tensorflow:global step 2347: loss = 1.7369 (0.495 sec/step)\n",
            "I1208 00:18:13.654917 140583164200832 learning.py:507] global step 2347: loss = 1.7369 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 2348: loss = 1.8876 (1.624 sec/step)\n",
            "I1208 00:18:15.281396 140583164200832 learning.py:507] global step 2348: loss = 1.8876 (1.624 sec/step)\n",
            "INFO:tensorflow:global step 2349: loss = 1.7232 (0.476 sec/step)\n",
            "I1208 00:18:15.759878 140583164200832 learning.py:507] global step 2349: loss = 1.7232 (0.476 sec/step)\n",
            "INFO:tensorflow:global step 2350: loss = 2.5227 (1.696 sec/step)\n",
            "I1208 00:18:17.458783 140583164200832 learning.py:507] global step 2350: loss = 2.5227 (1.696 sec/step)\n",
            "INFO:tensorflow:global step 2351: loss = 1.7950 (0.576 sec/step)\n",
            "I1208 00:18:18.037448 140583164200832 learning.py:507] global step 2351: loss = 1.7950 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 2352: loss = 2.2833 (1.783 sec/step)\n",
            "I1208 00:18:19.822731 140583164200832 learning.py:507] global step 2352: loss = 2.2833 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 2353: loss = 3.0510 (0.537 sec/step)\n",
            "I1208 00:18:20.361897 140583164200832 learning.py:507] global step 2353: loss = 3.0510 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 2354: loss = 2.1942 (1.595 sec/step)\n",
            "I1208 00:18:21.958686 140583164200832 learning.py:507] global step 2354: loss = 2.1942 (1.595 sec/step)\n",
            "INFO:tensorflow:global step 2355: loss = 2.3817 (1.300 sec/step)\n",
            "I1208 00:18:23.260500 140583164200832 learning.py:507] global step 2355: loss = 2.3817 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 2356: loss = 1.6588 (0.497 sec/step)\n",
            "I1208 00:18:23.758838 140583164200832 learning.py:507] global step 2356: loss = 1.6588 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 2357: loss = 2.0168 (1.763 sec/step)\n",
            "I1208 00:18:25.523548 140583164200832 learning.py:507] global step 2357: loss = 2.0168 (1.763 sec/step)\n",
            "INFO:tensorflow:global step 2358: loss = 2.1761 (0.680 sec/step)\n",
            "I1208 00:18:26.478433 140583164200832 learning.py:507] global step 2358: loss = 2.1761 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2359: loss = 2.8917 (0.721 sec/step)\n",
            "I1208 00:18:27.472950 140583164200832 learning.py:507] global step 2359: loss = 2.8917 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 2360: loss = 2.1783 (0.618 sec/step)\n",
            "I1208 00:18:28.223238 140583164200832 learning.py:507] global step 2360: loss = 2.1783 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2361: loss = 2.0173 (1.768 sec/step)\n",
            "I1208 00:18:29.992943 140583164200832 learning.py:507] global step 2361: loss = 2.0173 (1.768 sec/step)\n",
            "INFO:tensorflow:global step 2362: loss = 2.1496 (0.566 sec/step)\n",
            "I1208 00:18:30.839784 140583164200832 learning.py:507] global step 2362: loss = 2.1496 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2363: loss = 1.6074 (1.500 sec/step)\n",
            "I1208 00:18:32.468617 140583164200832 learning.py:507] global step 2363: loss = 1.6074 (1.500 sec/step)\n",
            "INFO:tensorflow:global step 2364: loss = 2.2466 (0.710 sec/step)\n",
            "I1208 00:18:33.450151 140583164200832 learning.py:507] global step 2364: loss = 2.2466 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 2365: loss = 1.9805 (1.347 sec/step)\n",
            "I1208 00:18:34.846945 140583164200832 learning.py:507] global step 2365: loss = 1.9805 (1.347 sec/step)\n",
            "INFO:tensorflow:global step 2366: loss = 1.8667 (0.585 sec/step)\n",
            "I1208 00:18:35.435739 140583164200832 learning.py:507] global step 2366: loss = 1.8667 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2367: loss = 2.3239 (1.526 sec/step)\n",
            "I1208 00:18:37.031124 140583164200832 learning.py:507] global step 2367: loss = 2.3239 (1.526 sec/step)\n",
            "INFO:tensorflow:global step 2368: loss = 2.0069 (0.674 sec/step)\n",
            "I1208 00:18:37.800256 140583164200832 learning.py:507] global step 2368: loss = 2.0069 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2369: loss = 2.2372 (0.659 sec/step)\n",
            "I1208 00:18:38.721103 140583164200832 learning.py:507] global step 2369: loss = 2.2372 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2370: loss = 2.1031 (1.673 sec/step)\n",
            "I1208 00:18:40.396445 140583164200832 learning.py:507] global step 2370: loss = 2.1031 (1.673 sec/step)\n",
            "INFO:tensorflow:global step 2371: loss = 2.0274 (0.550 sec/step)\n",
            "I1208 00:18:40.948734 140583164200832 learning.py:507] global step 2371: loss = 2.0274 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 2372: loss = 2.0349 (1.745 sec/step)\n",
            "I1208 00:18:42.695392 140583164200832 learning.py:507] global step 2372: loss = 2.0349 (1.745 sec/step)\n",
            "INFO:tensorflow:global step 2373: loss = 1.8866 (0.687 sec/step)\n",
            "I1208 00:18:43.692047 140583164200832 learning.py:507] global step 2373: loss = 1.8866 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 2374: loss = 2.0390 (1.508 sec/step)\n",
            "I1208 00:18:45.205404 140583164200832 learning.py:507] global step 2374: loss = 2.0390 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 2375: loss = 1.4809 (0.547 sec/step)\n",
            "I1208 00:18:46.113034 140583164200832 learning.py:507] global step 2375: loss = 1.4809 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 2376: loss = 1.6194 (0.694 sec/step)\n",
            "I1208 00:18:47.146016 140583164200832 learning.py:507] global step 2376: loss = 1.6194 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 2377: loss = 1.6073 (0.689 sec/step)\n",
            "I1208 00:18:48.176230 140583164200832 learning.py:507] global step 2377: loss = 1.6073 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 2378: loss = 1.6180 (0.658 sec/step)\n",
            "I1208 00:18:49.273386 140583164200832 learning.py:507] global step 2378: loss = 1.6180 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2379: loss = 2.8476 (0.491 sec/step)\n",
            "I1208 00:18:49.886576 140583164200832 learning.py:507] global step 2379: loss = 2.8476 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 2380: loss = 1.9094 (1.657 sec/step)\n",
            "I1208 00:18:51.598729 140583164200832 learning.py:507] global step 2380: loss = 1.9094 (1.657 sec/step)\n",
            "INFO:tensorflow:global step 2381: loss = 2.0410 (0.586 sec/step)\n",
            "I1208 00:18:52.433599 140583164200832 learning.py:507] global step 2381: loss = 2.0410 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 2382: loss = 2.0969 (0.577 sec/step)\n",
            "I1208 00:18:53.570958 140583164200832 learning.py:507] global step 2382: loss = 2.0969 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2383: loss = 2.3290 (2.673 sec/step)\n",
            "I1208 00:18:56.440770 140583164200832 learning.py:507] global step 2383: loss = 2.3290 (2.673 sec/step)\n",
            "INFO:tensorflow:global step 2384: loss = 1.7598 (0.589 sec/step)\n",
            "I1208 00:18:57.406082 140583164200832 learning.py:507] global step 2384: loss = 1.7598 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 2385: loss = 1.8263 (0.743 sec/step)\n",
            "I1208 00:18:58.412728 140583164200832 learning.py:507] global step 2385: loss = 1.8263 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2386: loss = 2.1119 (1.550 sec/step)\n",
            "I1208 00:19:00.021997 140583164200832 learning.py:507] global step 2386: loss = 2.1119 (1.550 sec/step)\n",
            "INFO:tensorflow:global step 2387: loss = 2.4887 (1.138 sec/step)\n",
            "I1208 00:19:01.161682 140583164200832 learning.py:507] global step 2387: loss = 2.4887 (1.138 sec/step)\n",
            "INFO:tensorflow:global step 2388: loss = 2.0877 (0.639 sec/step)\n",
            "I1208 00:19:02.115800 140583164200832 learning.py:507] global step 2388: loss = 2.0877 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2389: loss = 2.8511 (1.515 sec/step)\n",
            "I1208 00:19:03.665258 140583164200832 learning.py:507] global step 2389: loss = 2.8511 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 2390: loss = 1.8241 (0.722 sec/step)\n",
            "I1208 00:19:04.523823 140583164200832 learning.py:507] global step 2390: loss = 1.8241 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2391: loss = 2.0176 (0.611 sec/step)\n",
            "I1208 00:19:05.283298 140583164200832 learning.py:507] global step 2391: loss = 2.0176 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2392: loss = 1.8761 (2.074 sec/step)\n",
            "I1208 00:19:07.359006 140583164200832 learning.py:507] global step 2392: loss = 1.8761 (2.074 sec/step)\n",
            "INFO:tensorflow:global step 2393: loss = 2.1372 (0.500 sec/step)\n",
            "I1208 00:19:07.861152 140583164200832 learning.py:507] global step 2393: loss = 2.1372 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 2394: loss = 2.1621 (1.121 sec/step)\n",
            "I1208 00:19:09.016261 140583164200832 learning.py:507] global step 2394: loss = 2.1621 (1.121 sec/step)\n",
            "INFO:tensorflow:global step 2395: loss = 2.6734 (1.913 sec/step)\n",
            "I1208 00:19:11.179698 140583164200832 learning.py:507] global step 2395: loss = 2.6734 (1.913 sec/step)\n",
            "INFO:tensorflow:global step 2396: loss = 1.9154 (0.567 sec/step)\n",
            "I1208 00:19:11.749219 140583164200832 learning.py:507] global step 2396: loss = 1.9154 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 2397: loss = 2.2828 (1.740 sec/step)\n",
            "I1208 00:19:13.491011 140583164200832 learning.py:507] global step 2397: loss = 2.2828 (1.740 sec/step)\n",
            "INFO:tensorflow:global step 2398: loss = 2.1283 (0.953 sec/step)\n",
            "I1208 00:19:14.705203 140583164200832 learning.py:507] global step 2398: loss = 2.1283 (0.953 sec/step)\n",
            "INFO:tensorflow:global step 2399: loss = 1.9048 (0.592 sec/step)\n",
            "I1208 00:19:15.298986 140583164200832 learning.py:507] global step 2399: loss = 1.9048 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 2400: loss = 1.6867 (1.868 sec/step)\n",
            "I1208 00:19:17.460671 140583164200832 learning.py:507] global step 2400: loss = 1.6867 (1.868 sec/step)\n",
            "INFO:tensorflow:global step 2401: loss = 2.1275 (0.550 sec/step)\n",
            "I1208 00:19:18.019287 140583164200832 learning.py:507] global step 2401: loss = 2.1275 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 2402: loss = 1.8363 (2.022 sec/step)\n",
            "I1208 00:19:20.043251 140583164200832 learning.py:507] global step 2402: loss = 1.8363 (2.022 sec/step)\n",
            "INFO:tensorflow:global step 2403: loss = 2.2125 (0.568 sec/step)\n",
            "I1208 00:19:20.612890 140583164200832 learning.py:507] global step 2403: loss = 2.2125 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 2404: loss = 2.8192 (0.966 sec/step)\n",
            "I1208 00:19:21.617421 140583164200832 learning.py:507] global step 2404: loss = 2.8192 (0.966 sec/step)\n",
            "INFO:tensorflow:global step 2405: loss = 1.8766 (2.028 sec/step)\n",
            "I1208 00:19:23.932492 140583164200832 learning.py:507] global step 2405: loss = 1.8766 (2.028 sec/step)\n",
            "INFO:tensorflow:global step 2406: loss = 1.8854 (0.570 sec/step)\n",
            "I1208 00:19:24.504904 140583164200832 learning.py:507] global step 2406: loss = 1.8854 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 2407: loss = 1.5728 (1.816 sec/step)\n",
            "I1208 00:19:26.322955 140583164200832 learning.py:507] global step 2407: loss = 1.5728 (1.816 sec/step)\n",
            "INFO:tensorflow:global step 2408: loss = 2.3026 (0.553 sec/step)\n",
            "I1208 00:19:26.878343 140583164200832 learning.py:507] global step 2408: loss = 2.3026 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 2409: loss = 2.3416 (1.022 sec/step)\n",
            "I1208 00:19:27.939101 140583164200832 learning.py:507] global step 2409: loss = 2.3416 (1.022 sec/step)\n",
            "INFO:tensorflow:global step 2410: loss = 2.0613 (2.348 sec/step)\n",
            "I1208 00:19:30.385372 140583164200832 learning.py:507] global step 2410: loss = 2.0613 (2.348 sec/step)\n",
            "INFO:tensorflow:global step 2411: loss = 2.1163 (0.566 sec/step)\n",
            "I1208 00:19:30.952769 140583164200832 learning.py:507] global step 2411: loss = 2.1163 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2412: loss = 2.1368 (1.875 sec/step)\n",
            "I1208 00:19:32.829838 140583164200832 learning.py:507] global step 2412: loss = 2.1368 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 2413: loss = 1.8290 (0.581 sec/step)\n",
            "I1208 00:19:33.412937 140583164200832 learning.py:507] global step 2413: loss = 1.8290 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 2414: loss = 2.0288 (1.207 sec/step)\n",
            "I1208 00:19:34.933218 140583164200832 learning.py:507] global step 2414: loss = 2.0288 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2415: loss = 1.6738 (0.689 sec/step)\n",
            "I1208 00:19:35.780323 140583164200832 learning.py:507] global step 2415: loss = 1.6738 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 2416: loss = 1.6200 (0.681 sec/step)\n",
            "I1208 00:19:36.651968 140583164200832 learning.py:507] global step 2416: loss = 1.6200 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2417: loss = 1.7961 (2.139 sec/step)\n",
            "I1208 00:19:39.037722 140583164200832 learning.py:507] global step 2417: loss = 1.7961 (2.139 sec/step)\n",
            "INFO:tensorflow:global step 2418: loss = 2.3382 (0.671 sec/step)\n",
            "I1208 00:19:39.869523 140583164200832 learning.py:507] global step 2418: loss = 2.3382 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 2419: loss = 1.5645 (0.631 sec/step)\n",
            "I1208 00:19:41.055558 140583164200832 learning.py:507] global step 2419: loss = 1.5645 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2420: loss = 1.6067 (0.567 sec/step)\n",
            "I1208 00:19:42.050800 140583164200832 learning.py:507] global step 2420: loss = 1.6067 (0.567 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:19:42.076596 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W1208 00:19:42.477085 140579434034944 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global step 2421: loss = 2.4654 (2.127 sec/step)\n",
            "I1208 00:19:45.187583 140583164200832 learning.py:507] global step 2421: loss = 2.4654 (2.127 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2421.\n",
            "I1208 00:19:45.192359 140579459213056 supervisor.py:1050] Recording summary at step 2421.\n",
            "INFO:tensorflow:global step 2422: loss = 2.1711 (1.340 sec/step)\n",
            "I1208 00:19:46.808760 140583164200832 learning.py:507] global step 2422: loss = 2.1711 (1.340 sec/step)\n",
            "INFO:tensorflow:global step 2423: loss = 1.7764 (0.602 sec/step)\n",
            "I1208 00:19:47.525177 140583164200832 learning.py:507] global step 2423: loss = 1.7764 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2424: loss = 1.6913 (1.550 sec/step)\n",
            "I1208 00:19:49.359539 140583164200832 learning.py:507] global step 2424: loss = 1.6913 (1.550 sec/step)\n",
            "INFO:tensorflow:global step 2425: loss = 2.0110 (0.585 sec/step)\n",
            "I1208 00:19:49.946596 140583164200832 learning.py:507] global step 2425: loss = 2.0110 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2426: loss = 2.1529 (0.656 sec/step)\n",
            "I1208 00:19:50.604973 140583164200832 learning.py:507] global step 2426: loss = 2.1529 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2427: loss = 2.2050 (2.113 sec/step)\n",
            "I1208 00:19:52.762974 140583164200832 learning.py:507] global step 2427: loss = 2.2050 (2.113 sec/step)\n",
            "INFO:tensorflow:global step 2428: loss = 2.5436 (2.164 sec/step)\n",
            "I1208 00:19:54.945691 140583164200832 learning.py:507] global step 2428: loss = 2.5436 (2.164 sec/step)\n",
            "INFO:tensorflow:global step 2429: loss = 2.2703 (0.668 sec/step)\n",
            "I1208 00:19:55.846428 140583164200832 learning.py:507] global step 2429: loss = 2.2703 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 2430: loss = 2.3889 (1.880 sec/step)\n",
            "I1208 00:19:57.750459 140583164200832 learning.py:507] global step 2430: loss = 2.3889 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 2431: loss = 2.0316 (0.642 sec/step)\n",
            "I1208 00:19:58.541826 140583164200832 learning.py:507] global step 2431: loss = 2.0316 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 2432: loss = 2.0507 (1.465 sec/step)\n",
            "I1208 00:20:00.169342 140583164200832 learning.py:507] global step 2432: loss = 2.0507 (1.465 sec/step)\n",
            "INFO:tensorflow:global step 2433: loss = 2.1254 (0.582 sec/step)\n",
            "I1208 00:20:01.045991 140583164200832 learning.py:507] global step 2433: loss = 2.1254 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 2434: loss = 2.1814 (0.766 sec/step)\n",
            "I1208 00:20:02.066146 140583164200832 learning.py:507] global step 2434: loss = 2.1814 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 2435: loss = 2.1045 (0.598 sec/step)\n",
            "I1208 00:20:03.454153 140583164200832 learning.py:507] global step 2435: loss = 2.1045 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2436: loss = 1.9747 (0.797 sec/step)\n",
            "I1208 00:20:04.574492 140583164200832 learning.py:507] global step 2436: loss = 1.9747 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 2437: loss = 1.4133 (1.773 sec/step)\n",
            "I1208 00:20:06.372960 140583164200832 learning.py:507] global step 2437: loss = 1.4133 (1.773 sec/step)\n",
            "INFO:tensorflow:global step 2438: loss = 1.8371 (0.640 sec/step)\n",
            "I1208 00:20:07.014747 140583164200832 learning.py:507] global step 2438: loss = 1.8371 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2439: loss = 1.8027 (2.199 sec/step)\n",
            "I1208 00:20:09.215493 140583164200832 learning.py:507] global step 2439: loss = 1.8027 (2.199 sec/step)\n",
            "INFO:tensorflow:global step 2440: loss = 1.9518 (0.641 sec/step)\n",
            "I1208 00:20:10.149281 140583164200832 learning.py:507] global step 2440: loss = 1.9518 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2441: loss = 1.9589 (0.730 sec/step)\n",
            "I1208 00:20:11.348913 140583164200832 learning.py:507] global step 2441: loss = 1.9589 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 2442: loss = 1.3433 (0.627 sec/step)\n",
            "I1208 00:20:12.190973 140583164200832 learning.py:507] global step 2442: loss = 1.3433 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2443: loss = 2.4102 (0.636 sec/step)\n",
            "I1208 00:20:13.404738 140583164200832 learning.py:507] global step 2443: loss = 2.4102 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2444: loss = 1.7367 (1.130 sec/step)\n",
            "I1208 00:20:14.629052 140583164200832 learning.py:507] global step 2444: loss = 1.7367 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 2445: loss = 2.1722 (0.637 sec/step)\n",
            "I1208 00:20:15.469213 140583164200832 learning.py:507] global step 2445: loss = 2.1722 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2446: loss = 2.1966 (1.224 sec/step)\n",
            "I1208 00:20:16.819060 140583164200832 learning.py:507] global step 2446: loss = 2.1966 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2447: loss = 2.3336 (0.649 sec/step)\n",
            "I1208 00:20:17.516494 140583164200832 learning.py:507] global step 2447: loss = 2.3336 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2448: loss = 1.6834 (1.013 sec/step)\n",
            "I1208 00:20:18.762295 140583164200832 learning.py:507] global step 2448: loss = 1.6834 (1.013 sec/step)\n",
            "INFO:tensorflow:global step 2449: loss = 2.2017 (0.662 sec/step)\n",
            "I1208 00:20:19.479954 140583164200832 learning.py:507] global step 2449: loss = 2.2017 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2450: loss = 1.7457 (1.534 sec/step)\n",
            "I1208 00:20:21.051290 140583164200832 learning.py:507] global step 2450: loss = 1.7457 (1.534 sec/step)\n",
            "INFO:tensorflow:global step 2451: loss = 1.9860 (0.550 sec/step)\n",
            "I1208 00:20:21.864967 140583164200832 learning.py:507] global step 2451: loss = 1.9860 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 2452: loss = 2.2473 (1.164 sec/step)\n",
            "I1208 00:20:23.193394 140583164200832 learning.py:507] global step 2452: loss = 2.2473 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 2453: loss = 1.6268 (1.036 sec/step)\n",
            "I1208 00:20:24.230789 140583164200832 learning.py:507] global step 2453: loss = 1.6268 (1.036 sec/step)\n",
            "INFO:tensorflow:global step 2454: loss = 2.0156 (0.648 sec/step)\n",
            "I1208 00:20:25.044442 140583164200832 learning.py:507] global step 2454: loss = 2.0156 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2455: loss = 1.8913 (1.247 sec/step)\n",
            "I1208 00:20:26.496418 140583164200832 learning.py:507] global step 2455: loss = 1.8913 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2456: loss = 1.6749 (0.654 sec/step)\n",
            "I1208 00:20:27.390317 140583164200832 learning.py:507] global step 2456: loss = 1.6749 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2457: loss = 1.3276 (0.627 sec/step)\n",
            "I1208 00:20:28.346482 140583164200832 learning.py:507] global step 2457: loss = 1.3276 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2458: loss = 1.8458 (0.749 sec/step)\n",
            "I1208 00:20:29.283488 140583164200832 learning.py:507] global step 2458: loss = 1.8458 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 2459: loss = 1.6397 (1.594 sec/step)\n",
            "I1208 00:20:30.879052 140583164200832 learning.py:507] global step 2459: loss = 1.6397 (1.594 sec/step)\n",
            "INFO:tensorflow:global step 2460: loss = 2.1134 (0.566 sec/step)\n",
            "I1208 00:20:31.764987 140583164200832 learning.py:507] global step 2460: loss = 2.1134 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2461: loss = 2.5120 (0.528 sec/step)\n",
            "I1208 00:20:32.485592 140583164200832 learning.py:507] global step 2461: loss = 2.5120 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 2462: loss = 2.1324 (1.635 sec/step)\n",
            "I1208 00:20:34.122313 140583164200832 learning.py:507] global step 2462: loss = 2.1324 (1.635 sec/step)\n",
            "INFO:tensorflow:global step 2463: loss = 2.1551 (0.786 sec/step)\n",
            "I1208 00:20:34.997344 140583164200832 learning.py:507] global step 2463: loss = 2.1551 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 2464: loss = 2.0320 (0.675 sec/step)\n",
            "I1208 00:20:35.887578 140583164200832 learning.py:507] global step 2464: loss = 2.0320 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 2465: loss = 1.7009 (1.381 sec/step)\n",
            "I1208 00:20:37.273664 140583164200832 learning.py:507] global step 2465: loss = 1.7009 (1.381 sec/step)\n",
            "INFO:tensorflow:global step 2466: loss = 1.8829 (1.602 sec/step)\n",
            "I1208 00:20:38.876950 140583164200832 learning.py:507] global step 2466: loss = 1.8829 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 2467: loss = 2.2849 (0.470 sec/step)\n",
            "I1208 00:20:39.689747 140583164200832 learning.py:507] global step 2467: loss = 2.2849 (0.470 sec/step)\n",
            "INFO:tensorflow:global step 2468: loss = 2.0571 (0.622 sec/step)\n",
            "I1208 00:20:40.508168 140583164200832 learning.py:507] global step 2468: loss = 2.0571 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2469: loss = 2.2282 (0.710 sec/step)\n",
            "I1208 00:20:41.220160 140583164200832 learning.py:507] global step 2469: loss = 2.2282 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 2470: loss = 1.5470 (2.721 sec/step)\n",
            "I1208 00:20:43.942870 140583164200832 learning.py:507] global step 2470: loss = 1.5470 (2.721 sec/step)\n",
            "INFO:tensorflow:global step 2471: loss = 2.0749 (0.509 sec/step)\n",
            "I1208 00:20:44.453350 140583164200832 learning.py:507] global step 2471: loss = 2.0749 (0.509 sec/step)\n",
            "INFO:tensorflow:global step 2472: loss = 2.5168 (1.749 sec/step)\n",
            "I1208 00:20:46.204637 140583164200832 learning.py:507] global step 2472: loss = 2.5168 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 2473: loss = 2.0799 (0.522 sec/step)\n",
            "I1208 00:20:46.728797 140583164200832 learning.py:507] global step 2473: loss = 2.0799 (0.522 sec/step)\n",
            "INFO:tensorflow:global step 2474: loss = 2.1437 (1.643 sec/step)\n",
            "I1208 00:20:48.373724 140583164200832 learning.py:507] global step 2474: loss = 2.1437 (1.643 sec/step)\n",
            "INFO:tensorflow:global step 2475: loss = 2.2915 (0.932 sec/step)\n",
            "I1208 00:20:49.311553 140583164200832 learning.py:507] global step 2475: loss = 2.2915 (0.932 sec/step)\n",
            "INFO:tensorflow:global step 2476: loss = 2.7667 (0.629 sec/step)\n",
            "I1208 00:20:49.979113 140583164200832 learning.py:507] global step 2476: loss = 2.7667 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2477: loss = 2.8292 (0.616 sec/step)\n",
            "I1208 00:20:50.982211 140583164200832 learning.py:507] global step 2477: loss = 2.8292 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 2478: loss = 2.0892 (1.280 sec/step)\n",
            "I1208 00:20:52.542060 140583164200832 learning.py:507] global step 2478: loss = 2.0892 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2479: loss = 2.1265 (0.600 sec/step)\n",
            "I1208 00:20:53.444130 140583164200832 learning.py:507] global step 2479: loss = 2.1265 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 2480: loss = 2.0555 (0.530 sec/step)\n",
            "I1208 00:20:54.265575 140583164200832 learning.py:507] global step 2480: loss = 2.0555 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 2481: loss = 1.7812 (0.661 sec/step)\n",
            "I1208 00:20:55.566818 140583164200832 learning.py:507] global step 2481: loss = 1.7812 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2482: loss = 2.0643 (0.729 sec/step)\n",
            "I1208 00:20:56.751020 140583164200832 learning.py:507] global step 2482: loss = 2.0643 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 2483: loss = 2.5831 (0.572 sec/step)\n",
            "I1208 00:20:57.567012 140583164200832 learning.py:507] global step 2483: loss = 2.5831 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 2484: loss = 2.0652 (1.289 sec/step)\n",
            "I1208 00:20:59.059105 140583164200832 learning.py:507] global step 2484: loss = 2.0652 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 2485: loss = 1.6345 (0.580 sec/step)\n",
            "I1208 00:20:59.770423 140583164200832 learning.py:507] global step 2485: loss = 1.6345 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 2486: loss = 1.7230 (1.403 sec/step)\n",
            "I1208 00:21:01.298881 140583164200832 learning.py:507] global step 2486: loss = 1.7230 (1.403 sec/step)\n",
            "INFO:tensorflow:global step 2487: loss = 1.7894 (0.622 sec/step)\n",
            "I1208 00:21:02.166122 140583164200832 learning.py:507] global step 2487: loss = 1.7894 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2488: loss = 2.0523 (0.639 sec/step)\n",
            "I1208 00:21:03.020432 140583164200832 learning.py:507] global step 2488: loss = 2.0523 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2489: loss = 2.1928 (1.470 sec/step)\n",
            "I1208 00:21:04.510702 140583164200832 learning.py:507] global step 2489: loss = 2.1928 (1.470 sec/step)\n",
            "INFO:tensorflow:global step 2490: loss = 1.8795 (0.556 sec/step)\n",
            "I1208 00:21:05.339356 140583164200832 learning.py:507] global step 2490: loss = 1.8795 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 2491: loss = 2.2542 (2.198 sec/step)\n",
            "I1208 00:21:07.671946 140583164200832 learning.py:507] global step 2491: loss = 2.2542 (2.198 sec/step)\n",
            "INFO:tensorflow:global step 2492: loss = 2.0403 (0.736 sec/step)\n",
            "I1208 00:21:08.543367 140583164200832 learning.py:507] global step 2492: loss = 2.0403 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 2493: loss = 1.7735 (2.192 sec/step)\n",
            "I1208 00:21:10.737223 140583164200832 learning.py:507] global step 2493: loss = 1.7735 (2.192 sec/step)\n",
            "INFO:tensorflow:global step 2494: loss = 2.1828 (0.596 sec/step)\n",
            "I1208 00:21:11.578828 140583164200832 learning.py:507] global step 2494: loss = 2.1828 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2495: loss = 2.0197 (1.525 sec/step)\n",
            "I1208 00:21:13.288611 140583164200832 learning.py:507] global step 2495: loss = 2.0197 (1.525 sec/step)\n",
            "INFO:tensorflow:global step 2496: loss = 2.7887 (0.508 sec/step)\n",
            "I1208 00:21:13.798189 140583164200832 learning.py:507] global step 2496: loss = 2.7887 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 2497: loss = 2.0388 (0.831 sec/step)\n",
            "I1208 00:21:14.887215 140583164200832 learning.py:507] global step 2497: loss = 2.0388 (0.831 sec/step)\n",
            "INFO:tensorflow:global step 2498: loss = 1.8740 (1.736 sec/step)\n",
            "I1208 00:21:16.914154 140583164200832 learning.py:507] global step 2498: loss = 1.8740 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 2499: loss = 2.0055 (0.572 sec/step)\n",
            "I1208 00:21:17.616002 140583164200832 learning.py:507] global step 2499: loss = 2.0055 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 2500: loss = 1.8681 (2.957 sec/step)\n",
            "I1208 00:21:20.624428 140583164200832 learning.py:507] global step 2500: loss = 1.8681 (2.957 sec/step)\n",
            "INFO:tensorflow:global step 2501: loss = 2.1158 (0.722 sec/step)\n",
            "I1208 00:21:21.424800 140583164200832 learning.py:507] global step 2501: loss = 2.1158 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2502: loss = 2.6685 (0.636 sec/step)\n",
            "I1208 00:21:22.256916 140583164200832 learning.py:507] global step 2502: loss = 2.6685 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2503: loss = 1.4825 (1.652 sec/step)\n",
            "I1208 00:21:24.080172 140583164200832 learning.py:507] global step 2503: loss = 1.4825 (1.652 sec/step)\n",
            "INFO:tensorflow:global step 2504: loss = 1.2003 (0.754 sec/step)\n",
            "I1208 00:21:25.196867 140583164200832 learning.py:507] global step 2504: loss = 1.2003 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 2505: loss = 1.7514 (0.779 sec/step)\n",
            "I1208 00:21:26.079443 140583164200832 learning.py:507] global step 2505: loss = 1.7514 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 2506: loss = 2.0436 (1.355 sec/step)\n",
            "I1208 00:21:27.659384 140583164200832 learning.py:507] global step 2506: loss = 2.0436 (1.355 sec/step)\n",
            "INFO:tensorflow:global step 2507: loss = 2.2663 (0.689 sec/step)\n",
            "I1208 00:21:28.429609 140583164200832 learning.py:507] global step 2507: loss = 2.2663 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 2508: loss = 2.2625 (1.488 sec/step)\n",
            "I1208 00:21:29.919540 140583164200832 learning.py:507] global step 2508: loss = 2.2625 (1.488 sec/step)\n",
            "INFO:tensorflow:global step 2509: loss = 2.0512 (0.654 sec/step)\n",
            "I1208 00:21:30.830990 140583164200832 learning.py:507] global step 2509: loss = 2.0512 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2510: loss = 1.6570 (0.657 sec/step)\n",
            "I1208 00:21:31.992053 140583164200832 learning.py:507] global step 2510: loss = 1.6570 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2511: loss = 1.8220 (1.337 sec/step)\n",
            "I1208 00:21:33.471591 140583164200832 learning.py:507] global step 2511: loss = 1.8220 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 2512: loss = 2.3500 (0.647 sec/step)\n",
            "I1208 00:21:34.257551 140583164200832 learning.py:507] global step 2512: loss = 2.3500 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2513: loss = 1.7989 (1.529 sec/step)\n",
            "I1208 00:21:36.028967 140583164200832 learning.py:507] global step 2513: loss = 1.7989 (1.529 sec/step)\n",
            "INFO:tensorflow:global step 2514: loss = 2.0521 (0.613 sec/step)\n",
            "I1208 00:21:36.644708 140583164200832 learning.py:507] global step 2514: loss = 2.0521 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2515: loss = 2.0506 (2.050 sec/step)\n",
            "I1208 00:21:38.696618 140583164200832 learning.py:507] global step 2515: loss = 2.0506 (2.050 sec/step)\n",
            "INFO:tensorflow:global step 2516: loss = 2.0847 (0.581 sec/step)\n",
            "I1208 00:21:39.482434 140583164200832 learning.py:507] global step 2516: loss = 2.0847 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 2517: loss = 1.8131 (1.374 sec/step)\n",
            "I1208 00:21:41.020182 140583164200832 learning.py:507] global step 2517: loss = 1.8131 (1.374 sec/step)\n",
            "INFO:tensorflow:global step 2518: loss = 1.7696 (0.551 sec/step)\n",
            "I1208 00:21:41.858580 140583164200832 learning.py:507] global step 2518: loss = 1.7696 (0.551 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2518.\n",
            "I1208 00:21:44.210420 140579459213056 supervisor.py:1050] Recording summary at step 2518.\n",
            "INFO:tensorflow:global step 2519: loss = 1.9369 (2.441 sec/step)\n",
            "I1208 00:21:44.379414 140583164200832 learning.py:507] global step 2519: loss = 1.9369 (2.441 sec/step)\n",
            "INFO:tensorflow:global step 2520: loss = 1.8471 (0.614 sec/step)\n",
            "I1208 00:21:45.231203 140583164200832 learning.py:507] global step 2520: loss = 1.8471 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 2521: loss = 1.7916 (1.363 sec/step)\n",
            "I1208 00:21:46.693692 140583164200832 learning.py:507] global step 2521: loss = 1.7916 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 2522: loss = 2.2010 (0.563 sec/step)\n",
            "I1208 00:21:47.530111 140583164200832 learning.py:507] global step 2522: loss = 2.2010 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2523: loss = 1.9881 (1.297 sec/step)\n",
            "I1208 00:21:48.934080 140583164200832 learning.py:507] global step 2523: loss = 1.9881 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 2524: loss = 2.5662 (1.061 sec/step)\n",
            "I1208 00:21:49.996919 140583164200832 learning.py:507] global step 2524: loss = 2.5662 (1.061 sec/step)\n",
            "INFO:tensorflow:global step 2525: loss = 2.2934 (0.593 sec/step)\n",
            "I1208 00:21:50.591478 140583164200832 learning.py:507] global step 2525: loss = 2.2934 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 2526: loss = 1.9976 (1.032 sec/step)\n",
            "I1208 00:21:51.943208 140583164200832 learning.py:507] global step 2526: loss = 1.9976 (1.032 sec/step)\n",
            "INFO:tensorflow:global step 2527: loss = 1.8786 (1.711 sec/step)\n",
            "I1208 00:21:53.725384 140583164200832 learning.py:507] global step 2527: loss = 1.8786 (1.711 sec/step)\n",
            "INFO:tensorflow:global step 2528: loss = 2.0196 (0.653 sec/step)\n",
            "I1208 00:21:54.678149 140583164200832 learning.py:507] global step 2528: loss = 2.0196 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2529: loss = 1.6595 (0.650 sec/step)\n",
            "I1208 00:21:55.452302 140583164200832 learning.py:507] global step 2529: loss = 1.6595 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2530: loss = 1.9053 (2.099 sec/step)\n",
            "I1208 00:21:57.553143 140583164200832 learning.py:507] global step 2530: loss = 1.9053 (2.099 sec/step)\n",
            "INFO:tensorflow:global step 2531: loss = 2.2218 (0.774 sec/step)\n",
            "I1208 00:21:58.515697 140583164200832 learning.py:507] global step 2531: loss = 2.2218 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 2532: loss = 2.1566 (1.891 sec/step)\n",
            "I1208 00:22:00.441032 140583164200832 learning.py:507] global step 2532: loss = 2.1566 (1.891 sec/step)\n",
            "INFO:tensorflow:global step 2533: loss = 1.4147 (0.560 sec/step)\n",
            "I1208 00:22:01.003261 140583164200832 learning.py:507] global step 2533: loss = 1.4147 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 2534: loss = 2.2270 (1.951 sec/step)\n",
            "I1208 00:22:02.956438 140583164200832 learning.py:507] global step 2534: loss = 2.2270 (1.951 sec/step)\n",
            "INFO:tensorflow:global step 2535: loss = 1.8945 (0.506 sec/step)\n",
            "I1208 00:22:03.464480 140583164200832 learning.py:507] global step 2535: loss = 1.8945 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 2536: loss = 2.0938 (1.964 sec/step)\n",
            "I1208 00:22:05.430640 140583164200832 learning.py:507] global step 2536: loss = 2.0938 (1.964 sec/step)\n",
            "INFO:tensorflow:global step 2537: loss = 3.0070 (0.609 sec/step)\n",
            "I1208 00:22:06.041301 140583164200832 learning.py:507] global step 2537: loss = 3.0070 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2538: loss = 1.5905 (1.871 sec/step)\n",
            "I1208 00:22:07.914157 140583164200832 learning.py:507] global step 2538: loss = 1.5905 (1.871 sec/step)\n",
            "INFO:tensorflow:global step 2539: loss = 2.1441 (0.644 sec/step)\n",
            "I1208 00:22:08.854840 140583164200832 learning.py:507] global step 2539: loss = 2.1441 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2540: loss = 1.6529 (0.629 sec/step)\n",
            "I1208 00:22:09.855891 140583164200832 learning.py:507] global step 2540: loss = 1.6529 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2541: loss = 1.8007 (2.111 sec/step)\n",
            "I1208 00:22:12.148031 140583164200832 learning.py:507] global step 2541: loss = 1.8007 (2.111 sec/step)\n",
            "INFO:tensorflow:global step 2542: loss = 1.7812 (0.658 sec/step)\n",
            "I1208 00:22:13.066509 140583164200832 learning.py:507] global step 2542: loss = 1.7812 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2543: loss = 1.8821 (1.908 sec/step)\n",
            "I1208 00:22:15.064678 140583164200832 learning.py:507] global step 2543: loss = 1.8821 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 2544: loss = 2.2033 (0.636 sec/step)\n",
            "I1208 00:22:16.046907 140583164200832 learning.py:507] global step 2544: loss = 2.2033 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2545: loss = 1.8765 (1.452 sec/step)\n",
            "I1208 00:22:17.627475 140583164200832 learning.py:507] global step 2545: loss = 1.8765 (1.452 sec/step)\n",
            "INFO:tensorflow:global step 2546: loss = 1.8746 (0.627 sec/step)\n",
            "I1208 00:22:18.452583 140583164200832 learning.py:507] global step 2546: loss = 1.8746 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2547: loss = 1.4986 (1.685 sec/step)\n",
            "I1208 00:22:20.197837 140583164200832 learning.py:507] global step 2547: loss = 1.4986 (1.685 sec/step)\n",
            "INFO:tensorflow:global step 2548: loss = 1.9379 (0.606 sec/step)\n",
            "I1208 00:22:20.805320 140583164200832 learning.py:507] global step 2548: loss = 1.9379 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 2549: loss = 1.6619 (1.676 sec/step)\n",
            "I1208 00:22:22.494450 140583164200832 learning.py:507] global step 2549: loss = 1.6619 (1.676 sec/step)\n",
            "INFO:tensorflow:global step 2550: loss = 2.4079 (0.857 sec/step)\n",
            "I1208 00:22:23.608605 140583164200832 learning.py:507] global step 2550: loss = 2.4079 (0.857 sec/step)\n",
            "INFO:tensorflow:global step 2551: loss = 2.0584 (0.715 sec/step)\n",
            "I1208 00:22:24.843324 140583164200832 learning.py:507] global step 2551: loss = 2.0584 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 2552: loss = 1.5272 (0.633 sec/step)\n",
            "I1208 00:22:25.669160 140583164200832 learning.py:507] global step 2552: loss = 1.5272 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2553: loss = 2.1342 (1.383 sec/step)\n",
            "I1208 00:22:27.314727 140583164200832 learning.py:507] global step 2553: loss = 2.1342 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 2554: loss = 2.0470 (0.694 sec/step)\n",
            "I1208 00:22:28.288071 140583164200832 learning.py:507] global step 2554: loss = 2.0470 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 2555: loss = 1.6635 (2.101 sec/step)\n",
            "I1208 00:22:30.470248 140583164200832 learning.py:507] global step 2555: loss = 1.6635 (2.101 sec/step)\n",
            "INFO:tensorflow:global step 2556: loss = 3.0661 (0.648 sec/step)\n",
            "I1208 00:22:31.120257 140583164200832 learning.py:507] global step 2556: loss = 3.0661 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2557: loss = 2.2043 (0.563 sec/step)\n",
            "I1208 00:22:31.685667 140583164200832 learning.py:507] global step 2557: loss = 2.2043 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2558: loss = 1.9521 (2.623 sec/step)\n",
            "I1208 00:22:34.310702 140583164200832 learning.py:507] global step 2558: loss = 1.9521 (2.623 sec/step)\n",
            "INFO:tensorflow:global step 2559: loss = 2.3522 (0.644 sec/step)\n",
            "I1208 00:22:35.123013 140583164200832 learning.py:507] global step 2559: loss = 2.3522 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2560: loss = 1.8640 (0.554 sec/step)\n",
            "I1208 00:22:35.903345 140583164200832 learning.py:507] global step 2560: loss = 1.8640 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 2561: loss = 1.6309 (0.910 sec/step)\n",
            "I1208 00:22:36.894170 140583164200832 learning.py:507] global step 2561: loss = 1.6309 (0.910 sec/step)\n",
            "INFO:tensorflow:global step 2562: loss = 2.3273 (1.907 sec/step)\n",
            "I1208 00:22:38.958722 140583164200832 learning.py:507] global step 2562: loss = 2.3273 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 2563: loss = 1.7515 (0.524 sec/step)\n",
            "I1208 00:22:39.484444 140583164200832 learning.py:507] global step 2563: loss = 1.7515 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 2564: loss = 2.0890 (1.593 sec/step)\n",
            "I1208 00:22:41.148404 140583164200832 learning.py:507] global step 2564: loss = 2.0890 (1.593 sec/step)\n",
            "INFO:tensorflow:global step 2565: loss = 2.0284 (0.666 sec/step)\n",
            "I1208 00:22:42.177001 140583164200832 learning.py:507] global step 2565: loss = 2.0284 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 2566: loss = 2.4752 (1.839 sec/step)\n",
            "I1208 00:22:44.056828 140583164200832 learning.py:507] global step 2566: loss = 2.4752 (1.839 sec/step)\n",
            "INFO:tensorflow:global step 2567: loss = 2.4920 (0.582 sec/step)\n",
            "I1208 00:22:44.640031 140583164200832 learning.py:507] global step 2567: loss = 2.4920 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 2568: loss = 2.0705 (0.893 sec/step)\n",
            "I1208 00:22:45.953641 140583164200832 learning.py:507] global step 2568: loss = 2.0705 (0.893 sec/step)\n",
            "INFO:tensorflow:global step 2569: loss = 1.9067 (1.558 sec/step)\n",
            "I1208 00:22:47.666458 140583164200832 learning.py:507] global step 2569: loss = 1.9067 (1.558 sec/step)\n",
            "INFO:tensorflow:global step 2570: loss = 1.7137 (0.623 sec/step)\n",
            "I1208 00:22:48.512292 140583164200832 learning.py:507] global step 2570: loss = 1.7137 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2571: loss = 1.5801 (1.357 sec/step)\n",
            "I1208 00:22:50.255798 140583164200832 learning.py:507] global step 2571: loss = 1.5801 (1.357 sec/step)\n",
            "INFO:tensorflow:global step 2572: loss = 2.2116 (0.607 sec/step)\n",
            "I1208 00:22:51.179176 140583164200832 learning.py:507] global step 2572: loss = 2.2116 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2573: loss = 2.0376 (0.686 sec/step)\n",
            "I1208 00:22:52.382141 140583164200832 learning.py:507] global step 2573: loss = 2.0376 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 2574: loss = 1.9830 (0.638 sec/step)\n",
            "I1208 00:22:53.072495 140583164200832 learning.py:507] global step 2574: loss = 1.9830 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2575: loss = 2.0799 (1.826 sec/step)\n",
            "I1208 00:22:54.900045 140583164200832 learning.py:507] global step 2575: loss = 2.0799 (1.826 sec/step)\n",
            "INFO:tensorflow:global step 2576: loss = 1.9770 (1.214 sec/step)\n",
            "I1208 00:22:56.116451 140583164200832 learning.py:507] global step 2576: loss = 1.9770 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2577: loss = 1.9931 (0.503 sec/step)\n",
            "I1208 00:22:56.621241 140583164200832 learning.py:507] global step 2577: loss = 1.9931 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 2578: loss = 2.0780 (1.501 sec/step)\n",
            "I1208 00:22:58.368515 140583164200832 learning.py:507] global step 2578: loss = 2.0780 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 2579: loss = 2.0357 (1.406 sec/step)\n",
            "I1208 00:22:59.879449 140583164200832 learning.py:507] global step 2579: loss = 2.0357 (1.406 sec/step)\n",
            "INFO:tensorflow:global step 2580: loss = 1.7910 (0.726 sec/step)\n",
            "I1208 00:23:00.887763 140583164200832 learning.py:507] global step 2580: loss = 1.7910 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 2581: loss = 1.8460 (0.591 sec/step)\n",
            "I1208 00:23:01.583564 140583164200832 learning.py:507] global step 2581: loss = 1.8460 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 2582: loss = 2.2569 (1.475 sec/step)\n",
            "I1208 00:23:03.266285 140583164200832 learning.py:507] global step 2582: loss = 2.2569 (1.475 sec/step)\n",
            "INFO:tensorflow:global step 2583: loss = 1.9570 (1.247 sec/step)\n",
            "I1208 00:23:04.540366 140583164200832 learning.py:507] global step 2583: loss = 1.9570 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2584: loss = 1.6579 (0.590 sec/step)\n",
            "I1208 00:23:05.433298 140583164200832 learning.py:507] global step 2584: loss = 1.6579 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 2585: loss = 1.2366 (1.250 sec/step)\n",
            "I1208 00:23:06.829844 140583164200832 learning.py:507] global step 2585: loss = 1.2366 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2586: loss = 2.1467 (0.620 sec/step)\n",
            "I1208 00:23:07.826997 140583164200832 learning.py:507] global step 2586: loss = 2.1467 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2587: loss = 2.3643 (0.681 sec/step)\n",
            "I1208 00:23:09.079788 140583164200832 learning.py:507] global step 2587: loss = 2.3643 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2588: loss = 1.6833 (0.583 sec/step)\n",
            "I1208 00:23:09.741153 140583164200832 learning.py:507] global step 2588: loss = 1.6833 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 2589: loss = 1.5772 (2.029 sec/step)\n",
            "I1208 00:23:11.938826 140583164200832 learning.py:507] global step 2589: loss = 1.5772 (2.029 sec/step)\n",
            "INFO:tensorflow:global step 2590: loss = 1.5747 (0.634 sec/step)\n",
            "I1208 00:23:12.766971 140583164200832 learning.py:507] global step 2590: loss = 1.5747 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2591: loss = 2.8022 (1.492 sec/step)\n",
            "I1208 00:23:14.457567 140583164200832 learning.py:507] global step 2591: loss = 2.8022 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 2592: loss = 2.0577 (0.569 sec/step)\n",
            "I1208 00:23:15.262958 140583164200832 learning.py:507] global step 2592: loss = 2.0577 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 2593: loss = 1.6805 (1.566 sec/step)\n",
            "I1208 00:23:16.833279 140583164200832 learning.py:507] global step 2593: loss = 1.6805 (1.566 sec/step)\n",
            "INFO:tensorflow:global step 2594: loss = 1.9874 (0.623 sec/step)\n",
            "I1208 00:23:17.457847 140583164200832 learning.py:507] global step 2594: loss = 1.9874 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2595: loss = 1.6266 (1.626 sec/step)\n",
            "I1208 00:23:19.469856 140583164200832 learning.py:507] global step 2595: loss = 1.6266 (1.626 sec/step)\n",
            "INFO:tensorflow:global step 2596: loss = 2.0049 (1.681 sec/step)\n",
            "I1208 00:23:21.421662 140583164200832 learning.py:507] global step 2596: loss = 2.0049 (1.681 sec/step)\n",
            "INFO:tensorflow:global step 2597: loss = 1.9516 (0.638 sec/step)\n",
            "I1208 00:23:22.181725 140583164200832 learning.py:507] global step 2597: loss = 1.9516 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2598: loss = 2.4992 (0.532 sec/step)\n",
            "I1208 00:23:22.958418 140583164200832 learning.py:507] global step 2598: loss = 2.4992 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 2599: loss = 1.8131 (0.823 sec/step)\n",
            "I1208 00:23:24.135502 140583164200832 learning.py:507] global step 2599: loss = 1.8131 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 2600: loss = 1.7352 (1.436 sec/step)\n",
            "I1208 00:23:25.756517 140583164200832 learning.py:507] global step 2600: loss = 1.7352 (1.436 sec/step)\n",
            "INFO:tensorflow:global step 2601: loss = 1.8427 (0.652 sec/step)\n",
            "I1208 00:23:26.592679 140583164200832 learning.py:507] global step 2601: loss = 1.8427 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2602: loss = 1.7552 (0.626 sec/step)\n",
            "I1208 00:23:27.507925 140583164200832 learning.py:507] global step 2602: loss = 1.7552 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2603: loss = 2.1562 (1.358 sec/step)\n",
            "I1208 00:23:29.059514 140583164200832 learning.py:507] global step 2603: loss = 2.1562 (1.358 sec/step)\n",
            "INFO:tensorflow:global step 2604: loss = 2.0382 (0.572 sec/step)\n",
            "I1208 00:23:29.812452 140583164200832 learning.py:507] global step 2604: loss = 2.0382 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 2605: loss = 1.8720 (1.083 sec/step)\n",
            "I1208 00:23:31.062001 140583164200832 learning.py:507] global step 2605: loss = 1.8720 (1.083 sec/step)\n",
            "INFO:tensorflow:global step 2606: loss = 1.9994 (0.653 sec/step)\n",
            "I1208 00:23:31.886386 140583164200832 learning.py:507] global step 2606: loss = 1.9994 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2607: loss = 2.3827 (0.677 sec/step)\n",
            "I1208 00:23:33.106497 140583164200832 learning.py:507] global step 2607: loss = 2.3827 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 2608: loss = 2.1575 (0.618 sec/step)\n",
            "I1208 00:23:34.131120 140583164200832 learning.py:507] global step 2608: loss = 2.1575 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2609: loss = 2.0823 (1.158 sec/step)\n",
            "I1208 00:23:35.425073 140583164200832 learning.py:507] global step 2609: loss = 2.0823 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 2610: loss = 2.0852 (0.756 sec/step)\n",
            "I1208 00:23:36.360421 140583164200832 learning.py:507] global step 2610: loss = 2.0852 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 2611: loss = 1.7722 (1.125 sec/step)\n",
            "I1208 00:23:37.594362 140583164200832 learning.py:507] global step 2611: loss = 1.7722 (1.125 sec/step)\n",
            "INFO:tensorflow:global step 2612: loss = 2.4247 (0.601 sec/step)\n",
            "I1208 00:23:38.197533 140583164200832 learning.py:507] global step 2612: loss = 2.4247 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 2613: loss = 1.9834 (1.660 sec/step)\n",
            "I1208 00:23:39.860329 140583164200832 learning.py:507] global step 2613: loss = 1.9834 (1.660 sec/step)\n",
            "INFO:tensorflow:global step 2614: loss = 1.9954 (0.544 sec/step)\n",
            "I1208 00:23:40.622214 140583164200832 learning.py:507] global step 2614: loss = 1.9954 (0.544 sec/step)\n",
            "INFO:tensorflow:global step 2615: loss = 1.4867 (1.278 sec/step)\n",
            "I1208 00:23:42.576094 140583164200832 learning.py:507] global step 2615: loss = 1.4867 (1.278 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2615.\n",
            "I1208 00:23:44.118314 140579459213056 supervisor.py:1050] Recording summary at step 2615.\n",
            "INFO:tensorflow:global step 2616: loss = 2.3905 (1.507 sec/step)\n",
            "I1208 00:23:44.365965 140583164200832 learning.py:507] global step 2616: loss = 2.3905 (1.507 sec/step)\n",
            "INFO:tensorflow:global step 2617: loss = 1.8990 (0.570 sec/step)\n",
            "I1208 00:23:44.938108 140583164200832 learning.py:507] global step 2617: loss = 1.8990 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 2618: loss = 1.9126 (0.566 sec/step)\n",
            "I1208 00:23:45.505602 140583164200832 learning.py:507] global step 2618: loss = 1.9126 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2619: loss = 2.0921 (3.594 sec/step)\n",
            "I1208 00:23:49.124562 140583164200832 learning.py:507] global step 2619: loss = 2.0921 (3.594 sec/step)\n",
            "INFO:tensorflow:global step 2620: loss = 1.7419 (0.572 sec/step)\n",
            "I1208 00:23:49.698102 140583164200832 learning.py:507] global step 2620: loss = 1.7419 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 2621: loss = 1.7716 (1.769 sec/step)\n",
            "I1208 00:23:51.468996 140583164200832 learning.py:507] global step 2621: loss = 1.7716 (1.769 sec/step)\n",
            "INFO:tensorflow:global step 2622: loss = 1.7548 (0.644 sec/step)\n",
            "I1208 00:23:52.115294 140583164200832 learning.py:507] global step 2622: loss = 1.7548 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2623: loss = 2.7854 (1.698 sec/step)\n",
            "I1208 00:23:53.978936 140583164200832 learning.py:507] global step 2623: loss = 2.7854 (1.698 sec/step)\n",
            "INFO:tensorflow:global step 2624: loss = 1.5256 (0.709 sec/step)\n",
            "I1208 00:23:54.871743 140583164200832 learning.py:507] global step 2624: loss = 1.5256 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 2625: loss = 2.9506 (0.638 sec/step)\n",
            "I1208 00:23:56.087300 140583164200832 learning.py:507] global step 2625: loss = 2.9506 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2626: loss = 1.4962 (1.928 sec/step)\n",
            "I1208 00:23:58.353843 140583164200832 learning.py:507] global step 2626: loss = 1.4962 (1.928 sec/step)\n",
            "INFO:tensorflow:global step 2627: loss = 2.2990 (0.644 sec/step)\n",
            "I1208 00:23:59.329780 140583164200832 learning.py:507] global step 2627: loss = 2.2990 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2628: loss = 2.3204 (0.503 sec/step)\n",
            "I1208 00:24:00.022237 140583164200832 learning.py:507] global step 2628: loss = 2.3204 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 2629: loss = 1.6422 (0.705 sec/step)\n",
            "I1208 00:24:00.730739 140583164200832 learning.py:507] global step 2629: loss = 1.6422 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 2630: loss = 2.2033 (3.085 sec/step)\n",
            "I1208 00:24:03.830016 140583164200832 learning.py:507] global step 2630: loss = 2.2033 (3.085 sec/step)\n",
            "INFO:tensorflow:global step 2631: loss = 2.3791 (0.697 sec/step)\n",
            "I1208 00:24:04.538878 140583164200832 learning.py:507] global step 2631: loss = 2.3791 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 2632: loss = 1.9316 (0.955 sec/step)\n",
            "I1208 00:24:05.634783 140583164200832 learning.py:507] global step 2632: loss = 1.9316 (0.955 sec/step)\n",
            "INFO:tensorflow:global step 2633: loss = 2.5665 (2.167 sec/step)\n",
            "I1208 00:24:07.976577 140583164200832 learning.py:507] global step 2633: loss = 2.5665 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 2634: loss = 1.8058 (0.517 sec/step)\n",
            "I1208 00:24:08.495452 140583164200832 learning.py:507] global step 2634: loss = 1.8058 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 2635: loss = 2.0187 (1.281 sec/step)\n",
            "I1208 00:24:09.982597 140583164200832 learning.py:507] global step 2635: loss = 2.0187 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 2636: loss = 2.2202 (1.698 sec/step)\n",
            "I1208 00:24:11.866139 140583164200832 learning.py:507] global step 2636: loss = 2.2202 (1.698 sec/step)\n",
            "INFO:tensorflow:global step 2637: loss = 1.8626 (0.672 sec/step)\n",
            "I1208 00:24:12.539856 140583164200832 learning.py:507] global step 2637: loss = 1.8626 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2638: loss = 2.2907 (1.884 sec/step)\n",
            "I1208 00:24:14.556790 140583164200832 learning.py:507] global step 2638: loss = 2.2907 (1.884 sec/step)\n",
            "INFO:tensorflow:global step 2639: loss = 2.2441 (0.569 sec/step)\n",
            "I1208 00:24:15.464848 140583164200832 learning.py:507] global step 2639: loss = 2.2441 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 2640: loss = 1.3395 (2.048 sec/step)\n",
            "I1208 00:24:17.569713 140583164200832 learning.py:507] global step 2640: loss = 1.3395 (2.048 sec/step)\n",
            "INFO:tensorflow:global step 2641: loss = 2.8037 (0.695 sec/step)\n",
            "I1208 00:24:18.478677 140583164200832 learning.py:507] global step 2641: loss = 2.8037 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 2642: loss = 1.5898 (1.792 sec/step)\n",
            "I1208 00:24:20.543094 140583164200832 learning.py:507] global step 2642: loss = 1.5898 (1.792 sec/step)\n",
            "INFO:tensorflow:global step 2643: loss = 1.7559 (0.570 sec/step)\n",
            "I1208 00:24:21.360926 140583164200832 learning.py:507] global step 2643: loss = 1.7559 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 2644: loss = 1.4434 (2.040 sec/step)\n",
            "I1208 00:24:23.615328 140583164200832 learning.py:507] global step 2644: loss = 1.4434 (2.040 sec/step)\n",
            "INFO:tensorflow:global step 2645: loss = 2.0751 (0.694 sec/step)\n",
            "I1208 00:24:24.615573 140583164200832 learning.py:507] global step 2645: loss = 2.0751 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 2646: loss = 2.1215 (1.508 sec/step)\n",
            "I1208 00:24:26.228619 140583164200832 learning.py:507] global step 2646: loss = 2.1215 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 2647: loss = 1.7448 (0.552 sec/step)\n",
            "I1208 00:24:26.782460 140583164200832 learning.py:507] global step 2647: loss = 1.7448 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 2648: loss = 1.6325 (2.258 sec/step)\n",
            "I1208 00:24:29.041923 140583164200832 learning.py:507] global step 2648: loss = 1.6325 (2.258 sec/step)\n",
            "INFO:tensorflow:global step 2649: loss = 1.9175 (0.762 sec/step)\n",
            "I1208 00:24:29.964902 140583164200832 learning.py:507] global step 2649: loss = 1.9175 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 2650: loss = 2.2916 (1.919 sec/step)\n",
            "I1208 00:24:32.085355 140583164200832 learning.py:507] global step 2650: loss = 2.2916 (1.919 sec/step)\n",
            "INFO:tensorflow:global step 2651: loss = 2.0218 (0.553 sec/step)\n",
            "I1208 00:24:32.639777 140583164200832 learning.py:507] global step 2651: loss = 2.0218 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 2652: loss = 2.0692 (1.105 sec/step)\n",
            "I1208 00:24:33.919340 140583164200832 learning.py:507] global step 2652: loss = 2.0692 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 2653: loss = 1.6202 (1.756 sec/step)\n",
            "I1208 00:24:35.931038 140583164200832 learning.py:507] global step 2653: loss = 1.6202 (1.756 sec/step)\n",
            "INFO:tensorflow:global step 2654: loss = 1.9431 (0.488 sec/step)\n",
            "I1208 00:24:36.421403 140583164200832 learning.py:507] global step 2654: loss = 1.9431 (0.488 sec/step)\n",
            "INFO:tensorflow:global step 2655: loss = 2.1878 (1.423 sec/step)\n",
            "I1208 00:24:37.949144 140583164200832 learning.py:507] global step 2655: loss = 2.1878 (1.423 sec/step)\n",
            "INFO:tensorflow:global step 2656: loss = 2.2601 (1.461 sec/step)\n",
            "I1208 00:24:39.983246 140583164200832 learning.py:507] global step 2656: loss = 2.2601 (1.461 sec/step)\n",
            "INFO:tensorflow:global step 2657: loss = 2.0437 (0.631 sec/step)\n",
            "I1208 00:24:40.938193 140583164200832 learning.py:507] global step 2657: loss = 2.0437 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2658: loss = 2.4349 (1.748 sec/step)\n",
            "I1208 00:24:42.859947 140583164200832 learning.py:507] global step 2658: loss = 2.4349 (1.748 sec/step)\n",
            "INFO:tensorflow:global step 2659: loss = 1.8773 (0.534 sec/step)\n",
            "I1208 00:24:43.395785 140583164200832 learning.py:507] global step 2659: loss = 1.8773 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 2660: loss = 2.2754 (1.947 sec/step)\n",
            "I1208 00:24:45.344389 140583164200832 learning.py:507] global step 2660: loss = 2.2754 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 2661: loss = 2.0135 (0.637 sec/step)\n",
            "I1208 00:24:46.396538 140583164200832 learning.py:507] global step 2661: loss = 2.0135 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2662: loss = 1.7342 (1.796 sec/step)\n",
            "I1208 00:24:48.239202 140583164200832 learning.py:507] global step 2662: loss = 1.7342 (1.796 sec/step)\n",
            "INFO:tensorflow:global step 2663: loss = 1.9148 (0.562 sec/step)\n",
            "I1208 00:24:48.803523 140583164200832 learning.py:507] global step 2663: loss = 1.9148 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 2664: loss = 1.9860 (0.806 sec/step)\n",
            "I1208 00:24:49.875194 140583164200832 learning.py:507] global step 2664: loss = 1.9860 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 2665: loss = 2.1341 (2.234 sec/step)\n",
            "I1208 00:24:52.403621 140583164200832 learning.py:507] global step 2665: loss = 2.1341 (2.234 sec/step)\n",
            "INFO:tensorflow:global step 2666: loss = 1.6559 (0.536 sec/step)\n",
            "I1208 00:24:52.942167 140583164200832 learning.py:507] global step 2666: loss = 1.6559 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 2667: loss = 1.6411 (1.931 sec/step)\n",
            "I1208 00:24:54.875020 140583164200832 learning.py:507] global step 2667: loss = 1.6411 (1.931 sec/step)\n",
            "INFO:tensorflow:global step 2668: loss = 2.2672 (0.553 sec/step)\n",
            "I1208 00:24:55.430172 140583164200832 learning.py:507] global step 2668: loss = 2.2672 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 2669: loss = 2.0731 (0.665 sec/step)\n",
            "I1208 00:24:56.863028 140583164200832 learning.py:507] global step 2669: loss = 2.0731 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2670: loss = 1.9430 (2.208 sec/step)\n",
            "I1208 00:24:59.378167 140583164200832 learning.py:507] global step 2670: loss = 1.9430 (2.208 sec/step)\n",
            "INFO:tensorflow:global step 2671: loss = 1.5516 (0.525 sec/step)\n",
            "I1208 00:24:59.904739 140583164200832 learning.py:507] global step 2671: loss = 1.5516 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 2672: loss = 1.2481 (1.446 sec/step)\n",
            "I1208 00:25:01.538593 140583164200832 learning.py:507] global step 2672: loss = 1.2481 (1.446 sec/step)\n",
            "INFO:tensorflow:global step 2673: loss = 1.9424 (1.923 sec/step)\n",
            "I1208 00:25:03.462994 140583164200832 learning.py:507] global step 2673: loss = 1.9424 (1.923 sec/step)\n",
            "INFO:tensorflow:global step 2674: loss = 2.7814 (0.541 sec/step)\n",
            "I1208 00:25:04.005665 140583164200832 learning.py:507] global step 2674: loss = 2.7814 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 2675: loss = 1.9522 (1.753 sec/step)\n",
            "I1208 00:25:05.760822 140583164200832 learning.py:507] global step 2675: loss = 1.9522 (1.753 sec/step)\n",
            "INFO:tensorflow:global step 2676: loss = 1.8085 (0.612 sec/step)\n",
            "I1208 00:25:06.374749 140583164200832 learning.py:507] global step 2676: loss = 1.8085 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2677: loss = 1.9487 (1.908 sec/step)\n",
            "I1208 00:25:08.284638 140583164200832 learning.py:507] global step 2677: loss = 1.9487 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 2678: loss = 1.8304 (0.548 sec/step)\n",
            "I1208 00:25:08.834081 140583164200832 learning.py:507] global step 2678: loss = 1.8304 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 2679: loss = 1.8710 (2.139 sec/step)\n",
            "I1208 00:25:10.975161 140583164200832 learning.py:507] global step 2679: loss = 1.8710 (2.139 sec/step)\n",
            "INFO:tensorflow:global step 2680: loss = 1.7988 (0.607 sec/step)\n",
            "I1208 00:25:11.587032 140583164200832 learning.py:507] global step 2680: loss = 1.7988 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2681: loss = 2.1981 (1.943 sec/step)\n",
            "I1208 00:25:13.625912 140583164200832 learning.py:507] global step 2681: loss = 2.1981 (1.943 sec/step)\n",
            "INFO:tensorflow:global step 2682: loss = 2.5431 (0.625 sec/step)\n",
            "I1208 00:25:14.287254 140583164200832 learning.py:507] global step 2682: loss = 2.5431 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2683: loss = 1.8595 (2.067 sec/step)\n",
            "I1208 00:25:16.356516 140583164200832 learning.py:507] global step 2683: loss = 1.8595 (2.067 sec/step)\n",
            "INFO:tensorflow:global step 2684: loss = 2.1008 (0.534 sec/step)\n",
            "I1208 00:25:16.891908 140583164200832 learning.py:507] global step 2684: loss = 2.1008 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 2685: loss = 2.2156 (1.752 sec/step)\n",
            "I1208 00:25:18.802097 140583164200832 learning.py:507] global step 2685: loss = 2.2156 (1.752 sec/step)\n",
            "INFO:tensorflow:global step 2686: loss = 1.7309 (0.507 sec/step)\n",
            "I1208 00:25:19.406998 140583164200832 learning.py:507] global step 2686: loss = 1.7309 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 2687: loss = 2.2339 (1.753 sec/step)\n",
            "I1208 00:25:21.162095 140583164200832 learning.py:507] global step 2687: loss = 2.2339 (1.753 sec/step)\n",
            "INFO:tensorflow:global step 2688: loss = 1.9827 (0.593 sec/step)\n",
            "I1208 00:25:21.757853 140583164200832 learning.py:507] global step 2688: loss = 1.9827 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 2689: loss = 1.5434 (1.203 sec/step)\n",
            "I1208 00:25:23.267740 140583164200832 learning.py:507] global step 2689: loss = 1.5434 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2690: loss = 1.8049 (2.142 sec/step)\n",
            "I1208 00:25:25.607119 140583164200832 learning.py:507] global step 2690: loss = 1.8049 (2.142 sec/step)\n",
            "INFO:tensorflow:global step 2691: loss = 1.8321 (0.749 sec/step)\n",
            "I1208 00:25:26.397086 140583164200832 learning.py:507] global step 2691: loss = 1.8321 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 2692: loss = 2.2019 (2.031 sec/step)\n",
            "I1208 00:25:28.511515 140583164200832 learning.py:507] global step 2692: loss = 2.2019 (2.031 sec/step)\n",
            "INFO:tensorflow:global step 2693: loss = 1.8928 (0.621 sec/step)\n",
            "I1208 00:25:29.134627 140583164200832 learning.py:507] global step 2693: loss = 1.8928 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2694: loss = 1.9344 (1.814 sec/step)\n",
            "I1208 00:25:30.950572 140583164200832 learning.py:507] global step 2694: loss = 1.9344 (1.814 sec/step)\n",
            "INFO:tensorflow:global step 2695: loss = 2.2718 (0.591 sec/step)\n",
            "I1208 00:25:31.779008 140583164200832 learning.py:507] global step 2695: loss = 2.2718 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 2696: loss = 1.6727 (1.063 sec/step)\n",
            "I1208 00:25:32.985670 140583164200832 learning.py:507] global step 2696: loss = 1.6727 (1.063 sec/step)\n",
            "INFO:tensorflow:global step 2697: loss = 1.8520 (1.701 sec/step)\n",
            "I1208 00:25:34.915797 140583164200832 learning.py:507] global step 2697: loss = 1.8520 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 2698: loss = 1.5749 (0.602 sec/step)\n",
            "I1208 00:25:35.574729 140583164200832 learning.py:507] global step 2698: loss = 1.5749 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2699: loss = 1.7949 (2.041 sec/step)\n",
            "I1208 00:25:37.708193 140583164200832 learning.py:507] global step 2699: loss = 1.7949 (2.041 sec/step)\n",
            "INFO:tensorflow:global step 2700: loss = 1.9368 (0.579 sec/step)\n",
            "I1208 00:25:38.289094 140583164200832 learning.py:507] global step 2700: loss = 1.9368 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 2701: loss = 1.9403 (0.684 sec/step)\n",
            "I1208 00:25:39.422063 140583164200832 learning.py:507] global step 2701: loss = 1.9403 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 2702: loss = 2.1138 (1.976 sec/step)\n",
            "I1208 00:25:41.613753 140583164200832 learning.py:507] global step 2702: loss = 2.1138 (1.976 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2703.\n",
            "I1208 00:25:43.148515 140579459213056 supervisor.py:1050] Recording summary at step 2703.\n",
            "INFO:tensorflow:global step 2703: loss = 1.9788 (1.090 sec/step)\n",
            "I1208 00:25:43.425231 140583164200832 learning.py:507] global step 2703: loss = 1.9788 (1.090 sec/step)\n",
            "INFO:tensorflow:global step 2704: loss = 1.6416 (2.298 sec/step)\n",
            "I1208 00:25:45.944296 140583164200832 learning.py:507] global step 2704: loss = 1.6416 (2.298 sec/step)\n",
            "INFO:tensorflow:global step 2705: loss = 1.6382 (0.612 sec/step)\n",
            "I1208 00:25:46.675446 140583164200832 learning.py:507] global step 2705: loss = 1.6382 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 2706: loss = 1.7687 (2.117 sec/step)\n",
            "I1208 00:25:48.942673 140583164200832 learning.py:507] global step 2706: loss = 1.7687 (2.117 sec/step)\n",
            "INFO:tensorflow:global step 2707: loss = 2.2662 (0.605 sec/step)\n",
            "I1208 00:25:49.549380 140583164200832 learning.py:507] global step 2707: loss = 2.2662 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2708: loss = 1.7646 (1.875 sec/step)\n",
            "I1208 00:25:51.425932 140583164200832 learning.py:507] global step 2708: loss = 1.7646 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 2709: loss = 1.8490 (0.617 sec/step)\n",
            "I1208 00:25:52.045061 140583164200832 learning.py:507] global step 2709: loss = 1.8490 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 2710: loss = 1.8147 (2.150 sec/step)\n",
            "I1208 00:25:54.230956 140583164200832 learning.py:507] global step 2710: loss = 1.8147 (2.150 sec/step)\n",
            "INFO:tensorflow:global step 2711: loss = 2.2316 (0.582 sec/step)\n",
            "I1208 00:25:54.815049 140583164200832 learning.py:507] global step 2711: loss = 2.2316 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 2712: loss = 1.7918 (1.902 sec/step)\n",
            "I1208 00:25:56.719434 140583164200832 learning.py:507] global step 2712: loss = 1.7918 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 2713: loss = 2.2839 (0.558 sec/step)\n",
            "I1208 00:25:57.280271 140583164200832 learning.py:507] global step 2713: loss = 2.2839 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 2714: loss = 1.7702 (1.911 sec/step)\n",
            "I1208 00:25:59.193476 140583164200832 learning.py:507] global step 2714: loss = 1.7702 (1.911 sec/step)\n",
            "INFO:tensorflow:global step 2715: loss = 1.7434 (0.584 sec/step)\n",
            "I1208 00:25:59.779247 140583164200832 learning.py:507] global step 2715: loss = 1.7434 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 2716: loss = 2.2292 (1.489 sec/step)\n",
            "I1208 00:26:01.455387 140583164200832 learning.py:507] global step 2716: loss = 2.2292 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 2717: loss = 2.1743 (2.276 sec/step)\n",
            "I1208 00:26:03.795818 140583164200832 learning.py:507] global step 2717: loss = 2.1743 (2.276 sec/step)\n",
            "INFO:tensorflow:global step 2718: loss = 2.4899 (0.558 sec/step)\n",
            "I1208 00:26:04.355342 140583164200832 learning.py:507] global step 2718: loss = 2.4899 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 2719: loss = 2.0022 (1.147 sec/step)\n",
            "I1208 00:26:05.897024 140583164200832 learning.py:507] global step 2719: loss = 2.0022 (1.147 sec/step)\n",
            "INFO:tensorflow:global step 2720: loss = 2.0980 (1.581 sec/step)\n",
            "I1208 00:26:07.725153 140583164200832 learning.py:507] global step 2720: loss = 2.0980 (1.581 sec/step)\n",
            "INFO:tensorflow:global step 2721: loss = 2.1553 (0.738 sec/step)\n",
            "I1208 00:26:08.732766 140583164200832 learning.py:507] global step 2721: loss = 2.1553 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 2722: loss = 1.7173 (1.902 sec/step)\n",
            "I1208 00:26:10.675509 140583164200832 learning.py:507] global step 2722: loss = 1.7173 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 2723: loss = 2.1044 (0.591 sec/step)\n",
            "I1208 00:26:11.268722 140583164200832 learning.py:507] global step 2723: loss = 2.1044 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 2724: loss = 2.2028 (1.855 sec/step)\n",
            "I1208 00:26:13.126037 140583164200832 learning.py:507] global step 2724: loss = 2.2028 (1.855 sec/step)\n",
            "INFO:tensorflow:global step 2725: loss = 1.9048 (0.586 sec/step)\n",
            "I1208 00:26:13.713229 140583164200832 learning.py:507] global step 2725: loss = 1.9048 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 2726: loss = 2.3242 (1.784 sec/step)\n",
            "I1208 00:26:15.554624 140583164200832 learning.py:507] global step 2726: loss = 2.3242 (1.784 sec/step)\n",
            "INFO:tensorflow:global step 2727: loss = 2.0514 (0.693 sec/step)\n",
            "I1208 00:26:16.427176 140583164200832 learning.py:507] global step 2727: loss = 2.0514 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 2728: loss = 1.4944 (0.910 sec/step)\n",
            "I1208 00:26:17.525580 140583164200832 learning.py:507] global step 2728: loss = 1.4944 (0.910 sec/step)\n",
            "INFO:tensorflow:global step 2729: loss = 1.8268 (0.989 sec/step)\n",
            "I1208 00:26:18.643354 140583164200832 learning.py:507] global step 2729: loss = 1.8268 (0.989 sec/step)\n",
            "INFO:tensorflow:global step 2730: loss = 2.3024 (1.926 sec/step)\n",
            "I1208 00:26:20.623706 140583164200832 learning.py:507] global step 2730: loss = 2.3024 (1.926 sec/step)\n",
            "INFO:tensorflow:global step 2731: loss = 1.5792 (0.602 sec/step)\n",
            "I1208 00:26:21.448800 140583164200832 learning.py:507] global step 2731: loss = 1.5792 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2732: loss = 1.8053 (2.079 sec/step)\n",
            "I1208 00:26:23.658377 140583164200832 learning.py:507] global step 2732: loss = 1.8053 (2.079 sec/step)\n",
            "INFO:tensorflow:global step 2733: loss = 2.2187 (0.555 sec/step)\n",
            "I1208 00:26:24.215322 140583164200832 learning.py:507] global step 2733: loss = 2.2187 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2734: loss = 1.8479 (1.695 sec/step)\n",
            "I1208 00:26:26.064703 140583164200832 learning.py:507] global step 2734: loss = 1.8479 (1.695 sec/step)\n",
            "INFO:tensorflow:global step 2735: loss = 2.0674 (0.602 sec/step)\n",
            "I1208 00:26:27.027155 140583164200832 learning.py:507] global step 2735: loss = 2.0674 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2736: loss = 1.8686 (1.960 sec/step)\n",
            "I1208 00:26:29.079335 140583164200832 learning.py:507] global step 2736: loss = 1.8686 (1.960 sec/step)\n",
            "INFO:tensorflow:global step 2737: loss = 2.0626 (0.606 sec/step)\n",
            "I1208 00:26:29.687159 140583164200832 learning.py:507] global step 2737: loss = 2.0626 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 2738: loss = 2.2395 (2.134 sec/step)\n",
            "I1208 00:26:31.822770 140583164200832 learning.py:507] global step 2738: loss = 2.2395 (2.134 sec/step)\n",
            "INFO:tensorflow:global step 2739: loss = 1.5765 (0.577 sec/step)\n",
            "I1208 00:26:32.401734 140583164200832 learning.py:507] global step 2739: loss = 1.5765 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2740: loss = 2.2961 (1.779 sec/step)\n",
            "I1208 00:26:34.183375 140583164200832 learning.py:507] global step 2740: loss = 2.2961 (1.779 sec/step)\n",
            "INFO:tensorflow:global step 2741: loss = 2.0631 (0.582 sec/step)\n",
            "I1208 00:26:34.767081 140583164200832 learning.py:507] global step 2741: loss = 2.0631 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 2742: loss = 1.8454 (1.930 sec/step)\n",
            "I1208 00:26:36.699046 140583164200832 learning.py:507] global step 2742: loss = 1.8454 (1.930 sec/step)\n",
            "INFO:tensorflow:global step 2743: loss = 2.1692 (0.550 sec/step)\n",
            "I1208 00:26:37.250498 140583164200832 learning.py:507] global step 2743: loss = 2.1692 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 2744: loss = 2.6087 (0.701 sec/step)\n",
            "I1208 00:26:37.973019 140583164200832 learning.py:507] global step 2744: loss = 2.6087 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 2745: loss = 2.0544 (2.644 sec/step)\n",
            "I1208 00:26:41.364742 140583164200832 learning.py:507] global step 2745: loss = 2.0544 (2.644 sec/step)\n",
            "INFO:tensorflow:global step 2746: loss = 1.6780 (0.604 sec/step)\n",
            "I1208 00:26:42.270196 140583164200832 learning.py:507] global step 2746: loss = 1.6780 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2747: loss = 1.5653 (2.327 sec/step)\n",
            "I1208 00:26:44.897091 140583164200832 learning.py:507] global step 2747: loss = 1.5653 (2.327 sec/step)\n",
            "INFO:tensorflow:global step 2748: loss = 1.9884 (0.542 sec/step)\n",
            "I1208 00:26:45.441158 140583164200832 learning.py:507] global step 2748: loss = 1.9884 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 2749: loss = 2.2774 (1.789 sec/step)\n",
            "I1208 00:26:47.232553 140583164200832 learning.py:507] global step 2749: loss = 2.2774 (1.789 sec/step)\n",
            "INFO:tensorflow:global step 2750: loss = 2.1933 (0.490 sec/step)\n",
            "I1208 00:26:47.724837 140583164200832 learning.py:507] global step 2750: loss = 2.1933 (0.490 sec/step)\n",
            "INFO:tensorflow:global step 2751: loss = 2.0703 (0.987 sec/step)\n",
            "I1208 00:26:48.855677 140583164200832 learning.py:507] global step 2751: loss = 2.0703 (0.987 sec/step)\n",
            "INFO:tensorflow:global step 2752: loss = 1.7721 (1.736 sec/step)\n",
            "I1208 00:26:50.855039 140583164200832 learning.py:507] global step 2752: loss = 1.7721 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 2753: loss = 2.4699 (0.625 sec/step)\n",
            "I1208 00:26:51.482047 140583164200832 learning.py:507] global step 2753: loss = 2.4699 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2754: loss = 2.1909 (2.092 sec/step)\n",
            "I1208 00:26:53.576103 140583164200832 learning.py:507] global step 2754: loss = 2.1909 (2.092 sec/step)\n",
            "INFO:tensorflow:global step 2755: loss = 1.9154 (0.543 sec/step)\n",
            "I1208 00:26:54.121331 140583164200832 learning.py:507] global step 2755: loss = 1.9154 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 2756: loss = 1.8026 (1.764 sec/step)\n",
            "I1208 00:26:55.887051 140583164200832 learning.py:507] global step 2756: loss = 1.8026 (1.764 sec/step)\n",
            "INFO:tensorflow:global step 2757: loss = 2.2778 (0.565 sec/step)\n",
            "I1208 00:26:56.453611 140583164200832 learning.py:507] global step 2757: loss = 2.2778 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 2758: loss = 2.2640 (1.011 sec/step)\n",
            "I1208 00:26:57.667236 140583164200832 learning.py:507] global step 2758: loss = 2.2640 (1.011 sec/step)\n",
            "INFO:tensorflow:global step 2759: loss = 2.1625 (1.577 sec/step)\n",
            "I1208 00:26:59.507137 140583164200832 learning.py:507] global step 2759: loss = 2.1625 (1.577 sec/step)\n",
            "INFO:tensorflow:global step 2760: loss = 2.1428 (0.729 sec/step)\n",
            "I1208 00:27:00.247237 140583164200832 learning.py:507] global step 2760: loss = 2.1428 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 2761: loss = 2.1436 (1.457 sec/step)\n",
            "I1208 00:27:01.705390 140583164200832 learning.py:507] global step 2761: loss = 2.1436 (1.457 sec/step)\n",
            "INFO:tensorflow:global step 2762: loss = 2.1058 (0.542 sec/step)\n",
            "I1208 00:27:02.249238 140583164200832 learning.py:507] global step 2762: loss = 2.1058 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 2763: loss = 2.3464 (1.673 sec/step)\n",
            "I1208 00:27:03.924235 140583164200832 learning.py:507] global step 2763: loss = 2.3464 (1.673 sec/step)\n",
            "INFO:tensorflow:global step 2764: loss = 2.2228 (0.647 sec/step)\n",
            "I1208 00:27:04.751210 140583164200832 learning.py:507] global step 2764: loss = 2.2228 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2765: loss = 1.4595 (1.376 sec/step)\n",
            "I1208 00:27:06.251062 140583164200832 learning.py:507] global step 2765: loss = 1.4595 (1.376 sec/step)\n",
            "INFO:tensorflow:global step 2766: loss = 1.8137 (1.165 sec/step)\n",
            "I1208 00:27:07.417674 140583164200832 learning.py:507] global step 2766: loss = 1.8137 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 2767: loss = 1.6843 (0.713 sec/step)\n",
            "I1208 00:27:08.317744 140583164200832 learning.py:507] global step 2767: loss = 1.6843 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 2768: loss = 2.0556 (1.301 sec/step)\n",
            "I1208 00:27:09.772782 140583164200832 learning.py:507] global step 2768: loss = 2.0556 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 2769: loss = 1.8149 (0.530 sec/step)\n",
            "I1208 00:27:10.304300 140583164200832 learning.py:507] global step 2769: loss = 1.8149 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 2770: loss = 2.4580 (1.296 sec/step)\n",
            "I1208 00:27:11.654471 140583164200832 learning.py:507] global step 2770: loss = 2.4580 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 2771: loss = 2.0540 (1.415 sec/step)\n",
            "I1208 00:27:13.247897 140583164200832 learning.py:507] global step 2771: loss = 2.0540 (1.415 sec/step)\n",
            "INFO:tensorflow:global step 2772: loss = 2.0883 (0.586 sec/step)\n",
            "I1208 00:27:13.835863 140583164200832 learning.py:507] global step 2772: loss = 2.0883 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 2773: loss = 1.8362 (1.706 sec/step)\n",
            "I1208 00:27:15.543090 140583164200832 learning.py:507] global step 2773: loss = 1.8362 (1.706 sec/step)\n",
            "INFO:tensorflow:global step 2774: loss = 2.1160 (1.084 sec/step)\n",
            "I1208 00:27:16.628300 140583164200832 learning.py:507] global step 2774: loss = 2.1160 (1.084 sec/step)\n",
            "INFO:tensorflow:global step 2775: loss = 1.8508 (0.651 sec/step)\n",
            "I1208 00:27:17.327934 140583164200832 learning.py:507] global step 2775: loss = 1.8508 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2776: loss = 2.4660 (2.009 sec/step)\n",
            "I1208 00:27:19.385682 140583164200832 learning.py:507] global step 2776: loss = 2.4660 (2.009 sec/step)\n",
            "INFO:tensorflow:global step 2777: loss = 1.9112 (0.838 sec/step)\n",
            "I1208 00:27:20.248183 140583164200832 learning.py:507] global step 2777: loss = 1.9112 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 2778: loss = 2.5365 (1.318 sec/step)\n",
            "I1208 00:27:21.768195 140583164200832 learning.py:507] global step 2778: loss = 2.5365 (1.318 sec/step)\n",
            "INFO:tensorflow:global step 2779: loss = 2.0644 (0.580 sec/step)\n",
            "I1208 00:27:22.538099 140583164200832 learning.py:507] global step 2779: loss = 2.0644 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 2780: loss = 1.9798 (1.957 sec/step)\n",
            "I1208 00:27:24.691239 140583164200832 learning.py:507] global step 2780: loss = 1.9798 (1.957 sec/step)\n",
            "INFO:tensorflow:global step 2781: loss = 1.9736 (0.567 sec/step)\n",
            "I1208 00:27:25.259665 140583164200832 learning.py:507] global step 2781: loss = 1.9736 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 2782: loss = 1.8029 (2.057 sec/step)\n",
            "I1208 00:27:27.318835 140583164200832 learning.py:507] global step 2782: loss = 1.8029 (2.057 sec/step)\n",
            "INFO:tensorflow:global step 2783: loss = 1.9160 (0.470 sec/step)\n",
            "I1208 00:27:27.790946 140583164200832 learning.py:507] global step 2783: loss = 1.9160 (0.470 sec/step)\n",
            "INFO:tensorflow:global step 2784: loss = 2.7608 (1.838 sec/step)\n",
            "I1208 00:27:29.630850 140583164200832 learning.py:507] global step 2784: loss = 2.7608 (1.838 sec/step)\n",
            "INFO:tensorflow:global step 2785: loss = 1.7619 (0.675 sec/step)\n",
            "I1208 00:27:30.318387 140583164200832 learning.py:507] global step 2785: loss = 1.7619 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 2786: loss = 1.7572 (1.496 sec/step)\n",
            "I1208 00:27:31.815818 140583164200832 learning.py:507] global step 2786: loss = 1.7572 (1.496 sec/step)\n",
            "INFO:tensorflow:global step 2787: loss = 2.1059 (0.620 sec/step)\n",
            "I1208 00:27:32.673397 140583164200832 learning.py:507] global step 2787: loss = 2.1059 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2788: loss = 2.9242 (0.676 sec/step)\n",
            "I1208 00:27:33.625690 140583164200832 learning.py:507] global step 2788: loss = 2.9242 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 2789: loss = 1.7906 (0.626 sec/step)\n",
            "I1208 00:27:34.498119 140583164200832 learning.py:507] global step 2789: loss = 1.7906 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 2790: loss = 1.7924 (1.383 sec/step)\n",
            "I1208 00:27:36.054699 140583164200832 learning.py:507] global step 2790: loss = 1.7924 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 2791: loss = 2.2623 (0.634 sec/step)\n",
            "I1208 00:27:36.732517 140583164200832 learning.py:507] global step 2791: loss = 2.2623 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2792: loss = 1.5813 (1.560 sec/step)\n",
            "I1208 00:27:38.432893 140583164200832 learning.py:507] global step 2792: loss = 1.5813 (1.560 sec/step)\n",
            "INFO:tensorflow:global step 2793: loss = 2.0898 (0.464 sec/step)\n",
            "I1208 00:27:38.899358 140583164200832 learning.py:507] global step 2793: loss = 2.0898 (0.464 sec/step)\n",
            "INFO:tensorflow:global step 2794: loss = 2.4736 (1.725 sec/step)\n",
            "I1208 00:27:40.626264 140583164200832 learning.py:507] global step 2794: loss = 2.4736 (1.725 sec/step)\n",
            "INFO:tensorflow:global step 2795: loss = 1.9506 (0.636 sec/step)\n",
            "I1208 00:27:41.350912 140583164200832 learning.py:507] global step 2795: loss = 1.9506 (0.636 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2796.\n",
            "I1208 00:27:44.052154 140579459213056 supervisor.py:1050] Recording summary at step 2796.\n",
            "INFO:tensorflow:global step 2796: loss = 1.3971 (2.464 sec/step)\n",
            "I1208 00:27:44.073681 140583164200832 learning.py:507] global step 2796: loss = 1.3971 (2.464 sec/step)\n",
            "INFO:tensorflow:global step 2797: loss = 1.8795 (0.552 sec/step)\n",
            "I1208 00:27:44.899955 140583164200832 learning.py:507] global step 2797: loss = 1.8795 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 2798: loss = 2.0332 (1.251 sec/step)\n",
            "I1208 00:27:46.359467 140583164200832 learning.py:507] global step 2798: loss = 2.0332 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2799: loss = 2.5396 (0.658 sec/step)\n",
            "I1208 00:27:47.024880 140583164200832 learning.py:507] global step 2799: loss = 2.5396 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2800: loss = 1.8689 (1.587 sec/step)\n",
            "I1208 00:27:48.613847 140583164200832 learning.py:507] global step 2800: loss = 1.8689 (1.587 sec/step)\n",
            "INFO:tensorflow:global step 2801: loss = 2.2921 (0.654 sec/step)\n",
            "I1208 00:27:49.473706 140583164200832 learning.py:507] global step 2801: loss = 2.2921 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2802: loss = 2.2905 (0.673 sec/step)\n",
            "I1208 00:27:50.377336 140583164200832 learning.py:507] global step 2802: loss = 2.2905 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 2803: loss = 1.6769 (1.627 sec/step)\n",
            "I1208 00:27:52.095658 140583164200832 learning.py:507] global step 2803: loss = 1.6769 (1.627 sec/step)\n",
            "INFO:tensorflow:global step 2804: loss = 1.9362 (0.560 sec/step)\n",
            "I1208 00:27:52.940299 140583164200832 learning.py:507] global step 2804: loss = 1.9362 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 2805: loss = 2.6240 (0.537 sec/step)\n",
            "I1208 00:27:53.676359 140583164200832 learning.py:507] global step 2805: loss = 2.6240 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 2806: loss = 2.2626 (1.790 sec/step)\n",
            "I1208 00:27:55.468431 140583164200832 learning.py:507] global step 2806: loss = 2.2626 (1.790 sec/step)\n",
            "INFO:tensorflow:global step 2807: loss = 2.2707 (1.155 sec/step)\n",
            "I1208 00:27:56.624560 140583164200832 learning.py:507] global step 2807: loss = 2.2707 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 2808: loss = 1.7096 (1.178 sec/step)\n",
            "I1208 00:27:57.804431 140583164200832 learning.py:507] global step 2808: loss = 1.7096 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 2809: loss = 2.0627 (0.792 sec/step)\n",
            "I1208 00:27:58.792987 140583164200832 learning.py:507] global step 2809: loss = 2.0627 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 2810: loss = 1.8530 (1.257 sec/step)\n",
            "I1208 00:28:00.117347 140583164200832 learning.py:507] global step 2810: loss = 1.8530 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2811: loss = 1.9979 (0.632 sec/step)\n",
            "I1208 00:28:01.026190 140583164200832 learning.py:507] global step 2811: loss = 1.9979 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 2812: loss = 1.7517 (1.198 sec/step)\n",
            "I1208 00:28:02.460146 140583164200832 learning.py:507] global step 2812: loss = 1.7517 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2813: loss = 2.4661 (0.549 sec/step)\n",
            "I1208 00:28:03.296365 140583164200832 learning.py:507] global step 2813: loss = 2.4661 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 2814: loss = 1.9104 (1.186 sec/step)\n",
            "I1208 00:28:04.683638 140583164200832 learning.py:507] global step 2814: loss = 1.9104 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2815: loss = 2.3467 (0.468 sec/step)\n",
            "I1208 00:28:05.153151 140583164200832 learning.py:507] global step 2815: loss = 2.3467 (0.468 sec/step)\n",
            "INFO:tensorflow:global step 2816: loss = 1.9055 (0.672 sec/step)\n",
            "I1208 00:28:05.827399 140583164200832 learning.py:507] global step 2816: loss = 1.9055 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2817: loss = 2.7496 (2.796 sec/step)\n",
            "I1208 00:28:08.794865 140583164200832 learning.py:507] global step 2817: loss = 2.7496 (2.796 sec/step)\n",
            "INFO:tensorflow:global step 2818: loss = 1.4448 (0.629 sec/step)\n",
            "I1208 00:28:09.431925 140583164200832 learning.py:507] global step 2818: loss = 1.4448 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 2819: loss = 1.8478 (1.860 sec/step)\n",
            "I1208 00:28:11.294206 140583164200832 learning.py:507] global step 2819: loss = 1.8478 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 2820: loss = 1.9711 (0.598 sec/step)\n",
            "I1208 00:28:12.112967 140583164200832 learning.py:507] global step 2820: loss = 1.9711 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2821: loss = 1.6285 (1.374 sec/step)\n",
            "I1208 00:28:13.680518 140583164200832 learning.py:507] global step 2821: loss = 1.6285 (1.374 sec/step)\n",
            "INFO:tensorflow:global step 2822: loss = 2.1419 (0.577 sec/step)\n",
            "I1208 00:28:14.717758 140583164200832 learning.py:507] global step 2822: loss = 2.1419 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2823: loss = 2.1993 (0.659 sec/step)\n",
            "I1208 00:28:15.418569 140583164200832 learning.py:507] global step 2823: loss = 2.1993 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2824: loss = 2.0751 (1.821 sec/step)\n",
            "I1208 00:28:17.241348 140583164200832 learning.py:507] global step 2824: loss = 2.0751 (1.821 sec/step)\n",
            "INFO:tensorflow:global step 2825: loss = 1.8396 (0.596 sec/step)\n",
            "I1208 00:28:17.839465 140583164200832 learning.py:507] global step 2825: loss = 1.8396 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2826: loss = 1.3695 (1.838 sec/step)\n",
            "I1208 00:28:19.679179 140583164200832 learning.py:507] global step 2826: loss = 1.3695 (1.838 sec/step)\n",
            "INFO:tensorflow:global step 2827: loss = 2.3386 (0.613 sec/step)\n",
            "I1208 00:28:20.294202 140583164200832 learning.py:507] global step 2827: loss = 2.3386 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 2828: loss = 2.2108 (1.865 sec/step)\n",
            "I1208 00:28:22.161254 140583164200832 learning.py:507] global step 2828: loss = 2.2108 (1.865 sec/step)\n",
            "INFO:tensorflow:global step 2829: loss = 1.6689 (0.649 sec/step)\n",
            "I1208 00:28:22.811910 140583164200832 learning.py:507] global step 2829: loss = 1.6689 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 2830: loss = 1.9042 (1.093 sec/step)\n",
            "I1208 00:28:24.108954 140583164200832 learning.py:507] global step 2830: loss = 1.9042 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 2831: loss = 2.3664 (0.620 sec/step)\n",
            "I1208 00:28:25.237841 140583164200832 learning.py:507] global step 2831: loss = 2.3664 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2832: loss = 1.9962 (0.594 sec/step)\n",
            "I1208 00:28:26.164003 140583164200832 learning.py:507] global step 2832: loss = 1.9962 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2833: loss = 1.9527 (0.777 sec/step)\n",
            "I1208 00:28:27.146431 140583164200832 learning.py:507] global step 2833: loss = 1.9527 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 2834: loss = 2.3215 (1.264 sec/step)\n",
            "I1208 00:28:28.666597 140583164200832 learning.py:507] global step 2834: loss = 2.3215 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 2835: loss = 1.9110 (0.510 sec/step)\n",
            "I1208 00:28:29.674871 140583164200832 learning.py:507] global step 2835: loss = 1.9110 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 2836: loss = 1.5332 (1.154 sec/step)\n",
            "I1208 00:28:30.878360 140583164200832 learning.py:507] global step 2836: loss = 1.5332 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 2837: loss = 1.7214 (0.658 sec/step)\n",
            "I1208 00:28:31.561444 140583164200832 learning.py:507] global step 2837: loss = 1.7214 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2838: loss = 2.0771 (1.394 sec/step)\n",
            "I1208 00:28:33.147604 140583164200832 learning.py:507] global step 2838: loss = 2.0771 (1.394 sec/step)\n",
            "INFO:tensorflow:global step 2839: loss = 1.9636 (0.625 sec/step)\n",
            "I1208 00:28:33.966455 140583164200832 learning.py:507] global step 2839: loss = 1.9636 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2840: loss = 2.0834 (0.743 sec/step)\n",
            "I1208 00:28:35.071495 140583164200832 learning.py:507] global step 2840: loss = 2.0834 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2841: loss = 1.7312 (1.213 sec/step)\n",
            "I1208 00:28:36.437951 140583164200832 learning.py:507] global step 2841: loss = 1.7312 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 2842: loss = 1.9185 (0.587 sec/step)\n",
            "I1208 00:28:37.139486 140583164200832 learning.py:507] global step 2842: loss = 1.9185 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 2843: loss = 1.5989 (0.732 sec/step)\n",
            "I1208 00:28:38.039205 140583164200832 learning.py:507] global step 2843: loss = 1.5989 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 2844: loss = 1.9521 (0.639 sec/step)\n",
            "I1208 00:28:38.719691 140583164200832 learning.py:507] global step 2844: loss = 1.9521 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2845: loss = 1.8605 (2.131 sec/step)\n",
            "I1208 00:28:41.441043 140583164200832 learning.py:507] global step 2845: loss = 1.8605 (2.131 sec/step)\n",
            "INFO:tensorflow:global step 2846: loss = 1.8928 (0.590 sec/step)\n",
            "I1208 00:28:42.256204 140583164200832 learning.py:507] global step 2846: loss = 1.8928 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 2847: loss = 2.0881 (0.595 sec/step)\n",
            "I1208 00:28:43.071129 140583164200832 learning.py:507] global step 2847: loss = 2.0881 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 2848: loss = 1.6909 (1.919 sec/step)\n",
            "I1208 00:28:45.010773 140583164200832 learning.py:507] global step 2848: loss = 1.6909 (1.919 sec/step)\n",
            "INFO:tensorflow:global step 2849: loss = 2.0961 (0.624 sec/step)\n",
            "I1208 00:28:45.636931 140583164200832 learning.py:507] global step 2849: loss = 2.0961 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2850: loss = 2.1393 (1.905 sec/step)\n",
            "I1208 00:28:47.544079 140583164200832 learning.py:507] global step 2850: loss = 2.1393 (1.905 sec/step)\n",
            "INFO:tensorflow:global step 2851: loss = 2.0865 (0.696 sec/step)\n",
            "I1208 00:28:48.479743 140583164200832 learning.py:507] global step 2851: loss = 2.0865 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 2852: loss = 2.1426 (1.887 sec/step)\n",
            "I1208 00:28:50.468653 140583164200832 learning.py:507] global step 2852: loss = 2.1426 (1.887 sec/step)\n",
            "INFO:tensorflow:global step 2853: loss = 1.9635 (0.703 sec/step)\n",
            "I1208 00:28:51.451067 140583164200832 learning.py:507] global step 2853: loss = 1.9635 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 2854: loss = 2.3190 (0.663 sec/step)\n",
            "I1208 00:28:52.475313 140583164200832 learning.py:507] global step 2854: loss = 2.3190 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2855: loss = 1.9266 (0.585 sec/step)\n",
            "I1208 00:28:53.415575 140583164200832 learning.py:507] global step 2855: loss = 1.9266 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2856: loss = 1.9666 (0.738 sec/step)\n",
            "I1208 00:28:54.422742 140583164200832 learning.py:507] global step 2856: loss = 1.9666 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 2857: loss = 1.6028 (2.339 sec/step)\n",
            "I1208 00:28:56.763733 140583164200832 learning.py:507] global step 2857: loss = 1.6028 (2.339 sec/step)\n",
            "INFO:tensorflow:global step 2858: loss = 1.8047 (0.513 sec/step)\n",
            "I1208 00:28:57.278014 140583164200832 learning.py:507] global step 2858: loss = 1.8047 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 2859: loss = 2.1032 (1.267 sec/step)\n",
            "I1208 00:28:58.836453 140583164200832 learning.py:507] global step 2859: loss = 2.1032 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2860: loss = 2.1408 (0.669 sec/step)\n",
            "I1208 00:28:59.913123 140583164200832 learning.py:507] global step 2860: loss = 2.1408 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 2861: loss = 2.6470 (1.604 sec/step)\n",
            "I1208 00:29:01.738322 140583164200832 learning.py:507] global step 2861: loss = 2.6470 (1.604 sec/step)\n",
            "INFO:tensorflow:global step 2862: loss = 2.2701 (0.555 sec/step)\n",
            "I1208 00:29:02.541714 140583164200832 learning.py:507] global step 2862: loss = 2.2701 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2863: loss = 1.6091 (1.569 sec/step)\n",
            "I1208 00:29:04.363557 140583164200832 learning.py:507] global step 2863: loss = 1.6091 (1.569 sec/step)\n",
            "INFO:tensorflow:global step 2864: loss = 2.1952 (0.611 sec/step)\n",
            "I1208 00:29:05.290949 140583164200832 learning.py:507] global step 2864: loss = 2.1952 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 2865: loss = 2.7311 (0.562 sec/step)\n",
            "I1208 00:29:05.999039 140583164200832 learning.py:507] global step 2865: loss = 2.7311 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 2866: loss = 1.8055 (1.744 sec/step)\n",
            "I1208 00:29:07.744897 140583164200832 learning.py:507] global step 2866: loss = 1.8055 (1.744 sec/step)\n",
            "INFO:tensorflow:global step 2867: loss = 1.2934 (0.641 sec/step)\n",
            "I1208 00:29:08.632186 140583164200832 learning.py:507] global step 2867: loss = 1.2934 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2868: loss = 2.6738 (1.606 sec/step)\n",
            "I1208 00:29:10.431317 140583164200832 learning.py:507] global step 2868: loss = 2.6738 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 2869: loss = 2.5055 (0.562 sec/step)\n",
            "I1208 00:29:10.995300 140583164200832 learning.py:507] global step 2869: loss = 2.5055 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 2870: loss = 1.3989 (1.643 sec/step)\n",
            "I1208 00:29:12.797032 140583164200832 learning.py:507] global step 2870: loss = 1.3989 (1.643 sec/step)\n",
            "INFO:tensorflow:global step 2871: loss = 1.6006 (0.534 sec/step)\n",
            "I1208 00:29:13.668211 140583164200832 learning.py:507] global step 2871: loss = 1.6006 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 2872: loss = 1.8167 (1.143 sec/step)\n",
            "I1208 00:29:14.998481 140583164200832 learning.py:507] global step 2872: loss = 1.8167 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 2873: loss = 2.3140 (0.527 sec/step)\n",
            "I1208 00:29:15.781114 140583164200832 learning.py:507] global step 2873: loss = 2.3140 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 2874: loss = 2.4663 (0.463 sec/step)\n",
            "I1208 00:29:16.497901 140583164200832 learning.py:507] global step 2874: loss = 2.4663 (0.463 sec/step)\n",
            "INFO:tensorflow:global step 2875: loss = 1.6238 (1.606 sec/step)\n",
            "I1208 00:29:18.106299 140583164200832 learning.py:507] global step 2875: loss = 1.6238 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 2876: loss = 1.7992 (0.565 sec/step)\n",
            "I1208 00:29:18.868579 140583164200832 learning.py:507] global step 2876: loss = 1.7992 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 2877: loss = 1.6394 (1.828 sec/step)\n",
            "I1208 00:29:20.913911 140583164200832 learning.py:507] global step 2877: loss = 1.6394 (1.828 sec/step)\n",
            "INFO:tensorflow:global step 2878: loss = 2.2148 (0.751 sec/step)\n",
            "I1208 00:29:21.820978 140583164200832 learning.py:507] global step 2878: loss = 2.2148 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 2879: loss = 2.4321 (1.272 sec/step)\n",
            "I1208 00:29:23.178182 140583164200832 learning.py:507] global step 2879: loss = 2.4321 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2880: loss = 1.9807 (0.718 sec/step)\n",
            "I1208 00:29:24.115044 140583164200832 learning.py:507] global step 2880: loss = 1.9807 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 2881: loss = 2.0055 (1.792 sec/step)\n",
            "I1208 00:29:25.937439 140583164200832 learning.py:507] global step 2881: loss = 2.0055 (1.792 sec/step)\n",
            "INFO:tensorflow:global step 2882: loss = 2.2027 (0.677 sec/step)\n",
            "I1208 00:29:26.785016 140583164200832 learning.py:507] global step 2882: loss = 2.2027 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 2883: loss = 1.8008 (0.557 sec/step)\n",
            "I1208 00:29:27.573572 140583164200832 learning.py:507] global step 2883: loss = 1.8008 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 2884: loss = 2.0857 (0.885 sec/step)\n",
            "I1208 00:29:28.708966 140583164200832 learning.py:507] global step 2884: loss = 2.0857 (0.885 sec/step)\n",
            "INFO:tensorflow:global step 2885: loss = 2.0146 (1.512 sec/step)\n",
            "I1208 00:29:30.512760 140583164200832 learning.py:507] global step 2885: loss = 2.0146 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 2886: loss = 1.9607 (0.543 sec/step)\n",
            "I1208 00:29:31.057090 140583164200832 learning.py:507] global step 2886: loss = 1.9607 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 2887: loss = 2.2035 (0.635 sec/step)\n",
            "I1208 00:29:31.693703 140583164200832 learning.py:507] global step 2887: loss = 2.2035 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 2888: loss = 1.8981 (2.592 sec/step)\n",
            "I1208 00:29:34.651126 140583164200832 learning.py:507] global step 2888: loss = 1.8981 (2.592 sec/step)\n",
            "INFO:tensorflow:global step 2889: loss = 2.1362 (0.693 sec/step)\n",
            "I1208 00:29:35.380151 140583164200832 learning.py:507] global step 2889: loss = 2.1362 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 2890: loss = 2.0635 (1.647 sec/step)\n",
            "I1208 00:29:37.209477 140583164200832 learning.py:507] global step 2890: loss = 2.0635 (1.647 sec/step)\n",
            "INFO:tensorflow:global step 2891: loss = 2.1425 (0.569 sec/step)\n",
            "I1208 00:29:37.781027 140583164200832 learning.py:507] global step 2891: loss = 2.1425 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 2892: loss = 1.8652 (1.755 sec/step)\n",
            "I1208 00:29:39.539531 140583164200832 learning.py:507] global step 2892: loss = 1.8652 (1.755 sec/step)\n",
            "INFO:tensorflow:global step 2893: loss = 2.0573 (0.780 sec/step)\n",
            "I1208 00:29:40.401157 140583164200832 learning.py:507] global step 2893: loss = 2.0573 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 2894: loss = 2.1532 (0.552 sec/step)\n",
            "I1208 00:29:41.605747 140583164200832 learning.py:507] global step 2894: loss = 2.1532 (0.552 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:29:42.076468 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 2895: loss = 1.8330 (2.676 sec/step)\n",
            "I1208 00:29:44.923176 140583164200832 learning.py:507] global step 2895: loss = 1.8330 (2.676 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2895.\n",
            "I1208 00:29:45.148220 140579459213056 supervisor.py:1050] Recording summary at step 2895.\n",
            "INFO:tensorflow:global step 2896: loss = 2.0745 (0.734 sec/step)\n",
            "I1208 00:29:46.402312 140583164200832 learning.py:507] global step 2896: loss = 2.0745 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 2897: loss = 1.7451 (1.652 sec/step)\n",
            "I1208 00:29:48.315840 140583164200832 learning.py:507] global step 2897: loss = 1.7451 (1.652 sec/step)\n",
            "INFO:tensorflow:global step 2898: loss = 1.8951 (0.566 sec/step)\n",
            "I1208 00:29:49.180919 140583164200832 learning.py:507] global step 2898: loss = 1.8951 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2899: loss = 1.9547 (1.534 sec/step)\n",
            "I1208 00:29:50.826587 140583164200832 learning.py:507] global step 2899: loss = 1.9547 (1.534 sec/step)\n",
            "INFO:tensorflow:global step 2900: loss = 1.5759 (0.665 sec/step)\n",
            "I1208 00:29:51.713878 140583164200832 learning.py:507] global step 2900: loss = 1.5759 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2901: loss = 2.6603 (0.586 sec/step)\n",
            "I1208 00:29:52.467490 140583164200832 learning.py:507] global step 2901: loss = 2.6603 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 2902: loss = 2.5983 (1.862 sec/step)\n",
            "I1208 00:29:54.331633 140583164200832 learning.py:507] global step 2902: loss = 2.5983 (1.862 sec/step)\n",
            "INFO:tensorflow:global step 2903: loss = 2.0110 (0.582 sec/step)\n",
            "I1208 00:29:55.185853 140583164200832 learning.py:507] global step 2903: loss = 2.0110 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 2904: loss = 2.6132 (0.646 sec/step)\n",
            "I1208 00:29:56.284955 140583164200832 learning.py:507] global step 2904: loss = 2.6132 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 2905: loss = 1.3940 (0.565 sec/step)\n",
            "I1208 00:29:57.521778 140583164200832 learning.py:507] global step 2905: loss = 1.3940 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 2906: loss = 1.7324 (0.605 sec/step)\n",
            "I1208 00:29:58.560086 140583164200832 learning.py:507] global step 2906: loss = 1.7324 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2907: loss = 1.6443 (0.599 sec/step)\n",
            "I1208 00:29:59.624906 140583164200832 learning.py:507] global step 2907: loss = 1.6443 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 2908: loss = 2.1352 (1.568 sec/step)\n",
            "I1208 00:30:01.213330 140583164200832 learning.py:507] global step 2908: loss = 2.1352 (1.568 sec/step)\n",
            "INFO:tensorflow:global step 2909: loss = 1.6347 (0.475 sec/step)\n",
            "I1208 00:30:01.690288 140583164200832 learning.py:507] global step 2909: loss = 1.6347 (0.475 sec/step)\n",
            "INFO:tensorflow:global step 2910: loss = 1.9082 (1.923 sec/step)\n",
            "I1208 00:30:03.615432 140583164200832 learning.py:507] global step 2910: loss = 1.9082 (1.923 sec/step)\n",
            "INFO:tensorflow:global step 2911: loss = 2.0542 (0.676 sec/step)\n",
            "I1208 00:30:04.297412 140583164200832 learning.py:507] global step 2911: loss = 2.0542 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 2912: loss = 1.7417 (1.519 sec/step)\n",
            "I1208 00:30:05.863031 140583164200832 learning.py:507] global step 2912: loss = 1.7417 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 2913: loss = 1.8485 (0.595 sec/step)\n",
            "I1208 00:30:06.847788 140583164200832 learning.py:507] global step 2913: loss = 1.8485 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 2914: loss = 1.8852 (1.734 sec/step)\n",
            "I1208 00:30:08.641057 140583164200832 learning.py:507] global step 2914: loss = 1.8852 (1.734 sec/step)\n",
            "INFO:tensorflow:global step 2915: loss = 2.1161 (0.555 sec/step)\n",
            "I1208 00:30:09.367256 140583164200832 learning.py:507] global step 2915: loss = 2.1161 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 2916: loss = 1.8738 (1.776 sec/step)\n",
            "I1208 00:30:11.277536 140583164200832 learning.py:507] global step 2916: loss = 1.8738 (1.776 sec/step)\n",
            "INFO:tensorflow:global step 2917: loss = 1.9277 (0.523 sec/step)\n",
            "I1208 00:30:11.802333 140583164200832 learning.py:507] global step 2917: loss = 1.9277 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 2918: loss = 2.7037 (1.614 sec/step)\n",
            "I1208 00:30:13.452497 140583164200832 learning.py:507] global step 2918: loss = 2.7037 (1.614 sec/step)\n",
            "INFO:tensorflow:global step 2919: loss = 2.6109 (0.709 sec/step)\n",
            "I1208 00:30:14.304622 140583164200832 learning.py:507] global step 2919: loss = 2.6109 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 2920: loss = 2.3682 (0.753 sec/step)\n",
            "I1208 00:30:15.278035 140583164200832 learning.py:507] global step 2920: loss = 2.3682 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 2921: loss = 1.9736 (1.432 sec/step)\n",
            "I1208 00:30:16.775826 140583164200832 learning.py:507] global step 2921: loss = 1.9736 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 2922: loss = 1.7656 (0.572 sec/step)\n",
            "I1208 00:30:17.482792 140583164200832 learning.py:507] global step 2922: loss = 1.7656 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 2923: loss = 2.1548 (1.417 sec/step)\n",
            "I1208 00:30:19.072555 140583164200832 learning.py:507] global step 2923: loss = 2.1548 (1.417 sec/step)\n",
            "INFO:tensorflow:global step 2924: loss = 2.7112 (0.484 sec/step)\n",
            "I1208 00:30:19.558415 140583164200832 learning.py:507] global step 2924: loss = 2.7112 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 2925: loss = 2.4281 (0.952 sec/step)\n",
            "I1208 00:30:20.707696 140583164200832 learning.py:507] global step 2925: loss = 2.4281 (0.952 sec/step)\n",
            "INFO:tensorflow:global step 2926: loss = 1.5099 (1.620 sec/step)\n",
            "I1208 00:30:22.538544 140583164200832 learning.py:507] global step 2926: loss = 1.5099 (1.620 sec/step)\n",
            "INFO:tensorflow:global step 2927: loss = 1.8377 (0.658 sec/step)\n",
            "I1208 00:30:23.198442 140583164200832 learning.py:507] global step 2927: loss = 1.8377 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2928: loss = 2.1962 (1.455 sec/step)\n",
            "I1208 00:30:24.846213 140583164200832 learning.py:507] global step 2928: loss = 2.1962 (1.455 sec/step)\n",
            "INFO:tensorflow:global step 2929: loss = 1.6737 (1.187 sec/step)\n",
            "I1208 00:30:26.057127 140583164200832 learning.py:507] global step 2929: loss = 1.6737 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 2930: loss = 1.7643 (0.573 sec/step)\n",
            "I1208 00:30:26.632013 140583164200832 learning.py:507] global step 2930: loss = 1.7643 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 2931: loss = 2.0103 (1.823 sec/step)\n",
            "I1208 00:30:28.457007 140583164200832 learning.py:507] global step 2931: loss = 2.0103 (1.823 sec/step)\n",
            "INFO:tensorflow:global step 2932: loss = 1.8200 (0.589 sec/step)\n",
            "I1208 00:30:29.048442 140583164200832 learning.py:507] global step 2932: loss = 1.8200 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 2933: loss = 2.8543 (1.880 sec/step)\n",
            "I1208 00:30:30.930488 140583164200832 learning.py:507] global step 2933: loss = 2.8543 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 2934: loss = 1.7196 (0.701 sec/step)\n",
            "I1208 00:30:31.852448 140583164200832 learning.py:507] global step 2934: loss = 1.7196 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 2935: loss = 2.1315 (0.540 sec/step)\n",
            "I1208 00:30:32.571348 140583164200832 learning.py:507] global step 2935: loss = 2.1315 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 2936: loss = 2.4179 (1.667 sec/step)\n",
            "I1208 00:30:34.263081 140583164200832 learning.py:507] global step 2936: loss = 2.4179 (1.667 sec/step)\n",
            "INFO:tensorflow:global step 2937: loss = 1.5142 (0.622 sec/step)\n",
            "I1208 00:30:35.200440 140583164200832 learning.py:507] global step 2937: loss = 1.5142 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2938: loss = 2.4179 (0.563 sec/step)\n",
            "I1208 00:30:36.242954 140583164200832 learning.py:507] global step 2938: loss = 2.4179 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2939: loss = 2.0010 (0.621 sec/step)\n",
            "I1208 00:30:37.187169 140583164200832 learning.py:507] global step 2939: loss = 2.0010 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 2940: loss = 2.4509 (1.358 sec/step)\n",
            "I1208 00:30:38.826227 140583164200832 learning.py:507] global step 2940: loss = 2.4509 (1.358 sec/step)\n",
            "INFO:tensorflow:global step 2941: loss = 2.3305 (0.678 sec/step)\n",
            "I1208 00:30:39.565724 140583164200832 learning.py:507] global step 2941: loss = 2.3305 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2942: loss = 2.0864 (1.531 sec/step)\n",
            "I1208 00:30:41.122188 140583164200832 learning.py:507] global step 2942: loss = 2.0864 (1.531 sec/step)\n",
            "INFO:tensorflow:global step 2943: loss = 2.3089 (0.664 sec/step)\n",
            "I1208 00:30:41.868801 140583164200832 learning.py:507] global step 2943: loss = 2.3089 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 2944: loss = 1.5040 (1.952 sec/step)\n",
            "I1208 00:30:43.847748 140583164200832 learning.py:507] global step 2944: loss = 1.5040 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 2945: loss = 1.8762 (0.550 sec/step)\n",
            "I1208 00:30:44.400190 140583164200832 learning.py:507] global step 2945: loss = 1.8762 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 2946: loss = 2.0372 (1.905 sec/step)\n",
            "I1208 00:30:46.307594 140583164200832 learning.py:507] global step 2946: loss = 2.0372 (1.905 sec/step)\n",
            "INFO:tensorflow:global step 2947: loss = 1.6682 (0.570 sec/step)\n",
            "I1208 00:30:47.137587 140583164200832 learning.py:507] global step 2947: loss = 1.6682 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 2948: loss = 1.8093 (0.624 sec/step)\n",
            "I1208 00:30:48.125752 140583164200832 learning.py:507] global step 2948: loss = 1.8093 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2949: loss = 2.7246 (1.539 sec/step)\n",
            "I1208 00:30:49.909613 140583164200832 learning.py:507] global step 2949: loss = 2.7246 (1.539 sec/step)\n",
            "INFO:tensorflow:global step 2950: loss = 2.2189 (0.563 sec/step)\n",
            "I1208 00:30:50.699412 140583164200832 learning.py:507] global step 2950: loss = 2.2189 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 2951: loss = 1.6517 (0.575 sec/step)\n",
            "I1208 00:30:51.545371 140583164200832 learning.py:507] global step 2951: loss = 1.6517 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 2952: loss = 2.0803 (2.155 sec/step)\n",
            "I1208 00:30:53.701752 140583164200832 learning.py:507] global step 2952: loss = 2.0803 (2.155 sec/step)\n",
            "INFO:tensorflow:global step 2953: loss = 1.4871 (0.495 sec/step)\n",
            "I1208 00:30:54.198695 140583164200832 learning.py:507] global step 2953: loss = 1.4871 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 2954: loss = 2.2958 (2.037 sec/step)\n",
            "I1208 00:30:56.264986 140583164200832 learning.py:507] global step 2954: loss = 2.2958 (2.037 sec/step)\n",
            "INFO:tensorflow:global step 2955: loss = 1.6051 (0.588 sec/step)\n",
            "I1208 00:30:56.855491 140583164200832 learning.py:507] global step 2955: loss = 1.6051 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 2956: loss = 1.8872 (0.841 sec/step)\n",
            "I1208 00:30:57.958076 140583164200832 learning.py:507] global step 2956: loss = 1.8872 (0.841 sec/step)\n",
            "INFO:tensorflow:global step 2957: loss = 1.4638 (1.606 sec/step)\n",
            "I1208 00:30:59.862120 140583164200832 learning.py:507] global step 2957: loss = 1.4638 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 2958: loss = 1.7045 (0.615 sec/step)\n",
            "I1208 00:31:00.479472 140583164200832 learning.py:507] global step 2958: loss = 1.7045 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2959: loss = 1.4532 (1.715 sec/step)\n",
            "I1208 00:31:02.196543 140583164200832 learning.py:507] global step 2959: loss = 1.4532 (1.715 sec/step)\n",
            "INFO:tensorflow:global step 2960: loss = 1.5008 (0.792 sec/step)\n",
            "I1208 00:31:03.175430 140583164200832 learning.py:507] global step 2960: loss = 1.5008 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 2961: loss = 2.0862 (0.523 sec/step)\n",
            "I1208 00:31:03.789303 140583164200832 learning.py:507] global step 2961: loss = 2.0862 (0.523 sec/step)\n",
            "INFO:tensorflow:global step 2962: loss = 1.7918 (0.797 sec/step)\n",
            "I1208 00:31:04.740606 140583164200832 learning.py:507] global step 2962: loss = 1.7918 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 2963: loss = 1.7005 (1.869 sec/step)\n",
            "I1208 00:31:06.846394 140583164200832 learning.py:507] global step 2963: loss = 1.7005 (1.869 sec/step)\n",
            "INFO:tensorflow:global step 2964: loss = 2.0420 (1.106 sec/step)\n",
            "I1208 00:31:07.953662 140583164200832 learning.py:507] global step 2964: loss = 2.0420 (1.106 sec/step)\n",
            "INFO:tensorflow:global step 2965: loss = 2.2287 (0.695 sec/step)\n",
            "I1208 00:31:08.729574 140583164200832 learning.py:507] global step 2965: loss = 2.2287 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 2966: loss = 1.8547 (1.655 sec/step)\n",
            "I1208 00:31:10.430605 140583164200832 learning.py:507] global step 2966: loss = 1.8547 (1.655 sec/step)\n",
            "INFO:tensorflow:global step 2967: loss = 2.2336 (0.510 sec/step)\n",
            "I1208 00:31:10.942925 140583164200832 learning.py:507] global step 2967: loss = 2.2336 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 2968: loss = 2.4807 (1.909 sec/step)\n",
            "I1208 00:31:12.853949 140583164200832 learning.py:507] global step 2968: loss = 2.4807 (1.909 sec/step)\n",
            "INFO:tensorflow:global step 2969: loss = 2.0265 (0.685 sec/step)\n",
            "I1208 00:31:13.790777 140583164200832 learning.py:507] global step 2969: loss = 2.0265 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 2970: loss = 2.1803 (1.619 sec/step)\n",
            "I1208 00:31:15.542701 140583164200832 learning.py:507] global step 2970: loss = 2.1803 (1.619 sec/step)\n",
            "INFO:tensorflow:global step 2971: loss = 1.9572 (0.580 sec/step)\n",
            "I1208 00:31:16.339372 140583164200832 learning.py:507] global step 2971: loss = 1.9572 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 2972: loss = 1.4963 (1.387 sec/step)\n",
            "I1208 00:31:17.930720 140583164200832 learning.py:507] global step 2972: loss = 1.4963 (1.387 sec/step)\n",
            "INFO:tensorflow:global step 2973: loss = 1.8244 (0.545 sec/step)\n",
            "I1208 00:31:18.477765 140583164200832 learning.py:507] global step 2973: loss = 1.8244 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 2974: loss = 1.7611 (0.973 sec/step)\n",
            "I1208 00:31:19.631905 140583164200832 learning.py:507] global step 2974: loss = 1.7611 (0.973 sec/step)\n",
            "INFO:tensorflow:global step 2975: loss = 1.8970 (1.664 sec/step)\n",
            "I1208 00:31:21.581362 140583164200832 learning.py:507] global step 2975: loss = 1.8970 (1.664 sec/step)\n",
            "INFO:tensorflow:global step 2976: loss = 2.0213 (0.609 sec/step)\n",
            "I1208 00:31:22.284633 140583164200832 learning.py:507] global step 2976: loss = 2.0213 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2977: loss = 1.3428 (1.243 sec/step)\n",
            "I1208 00:31:23.758473 140583164200832 learning.py:507] global step 2977: loss = 1.3428 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2978: loss = 1.6937 (1.091 sec/step)\n",
            "I1208 00:31:24.850525 140583164200832 learning.py:507] global step 2978: loss = 1.6937 (1.091 sec/step)\n",
            "INFO:tensorflow:global step 2979: loss = 1.7281 (0.608 sec/step)\n",
            "I1208 00:31:25.459993 140583164200832 learning.py:507] global step 2979: loss = 1.7281 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 2980: loss = 2.4738 (0.809 sec/step)\n",
            "I1208 00:31:26.443163 140583164200832 learning.py:507] global step 2980: loss = 2.4738 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 2981: loss = 1.4817 (1.622 sec/step)\n",
            "I1208 00:31:28.299008 140583164200832 learning.py:507] global step 2981: loss = 1.4817 (1.622 sec/step)\n",
            "INFO:tensorflow:global step 2982: loss = 1.5802 (0.623 sec/step)\n",
            "I1208 00:31:29.216857 140583164200832 learning.py:507] global step 2982: loss = 1.5802 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2983: loss = 1.8503 (0.729 sec/step)\n",
            "I1208 00:31:30.387029 140583164200832 learning.py:507] global step 2983: loss = 1.8503 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 2984: loss = 1.8852 (1.649 sec/step)\n",
            "I1208 00:31:32.211134 140583164200832 learning.py:507] global step 2984: loss = 1.8852 (1.649 sec/step)\n",
            "INFO:tensorflow:global step 2985: loss = 1.9708 (0.674 sec/step)\n",
            "I1208 00:31:33.194013 140583164200832 learning.py:507] global step 2985: loss = 1.9708 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2986: loss = 1.9025 (0.569 sec/step)\n",
            "I1208 00:31:33.939224 140583164200832 learning.py:507] global step 2986: loss = 1.9025 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 2987: loss = 2.3900 (0.637 sec/step)\n",
            "I1208 00:31:35.252591 140583164200832 learning.py:507] global step 2987: loss = 2.3900 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2988: loss = 1.7878 (2.039 sec/step)\n",
            "I1208 00:31:37.568700 140583164200832 learning.py:507] global step 2988: loss = 1.7878 (2.039 sec/step)\n",
            "INFO:tensorflow:global step 2989: loss = 1.9091 (0.581 sec/step)\n",
            "I1208 00:31:38.151179 140583164200832 learning.py:507] global step 2989: loss = 1.9091 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 2990: loss = 1.8826 (2.000 sec/step)\n",
            "I1208 00:31:40.152580 140583164200832 learning.py:507] global step 2990: loss = 1.8826 (2.000 sec/step)\n",
            "INFO:tensorflow:global step 2991: loss = 1.8511 (0.531 sec/step)\n",
            "I1208 00:31:40.685766 140583164200832 learning.py:507] global step 2991: loss = 1.8511 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 2992: loss = 1.7865 (3.245 sec/step)\n",
            "I1208 00:31:43.933563 140583164200832 learning.py:507] global step 2992: loss = 1.7865 (3.245 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2992.\n",
            "I1208 00:31:43.951673 140579459213056 supervisor.py:1050] Recording summary at step 2992.\n",
            "INFO:tensorflow:global step 2993: loss = 1.5523 (0.679 sec/step)\n",
            "I1208 00:31:44.836655 140583164200832 learning.py:507] global step 2993: loss = 1.5523 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 2994: loss = 2.0591 (0.853 sec/step)\n",
            "I1208 00:31:46.060708 140583164200832 learning.py:507] global step 2994: loss = 2.0591 (0.853 sec/step)\n",
            "INFO:tensorflow:global step 2995: loss = 1.8815 (2.107 sec/step)\n",
            "I1208 00:31:48.230764 140583164200832 learning.py:507] global step 2995: loss = 1.8815 (2.107 sec/step)\n",
            "INFO:tensorflow:global step 2996: loss = 2.3122 (0.588 sec/step)\n",
            "I1208 00:31:48.820415 140583164200832 learning.py:507] global step 2996: loss = 2.3122 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 2997: loss = 2.0587 (0.779 sec/step)\n",
            "I1208 00:31:49.869164 140583164200832 learning.py:507] global step 2997: loss = 2.0587 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 2998: loss = 2.3558 (1.884 sec/step)\n",
            "I1208 00:31:52.003592 140583164200832 learning.py:507] global step 2998: loss = 2.3558 (1.884 sec/step)\n",
            "INFO:tensorflow:global step 2999: loss = 2.0691 (0.653 sec/step)\n",
            "I1208 00:31:52.728541 140583164200832 learning.py:507] global step 2999: loss = 2.0691 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 3000: loss = 2.8590 (1.502 sec/step)\n",
            "I1208 00:31:54.443608 140583164200832 learning.py:507] global step 3000: loss = 2.8590 (1.502 sec/step)\n",
            "INFO:tensorflow:global step 3001: loss = 1.8430 (0.659 sec/step)\n",
            "I1208 00:31:55.266554 140583164200832 learning.py:507] global step 3001: loss = 1.8430 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 3002: loss = 1.5062 (0.723 sec/step)\n",
            "I1208 00:31:56.409465 140583164200832 learning.py:507] global step 3002: loss = 1.5062 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 3003: loss = 1.7893 (1.371 sec/step)\n",
            "I1208 00:31:57.923948 140583164200832 learning.py:507] global step 3003: loss = 1.7893 (1.371 sec/step)\n",
            "INFO:tensorflow:global step 3004: loss = 1.6286 (0.753 sec/step)\n",
            "I1208 00:31:58.823839 140583164200832 learning.py:507] global step 3004: loss = 1.6286 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 3005: loss = 1.8032 (1.384 sec/step)\n",
            "I1208 00:32:00.222215 140583164200832 learning.py:507] global step 3005: loss = 1.8032 (1.384 sec/step)\n",
            "INFO:tensorflow:global step 3006: loss = 1.5254 (0.586 sec/step)\n",
            "I1208 00:32:00.810079 140583164200832 learning.py:507] global step 3006: loss = 1.5254 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3007: loss = 1.8920 (1.785 sec/step)\n",
            "I1208 00:32:02.596840 140583164200832 learning.py:507] global step 3007: loss = 1.8920 (1.785 sec/step)\n",
            "INFO:tensorflow:global step 3008: loss = 1.6217 (0.534 sec/step)\n",
            "I1208 00:32:03.132626 140583164200832 learning.py:507] global step 3008: loss = 1.6217 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 3009: loss = 2.1764 (0.823 sec/step)\n",
            "I1208 00:32:04.151044 140583164200832 learning.py:507] global step 3009: loss = 2.1764 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 3010: loss = 2.1570 (1.551 sec/step)\n",
            "I1208 00:32:05.940875 140583164200832 learning.py:507] global step 3010: loss = 2.1570 (1.551 sec/step)\n",
            "INFO:tensorflow:global step 3011: loss = 1.8564 (0.602 sec/step)\n",
            "I1208 00:32:06.545157 140583164200832 learning.py:507] global step 3011: loss = 1.8564 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 3012: loss = 1.4943 (1.717 sec/step)\n",
            "I1208 00:32:08.264272 140583164200832 learning.py:507] global step 3012: loss = 1.4943 (1.717 sec/step)\n",
            "INFO:tensorflow:global step 3013: loss = 1.4845 (0.697 sec/step)\n",
            "I1208 00:32:09.233034 140583164200832 learning.py:507] global step 3013: loss = 1.4845 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 3014: loss = 2.2904 (1.706 sec/step)\n",
            "I1208 00:32:11.008144 140583164200832 learning.py:507] global step 3014: loss = 2.2904 (1.706 sec/step)\n",
            "INFO:tensorflow:global step 3015: loss = 1.9119 (0.569 sec/step)\n",
            "I1208 00:32:11.809175 140583164200832 learning.py:507] global step 3015: loss = 1.9119 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 3016: loss = 2.0312 (0.626 sec/step)\n",
            "I1208 00:32:12.977926 140583164200832 learning.py:507] global step 3016: loss = 2.0312 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 3017: loss = 1.7688 (0.596 sec/step)\n",
            "I1208 00:32:13.841834 140583164200832 learning.py:507] global step 3017: loss = 1.7688 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 3018: loss = 1.9037 (1.276 sec/step)\n",
            "I1208 00:32:15.410830 140583164200832 learning.py:507] global step 3018: loss = 1.9037 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 3019: loss = 1.5440 (0.588 sec/step)\n",
            "I1208 00:32:16.000690 140583164200832 learning.py:507] global step 3019: loss = 1.5440 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 3020: loss = 1.6266 (1.726 sec/step)\n",
            "I1208 00:32:17.728956 140583164200832 learning.py:507] global step 3020: loss = 1.6266 (1.726 sec/step)\n",
            "INFO:tensorflow:global step 3021: loss = 1.7944 (0.757 sec/step)\n",
            "I1208 00:32:18.801340 140583164200832 learning.py:507] global step 3021: loss = 1.7944 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 3022: loss = 1.3177 (0.593 sec/step)\n",
            "I1208 00:32:19.510562 140583164200832 learning.py:507] global step 3022: loss = 1.3177 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3023: loss = 2.1776 (0.646 sec/step)\n",
            "I1208 00:32:20.686051 140583164200832 learning.py:507] global step 3023: loss = 2.1776 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 3024: loss = 2.1147 (1.197 sec/step)\n",
            "I1208 00:32:22.004722 140583164200832 learning.py:507] global step 3024: loss = 2.1147 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 3025: loss = 1.9292 (0.641 sec/step)\n",
            "I1208 00:32:22.813540 140583164200832 learning.py:507] global step 3025: loss = 1.9292 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 3026: loss = 1.5015 (1.584 sec/step)\n",
            "I1208 00:32:24.492421 140583164200832 learning.py:507] global step 3026: loss = 1.5015 (1.584 sec/step)\n",
            "INFO:tensorflow:global step 3027: loss = 2.1106 (0.532 sec/step)\n",
            "I1208 00:32:25.026751 140583164200832 learning.py:507] global step 3027: loss = 2.1106 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 3028: loss = 1.6665 (1.382 sec/step)\n",
            "I1208 00:32:26.729184 140583164200832 learning.py:507] global step 3028: loss = 1.6665 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 3029: loss = 1.6262 (0.543 sec/step)\n",
            "I1208 00:32:27.352470 140583164200832 learning.py:507] global step 3029: loss = 1.6262 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 3030: loss = 1.5730 (1.676 sec/step)\n",
            "I1208 00:32:29.188198 140583164200832 learning.py:507] global step 3030: loss = 1.5730 (1.676 sec/step)\n",
            "INFO:tensorflow:global step 3031: loss = 1.6697 (0.550 sec/step)\n",
            "I1208 00:32:30.059232 140583164200832 learning.py:507] global step 3031: loss = 1.6697 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 3032: loss = 1.9454 (1.552 sec/step)\n",
            "I1208 00:32:31.772252 140583164200832 learning.py:507] global step 3032: loss = 1.9454 (1.552 sec/step)\n",
            "INFO:tensorflow:global step 3033: loss = 1.9178 (0.674 sec/step)\n",
            "I1208 00:32:32.646019 140583164200832 learning.py:507] global step 3033: loss = 1.9178 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 3034: loss = 2.0640 (1.341 sec/step)\n",
            "I1208 00:32:34.146255 140583164200832 learning.py:507] global step 3034: loss = 2.0640 (1.341 sec/step)\n",
            "INFO:tensorflow:global step 3035: loss = 1.9931 (1.065 sec/step)\n",
            "I1208 00:32:35.212638 140583164200832 learning.py:507] global step 3035: loss = 1.9931 (1.065 sec/step)\n",
            "INFO:tensorflow:global step 3036: loss = 2.1250 (0.588 sec/step)\n",
            "I1208 00:32:36.103131 140583164200832 learning.py:507] global step 3036: loss = 2.1250 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 3037: loss = 1.9173 (0.536 sec/step)\n",
            "I1208 00:32:36.699039 140583164200832 learning.py:507] global step 3037: loss = 1.9173 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 3038: loss = 1.6877 (1.667 sec/step)\n",
            "I1208 00:32:38.377180 140583164200832 learning.py:507] global step 3038: loss = 1.6877 (1.667 sec/step)\n",
            "INFO:tensorflow:global step 3039: loss = 2.0713 (0.672 sec/step)\n",
            "I1208 00:32:39.342787 140583164200832 learning.py:507] global step 3039: loss = 2.0713 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 3040: loss = 1.8251 (0.639 sec/step)\n",
            "I1208 00:32:40.063347 140583164200832 learning.py:507] global step 3040: loss = 1.8251 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 3041: loss = 2.4818 (0.941 sec/step)\n",
            "I1208 00:32:41.118839 140583164200832 learning.py:507] global step 3041: loss = 2.4818 (0.941 sec/step)\n",
            "INFO:tensorflow:global step 3042: loss = 1.4575 (1.386 sec/step)\n",
            "I1208 00:32:42.985501 140583164200832 learning.py:507] global step 3042: loss = 1.4575 (1.386 sec/step)\n",
            "INFO:tensorflow:global step 3043: loss = 2.2871 (0.713 sec/step)\n",
            "I1208 00:32:43.850001 140583164200832 learning.py:507] global step 3043: loss = 2.2871 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 3044: loss = 1.7700 (0.635 sec/step)\n",
            "I1208 00:32:44.888031 140583164200832 learning.py:507] global step 3044: loss = 1.7700 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 3045: loss = 1.8313 (1.273 sec/step)\n",
            "I1208 00:32:46.393321 140583164200832 learning.py:507] global step 3045: loss = 1.8313 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 3046: loss = 1.9456 (0.492 sec/step)\n",
            "I1208 00:32:46.887263 140583164200832 learning.py:507] global step 3046: loss = 1.9456 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3047: loss = 1.9429 (1.465 sec/step)\n",
            "I1208 00:32:48.421406 140583164200832 learning.py:507] global step 3047: loss = 1.9429 (1.465 sec/step)\n",
            "INFO:tensorflow:global step 3048: loss = 1.2494 (0.646 sec/step)\n",
            "I1208 00:32:49.426199 140583164200832 learning.py:507] global step 3048: loss = 1.2494 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 3049: loss = 2.2488 (0.654 sec/step)\n",
            "I1208 00:32:50.547418 140583164200832 learning.py:507] global step 3049: loss = 2.2488 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 3050: loss = 1.6200 (0.612 sec/step)\n",
            "I1208 00:32:51.466996 140583164200832 learning.py:507] global step 3050: loss = 1.6200 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 3051: loss = 2.1862 (1.314 sec/step)\n",
            "I1208 00:32:52.823414 140583164200832 learning.py:507] global step 3051: loss = 2.1862 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 3052: loss = 2.1234 (0.582 sec/step)\n",
            "I1208 00:32:53.614405 140583164200832 learning.py:507] global step 3052: loss = 2.1234 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 3053: loss = 1.6437 (1.571 sec/step)\n",
            "I1208 00:32:55.207093 140583164200832 learning.py:507] global step 3053: loss = 1.6437 (1.571 sec/step)\n",
            "INFO:tensorflow:global step 3054: loss = 1.9367 (0.679 sec/step)\n",
            "I1208 00:32:56.100364 140583164200832 learning.py:507] global step 3054: loss = 1.9367 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 3055: loss = 1.5088 (0.681 sec/step)\n",
            "I1208 00:32:57.087922 140583164200832 learning.py:507] global step 3055: loss = 1.5088 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 3056: loss = 1.8528 (0.562 sec/step)\n",
            "I1208 00:32:57.863356 140583164200832 learning.py:507] global step 3056: loss = 1.8528 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3057: loss = 1.2859 (1.854 sec/step)\n",
            "I1208 00:32:59.719341 140583164200832 learning.py:507] global step 3057: loss = 1.2859 (1.854 sec/step)\n",
            "INFO:tensorflow:global step 3058: loss = 1.9314 (1.120 sec/step)\n",
            "I1208 00:33:00.840523 140583164200832 learning.py:507] global step 3058: loss = 1.9314 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 3059: loss = 2.1537 (0.481 sec/step)\n",
            "I1208 00:33:01.322733 140583164200832 learning.py:507] global step 3059: loss = 2.1537 (0.481 sec/step)\n",
            "INFO:tensorflow:global step 3060: loss = 1.9242 (1.735 sec/step)\n",
            "I1208 00:33:03.059656 140583164200832 learning.py:507] global step 3060: loss = 1.9242 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 3061: loss = 1.6137 (1.078 sec/step)\n",
            "I1208 00:33:04.171656 140583164200832 learning.py:507] global step 3061: loss = 1.6137 (1.078 sec/step)\n",
            "INFO:tensorflow:global step 3062: loss = 1.5849 (0.560 sec/step)\n",
            "I1208 00:33:04.738285 140583164200832 learning.py:507] global step 3062: loss = 1.5849 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 3063: loss = 1.5033 (0.801 sec/step)\n",
            "I1208 00:33:05.769235 140583164200832 learning.py:507] global step 3063: loss = 1.5033 (0.801 sec/step)\n",
            "INFO:tensorflow:global step 3064: loss = 2.1599 (1.481 sec/step)\n",
            "I1208 00:33:07.479572 140583164200832 learning.py:507] global step 3064: loss = 2.1599 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 3065: loss = 1.5448 (1.270 sec/step)\n",
            "I1208 00:33:08.751472 140583164200832 learning.py:507] global step 3065: loss = 1.5448 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 3066: loss = 2.0108 (0.537 sec/step)\n",
            "I1208 00:33:09.290888 140583164200832 learning.py:507] global step 3066: loss = 2.0108 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 3067: loss = 1.7700 (1.770 sec/step)\n",
            "I1208 00:33:11.241211 140583164200832 learning.py:507] global step 3067: loss = 1.7700 (1.770 sec/step)\n",
            "INFO:tensorflow:global step 3068: loss = 2.5413 (0.618 sec/step)\n",
            "I1208 00:33:11.883983 140583164200832 learning.py:507] global step 3068: loss = 2.5413 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 3069: loss = 3.0873 (1.567 sec/step)\n",
            "I1208 00:33:13.563962 140583164200832 learning.py:507] global step 3069: loss = 3.0873 (1.567 sec/step)\n",
            "INFO:tensorflow:global step 3070: loss = 1.6899 (0.567 sec/step)\n",
            "I1208 00:33:14.243737 140583164200832 learning.py:507] global step 3070: loss = 1.6899 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 3071: loss = 2.4039 (1.460 sec/step)\n",
            "I1208 00:33:15.916057 140583164200832 learning.py:507] global step 3071: loss = 2.4039 (1.460 sec/step)\n",
            "INFO:tensorflow:global step 3072: loss = 1.9461 (0.513 sec/step)\n",
            "I1208 00:33:16.430742 140583164200832 learning.py:507] global step 3072: loss = 1.9461 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 3073: loss = 1.4761 (1.819 sec/step)\n",
            "I1208 00:33:18.251721 140583164200832 learning.py:507] global step 3073: loss = 1.4761 (1.819 sec/step)\n",
            "INFO:tensorflow:global step 3074: loss = 1.8537 (0.604 sec/step)\n",
            "I1208 00:33:18.878477 140583164200832 learning.py:507] global step 3074: loss = 1.8537 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 3075: loss = 2.1547 (0.832 sec/step)\n",
            "I1208 00:33:19.779554 140583164200832 learning.py:507] global step 3075: loss = 2.1547 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 3076: loss = 1.8335 (1.655 sec/step)\n",
            "I1208 00:33:21.742466 140583164200832 learning.py:507] global step 3076: loss = 1.8335 (1.655 sec/step)\n",
            "INFO:tensorflow:global step 3077: loss = 2.0150 (0.562 sec/step)\n",
            "I1208 00:33:22.306673 140583164200832 learning.py:507] global step 3077: loss = 2.0150 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3078: loss = 1.4951 (1.739 sec/step)\n",
            "I1208 00:33:24.047750 140583164200832 learning.py:507] global step 3078: loss = 1.4951 (1.739 sec/step)\n",
            "INFO:tensorflow:global step 3079: loss = 1.6507 (0.604 sec/step)\n",
            "I1208 00:33:25.052846 140583164200832 learning.py:507] global step 3079: loss = 1.6507 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 3080: loss = 2.6030 (1.352 sec/step)\n",
            "I1208 00:33:26.589365 140583164200832 learning.py:507] global step 3080: loss = 2.6030 (1.352 sec/step)\n",
            "INFO:tensorflow:global step 3081: loss = 1.6743 (0.469 sec/step)\n",
            "I1208 00:33:27.059926 140583164200832 learning.py:507] global step 3081: loss = 1.6743 (0.469 sec/step)\n",
            "INFO:tensorflow:global step 3082: loss = 2.2236 (0.626 sec/step)\n",
            "I1208 00:33:27.688001 140583164200832 learning.py:507] global step 3082: loss = 2.2236 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 3083: loss = 2.3271 (1.141 sec/step)\n",
            "I1208 00:33:29.104415 140583164200832 learning.py:507] global step 3083: loss = 2.3271 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 3084: loss = 1.7857 (1.517 sec/step)\n",
            "I1208 00:33:31.179628 140583164200832 learning.py:507] global step 3084: loss = 1.7857 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 3085: loss = 2.1283 (1.822 sec/step)\n",
            "I1208 00:33:33.006590 140583164200832 learning.py:507] global step 3085: loss = 2.1283 (1.822 sec/step)\n",
            "INFO:tensorflow:global step 3086: loss = 1.3176 (0.704 sec/step)\n",
            "I1208 00:33:33.991098 140583164200832 learning.py:507] global step 3086: loss = 1.3176 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 3087: loss = 1.9004 (0.530 sec/step)\n",
            "I1208 00:33:34.575501 140583164200832 learning.py:507] global step 3087: loss = 1.9004 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 3088: loss = 2.2509 (0.562 sec/step)\n",
            "I1208 00:33:35.582051 140583164200832 learning.py:507] global step 3088: loss = 2.2509 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3089: loss = 1.9185 (1.942 sec/step)\n",
            "I1208 00:33:37.708690 140583164200832 learning.py:507] global step 3089: loss = 1.9185 (1.942 sec/step)\n",
            "INFO:tensorflow:global step 3090: loss = 2.1251 (0.586 sec/step)\n",
            "I1208 00:33:38.531235 140583164200832 learning.py:507] global step 3090: loss = 2.1251 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3091: loss = 1.7309 (1.490 sec/step)\n",
            "I1208 00:33:40.084943 140583164200832 learning.py:507] global step 3091: loss = 1.7309 (1.490 sec/step)\n",
            "INFO:tensorflow:global step 3092: loss = 1.9324 (0.578 sec/step)\n",
            "I1208 00:33:40.665192 140583164200832 learning.py:507] global step 3092: loss = 1.9324 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 3093: loss = 1.7027 (2.300 sec/step)\n",
            "I1208 00:33:43.112295 140583164200832 learning.py:507] global step 3093: loss = 1.7027 (2.300 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3093.\n",
            "I1208 00:33:44.615614 140579459213056 supervisor.py:1050] Recording summary at step 3093.\n",
            "INFO:tensorflow:global step 3094: loss = 1.9515 (1.711 sec/step)\n",
            "I1208 00:33:44.825193 140583164200832 learning.py:507] global step 3094: loss = 1.9515 (1.711 sec/step)\n",
            "INFO:tensorflow:global step 3095: loss = 2.1432 (0.519 sec/step)\n",
            "I1208 00:33:45.345921 140583164200832 learning.py:507] global step 3095: loss = 2.1432 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 3096: loss = 1.6186 (2.147 sec/step)\n",
            "I1208 00:33:47.494979 140583164200832 learning.py:507] global step 3096: loss = 1.6186 (2.147 sec/step)\n",
            "INFO:tensorflow:global step 3097: loss = 1.6681 (0.482 sec/step)\n",
            "I1208 00:33:47.978914 140583164200832 learning.py:507] global step 3097: loss = 1.6681 (0.482 sec/step)\n",
            "INFO:tensorflow:global step 3098: loss = 1.4998 (1.695 sec/step)\n",
            "I1208 00:33:49.676208 140583164200832 learning.py:507] global step 3098: loss = 1.4998 (1.695 sec/step)\n",
            "INFO:tensorflow:global step 3099: loss = 2.3637 (0.502 sec/step)\n",
            "I1208 00:33:50.180286 140583164200832 learning.py:507] global step 3099: loss = 2.3637 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 3100: loss = 1.8455 (0.560 sec/step)\n",
            "I1208 00:33:50.742217 140583164200832 learning.py:507] global step 3100: loss = 1.8455 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 3101: loss = 1.6447 (1.193 sec/step)\n",
            "I1208 00:33:52.078372 140583164200832 learning.py:507] global step 3101: loss = 1.6447 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 3102: loss = 2.1787 (2.208 sec/step)\n",
            "I1208 00:33:54.489222 140583164200832 learning.py:507] global step 3102: loss = 2.1787 (2.208 sec/step)\n",
            "INFO:tensorflow:global step 3103: loss = 2.2339 (0.722 sec/step)\n",
            "I1208 00:33:55.388436 140583164200832 learning.py:507] global step 3103: loss = 2.2339 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 3104: loss = 1.8166 (0.546 sec/step)\n",
            "I1208 00:33:56.083923 140583164200832 learning.py:507] global step 3104: loss = 1.8166 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 3105: loss = 1.6108 (0.729 sec/step)\n",
            "I1208 00:33:57.114589 140583164200832 learning.py:507] global step 3105: loss = 1.6108 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 3106: loss = 1.7245 (1.509 sec/step)\n",
            "I1208 00:33:58.894274 140583164200832 learning.py:507] global step 3106: loss = 1.7245 (1.509 sec/step)\n",
            "INFO:tensorflow:global step 3107: loss = 2.0558 (0.645 sec/step)\n",
            "I1208 00:33:59.869523 140583164200832 learning.py:507] global step 3107: loss = 2.0558 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 3108: loss = 1.5551 (0.578 sec/step)\n",
            "I1208 00:34:00.785172 140583164200832 learning.py:507] global step 3108: loss = 1.5551 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 3109: loss = 2.0813 (1.287 sec/step)\n",
            "I1208 00:34:02.249122 140583164200832 learning.py:507] global step 3109: loss = 2.0813 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 3110: loss = 1.9784 (0.533 sec/step)\n",
            "I1208 00:34:03.168535 140583164200832 learning.py:507] global step 3110: loss = 1.9784 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 3111: loss = 2.1971 (1.271 sec/step)\n",
            "I1208 00:34:04.663337 140583164200832 learning.py:507] global step 3111: loss = 2.1971 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 3112: loss = 1.6192 (0.525 sec/step)\n",
            "I1208 00:34:05.373962 140583164200832 learning.py:507] global step 3112: loss = 1.6192 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 3113: loss = 1.4871 (1.354 sec/step)\n",
            "I1208 00:34:06.821231 140583164200832 learning.py:507] global step 3113: loss = 1.4871 (1.354 sec/step)\n",
            "INFO:tensorflow:global step 3114: loss = 1.2295 (0.679 sec/step)\n",
            "I1208 00:34:07.667141 140583164200832 learning.py:507] global step 3114: loss = 1.2295 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 3115: loss = 1.7830 (1.280 sec/step)\n",
            "I1208 00:34:09.075217 140583164200832 learning.py:507] global step 3115: loss = 1.7830 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 3116: loss = 1.6636 (0.681 sec/step)\n",
            "I1208 00:34:09.954322 140583164200832 learning.py:507] global step 3116: loss = 1.6636 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 3117: loss = 1.7162 (1.208 sec/step)\n",
            "I1208 00:34:11.208983 140583164200832 learning.py:507] global step 3117: loss = 1.7162 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 3118: loss = 1.6840 (0.491 sec/step)\n",
            "I1208 00:34:11.701294 140583164200832 learning.py:507] global step 3118: loss = 1.6840 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 3119: loss = 1.8396 (0.756 sec/step)\n",
            "I1208 00:34:12.610353 140583164200832 learning.py:507] global step 3119: loss = 1.8396 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 3120: loss = 2.2787 (2.220 sec/step)\n",
            "I1208 00:34:15.053529 140583164200832 learning.py:507] global step 3120: loss = 2.2787 (2.220 sec/step)\n",
            "INFO:tensorflow:global step 3121: loss = 1.5521 (0.505 sec/step)\n",
            "I1208 00:34:15.560476 140583164200832 learning.py:507] global step 3121: loss = 1.5521 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 3122: loss = 2.2401 (1.539 sec/step)\n",
            "I1208 00:34:17.201439 140583164200832 learning.py:507] global step 3122: loss = 2.2401 (1.539 sec/step)\n",
            "INFO:tensorflow:global step 3123: loss = 1.8562 (1.644 sec/step)\n",
            "I1208 00:34:18.859857 140583164200832 learning.py:507] global step 3123: loss = 1.8562 (1.644 sec/step)\n",
            "INFO:tensorflow:global step 3124: loss = 2.0717 (0.643 sec/step)\n",
            "I1208 00:34:19.787544 140583164200832 learning.py:507] global step 3124: loss = 2.0717 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 3125: loss = 1.5883 (2.266 sec/step)\n",
            "I1208 00:34:22.148022 140583164200832 learning.py:507] global step 3125: loss = 1.5883 (2.266 sec/step)\n",
            "INFO:tensorflow:global step 3126: loss = 1.5694 (0.504 sec/step)\n",
            "I1208 00:34:22.654296 140583164200832 learning.py:507] global step 3126: loss = 1.5694 (0.504 sec/step)\n",
            "INFO:tensorflow:global step 3127: loss = 1.5204 (1.849 sec/step)\n",
            "I1208 00:34:24.576027 140583164200832 learning.py:507] global step 3127: loss = 1.5204 (1.849 sec/step)\n",
            "INFO:tensorflow:global step 3128: loss = 1.6479 (0.648 sec/step)\n",
            "I1208 00:34:25.307891 140583164200832 learning.py:507] global step 3128: loss = 1.6479 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 3129: loss = 2.0820 (1.793 sec/step)\n",
            "I1208 00:34:27.102174 140583164200832 learning.py:507] global step 3129: loss = 2.0820 (1.793 sec/step)\n",
            "INFO:tensorflow:global step 3130: loss = 1.5281 (0.744 sec/step)\n",
            "I1208 00:34:28.267371 140583164200832 learning.py:507] global step 3130: loss = 1.5281 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 3131: loss = 2.3051 (0.591 sec/step)\n",
            "I1208 00:34:28.860031 140583164200832 learning.py:507] global step 3131: loss = 2.3051 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 3132: loss = 2.4352 (0.605 sec/step)\n",
            "I1208 00:34:30.076047 140583164200832 learning.py:507] global step 3132: loss = 2.4352 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 3133: loss = 2.1044 (2.031 sec/step)\n",
            "I1208 00:34:32.380515 140583164200832 learning.py:507] global step 3133: loss = 2.1044 (2.031 sec/step)\n",
            "INFO:tensorflow:global step 3134: loss = 1.5436 (0.619 sec/step)\n",
            "I1208 00:34:33.001156 140583164200832 learning.py:507] global step 3134: loss = 1.5436 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 3135: loss = 1.5158 (1.748 sec/step)\n",
            "I1208 00:34:34.750842 140583164200832 learning.py:507] global step 3135: loss = 1.5158 (1.748 sec/step)\n",
            "INFO:tensorflow:global step 3136: loss = 1.7411 (0.816 sec/step)\n",
            "I1208 00:34:35.695469 140583164200832 learning.py:507] global step 3136: loss = 1.7411 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 3137: loss = 1.5287 (1.849 sec/step)\n",
            "I1208 00:34:37.574787 140583164200832 learning.py:507] global step 3137: loss = 1.5287 (1.849 sec/step)\n",
            "INFO:tensorflow:global step 3138: loss = 1.9165 (0.603 sec/step)\n",
            "I1208 00:34:38.180082 140583164200832 learning.py:507] global step 3138: loss = 1.9165 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 3139: loss = 2.0937 (1.924 sec/step)\n",
            "I1208 00:34:40.106375 140583164200832 learning.py:507] global step 3139: loss = 2.0937 (1.924 sec/step)\n",
            "INFO:tensorflow:global step 3140: loss = 2.4000 (0.634 sec/step)\n",
            "I1208 00:34:41.068659 140583164200832 learning.py:507] global step 3140: loss = 2.4000 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 3141: loss = 1.8563 (0.508 sec/step)\n",
            "I1208 00:34:41.716501 140583164200832 learning.py:507] global step 3141: loss = 1.8563 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 3142: loss = 1.5523 (1.105 sec/step)\n",
            "I1208 00:34:42.836694 140583164200832 learning.py:507] global step 3142: loss = 1.5523 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 3143: loss = 1.8728 (0.939 sec/step)\n",
            "I1208 00:34:44.224411 140583164200832 learning.py:507] global step 3143: loss = 1.8728 (0.939 sec/step)\n",
            "INFO:tensorflow:global step 3144: loss = 1.6545 (2.002 sec/step)\n",
            "I1208 00:34:46.288428 140583164200832 learning.py:507] global step 3144: loss = 1.6545 (2.002 sec/step)\n",
            "INFO:tensorflow:global step 3145: loss = 1.7672 (0.603 sec/step)\n",
            "I1208 00:34:46.894337 140583164200832 learning.py:507] global step 3145: loss = 1.7672 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 3146: loss = 1.3600 (1.998 sec/step)\n",
            "I1208 00:34:48.894875 140583164200832 learning.py:507] global step 3146: loss = 1.3600 (1.998 sec/step)\n",
            "INFO:tensorflow:global step 3147: loss = 1.7106 (0.541 sec/step)\n",
            "I1208 00:34:49.713100 140583164200832 learning.py:507] global step 3147: loss = 1.7106 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 3148: loss = 3.1221 (0.614 sec/step)\n",
            "I1208 00:34:50.705386 140583164200832 learning.py:507] global step 3148: loss = 3.1221 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 3149: loss = 1.8415 (1.977 sec/step)\n",
            "I1208 00:34:52.719998 140583164200832 learning.py:507] global step 3149: loss = 1.8415 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 3150: loss = 1.8620 (0.675 sec/step)\n",
            "I1208 00:34:53.738090 140583164200832 learning.py:507] global step 3150: loss = 1.8620 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 3151: loss = 2.0447 (0.670 sec/step)\n",
            "I1208 00:34:54.679992 140583164200832 learning.py:507] global step 3151: loss = 2.0447 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 3152: loss = 1.3804 (1.377 sec/step)\n",
            "I1208 00:34:56.413216 140583164200832 learning.py:507] global step 3152: loss = 1.3804 (1.377 sec/step)\n",
            "INFO:tensorflow:global step 3153: loss = 1.2853 (0.664 sec/step)\n",
            "I1208 00:34:57.410048 140583164200832 learning.py:507] global step 3153: loss = 1.2853 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 3154: loss = 1.3809 (1.285 sec/step)\n",
            "I1208 00:34:58.713031 140583164200832 learning.py:507] global step 3154: loss = 1.3809 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 3155: loss = 1.7931 (0.631 sec/step)\n",
            "I1208 00:34:59.566581 140583164200832 learning.py:507] global step 3155: loss = 1.7931 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 3156: loss = 2.8068 (0.689 sec/step)\n",
            "I1208 00:35:00.708789 140583164200832 learning.py:507] global step 3156: loss = 2.8068 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 3157: loss = 2.2581 (0.734 sec/step)\n",
            "I1208 00:35:01.846054 140583164200832 learning.py:507] global step 3157: loss = 2.2581 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 3158: loss = 2.1918 (0.513 sec/step)\n",
            "I1208 00:35:02.786754 140583164200832 learning.py:507] global step 3158: loss = 2.1918 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 3159: loss = 2.0647 (0.862 sec/step)\n",
            "I1208 00:35:03.890045 140583164200832 learning.py:507] global step 3159: loss = 2.0647 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 3160: loss = 1.5540 (1.428 sec/step)\n",
            "I1208 00:35:05.347116 140583164200832 learning.py:507] global step 3160: loss = 1.5540 (1.428 sec/step)\n",
            "INFO:tensorflow:global step 3161: loss = 2.3912 (0.783 sec/step)\n",
            "I1208 00:35:06.264991 140583164200832 learning.py:507] global step 3161: loss = 2.3912 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 3162: loss = 1.6386 (0.720 sec/step)\n",
            "I1208 00:35:07.168704 140583164200832 learning.py:507] global step 3162: loss = 1.6386 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 3163: loss = 1.6968 (1.449 sec/step)\n",
            "I1208 00:35:08.662096 140583164200832 learning.py:507] global step 3163: loss = 1.6968 (1.449 sec/step)\n",
            "INFO:tensorflow:global step 3164: loss = 1.7648 (0.493 sec/step)\n",
            "I1208 00:35:09.156615 140583164200832 learning.py:507] global step 3164: loss = 1.7648 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 3165: loss = 1.6294 (1.596 sec/step)\n",
            "I1208 00:35:10.866485 140583164200832 learning.py:507] global step 3165: loss = 1.6294 (1.596 sec/step)\n",
            "INFO:tensorflow:global step 3166: loss = 1.7649 (0.576 sec/step)\n",
            "I1208 00:35:11.626090 140583164200832 learning.py:507] global step 3166: loss = 1.7649 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 3167: loss = 1.9246 (1.732 sec/step)\n",
            "I1208 00:35:13.512512 140583164200832 learning.py:507] global step 3167: loss = 1.9246 (1.732 sec/step)\n",
            "INFO:tensorflow:global step 3168: loss = 1.9071 (0.639 sec/step)\n",
            "I1208 00:35:14.397663 140583164200832 learning.py:507] global step 3168: loss = 1.9071 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 3169: loss = 1.5128 (0.614 sec/step)\n",
            "I1208 00:35:15.572114 140583164200832 learning.py:507] global step 3169: loss = 1.5128 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 3170: loss = 1.4226 (0.695 sec/step)\n",
            "I1208 00:35:16.691074 140583164200832 learning.py:507] global step 3170: loss = 1.4226 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 3171: loss = 1.4926 (0.546 sec/step)\n",
            "I1208 00:35:17.424407 140583164200832 learning.py:507] global step 3171: loss = 1.4926 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 3172: loss = 1.4624 (1.452 sec/step)\n",
            "I1208 00:35:18.920485 140583164200832 learning.py:507] global step 3172: loss = 1.4624 (1.452 sec/step)\n",
            "INFO:tensorflow:global step 3173: loss = 1.9675 (0.562 sec/step)\n",
            "I1208 00:35:19.824884 140583164200832 learning.py:507] global step 3173: loss = 1.9675 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3174: loss = 2.0618 (1.208 sec/step)\n",
            "I1208 00:35:21.220100 140583164200832 learning.py:507] global step 3174: loss = 2.0618 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 3175: loss = 1.5011 (0.635 sec/step)\n",
            "I1208 00:35:21.995515 140583164200832 learning.py:507] global step 3175: loss = 1.5011 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 3176: loss = 1.8263 (1.179 sec/step)\n",
            "I1208 00:35:23.405002 140583164200832 learning.py:507] global step 3176: loss = 1.8263 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 3177: loss = 1.9236 (0.622 sec/step)\n",
            "I1208 00:35:24.274132 140583164200832 learning.py:507] global step 3177: loss = 1.9236 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 3178: loss = 1.2812 (0.575 sec/step)\n",
            "I1208 00:35:25.401481 140583164200832 learning.py:507] global step 3178: loss = 1.2812 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 3179: loss = 2.0176 (0.695 sec/step)\n",
            "I1208 00:35:26.434897 140583164200832 learning.py:507] global step 3179: loss = 2.0176 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 3180: loss = 2.0895 (1.214 sec/step)\n",
            "I1208 00:35:27.771582 140583164200832 learning.py:507] global step 3180: loss = 2.0895 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 3181: loss = 2.5019 (0.571 sec/step)\n",
            "I1208 00:35:28.658368 140583164200832 learning.py:507] global step 3181: loss = 2.5019 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 3182: loss = 1.9430 (0.613 sec/step)\n",
            "I1208 00:35:29.696306 140583164200832 learning.py:507] global step 3182: loss = 1.9430 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 3183: loss = 1.6652 (0.593 sec/step)\n",
            "I1208 00:35:30.750643 140583164200832 learning.py:507] global step 3183: loss = 1.6652 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3184: loss = 1.2976 (0.562 sec/step)\n",
            "I1208 00:35:31.507235 140583164200832 learning.py:507] global step 3184: loss = 1.2976 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3185: loss = 1.7354 (0.642 sec/step)\n",
            "I1208 00:35:32.742476 140583164200832 learning.py:507] global step 3185: loss = 1.7354 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 3186: loss = 2.2186 (2.084 sec/step)\n",
            "I1208 00:35:34.851693 140583164200832 learning.py:507] global step 3186: loss = 2.2186 (2.084 sec/step)\n",
            "INFO:tensorflow:global step 3187: loss = 1.7621 (0.662 sec/step)\n",
            "I1208 00:35:35.708915 140583164200832 learning.py:507] global step 3187: loss = 1.7621 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 3188: loss = 2.0150 (1.436 sec/step)\n",
            "I1208 00:35:37.164220 140583164200832 learning.py:507] global step 3188: loss = 2.0150 (1.436 sec/step)\n",
            "INFO:tensorflow:global step 3189: loss = 1.7484 (1.086 sec/step)\n",
            "I1208 00:35:38.252173 140583164200832 learning.py:507] global step 3189: loss = 1.7484 (1.086 sec/step)\n",
            "INFO:tensorflow:global step 3190: loss = 1.7653 (1.038 sec/step)\n",
            "I1208 00:35:39.291653 140583164200832 learning.py:507] global step 3190: loss = 1.7653 (1.038 sec/step)\n",
            "INFO:tensorflow:global step 3191: loss = 2.0569 (0.542 sec/step)\n",
            "I1208 00:35:39.835622 140583164200832 learning.py:507] global step 3191: loss = 2.0569 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 3192: loss = 1.4722 (0.784 sec/step)\n",
            "I1208 00:35:40.898037 140583164200832 learning.py:507] global step 3192: loss = 1.4722 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 3193: loss = 1.4489 (2.794 sec/step)\n",
            "I1208 00:35:43.968494 140583164200832 learning.py:507] global step 3193: loss = 1.4489 (2.794 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3193.\n",
            "I1208 00:35:44.003968 140579459213056 supervisor.py:1050] Recording summary at step 3193.\n",
            "INFO:tensorflow:global step 3194: loss = 1.5970 (0.649 sec/step)\n",
            "I1208 00:35:44.760025 140583164200832 learning.py:507] global step 3194: loss = 1.5970 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 3195: loss = 3.1663 (1.434 sec/step)\n",
            "I1208 00:35:46.359224 140583164200832 learning.py:507] global step 3195: loss = 3.1663 (1.434 sec/step)\n",
            "INFO:tensorflow:global step 3196: loss = 1.6439 (0.540 sec/step)\n",
            "I1208 00:35:47.136471 140583164200832 learning.py:507] global step 3196: loss = 1.6439 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 3197: loss = 2.0596 (1.438 sec/step)\n",
            "I1208 00:35:48.597713 140583164200832 learning.py:507] global step 3197: loss = 2.0596 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 3198: loss = 2.1125 (0.686 sec/step)\n",
            "I1208 00:35:49.565576 140583164200832 learning.py:507] global step 3198: loss = 2.1125 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 3199: loss = 1.6592 (0.556 sec/step)\n",
            "I1208 00:35:50.458121 140583164200832 learning.py:507] global step 3199: loss = 1.6592 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 3200: loss = 1.8791 (0.844 sec/step)\n",
            "I1208 00:35:51.682679 140583164200832 learning.py:507] global step 3200: loss = 1.8791 (0.844 sec/step)\n",
            "INFO:tensorflow:global step 3201: loss = 1.7031 (0.566 sec/step)\n",
            "I1208 00:35:52.282523 140583164200832 learning.py:507] global step 3201: loss = 1.7031 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 3202: loss = 1.3735 (0.835 sec/step)\n",
            "I1208 00:35:53.386097 140583164200832 learning.py:507] global step 3202: loss = 1.3735 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 3203: loss = 1.1876 (1.509 sec/step)\n",
            "I1208 00:35:55.120366 140583164200832 learning.py:507] global step 3203: loss = 1.1876 (1.509 sec/step)\n",
            "INFO:tensorflow:global step 3204: loss = 1.5906 (0.551 sec/step)\n",
            "I1208 00:35:56.047988 140583164200832 learning.py:507] global step 3204: loss = 1.5906 (0.551 sec/step)\n",
            "INFO:tensorflow:global step 3205: loss = 2.0277 (0.602 sec/step)\n",
            "I1208 00:35:57.095613 140583164200832 learning.py:507] global step 3205: loss = 2.0277 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 3206: loss = 1.9582 (0.581 sec/step)\n",
            "I1208 00:35:57.815719 140583164200832 learning.py:507] global step 3206: loss = 1.9582 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 3207: loss = 2.0927 (0.641 sec/step)\n",
            "I1208 00:35:58.767688 140583164200832 learning.py:507] global step 3207: loss = 2.0927 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 3208: loss = 1.6906 (1.891 sec/step)\n",
            "I1208 00:36:00.975198 140583164200832 learning.py:507] global step 3208: loss = 1.6906 (1.891 sec/step)\n",
            "INFO:tensorflow:global step 3209: loss = 1.5334 (0.599 sec/step)\n",
            "I1208 00:36:02.020964 140583164200832 learning.py:507] global step 3209: loss = 1.5334 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 3210: loss = 2.1178 (0.548 sec/step)\n",
            "I1208 00:36:02.661887 140583164200832 learning.py:507] global step 3210: loss = 2.1178 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 3211: loss = 1.8348 (0.823 sec/step)\n",
            "I1208 00:36:03.724417 140583164200832 learning.py:507] global step 3211: loss = 1.8348 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 3212: loss = 2.4071 (2.091 sec/step)\n",
            "I1208 00:36:06.105459 140583164200832 learning.py:507] global step 3212: loss = 2.4071 (2.091 sec/step)\n",
            "INFO:tensorflow:global step 3213: loss = 1.5173 (0.570 sec/step)\n",
            "I1208 00:36:06.874651 140583164200832 learning.py:507] global step 3213: loss = 1.5173 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 3214: loss = 1.7591 (1.343 sec/step)\n",
            "I1208 00:36:08.381631 140583164200832 learning.py:507] global step 3214: loss = 1.7591 (1.343 sec/step)\n",
            "INFO:tensorflow:global step 3215: loss = 2.1252 (0.517 sec/step)\n",
            "I1208 00:36:09.173448 140583164200832 learning.py:507] global step 3215: loss = 2.1252 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 3216: loss = 1.6343 (0.642 sec/step)\n",
            "I1208 00:36:09.994130 140583164200832 learning.py:507] global step 3216: loss = 1.6343 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 3217: loss = 1.6255 (1.431 sec/step)\n",
            "I1208 00:36:11.642850 140583164200832 learning.py:507] global step 3217: loss = 1.6255 (1.431 sec/step)\n",
            "INFO:tensorflow:global step 3218: loss = 2.0892 (0.492 sec/step)\n",
            "I1208 00:36:12.137294 140583164200832 learning.py:507] global step 3218: loss = 2.0892 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3219: loss = 2.6016 (1.744 sec/step)\n",
            "I1208 00:36:13.883600 140583164200832 learning.py:507] global step 3219: loss = 2.6016 (1.744 sec/step)\n",
            "INFO:tensorflow:global step 3220: loss = 1.7347 (0.533 sec/step)\n",
            "I1208 00:36:14.702056 140583164200832 learning.py:507] global step 3220: loss = 1.7347 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 3221: loss = 1.9955 (1.233 sec/step)\n",
            "I1208 00:36:16.092383 140583164200832 learning.py:507] global step 3221: loss = 1.9955 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 3222: loss = 2.4855 (0.656 sec/step)\n",
            "I1208 00:36:16.825376 140583164200832 learning.py:507] global step 3222: loss = 2.4855 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 3223: loss = 1.9253 (1.300 sec/step)\n",
            "I1208 00:36:18.279632 140583164200832 learning.py:507] global step 3223: loss = 1.9253 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 3224: loss = 1.9174 (0.718 sec/step)\n",
            "I1208 00:36:19.011115 140583164200832 learning.py:507] global step 3224: loss = 1.9174 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 3225: loss = 1.8438 (1.336 sec/step)\n",
            "I1208 00:36:20.472432 140583164200832 learning.py:507] global step 3225: loss = 1.8438 (1.336 sec/step)\n",
            "INFO:tensorflow:global step 3226: loss = 1.9418 (0.560 sec/step)\n",
            "I1208 00:36:21.034508 140583164200832 learning.py:507] global step 3226: loss = 1.9418 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 3227: loss = 1.7647 (0.787 sec/step)\n",
            "I1208 00:36:22.003731 140583164200832 learning.py:507] global step 3227: loss = 1.7647 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 3228: loss = 1.5740 (1.502 sec/step)\n",
            "I1208 00:36:23.817782 140583164200832 learning.py:507] global step 3228: loss = 1.5740 (1.502 sec/step)\n",
            "INFO:tensorflow:global step 3229: loss = 2.0028 (0.559 sec/step)\n",
            "I1208 00:36:24.570657 140583164200832 learning.py:507] global step 3229: loss = 2.0028 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 3230: loss = 1.9028 (1.281 sec/step)\n",
            "I1208 00:36:26.032077 140583164200832 learning.py:507] global step 3230: loss = 1.9028 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 3231: loss = 2.3180 (0.510 sec/step)\n",
            "I1208 00:36:26.543689 140583164200832 learning.py:507] global step 3231: loss = 2.3180 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 3232: loss = 1.9068 (1.726 sec/step)\n",
            "I1208 00:36:28.271067 140583164200832 learning.py:507] global step 3232: loss = 1.9068 (1.726 sec/step)\n",
            "INFO:tensorflow:global step 3233: loss = 2.4966 (0.676 sec/step)\n",
            "I1208 00:36:29.218037 140583164200832 learning.py:507] global step 3233: loss = 2.4966 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 3234: loss = 1.6660 (0.797 sec/step)\n",
            "I1208 00:36:30.431530 140583164200832 learning.py:507] global step 3234: loss = 1.6660 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 3235: loss = 1.5454 (1.329 sec/step)\n",
            "I1208 00:36:31.767157 140583164200832 learning.py:507] global step 3235: loss = 1.5454 (1.329 sec/step)\n",
            "INFO:tensorflow:global step 3236: loss = 1.9857 (0.718 sec/step)\n",
            "I1208 00:36:32.925615 140583164200832 learning.py:507] global step 3236: loss = 1.9857 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 3237: loss = 1.9219 (0.707 sec/step)\n",
            "I1208 00:36:33.795012 140583164200832 learning.py:507] global step 3237: loss = 1.9219 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 3238: loss = 2.0666 (1.155 sec/step)\n",
            "I1208 00:36:35.159587 140583164200832 learning.py:507] global step 3238: loss = 2.0666 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 3239: loss = 1.7874 (0.721 sec/step)\n",
            "I1208 00:36:35.988038 140583164200832 learning.py:507] global step 3239: loss = 1.7874 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 3240: loss = 2.4645 (0.584 sec/step)\n",
            "I1208 00:36:37.051774 140583164200832 learning.py:507] global step 3240: loss = 2.4645 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 3241: loss = 1.5596 (0.667 sec/step)\n",
            "I1208 00:36:37.886063 140583164200832 learning.py:507] global step 3241: loss = 1.5596 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3242: loss = 1.9479 (0.805 sec/step)\n",
            "I1208 00:36:39.039669 140583164200832 learning.py:507] global step 3242: loss = 1.9479 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 3243: loss = 1.8048 (0.651 sec/step)\n",
            "I1208 00:36:40.480398 140583164200832 learning.py:507] global step 3243: loss = 1.8048 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 3244: loss = 1.7856 (1.257 sec/step)\n",
            "I1208 00:36:41.864287 140583164200832 learning.py:507] global step 3244: loss = 1.7856 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 3245: loss = 2.4896 (0.624 sec/step)\n",
            "I1208 00:36:42.672112 140583164200832 learning.py:507] global step 3245: loss = 2.4896 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 3246: loss = 1.9290 (0.607 sec/step)\n",
            "I1208 00:36:43.522390 140583164200832 learning.py:507] global step 3246: loss = 1.9290 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 3247: loss = 1.9696 (0.738 sec/step)\n",
            "I1208 00:36:44.262662 140583164200832 learning.py:507] global step 3247: loss = 1.9696 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 3248: loss = 2.2337 (3.123 sec/step)\n",
            "I1208 00:36:47.387956 140583164200832 learning.py:507] global step 3248: loss = 2.2337 (3.123 sec/step)\n",
            "INFO:tensorflow:global step 3249: loss = 2.0004 (0.650 sec/step)\n",
            "I1208 00:36:48.039779 140583164200832 learning.py:507] global step 3249: loss = 2.0004 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 3250: loss = 1.7025 (2.186 sec/step)\n",
            "I1208 00:36:50.227783 140583164200832 learning.py:507] global step 3250: loss = 1.7025 (2.186 sec/step)\n",
            "INFO:tensorflow:global step 3251: loss = 1.7346 (0.586 sec/step)\n",
            "I1208 00:36:51.086410 140583164200832 learning.py:507] global step 3251: loss = 1.7346 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3252: loss = 1.8454 (0.545 sec/step)\n",
            "I1208 00:36:51.866141 140583164200832 learning.py:507] global step 3252: loss = 1.8454 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 3253: loss = 1.6676 (1.796 sec/step)\n",
            "I1208 00:36:53.663979 140583164200832 learning.py:507] global step 3253: loss = 1.6676 (1.796 sec/step)\n",
            "INFO:tensorflow:global step 3254: loss = 1.5949 (0.724 sec/step)\n",
            "I1208 00:36:54.532090 140583164200832 learning.py:507] global step 3254: loss = 1.5949 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 3255: loss = 1.7032 (1.357 sec/step)\n",
            "I1208 00:36:56.072756 140583164200832 learning.py:507] global step 3255: loss = 1.7032 (1.357 sec/step)\n",
            "INFO:tensorflow:global step 3256: loss = 2.1771 (1.150 sec/step)\n",
            "I1208 00:36:57.224440 140583164200832 learning.py:507] global step 3256: loss = 2.1771 (1.150 sec/step)\n",
            "INFO:tensorflow:global step 3257: loss = 2.4426 (0.514 sec/step)\n",
            "I1208 00:36:57.929137 140583164200832 learning.py:507] global step 3257: loss = 2.4426 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 3258: loss = 2.2854 (1.243 sec/step)\n",
            "I1208 00:36:59.431837 140583164200832 learning.py:507] global step 3258: loss = 2.2854 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 3259: loss = 2.3402 (0.645 sec/step)\n",
            "I1208 00:37:00.238833 140583164200832 learning.py:507] global step 3259: loss = 2.3402 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 3260: loss = 2.1125 (1.369 sec/step)\n",
            "I1208 00:37:01.740823 140583164200832 learning.py:507] global step 3260: loss = 2.1125 (1.369 sec/step)\n",
            "INFO:tensorflow:global step 3261: loss = 2.1647 (0.506 sec/step)\n",
            "I1208 00:37:02.559600 140583164200832 learning.py:507] global step 3261: loss = 2.1647 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 3262: loss = 2.1202 (0.593 sec/step)\n",
            "I1208 00:37:03.299541 140583164200832 learning.py:507] global step 3262: loss = 2.1202 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3263: loss = 1.9881 (1.541 sec/step)\n",
            "I1208 00:37:05.049583 140583164200832 learning.py:507] global step 3263: loss = 1.9881 (1.541 sec/step)\n",
            "INFO:tensorflow:global step 3264: loss = 1.8808 (0.591 sec/step)\n",
            "I1208 00:37:05.677447 140583164200832 learning.py:507] global step 3264: loss = 1.8808 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 3265: loss = 1.6723 (1.583 sec/step)\n",
            "I1208 00:37:07.368222 140583164200832 learning.py:507] global step 3265: loss = 1.6723 (1.583 sec/step)\n",
            "INFO:tensorflow:global step 3266: loss = 1.9376 (0.533 sec/step)\n",
            "I1208 00:37:07.907455 140583164200832 learning.py:507] global step 3266: loss = 1.9376 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 3267: loss = 1.9750 (0.900 sec/step)\n",
            "I1208 00:37:08.840387 140583164200832 learning.py:507] global step 3267: loss = 1.9750 (0.900 sec/step)\n",
            "INFO:tensorflow:global step 3268: loss = 1.8934 (2.137 sec/step)\n",
            "I1208 00:37:11.286919 140583164200832 learning.py:507] global step 3268: loss = 1.8934 (2.137 sec/step)\n",
            "INFO:tensorflow:global step 3269: loss = 2.0844 (0.792 sec/step)\n",
            "I1208 00:37:12.389225 140583164200832 learning.py:507] global step 3269: loss = 2.0844 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 3270: loss = 2.1838 (0.584 sec/step)\n",
            "I1208 00:37:12.977415 140583164200832 learning.py:507] global step 3270: loss = 2.1838 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 3271: loss = 1.9529 (0.769 sec/step)\n",
            "I1208 00:37:14.106319 140583164200832 learning.py:507] global step 3271: loss = 1.9529 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 3272: loss = 1.6469 (2.649 sec/step)\n",
            "I1208 00:37:17.176591 140583164200832 learning.py:507] global step 3272: loss = 1.6469 (2.649 sec/step)\n",
            "INFO:tensorflow:global step 3273: loss = 1.8186 (0.528 sec/step)\n",
            "I1208 00:37:17.706885 140583164200832 learning.py:507] global step 3273: loss = 1.8186 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 3274: loss = 1.8614 (1.840 sec/step)\n",
            "I1208 00:37:19.548219 140583164200832 learning.py:507] global step 3274: loss = 1.8614 (1.840 sec/step)\n",
            "INFO:tensorflow:global step 3275: loss = 1.4190 (0.595 sec/step)\n",
            "I1208 00:37:20.145236 140583164200832 learning.py:507] global step 3275: loss = 1.4190 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 3276: loss = 2.1022 (1.195 sec/step)\n",
            "I1208 00:37:21.602796 140583164200832 learning.py:507] global step 3276: loss = 2.1022 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 3277: loss = 2.4119 (0.729 sec/step)\n",
            "I1208 00:37:22.830417 140583164200832 learning.py:507] global step 3277: loss = 2.4119 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 3278: loss = 1.4068 (1.344 sec/step)\n",
            "I1208 00:37:24.195425 140583164200832 learning.py:507] global step 3278: loss = 1.4068 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 3279: loss = 2.2463 (0.569 sec/step)\n",
            "I1208 00:37:25.119600 140583164200832 learning.py:507] global step 3279: loss = 2.2463 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 3280: loss = 1.9497 (0.687 sec/step)\n",
            "I1208 00:37:26.219725 140583164200832 learning.py:507] global step 3280: loss = 1.9497 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 3281: loss = 1.9514 (0.548 sec/step)\n",
            "I1208 00:37:26.829275 140583164200832 learning.py:507] global step 3281: loss = 1.9514 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 3282: loss = 1.9557 (1.640 sec/step)\n",
            "I1208 00:37:28.471066 140583164200832 learning.py:507] global step 3282: loss = 1.9557 (1.640 sec/step)\n",
            "INFO:tensorflow:global step 3283: loss = 2.1260 (1.184 sec/step)\n",
            "I1208 00:37:29.663621 140583164200832 learning.py:507] global step 3283: loss = 2.1260 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 3284: loss = 2.0070 (0.669 sec/step)\n",
            "I1208 00:37:30.361060 140583164200832 learning.py:507] global step 3284: loss = 2.0070 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 3285: loss = 1.7480 (0.870 sec/step)\n",
            "I1208 00:37:31.456274 140583164200832 learning.py:507] global step 3285: loss = 1.7480 (0.870 sec/step)\n",
            "INFO:tensorflow:global step 3286: loss = 1.9808 (1.664 sec/step)\n",
            "I1208 00:37:33.323690 140583164200832 learning.py:507] global step 3286: loss = 1.9808 (1.664 sec/step)\n",
            "INFO:tensorflow:global step 3287: loss = 1.3846 (0.529 sec/step)\n",
            "I1208 00:37:34.134641 140583164200832 learning.py:507] global step 3287: loss = 1.3846 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 3288: loss = 1.8738 (0.533 sec/step)\n",
            "I1208 00:37:34.993433 140583164200832 learning.py:507] global step 3288: loss = 1.8738 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 3289: loss = 1.6239 (1.641 sec/step)\n",
            "I1208 00:37:36.636170 140583164200832 learning.py:507] global step 3289: loss = 1.6239 (1.641 sec/step)\n",
            "INFO:tensorflow:global step 3290: loss = 1.8997 (0.709 sec/step)\n",
            "I1208 00:37:37.394072 140583164200832 learning.py:507] global step 3290: loss = 1.8997 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 3291: loss = 2.2976 (1.432 sec/step)\n",
            "I1208 00:37:39.004758 140583164200832 learning.py:507] global step 3291: loss = 2.2976 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 3292: loss = 2.4435 (0.539 sec/step)\n",
            "I1208 00:37:39.545492 140583164200832 learning.py:507] global step 3292: loss = 2.4435 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 3293: loss = 1.6955 (0.945 sec/step)\n",
            "I1208 00:37:40.675559 140583164200832 learning.py:507] global step 3293: loss = 1.6955 (0.945 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3294.\n",
            "I1208 00:37:43.873297 140579459213056 supervisor.py:1050] Recording summary at step 3294.\n",
            "INFO:tensorflow:global step 3294: loss = 1.8907 (2.913 sec/step)\n",
            "I1208 00:37:43.879281 140583164200832 learning.py:507] global step 3294: loss = 1.8907 (2.913 sec/step)\n",
            "INFO:tensorflow:global step 3295: loss = 1.6681 (0.558 sec/step)\n",
            "I1208 00:37:44.439266 140583164200832 learning.py:507] global step 3295: loss = 1.6681 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 3296: loss = 1.6109 (1.894 sec/step)\n",
            "I1208 00:37:46.335326 140583164200832 learning.py:507] global step 3296: loss = 1.6109 (1.894 sec/step)\n",
            "INFO:tensorflow:global step 3297: loss = 1.7779 (0.507 sec/step)\n",
            "I1208 00:37:47.227681 140583164200832 learning.py:507] global step 3297: loss = 1.7779 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 3298: loss = 1.9433 (1.131 sec/step)\n",
            "I1208 00:37:48.506179 140583164200832 learning.py:507] global step 3298: loss = 1.9433 (1.131 sec/step)\n",
            "INFO:tensorflow:global step 3299: loss = 1.8429 (1.105 sec/step)\n",
            "I1208 00:37:49.612673 140583164200832 learning.py:507] global step 3299: loss = 1.8429 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 3300: loss = 1.6785 (0.593 sec/step)\n",
            "I1208 00:37:50.543649 140583164200832 learning.py:507] global step 3300: loss = 1.6785 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3301: loss = 2.0010 (0.578 sec/step)\n",
            "I1208 00:37:51.503145 140583164200832 learning.py:507] global step 3301: loss = 2.0010 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 3302: loss = 2.1452 (1.311 sec/step)\n",
            "I1208 00:37:52.881091 140583164200832 learning.py:507] global step 3302: loss = 2.1452 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 3303: loss = 2.2443 (0.781 sec/step)\n",
            "I1208 00:37:53.793770 140583164200832 learning.py:507] global step 3303: loss = 2.2443 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 3304: loss = 1.5853 (0.569 sec/step)\n",
            "I1208 00:37:54.509094 140583164200832 learning.py:507] global step 3304: loss = 1.5853 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 3305: loss = 1.4427 (1.730 sec/step)\n",
            "I1208 00:37:56.240682 140583164200832 learning.py:507] global step 3305: loss = 1.4427 (1.730 sec/step)\n",
            "INFO:tensorflow:global step 3306: loss = 2.1456 (0.702 sec/step)\n",
            "I1208 00:37:56.995940 140583164200832 learning.py:507] global step 3306: loss = 2.1456 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 3307: loss = 1.5373 (0.748 sec/step)\n",
            "I1208 00:37:58.147464 140583164200832 learning.py:507] global step 3307: loss = 1.5373 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 3308: loss = 1.4634 (0.621 sec/step)\n",
            "I1208 00:37:59.187221 140583164200832 learning.py:507] global step 3308: loss = 1.4634 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 3309: loss = 1.9821 (1.214 sec/step)\n",
            "I1208 00:38:00.512241 140583164200832 learning.py:507] global step 3309: loss = 1.9821 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 3310: loss = 1.8501 (1.081 sec/step)\n",
            "I1208 00:38:01.594802 140583164200832 learning.py:507] global step 3310: loss = 1.8501 (1.081 sec/step)\n",
            "INFO:tensorflow:global step 3311: loss = 1.9350 (0.666 sec/step)\n",
            "I1208 00:38:02.349795 140583164200832 learning.py:507] global step 3311: loss = 1.9350 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 3312: loss = 1.7672 (1.228 sec/step)\n",
            "I1208 00:38:03.842135 140583164200832 learning.py:507] global step 3312: loss = 1.7672 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 3313: loss = 1.9591 (0.527 sec/step)\n",
            "I1208 00:38:04.647472 140583164200832 learning.py:507] global step 3313: loss = 1.9591 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 3314: loss = 2.3951 (1.354 sec/step)\n",
            "I1208 00:38:06.150533 140583164200832 learning.py:507] global step 3314: loss = 2.3951 (1.354 sec/step)\n",
            "INFO:tensorflow:global step 3315: loss = 1.6725 (1.092 sec/step)\n",
            "I1208 00:38:07.243968 140583164200832 learning.py:507] global step 3315: loss = 1.6725 (1.092 sec/step)\n",
            "INFO:tensorflow:global step 3316: loss = 1.9840 (0.766 sec/step)\n",
            "I1208 00:38:08.277551 140583164200832 learning.py:507] global step 3316: loss = 1.9840 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 3317: loss = 2.3732 (0.692 sec/step)\n",
            "I1208 00:38:09.109997 140583164200832 learning.py:507] global step 3317: loss = 2.3732 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 3318: loss = 1.5137 (1.484 sec/step)\n",
            "I1208 00:38:10.702855 140583164200832 learning.py:507] global step 3318: loss = 1.5137 (1.484 sec/step)\n",
            "INFO:tensorflow:global step 3319: loss = 1.8016 (0.583 sec/step)\n",
            "I1208 00:38:11.587001 140583164200832 learning.py:507] global step 3319: loss = 1.8016 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 3320: loss = 1.7395 (1.349 sec/step)\n",
            "I1208 00:38:13.032513 140583164200832 learning.py:507] global step 3320: loss = 1.7395 (1.349 sec/step)\n",
            "INFO:tensorflow:global step 3321: loss = 1.4098 (0.733 sec/step)\n",
            "I1208 00:38:14.041012 140583164200832 learning.py:507] global step 3321: loss = 1.4098 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 3322: loss = 2.1180 (0.485 sec/step)\n",
            "I1208 00:38:14.578015 140583164200832 learning.py:507] global step 3322: loss = 2.1180 (0.485 sec/step)\n",
            "INFO:tensorflow:global step 3323: loss = 1.7577 (0.608 sec/step)\n",
            "I1208 00:38:15.188479 140583164200832 learning.py:507] global step 3323: loss = 1.7577 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 3324: loss = 1.4966 (2.543 sec/step)\n",
            "I1208 00:38:17.733448 140583164200832 learning.py:507] global step 3324: loss = 1.4966 (2.543 sec/step)\n",
            "INFO:tensorflow:global step 3325: loss = 1.3550 (0.571 sec/step)\n",
            "I1208 00:38:18.501992 140583164200832 learning.py:507] global step 3325: loss = 1.3550 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 3326: loss = 1.8260 (0.612 sec/step)\n",
            "I1208 00:38:19.691097 140583164200832 learning.py:507] global step 3326: loss = 1.8260 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 3327: loss = 2.0834 (0.691 sec/step)\n",
            "I1208 00:38:20.634902 140583164200832 learning.py:507] global step 3327: loss = 2.0834 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 3328: loss = 2.2295 (1.675 sec/step)\n",
            "I1208 00:38:22.506767 140583164200832 learning.py:507] global step 3328: loss = 2.2295 (1.675 sec/step)\n",
            "INFO:tensorflow:global step 3329: loss = 1.9031 (0.498 sec/step)\n",
            "I1208 00:38:23.006162 140583164200832 learning.py:507] global step 3329: loss = 1.9031 (0.498 sec/step)\n",
            "INFO:tensorflow:global step 3330: loss = 1.7821 (0.960 sec/step)\n",
            "I1208 00:38:24.154874 140583164200832 learning.py:507] global step 3330: loss = 1.7821 (0.960 sec/step)\n",
            "INFO:tensorflow:global step 3331: loss = 1.7042 (2.346 sec/step)\n",
            "I1208 00:38:26.843519 140583164200832 learning.py:507] global step 3331: loss = 1.7042 (2.346 sec/step)\n",
            "INFO:tensorflow:global step 3332: loss = 1.8697 (0.532 sec/step)\n",
            "I1208 00:38:27.377211 140583164200832 learning.py:507] global step 3332: loss = 1.8697 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 3333: loss = 1.6829 (1.818 sec/step)\n",
            "I1208 00:38:29.197627 140583164200832 learning.py:507] global step 3333: loss = 1.6829 (1.818 sec/step)\n",
            "INFO:tensorflow:global step 3334: loss = 1.4544 (1.084 sec/step)\n",
            "I1208 00:38:30.283643 140583164200832 learning.py:507] global step 3334: loss = 1.4544 (1.084 sec/step)\n",
            "INFO:tensorflow:global step 3335: loss = 1.3543 (0.501 sec/step)\n",
            "I1208 00:38:30.786598 140583164200832 learning.py:507] global step 3335: loss = 1.3543 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 3336: loss = 1.6367 (1.904 sec/step)\n",
            "I1208 00:38:32.692419 140583164200832 learning.py:507] global step 3336: loss = 1.6367 (1.904 sec/step)\n",
            "INFO:tensorflow:global step 3337: loss = 1.7367 (0.593 sec/step)\n",
            "I1208 00:38:33.287577 140583164200832 learning.py:507] global step 3337: loss = 1.7367 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3338: loss = 1.8977 (2.362 sec/step)\n",
            "I1208 00:38:35.651578 140583164200832 learning.py:507] global step 3338: loss = 1.8977 (2.362 sec/step)\n",
            "INFO:tensorflow:global step 3339: loss = 1.7138 (0.603 sec/step)\n",
            "I1208 00:38:36.424720 140583164200832 learning.py:507] global step 3339: loss = 1.7138 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 3340: loss = 1.5325 (1.324 sec/step)\n",
            "I1208 00:38:37.998868 140583164200832 learning.py:507] global step 3340: loss = 1.5325 (1.324 sec/step)\n",
            "INFO:tensorflow:global step 3341: loss = 1.8073 (0.586 sec/step)\n",
            "I1208 00:38:38.587269 140583164200832 learning.py:507] global step 3341: loss = 1.8073 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3342: loss = 2.0842 (0.992 sec/step)\n",
            "I1208 00:38:39.588380 140583164200832 learning.py:507] global step 3342: loss = 2.0842 (0.992 sec/step)\n",
            "INFO:tensorflow:global step 3343: loss = 1.9088 (2.486 sec/step)\n",
            "I1208 00:38:42.076493 140583164200832 learning.py:507] global step 3343: loss = 1.9088 (2.486 sec/step)\n",
            "INFO:tensorflow:global step 3344: loss = 1.8253 (0.584 sec/step)\n",
            "I1208 00:38:42.766457 140583164200832 learning.py:507] global step 3344: loss = 1.8253 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 3345: loss = 2.0105 (1.701 sec/step)\n",
            "I1208 00:38:44.472647 140583164200832 learning.py:507] global step 3345: loss = 2.0105 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 3346: loss = 2.2764 (0.719 sec/step)\n",
            "I1208 00:38:45.538848 140583164200832 learning.py:507] global step 3346: loss = 2.2764 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 3347: loss = 1.7022 (0.587 sec/step)\n",
            "I1208 00:38:46.146330 140583164200832 learning.py:507] global step 3347: loss = 1.7022 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 3348: loss = 1.8605 (0.973 sec/step)\n",
            "I1208 00:38:47.134989 140583164200832 learning.py:507] global step 3348: loss = 1.8605 (0.973 sec/step)\n",
            "INFO:tensorflow:global step 3349: loss = 2.3078 (1.827 sec/step)\n",
            "I1208 00:38:49.164525 140583164200832 learning.py:507] global step 3349: loss = 2.3078 (1.827 sec/step)\n",
            "INFO:tensorflow:global step 3350: loss = 2.1148 (0.582 sec/step)\n",
            "I1208 00:38:49.748782 140583164200832 learning.py:507] global step 3350: loss = 2.1148 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 3351: loss = 1.6835 (1.755 sec/step)\n",
            "I1208 00:38:51.505789 140583164200832 learning.py:507] global step 3351: loss = 1.6835 (1.755 sec/step)\n",
            "INFO:tensorflow:global step 3352: loss = 1.9327 (0.756 sec/step)\n",
            "I1208 00:38:52.401375 140583164200832 learning.py:507] global step 3352: loss = 1.9327 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 3353: loss = 2.1018 (1.331 sec/step)\n",
            "I1208 00:38:53.856885 140583164200832 learning.py:507] global step 3353: loss = 2.1018 (1.331 sec/step)\n",
            "INFO:tensorflow:global step 3354: loss = 1.8327 (0.576 sec/step)\n",
            "I1208 00:38:54.610040 140583164200832 learning.py:507] global step 3354: loss = 1.8327 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 3355: loss = 1.7092 (0.637 sec/step)\n",
            "I1208 00:38:55.728797 140583164200832 learning.py:507] global step 3355: loss = 1.7092 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 3356: loss = 1.5941 (1.994 sec/step)\n",
            "I1208 00:38:57.886454 140583164200832 learning.py:507] global step 3356: loss = 1.5941 (1.994 sec/step)\n",
            "INFO:tensorflow:global step 3357: loss = 1.6665 (0.587 sec/step)\n",
            "I1208 00:38:58.475670 140583164200832 learning.py:507] global step 3357: loss = 1.6665 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 3358: loss = 1.7936 (2.010 sec/step)\n",
            "I1208 00:39:00.487986 140583164200832 learning.py:507] global step 3358: loss = 1.7936 (2.010 sec/step)\n",
            "INFO:tensorflow:global step 3359: loss = 1.6027 (0.590 sec/step)\n",
            "I1208 00:39:01.079616 140583164200832 learning.py:507] global step 3359: loss = 1.6027 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3360: loss = 2.3979 (1.954 sec/step)\n",
            "I1208 00:39:03.035287 140583164200832 learning.py:507] global step 3360: loss = 2.3979 (1.954 sec/step)\n",
            "INFO:tensorflow:global step 3361: loss = 2.1222 (0.726 sec/step)\n",
            "I1208 00:39:04.018916 140583164200832 learning.py:507] global step 3361: loss = 2.1222 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 3362: loss = 1.6999 (1.406 sec/step)\n",
            "I1208 00:39:05.581721 140583164200832 learning.py:507] global step 3362: loss = 1.6999 (1.406 sec/step)\n",
            "INFO:tensorflow:global step 3363: loss = 2.0837 (0.542 sec/step)\n",
            "I1208 00:39:06.544742 140583164200832 learning.py:507] global step 3363: loss = 2.0837 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 3364: loss = 1.7396 (1.245 sec/step)\n",
            "I1208 00:39:07.906115 140583164200832 learning.py:507] global step 3364: loss = 1.7396 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 3365: loss = 1.7453 (1.142 sec/step)\n",
            "I1208 00:39:09.049386 140583164200832 learning.py:507] global step 3365: loss = 1.7453 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 3366: loss = 2.3431 (0.715 sec/step)\n",
            "I1208 00:39:09.959760 140583164200832 learning.py:507] global step 3366: loss = 2.3431 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 3367: loss = 2.2775 (0.684 sec/step)\n",
            "I1208 00:39:11.053243 140583164200832 learning.py:507] global step 3367: loss = 2.2775 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 3368: loss = 2.1204 (0.551 sec/step)\n",
            "I1208 00:39:11.751689 140583164200832 learning.py:507] global step 3368: loss = 2.1204 (0.551 sec/step)\n",
            "INFO:tensorflow:global step 3369: loss = 1.7507 (1.433 sec/step)\n",
            "I1208 00:39:13.476702 140583164200832 learning.py:507] global step 3369: loss = 1.7507 (1.433 sec/step)\n",
            "INFO:tensorflow:global step 3370: loss = 2.0842 (1.070 sec/step)\n",
            "I1208 00:39:14.597574 140583164200832 learning.py:507] global step 3370: loss = 2.0842 (1.070 sec/step)\n",
            "INFO:tensorflow:global step 3371: loss = 1.8228 (0.502 sec/step)\n",
            "I1208 00:39:15.100981 140583164200832 learning.py:507] global step 3371: loss = 1.8228 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 3372: loss = 1.6870 (1.001 sec/step)\n",
            "I1208 00:39:16.153690 140583164200832 learning.py:507] global step 3372: loss = 1.6870 (1.001 sec/step)\n",
            "INFO:tensorflow:global step 3373: loss = 1.9545 (1.792 sec/step)\n",
            "I1208 00:39:18.062565 140583164200832 learning.py:507] global step 3373: loss = 1.9545 (1.792 sec/step)\n",
            "INFO:tensorflow:global step 3374: loss = 1.7817 (1.087 sec/step)\n",
            "I1208 00:39:19.150964 140583164200832 learning.py:507] global step 3374: loss = 1.7817 (1.087 sec/step)\n",
            "INFO:tensorflow:global step 3375: loss = 1.8646 (0.586 sec/step)\n",
            "I1208 00:39:19.943932 140583164200832 learning.py:507] global step 3375: loss = 1.8646 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3376: loss = 1.5229 (1.265 sec/step)\n",
            "I1208 00:39:21.434255 140583164200832 learning.py:507] global step 3376: loss = 1.5229 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 3377: loss = 1.7424 (0.494 sec/step)\n",
            "I1208 00:39:21.930078 140583164200832 learning.py:507] global step 3377: loss = 1.7424 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 3378: loss = 1.7327 (0.561 sec/step)\n",
            "I1208 00:39:22.492336 140583164200832 learning.py:507] global step 3378: loss = 1.7327 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3379: loss = 1.6561 (0.860 sec/step)\n",
            "I1208 00:39:23.636090 140583164200832 learning.py:507] global step 3379: loss = 1.6561 (0.860 sec/step)\n",
            "INFO:tensorflow:global step 3380: loss = 1.7950 (2.306 sec/step)\n",
            "I1208 00:39:26.260225 140583164200832 learning.py:507] global step 3380: loss = 1.7950 (2.306 sec/step)\n",
            "INFO:tensorflow:global step 3381: loss = 2.4020 (1.121 sec/step)\n",
            "I1208 00:39:27.383215 140583164200832 learning.py:507] global step 3381: loss = 2.4020 (1.121 sec/step)\n",
            "INFO:tensorflow:global step 3382: loss = 1.6150 (0.590 sec/step)\n",
            "I1208 00:39:28.205786 140583164200832 learning.py:507] global step 3382: loss = 1.6150 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3383: loss = 1.3647 (2.027 sec/step)\n",
            "I1208 00:39:30.352209 140583164200832 learning.py:507] global step 3383: loss = 1.3647 (2.027 sec/step)\n",
            "INFO:tensorflow:global step 3384: loss = 2.0715 (0.476 sec/step)\n",
            "I1208 00:39:30.829466 140583164200832 learning.py:507] global step 3384: loss = 2.0715 (0.476 sec/step)\n",
            "INFO:tensorflow:global step 3385: loss = 1.8290 (0.587 sec/step)\n",
            "I1208 00:39:31.418487 140583164200832 learning.py:507] global step 3385: loss = 1.8290 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 3386: loss = 1.6669 (2.741 sec/step)\n",
            "I1208 00:39:34.181138 140583164200832 learning.py:507] global step 3386: loss = 1.6669 (2.741 sec/step)\n",
            "INFO:tensorflow:global step 3387: loss = 1.6104 (1.137 sec/step)\n",
            "I1208 00:39:35.320106 140583164200832 learning.py:507] global step 3387: loss = 1.6104 (1.137 sec/step)\n",
            "INFO:tensorflow:global step 3388: loss = 1.4049 (0.640 sec/step)\n",
            "I1208 00:39:36.134264 140583164200832 learning.py:507] global step 3388: loss = 1.4049 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 3389: loss = 1.7614 (0.710 sec/step)\n",
            "I1208 00:39:37.547141 140583164200832 learning.py:507] global step 3389: loss = 1.7614 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 3390: loss = 2.2369 (1.447 sec/step)\n",
            "I1208 00:39:39.009597 140583164200832 learning.py:507] global step 3390: loss = 2.2369 (1.447 sec/step)\n",
            "INFO:tensorflow:global step 3391: loss = 1.8326 (0.777 sec/step)\n",
            "I1208 00:39:40.046994 140583164200832 learning.py:507] global step 3391: loss = 1.8326 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 3392: loss = 2.2969 (1.363 sec/step)\n",
            "I1208 00:39:41.529072 140583164200832 learning.py:507] global step 3392: loss = 2.2969 (1.363 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:39:42.077094 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 3393: loss = 1.6245 (0.575 sec/step)\n",
            "I1208 00:39:42.328937 140583164200832 learning.py:507] global step 3393: loss = 1.6245 (0.575 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3394.\n",
            "I1208 00:39:45.755942 140579459213056 supervisor.py:1050] Recording summary at step 3394.\n",
            "INFO:tensorflow:global step 3394: loss = 2.0010 (3.426 sec/step)\n",
            "I1208 00:39:46.327278 140583164200832 learning.py:507] global step 3394: loss = 2.0010 (3.426 sec/step)\n",
            "INFO:tensorflow:global step 3395: loss = 1.4670 (0.667 sec/step)\n",
            "I1208 00:39:47.177040 140583164200832 learning.py:507] global step 3395: loss = 1.4670 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3396: loss = 1.9269 (1.493 sec/step)\n",
            "I1208 00:39:48.671626 140583164200832 learning.py:507] global step 3396: loss = 1.9269 (1.493 sec/step)\n",
            "INFO:tensorflow:global step 3397: loss = 2.4920 (0.688 sec/step)\n",
            "I1208 00:39:49.400648 140583164200832 learning.py:507] global step 3397: loss = 2.4920 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 3398: loss = 1.8209 (1.322 sec/step)\n",
            "I1208 00:39:50.938939 140583164200832 learning.py:507] global step 3398: loss = 1.8209 (1.322 sec/step)\n",
            "INFO:tensorflow:global step 3399: loss = 1.4583 (0.620 sec/step)\n",
            "I1208 00:39:51.859492 140583164200832 learning.py:507] global step 3399: loss = 1.4583 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 3400: loss = 1.6347 (1.126 sec/step)\n",
            "I1208 00:39:53.091022 140583164200832 learning.py:507] global step 3400: loss = 1.6347 (1.126 sec/step)\n",
            "INFO:tensorflow:global step 3401: loss = 2.1073 (0.501 sec/step)\n",
            "I1208 00:39:53.794404 140583164200832 learning.py:507] global step 3401: loss = 2.1073 (0.501 sec/step)\n",
            "INFO:tensorflow:global step 3402: loss = 1.7694 (1.882 sec/step)\n",
            "I1208 00:39:55.841456 140583164200832 learning.py:507] global step 3402: loss = 1.7694 (1.882 sec/step)\n",
            "INFO:tensorflow:global step 3403: loss = 2.0872 (0.567 sec/step)\n",
            "I1208 00:39:56.410168 140583164200832 learning.py:507] global step 3403: loss = 2.0872 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 3404: loss = 2.0008 (1.772 sec/step)\n",
            "I1208 00:39:58.183726 140583164200832 learning.py:507] global step 3404: loss = 2.0008 (1.772 sec/step)\n",
            "INFO:tensorflow:global step 3405: loss = 1.4381 (0.479 sec/step)\n",
            "I1208 00:39:58.664802 140583164200832 learning.py:507] global step 3405: loss = 1.4381 (0.479 sec/step)\n",
            "INFO:tensorflow:global step 3406: loss = 1.6704 (1.679 sec/step)\n",
            "I1208 00:40:00.345061 140583164200832 learning.py:507] global step 3406: loss = 1.6704 (1.679 sec/step)\n",
            "INFO:tensorflow:global step 3407: loss = 1.5760 (0.697 sec/step)\n",
            "I1208 00:40:01.275990 140583164200832 learning.py:507] global step 3407: loss = 1.5760 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 3408: loss = 1.7401 (1.282 sec/step)\n",
            "I1208 00:40:02.573684 140583164200832 learning.py:507] global step 3408: loss = 1.7401 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 3409: loss = 1.8124 (1.097 sec/step)\n",
            "I1208 00:40:03.672697 140583164200832 learning.py:507] global step 3409: loss = 1.8124 (1.097 sec/step)\n",
            "INFO:tensorflow:global step 3410: loss = 1.7626 (0.633 sec/step)\n",
            "I1208 00:40:04.622737 140583164200832 learning.py:507] global step 3410: loss = 1.7626 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 3411: loss = 1.5214 (0.572 sec/step)\n",
            "I1208 00:40:05.248201 140583164200832 learning.py:507] global step 3411: loss = 1.5214 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 3412: loss = 2.3471 (1.317 sec/step)\n",
            "I1208 00:40:06.579369 140583164200832 learning.py:507] global step 3412: loss = 2.3471 (1.317 sec/step)\n",
            "INFO:tensorflow:global step 3413: loss = 1.2677 (1.955 sec/step)\n",
            "I1208 00:40:08.823776 140583164200832 learning.py:507] global step 3413: loss = 1.2677 (1.955 sec/step)\n",
            "INFO:tensorflow:global step 3414: loss = 2.0095 (0.692 sec/step)\n",
            "I1208 00:40:09.732386 140583164200832 learning.py:507] global step 3414: loss = 2.0095 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 3415: loss = 1.7169 (0.591 sec/step)\n",
            "I1208 00:40:10.506276 140583164200832 learning.py:507] global step 3415: loss = 1.7169 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 3416: loss = 1.6656 (1.753 sec/step)\n",
            "I1208 00:40:12.261434 140583164200832 learning.py:507] global step 3416: loss = 1.6656 (1.753 sec/step)\n",
            "INFO:tensorflow:global step 3417: loss = 2.0036 (0.575 sec/step)\n",
            "I1208 00:40:12.838348 140583164200832 learning.py:507] global step 3417: loss = 2.0036 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 3418: loss = 1.8752 (1.861 sec/step)\n",
            "I1208 00:40:14.701501 140583164200832 learning.py:507] global step 3418: loss = 1.8752 (1.861 sec/step)\n",
            "INFO:tensorflow:global step 3419: loss = 1.5181 (0.476 sec/step)\n",
            "I1208 00:40:15.179650 140583164200832 learning.py:507] global step 3419: loss = 1.5181 (0.476 sec/step)\n",
            "INFO:tensorflow:global step 3420: loss = 1.6294 (0.929 sec/step)\n",
            "I1208 00:40:16.299033 140583164200832 learning.py:507] global step 3420: loss = 1.6294 (0.929 sec/step)\n",
            "INFO:tensorflow:global step 3421: loss = 1.6546 (1.582 sec/step)\n",
            "I1208 00:40:18.160997 140583164200832 learning.py:507] global step 3421: loss = 1.6546 (1.582 sec/step)\n",
            "INFO:tensorflow:global step 3422: loss = 1.5001 (0.939 sec/step)\n",
            "I1208 00:40:19.132231 140583164200832 learning.py:507] global step 3422: loss = 1.5001 (0.939 sec/step)\n",
            "INFO:tensorflow:global step 3423: loss = 1.6959 (1.249 sec/step)\n",
            "I1208 00:40:20.383449 140583164200832 learning.py:507] global step 3423: loss = 1.6959 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 3424: loss = 1.8232 (0.634 sec/step)\n",
            "I1208 00:40:21.319415 140583164200832 learning.py:507] global step 3424: loss = 1.8232 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 3425: loss = 2.4575 (0.509 sec/step)\n",
            "I1208 00:40:22.175355 140583164200832 learning.py:507] global step 3425: loss = 2.4575 (0.509 sec/step)\n",
            "INFO:tensorflow:global step 3426: loss = 1.6136 (0.965 sec/step)\n",
            "I1208 00:40:23.375672 140583164200832 learning.py:507] global step 3426: loss = 1.6136 (0.965 sec/step)\n",
            "INFO:tensorflow:global step 3427: loss = 1.9176 (1.451 sec/step)\n",
            "I1208 00:40:24.853799 140583164200832 learning.py:507] global step 3427: loss = 1.9176 (1.451 sec/step)\n",
            "INFO:tensorflow:global step 3428: loss = 1.9573 (0.732 sec/step)\n",
            "I1208 00:40:25.810238 140583164200832 learning.py:507] global step 3428: loss = 1.9573 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 3429: loss = 2.2712 (0.798 sec/step)\n",
            "I1208 00:40:26.874578 140583164200832 learning.py:507] global step 3429: loss = 2.2712 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 3430: loss = 1.7766 (0.497 sec/step)\n",
            "I1208 00:40:27.373663 140583164200832 learning.py:507] global step 3430: loss = 1.7766 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 3431: loss = 1.7616 (1.505 sec/step)\n",
            "I1208 00:40:28.924726 140583164200832 learning.py:507] global step 3431: loss = 1.7616 (1.505 sec/step)\n",
            "INFO:tensorflow:global step 3432: loss = 1.7313 (0.638 sec/step)\n",
            "I1208 00:40:29.745984 140583164200832 learning.py:507] global step 3432: loss = 1.7313 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 3433: loss = 1.7898 (1.265 sec/step)\n",
            "I1208 00:40:31.131748 140583164200832 learning.py:507] global step 3433: loss = 1.7898 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 3434: loss = 2.1748 (0.509 sec/step)\n",
            "I1208 00:40:31.813159 140583164200832 learning.py:507] global step 3434: loss = 2.1748 (0.509 sec/step)\n",
            "INFO:tensorflow:global step 3435: loss = 1.9371 (0.565 sec/step)\n",
            "I1208 00:40:32.715291 140583164200832 learning.py:507] global step 3435: loss = 1.9371 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 3436: loss = 1.5207 (0.577 sec/step)\n",
            "I1208 00:40:33.294340 140583164200832 learning.py:507] global step 3436: loss = 1.5207 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 3437: loss = 1.6867 (3.242 sec/step)\n",
            "I1208 00:40:36.538690 140583164200832 learning.py:507] global step 3437: loss = 1.6867 (3.242 sec/step)\n",
            "INFO:tensorflow:global step 3438: loss = 1.7393 (0.558 sec/step)\n",
            "I1208 00:40:37.398089 140583164200832 learning.py:507] global step 3438: loss = 1.7393 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 3439: loss = 1.4707 (0.723 sec/step)\n",
            "I1208 00:40:38.855855 140583164200832 learning.py:507] global step 3439: loss = 1.4707 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 3440: loss = 1.9303 (0.647 sec/step)\n",
            "I1208 00:40:39.660531 140583164200832 learning.py:507] global step 3440: loss = 1.9303 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 3441: loss = 2.0529 (0.850 sec/step)\n",
            "I1208 00:40:40.681122 140583164200832 learning.py:507] global step 3441: loss = 2.0529 (0.850 sec/step)\n",
            "INFO:tensorflow:global step 3442: loss = 1.9249 (1.515 sec/step)\n",
            "I1208 00:40:42.667581 140583164200832 learning.py:507] global step 3442: loss = 1.9249 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 3443: loss = 1.9515 (0.622 sec/step)\n",
            "I1208 00:40:43.391071 140583164200832 learning.py:507] global step 3443: loss = 1.9515 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 3444: loss = 2.2303 (1.497 sec/step)\n",
            "I1208 00:40:45.093738 140583164200832 learning.py:507] global step 3444: loss = 2.2303 (1.497 sec/step)\n",
            "INFO:tensorflow:global step 3445: loss = 1.8832 (0.586 sec/step)\n",
            "I1208 00:40:45.941414 140583164200832 learning.py:507] global step 3445: loss = 1.8832 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3446: loss = 1.6329 (1.072 sec/step)\n",
            "I1208 00:40:47.187696 140583164200832 learning.py:507] global step 3446: loss = 1.6329 (1.072 sec/step)\n",
            "INFO:tensorflow:global step 3447: loss = 1.5185 (0.567 sec/step)\n",
            "I1208 00:40:47.960489 140583164200832 learning.py:507] global step 3447: loss = 1.5185 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 3448: loss = 2.1337 (0.644 sec/step)\n",
            "I1208 00:40:49.181054 140583164200832 learning.py:507] global step 3448: loss = 2.1337 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 3449: loss = 1.5479 (0.542 sec/step)\n",
            "I1208 00:40:49.843005 140583164200832 learning.py:507] global step 3449: loss = 1.5479 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 3450: loss = 2.3694 (0.872 sec/step)\n",
            "I1208 00:40:50.873530 140583164200832 learning.py:507] global step 3450: loss = 2.3694 (0.872 sec/step)\n",
            "INFO:tensorflow:global step 3451: loss = 1.9804 (1.602 sec/step)\n",
            "I1208 00:40:52.684404 140583164200832 learning.py:507] global step 3451: loss = 1.9804 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 3452: loss = 1.9629 (0.571 sec/step)\n",
            "I1208 00:40:53.257518 140583164200832 learning.py:507] global step 3452: loss = 1.9629 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 3453: loss = 2.5673 (1.671 sec/step)\n",
            "I1208 00:40:54.930794 140583164200832 learning.py:507] global step 3453: loss = 2.5673 (1.671 sec/step)\n",
            "INFO:tensorflow:global step 3454: loss = 2.1825 (0.528 sec/step)\n",
            "I1208 00:40:55.461485 140583164200832 learning.py:507] global step 3454: loss = 2.1825 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 3455: loss = 2.0494 (1.566 sec/step)\n",
            "I1208 00:40:57.029242 140583164200832 learning.py:507] global step 3455: loss = 2.0494 (1.566 sec/step)\n",
            "INFO:tensorflow:global step 3456: loss = 2.3782 (0.704 sec/step)\n",
            "I1208 00:40:57.902024 140583164200832 learning.py:507] global step 3456: loss = 2.3782 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 3457: loss = 1.4988 (0.771 sec/step)\n",
            "I1208 00:40:58.987965 140583164200832 learning.py:507] global step 3457: loss = 1.4988 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 3458: loss = 1.5119 (0.744 sec/step)\n",
            "I1208 00:41:00.040184 140583164200832 learning.py:507] global step 3458: loss = 1.5119 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 3459: loss = 1.9345 (0.597 sec/step)\n",
            "I1208 00:41:01.104712 140583164200832 learning.py:507] global step 3459: loss = 1.9345 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 3460: loss = 2.0690 (1.240 sec/step)\n",
            "I1208 00:41:02.618497 140583164200832 learning.py:507] global step 3460: loss = 2.0690 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 3461: loss = 1.5400 (1.110 sec/step)\n",
            "I1208 00:41:03.730935 140583164200832 learning.py:507] global step 3461: loss = 1.5400 (1.110 sec/step)\n",
            "INFO:tensorflow:global step 3462: loss = 1.7326 (0.542 sec/step)\n",
            "I1208 00:41:04.274303 140583164200832 learning.py:507] global step 3462: loss = 1.7326 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 3463: loss = 1.7755 (0.569 sec/step)\n",
            "I1208 00:41:04.845748 140583164200832 learning.py:507] global step 3463: loss = 1.7755 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 3464: loss = 1.7214 (2.497 sec/step)\n",
            "I1208 00:41:07.344939 140583164200832 learning.py:507] global step 3464: loss = 1.7214 (2.497 sec/step)\n",
            "INFO:tensorflow:global step 3465: loss = 1.8600 (0.595 sec/step)\n",
            "I1208 00:41:08.236309 140583164200832 learning.py:507] global step 3465: loss = 1.8600 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 3466: loss = 1.9313 (1.120 sec/step)\n",
            "I1208 00:41:09.515763 140583164200832 learning.py:507] global step 3466: loss = 1.9313 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 3467: loss = 1.5443 (0.683 sec/step)\n",
            "I1208 00:41:10.463546 140583164200832 learning.py:507] global step 3467: loss = 1.5443 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 3468: loss = 2.0167 (0.620 sec/step)\n",
            "I1208 00:41:11.476180 140583164200832 learning.py:507] global step 3468: loss = 2.0167 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 3469: loss = 2.5924 (1.856 sec/step)\n",
            "I1208 00:41:13.452601 140583164200832 learning.py:507] global step 3469: loss = 2.5924 (1.856 sec/step)\n",
            "INFO:tensorflow:global step 3470: loss = 1.6951 (0.486 sec/step)\n",
            "I1208 00:41:13.940083 140583164200832 learning.py:507] global step 3470: loss = 1.6951 (0.486 sec/step)\n",
            "INFO:tensorflow:global step 3471: loss = 1.6807 (0.573 sec/step)\n",
            "I1208 00:41:14.515353 140583164200832 learning.py:507] global step 3471: loss = 1.6807 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 3472: loss = 2.6547 (2.065 sec/step)\n",
            "I1208 00:41:16.776224 140583164200832 learning.py:507] global step 3472: loss = 2.6547 (2.065 sec/step)\n",
            "INFO:tensorflow:global step 3473: loss = 2.2419 (1.914 sec/step)\n",
            "I1208 00:41:18.849530 140583164200832 learning.py:507] global step 3473: loss = 2.2419 (1.914 sec/step)\n",
            "INFO:tensorflow:global step 3474: loss = 1.8804 (0.697 sec/step)\n",
            "I1208 00:41:19.809932 140583164200832 learning.py:507] global step 3474: loss = 1.8804 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 3475: loss = 1.6524 (1.674 sec/step)\n",
            "I1208 00:41:21.794819 140583164200832 learning.py:507] global step 3475: loss = 1.6524 (1.674 sec/step)\n",
            "INFO:tensorflow:global step 3476: loss = 2.1585 (0.594 sec/step)\n",
            "I1208 00:41:22.621002 140583164200832 learning.py:507] global step 3476: loss = 2.1585 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 3477: loss = 2.1795 (0.855 sec/step)\n",
            "I1208 00:41:23.653794 140583164200832 learning.py:507] global step 3477: loss = 2.1795 (0.855 sec/step)\n",
            "INFO:tensorflow:global step 3478: loss = 2.0287 (0.532 sec/step)\n",
            "I1208 00:41:24.531127 140583164200832 learning.py:507] global step 3478: loss = 2.0287 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 3479: loss = 1.7114 (1.650 sec/step)\n",
            "I1208 00:41:26.182709 140583164200832 learning.py:507] global step 3479: loss = 1.7114 (1.650 sec/step)\n",
            "INFO:tensorflow:global step 3480: loss = 1.9106 (0.772 sec/step)\n",
            "I1208 00:41:27.249996 140583164200832 learning.py:507] global step 3480: loss = 1.9106 (0.772 sec/step)\n",
            "INFO:tensorflow:global step 3481: loss = 1.3494 (0.539 sec/step)\n",
            "I1208 00:41:27.792211 140583164200832 learning.py:507] global step 3481: loss = 1.3494 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 3482: loss = 1.9268 (0.815 sec/step)\n",
            "I1208 00:41:28.853707 140583164200832 learning.py:507] global step 3482: loss = 1.9268 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 3483: loss = 1.5672 (1.634 sec/step)\n",
            "I1208 00:41:30.608212 140583164200832 learning.py:507] global step 3483: loss = 1.5672 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 3484: loss = 1.9482 (1.180 sec/step)\n",
            "I1208 00:41:31.789438 140583164200832 learning.py:507] global step 3484: loss = 1.9482 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 3485: loss = 2.3979 (0.527 sec/step)\n",
            "I1208 00:41:32.318366 140583164200832 learning.py:507] global step 3485: loss = 2.3979 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 3486: loss = 2.0148 (0.939 sec/step)\n",
            "I1208 00:41:33.465498 140583164200832 learning.py:507] global step 3486: loss = 2.0148 (0.939 sec/step)\n",
            "INFO:tensorflow:global step 3487: loss = 1.6148 (1.588 sec/step)\n",
            "I1208 00:41:35.318834 140583164200832 learning.py:507] global step 3487: loss = 1.6148 (1.588 sec/step)\n",
            "INFO:tensorflow:global step 3488: loss = 1.9136 (0.632 sec/step)\n",
            "I1208 00:41:36.154309 140583164200832 learning.py:507] global step 3488: loss = 1.9136 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 3489: loss = 2.3455 (1.275 sec/step)\n",
            "I1208 00:41:37.618173 140583164200832 learning.py:507] global step 3489: loss = 2.3455 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 3490: loss = 1.9705 (0.693 sec/step)\n",
            "I1208 00:41:38.352561 140583164200832 learning.py:507] global step 3490: loss = 1.9705 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 3491: loss = 1.9152 (1.509 sec/step)\n",
            "I1208 00:41:39.923445 140583164200832 learning.py:507] global step 3491: loss = 1.9152 (1.509 sec/step)\n",
            "INFO:tensorflow:global step 3492: loss = 2.0389 (0.484 sec/step)\n",
            "I1208 00:41:40.408715 140583164200832 learning.py:507] global step 3492: loss = 2.0389 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 3493: loss = 1.7668 (0.611 sec/step)\n",
            "I1208 00:41:41.021552 140583164200832 learning.py:507] global step 3493: loss = 1.7668 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 3494: loss = 1.7107 (3.724 sec/step)\n",
            "I1208 00:41:44.814086 140583164200832 learning.py:507] global step 3494: loss = 1.7107 (3.724 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3494.\n",
            "I1208 00:41:45.123792 140579459213056 supervisor.py:1050] Recording summary at step 3494.\n",
            "INFO:tensorflow:global step 3495: loss = 2.1645 (0.716 sec/step)\n",
            "I1208 00:41:45.781930 140583164200832 learning.py:507] global step 3495: loss = 2.1645 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 3496: loss = 1.9006 (0.511 sec/step)\n",
            "I1208 00:41:46.414417 140583164200832 learning.py:507] global step 3496: loss = 1.9006 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 3497: loss = 1.5467 (2.240 sec/step)\n",
            "I1208 00:41:48.656070 140583164200832 learning.py:507] global step 3497: loss = 1.5467 (2.240 sec/step)\n",
            "INFO:tensorflow:global step 3498: loss = 1.9516 (0.515 sec/step)\n",
            "I1208 00:41:49.173242 140583164200832 learning.py:507] global step 3498: loss = 1.9516 (0.515 sec/step)\n",
            "INFO:tensorflow:global step 3499: loss = 1.8569 (1.605 sec/step)\n",
            "I1208 00:41:50.780204 140583164200832 learning.py:507] global step 3499: loss = 1.8569 (1.605 sec/step)\n",
            "INFO:tensorflow:global step 3500: loss = 2.0448 (0.714 sec/step)\n",
            "I1208 00:41:51.708976 140583164200832 learning.py:507] global step 3500: loss = 2.0448 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 3501: loss = 1.2618 (0.780 sec/step)\n",
            "I1208 00:41:52.802781 140583164200832 learning.py:507] global step 3501: loss = 1.2618 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 3502: loss = 1.6629 (0.493 sec/step)\n",
            "I1208 00:41:53.343593 140583164200832 learning.py:507] global step 3502: loss = 1.6629 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 3503: loss = 1.7248 (1.555 sec/step)\n",
            "I1208 00:41:55.001088 140583164200832 learning.py:507] global step 3503: loss = 1.7248 (1.555 sec/step)\n",
            "INFO:tensorflow:global step 3504: loss = 1.5988 (1.126 sec/step)\n",
            "I1208 00:41:56.128611 140583164200832 learning.py:507] global step 3504: loss = 1.5988 (1.126 sec/step)\n",
            "INFO:tensorflow:global step 3505: loss = 2.1276 (0.555 sec/step)\n",
            "I1208 00:41:56.686068 140583164200832 learning.py:507] global step 3505: loss = 2.1276 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 3506: loss = 2.3664 (0.613 sec/step)\n",
            "I1208 00:41:57.301550 140583164200832 learning.py:507] global step 3506: loss = 2.3664 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 3507: loss = 1.5192 (1.073 sec/step)\n",
            "I1208 00:41:58.664572 140583164200832 learning.py:507] global step 3507: loss = 1.5192 (1.073 sec/step)\n",
            "INFO:tensorflow:global step 3508: loss = 1.8809 (1.944 sec/step)\n",
            "I1208 00:42:01.029870 140583164200832 learning.py:507] global step 3508: loss = 1.8809 (1.944 sec/step)\n",
            "INFO:tensorflow:global step 3509: loss = 2.2351 (0.583 sec/step)\n",
            "I1208 00:42:01.982656 140583164200832 learning.py:507] global step 3509: loss = 2.2351 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 3510: loss = 1.7245 (0.556 sec/step)\n",
            "I1208 00:42:02.679007 140583164200832 learning.py:507] global step 3510: loss = 1.7245 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 3511: loss = 1.8695 (0.568 sec/step)\n",
            "I1208 00:42:03.868101 140583164200832 learning.py:507] global step 3511: loss = 1.8695 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 3512: loss = 1.7217 (1.964 sec/step)\n",
            "I1208 00:42:06.182749 140583164200832 learning.py:507] global step 3512: loss = 1.7217 (1.964 sec/step)\n",
            "INFO:tensorflow:global step 3513: loss = 1.6777 (0.669 sec/step)\n",
            "I1208 00:42:06.872912 140583164200832 learning.py:507] global step 3513: loss = 1.6777 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 3514: loss = 1.5936 (1.514 sec/step)\n",
            "I1208 00:42:08.388527 140583164200832 learning.py:507] global step 3514: loss = 1.5936 (1.514 sec/step)\n",
            "INFO:tensorflow:global step 3515: loss = 1.9884 (1.116 sec/step)\n",
            "I1208 00:42:09.506315 140583164200832 learning.py:507] global step 3515: loss = 1.9884 (1.116 sec/step)\n",
            "INFO:tensorflow:global step 3516: loss = 1.7197 (0.710 sec/step)\n",
            "I1208 00:42:10.368166 140583164200832 learning.py:507] global step 3516: loss = 1.7197 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 3517: loss = 1.4735 (1.374 sec/step)\n",
            "I1208 00:42:11.743638 140583164200832 learning.py:507] global step 3517: loss = 1.4735 (1.374 sec/step)\n",
            "INFO:tensorflow:global step 3518: loss = 1.7411 (0.589 sec/step)\n",
            "I1208 00:42:12.660987 140583164200832 learning.py:507] global step 3518: loss = 1.7411 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 3519: loss = 1.6074 (1.278 sec/step)\n",
            "I1208 00:42:14.015049 140583164200832 learning.py:507] global step 3519: loss = 1.6074 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 3520: loss = 1.7403 (0.497 sec/step)\n",
            "I1208 00:42:14.515012 140583164200832 learning.py:507] global step 3520: loss = 1.7403 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 3521: loss = 1.3760 (1.730 sec/step)\n",
            "I1208 00:42:16.246990 140583164200832 learning.py:507] global step 3521: loss = 1.3760 (1.730 sec/step)\n",
            "INFO:tensorflow:global step 3522: loss = 1.7043 (0.640 sec/step)\n",
            "I1208 00:42:17.007416 140583164200832 learning.py:507] global step 3522: loss = 1.7043 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 3523: loss = 1.5797 (0.634 sec/step)\n",
            "I1208 00:42:18.229140 140583164200832 learning.py:507] global step 3523: loss = 1.5797 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 3524: loss = 2.2699 (1.155 sec/step)\n",
            "I1208 00:42:19.472249 140583164200832 learning.py:507] global step 3524: loss = 2.2699 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 3525: loss = 2.1793 (0.538 sec/step)\n",
            "I1208 00:42:20.012302 140583164200832 learning.py:507] global step 3525: loss = 2.1793 (0.538 sec/step)\n",
            "INFO:tensorflow:global step 3526: loss = 1.9438 (2.051 sec/step)\n",
            "I1208 00:42:22.080780 140583164200832 learning.py:507] global step 3526: loss = 1.9438 (2.051 sec/step)\n",
            "INFO:tensorflow:global step 3527: loss = 1.5577 (0.632 sec/step)\n",
            "I1208 00:42:23.007243 140583164200832 learning.py:507] global step 3527: loss = 1.5577 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 3528: loss = 1.8024 (1.288 sec/step)\n",
            "I1208 00:42:24.539930 140583164200832 learning.py:507] global step 3528: loss = 1.8024 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 3529: loss = 1.5155 (0.511 sec/step)\n",
            "I1208 00:42:25.264271 140583164200832 learning.py:507] global step 3529: loss = 1.5155 (0.511 sec/step)\n",
            "INFO:tensorflow:global step 3530: loss = 1.7483 (0.640 sec/step)\n",
            "I1208 00:42:26.452099 140583164200832 learning.py:507] global step 3530: loss = 1.7483 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 3531: loss = 1.5617 (1.268 sec/step)\n",
            "I1208 00:42:27.855911 140583164200832 learning.py:507] global step 3531: loss = 1.5617 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 3532: loss = 1.7359 (1.135 sec/step)\n",
            "I1208 00:42:28.992518 140583164200832 learning.py:507] global step 3532: loss = 1.7359 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 3533: loss = 2.2514 (0.661 sec/step)\n",
            "I1208 00:42:29.909466 140583164200832 learning.py:507] global step 3533: loss = 2.2514 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 3534: loss = 1.5075 (0.613 sec/step)\n",
            "I1208 00:42:30.838485 140583164200832 learning.py:507] global step 3534: loss = 1.5075 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 3535: loss = 1.5247 (1.272 sec/step)\n",
            "I1208 00:42:32.359936 140583164200832 learning.py:507] global step 3535: loss = 1.5247 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 3536: loss = 1.1712 (1.074 sec/step)\n",
            "I1208 00:42:33.435916 140583164200832 learning.py:507] global step 3536: loss = 1.1712 (1.074 sec/step)\n",
            "INFO:tensorflow:global step 3537: loss = 1.7026 (0.497 sec/step)\n",
            "I1208 00:42:33.936364 140583164200832 learning.py:507] global step 3537: loss = 1.7026 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 3538: loss = 1.5886 (0.758 sec/step)\n",
            "I1208 00:42:35.022488 140583164200832 learning.py:507] global step 3538: loss = 1.5886 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 3539: loss = 1.9599 (1.569 sec/step)\n",
            "I1208 00:42:36.750078 140583164200832 learning.py:507] global step 3539: loss = 1.9599 (1.569 sec/step)\n",
            "INFO:tensorflow:global step 3540: loss = 2.0533 (0.556 sec/step)\n",
            "I1208 00:42:37.621213 140583164200832 learning.py:507] global step 3540: loss = 2.0533 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 3541: loss = 1.7847 (0.590 sec/step)\n",
            "I1208 00:42:38.346109 140583164200832 learning.py:507] global step 3541: loss = 1.7847 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3542: loss = 2.0224 (1.816 sec/step)\n",
            "I1208 00:42:40.165104 140583164200832 learning.py:507] global step 3542: loss = 2.0224 (1.816 sec/step)\n",
            "INFO:tensorflow:global step 3543: loss = 1.8753 (0.554 sec/step)\n",
            "I1208 00:42:40.966140 140583164200832 learning.py:507] global step 3543: loss = 1.8753 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 3544: loss = 1.7177 (0.745 sec/step)\n",
            "I1208 00:42:42.143638 140583164200832 learning.py:507] global step 3544: loss = 1.7177 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 3545: loss = 1.9542 (1.138 sec/step)\n",
            "I1208 00:42:43.400204 140583164200832 learning.py:507] global step 3545: loss = 1.9542 (1.138 sec/step)\n",
            "INFO:tensorflow:global step 3546: loss = 2.1557 (0.561 sec/step)\n",
            "I1208 00:42:43.963520 140583164200832 learning.py:507] global step 3546: loss = 2.1557 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3547: loss = 1.7591 (1.726 sec/step)\n",
            "I1208 00:42:45.826596 140583164200832 learning.py:507] global step 3547: loss = 1.7591 (1.726 sec/step)\n",
            "INFO:tensorflow:global step 3548: loss = 1.6263 (1.110 sec/step)\n",
            "I1208 00:42:46.937996 140583164200832 learning.py:507] global step 3548: loss = 1.6263 (1.110 sec/step)\n",
            "INFO:tensorflow:global step 3549: loss = 1.3745 (0.679 sec/step)\n",
            "I1208 00:42:47.863077 140583164200832 learning.py:507] global step 3549: loss = 1.3745 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 3550: loss = 2.1781 (1.372 sec/step)\n",
            "I1208 00:42:49.289422 140583164200832 learning.py:507] global step 3550: loss = 2.1781 (1.372 sec/step)\n",
            "INFO:tensorflow:global step 3551: loss = 1.6530 (0.763 sec/step)\n",
            "I1208 00:42:50.057340 140583164200832 learning.py:507] global step 3551: loss = 1.6530 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 3552: loss = 1.7596 (1.514 sec/step)\n",
            "I1208 00:42:51.573317 140583164200832 learning.py:507] global step 3552: loss = 1.7596 (1.514 sec/step)\n",
            "INFO:tensorflow:global step 3553: loss = 1.7595 (0.639 sec/step)\n",
            "I1208 00:42:52.566177 140583164200832 learning.py:507] global step 3553: loss = 1.7595 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 3554: loss = 2.0555 (0.524 sec/step)\n",
            "I1208 00:42:53.194064 140583164200832 learning.py:507] global step 3554: loss = 2.0555 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 3555: loss = 1.8700 (1.836 sec/step)\n",
            "I1208 00:42:55.031403 140583164200832 learning.py:507] global step 3555: loss = 1.8700 (1.836 sec/step)\n",
            "INFO:tensorflow:global step 3556: loss = 1.6828 (0.603 sec/step)\n",
            "I1208 00:42:56.068763 140583164200832 learning.py:507] global step 3556: loss = 1.6828 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 3557: loss = 2.1451 (0.586 sec/step)\n",
            "I1208 00:42:56.783852 140583164200832 learning.py:507] global step 3557: loss = 2.1451 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3558: loss = 1.8901 (1.687 sec/step)\n",
            "I1208 00:42:58.473026 140583164200832 learning.py:507] global step 3558: loss = 1.8901 (1.687 sec/step)\n",
            "INFO:tensorflow:global step 3559: loss = 1.8477 (0.660 sec/step)\n",
            "I1208 00:42:59.134396 140583164200832 learning.py:507] global step 3559: loss = 1.8477 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 3560: loss = 2.0269 (0.988 sec/step)\n",
            "I1208 00:43:00.383838 140583164200832 learning.py:507] global step 3560: loss = 2.0269 (0.988 sec/step)\n",
            "INFO:tensorflow:global step 3561: loss = 1.7686 (1.610 sec/step)\n",
            "I1208 00:43:02.099482 140583164200832 learning.py:507] global step 3561: loss = 1.7686 (1.610 sec/step)\n",
            "INFO:tensorflow:global step 3562: loss = 1.3838 (0.561 sec/step)\n",
            "I1208 00:43:02.662792 140583164200832 learning.py:507] global step 3562: loss = 1.3838 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3563: loss = 1.4664 (1.785 sec/step)\n",
            "I1208 00:43:04.450131 140583164200832 learning.py:507] global step 3563: loss = 1.4664 (1.785 sec/step)\n",
            "INFO:tensorflow:global step 3564: loss = 1.6234 (0.612 sec/step)\n",
            "I1208 00:43:05.326880 140583164200832 learning.py:507] global step 3564: loss = 1.6234 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 3565: loss = 1.5951 (1.449 sec/step)\n",
            "I1208 00:43:06.899181 140583164200832 learning.py:507] global step 3565: loss = 1.5951 (1.449 sec/step)\n",
            "INFO:tensorflow:global step 3566: loss = 1.5627 (0.676 sec/step)\n",
            "I1208 00:43:07.711976 140583164200832 learning.py:507] global step 3566: loss = 1.5627 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 3567: loss = 2.4737 (0.708 sec/step)\n",
            "I1208 00:43:09.052073 140583164200832 learning.py:507] global step 3567: loss = 2.4737 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 3568: loss = 1.5336 (1.437 sec/step)\n",
            "I1208 00:43:10.606441 140583164200832 learning.py:507] global step 3568: loss = 1.5336 (1.437 sec/step)\n",
            "INFO:tensorflow:global step 3569: loss = 2.2064 (0.614 sec/step)\n",
            "I1208 00:43:11.437114 140583164200832 learning.py:507] global step 3569: loss = 2.2064 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 3570: loss = 1.9512 (1.364 sec/step)\n",
            "I1208 00:43:13.002637 140583164200832 learning.py:507] global step 3570: loss = 1.9512 (1.364 sec/step)\n",
            "INFO:tensorflow:global step 3571: loss = 1.8016 (0.761 sec/step)\n",
            "I1208 00:43:14.033386 140583164200832 learning.py:507] global step 3571: loss = 1.8016 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 3572: loss = 1.5116 (0.832 sec/step)\n",
            "I1208 00:43:15.107323 140583164200832 learning.py:507] global step 3572: loss = 1.5116 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 3573: loss = 1.9223 (0.585 sec/step)\n",
            "I1208 00:43:15.940066 140583164200832 learning.py:507] global step 3573: loss = 1.9223 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 3574: loss = 1.4571 (0.891 sec/step)\n",
            "I1208 00:43:17.376523 140583164200832 learning.py:507] global step 3574: loss = 1.4571 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 3575: loss = 1.5134 (1.360 sec/step)\n",
            "I1208 00:43:18.738393 140583164200832 learning.py:507] global step 3575: loss = 1.5134 (1.360 sec/step)\n",
            "INFO:tensorflow:global step 3576: loss = 1.9030 (0.547 sec/step)\n",
            "I1208 00:43:19.637788 140583164200832 learning.py:507] global step 3576: loss = 1.9030 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 3577: loss = 1.9162 (0.753 sec/step)\n",
            "I1208 00:43:20.607979 140583164200832 learning.py:507] global step 3577: loss = 1.9162 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 3578: loss = 2.7053 (1.416 sec/step)\n",
            "I1208 00:43:22.179023 140583164200832 learning.py:507] global step 3578: loss = 2.7053 (1.416 sec/step)\n",
            "INFO:tensorflow:global step 3579: loss = 2.3781 (0.510 sec/step)\n",
            "I1208 00:43:23.061646 140583164200832 learning.py:507] global step 3579: loss = 2.3781 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 3580: loss = 2.0108 (1.417 sec/step)\n",
            "I1208 00:43:24.572551 140583164200832 learning.py:507] global step 3580: loss = 2.0108 (1.417 sec/step)\n",
            "INFO:tensorflow:global step 3581: loss = 1.5537 (0.574 sec/step)\n",
            "I1208 00:43:25.149159 140583164200832 learning.py:507] global step 3581: loss = 1.5537 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 3582: loss = 1.9573 (0.743 sec/step)\n",
            "I1208 00:43:26.257975 140583164200832 learning.py:507] global step 3582: loss = 1.9573 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 3583: loss = 1.9117 (1.477 sec/step)\n",
            "I1208 00:43:28.045329 140583164200832 learning.py:507] global step 3583: loss = 1.9117 (1.477 sec/step)\n",
            "INFO:tensorflow:global step 3584: loss = 1.6249 (0.586 sec/step)\n",
            "I1208 00:43:29.004964 140583164200832 learning.py:507] global step 3584: loss = 1.6249 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3585: loss = 1.6409 (0.732 sec/step)\n",
            "I1208 00:43:30.041753 140583164200832 learning.py:507] global step 3585: loss = 1.6409 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 3586: loss = 2.0112 (0.701 sec/step)\n",
            "I1208 00:43:31.198149 140583164200832 learning.py:507] global step 3586: loss = 2.0112 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 3587: loss = 2.3032 (0.733 sec/step)\n",
            "I1208 00:43:32.250232 140583164200832 learning.py:507] global step 3587: loss = 2.3032 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 3588: loss = 1.5882 (0.565 sec/step)\n",
            "I1208 00:43:33.312613 140583164200832 learning.py:507] global step 3588: loss = 1.5882 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 3589: loss = 1.6599 (1.250 sec/step)\n",
            "I1208 00:43:34.797312 140583164200832 learning.py:507] global step 3589: loss = 1.6599 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 3590: loss = 1.7528 (0.684 sec/step)\n",
            "I1208 00:43:35.776948 140583164200832 learning.py:507] global step 3590: loss = 1.7528 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 3591: loss = 1.7051 (0.637 sec/step)\n",
            "I1208 00:43:36.718776 140583164200832 learning.py:507] global step 3591: loss = 1.7051 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 3592: loss = 2.3390 (0.698 sec/step)\n",
            "I1208 00:43:37.579027 140583164200832 learning.py:507] global step 3592: loss = 2.3390 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 3593: loss = 1.4401 (1.438 sec/step)\n",
            "I1208 00:43:39.070565 140583164200832 learning.py:507] global step 3593: loss = 1.4401 (1.438 sec/step)\n",
            "INFO:tensorflow:global step 3594: loss = 1.3922 (0.512 sec/step)\n",
            "I1208 00:43:39.732906 140583164200832 learning.py:507] global step 3594: loss = 1.3922 (0.512 sec/step)\n",
            "INFO:tensorflow:global step 3595: loss = 1.5458 (1.437 sec/step)\n",
            "I1208 00:43:41.389723 140583164200832 learning.py:507] global step 3595: loss = 1.5458 (1.437 sec/step)\n",
            "INFO:tensorflow:global step 3596: loss = 2.0576 (0.607 sec/step)\n",
            "I1208 00:43:42.647251 140583164200832 learning.py:507] global step 3596: loss = 2.0576 (0.607 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3596.\n",
            "I1208 00:43:44.759319 140579459213056 supervisor.py:1050] Recording summary at step 3596.\n",
            "INFO:tensorflow:global step 3597: loss = 1.5271 (2.134 sec/step)\n",
            "I1208 00:43:44.955647 140583164200832 learning.py:507] global step 3597: loss = 1.5271 (2.134 sec/step)\n",
            "INFO:tensorflow:global step 3598: loss = 2.0431 (0.520 sec/step)\n",
            "I1208 00:43:45.478172 140583164200832 learning.py:507] global step 3598: loss = 2.0431 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 3599: loss = 2.0780 (1.622 sec/step)\n",
            "I1208 00:43:47.101647 140583164200832 learning.py:507] global step 3599: loss = 2.0780 (1.622 sec/step)\n",
            "INFO:tensorflow:global step 3600: loss = 1.9394 (0.583 sec/step)\n",
            "I1208 00:43:47.798097 140583164200832 learning.py:507] global step 3600: loss = 1.9394 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 3601: loss = 1.7642 (0.558 sec/step)\n",
            "I1208 00:43:48.940952 140583164200832 learning.py:507] global step 3601: loss = 1.7642 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 3602: loss = 1.7693 (0.600 sec/step)\n",
            "I1208 00:43:50.009801 140583164200832 learning.py:507] global step 3602: loss = 1.7693 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 3603: loss = 1.6930 (1.094 sec/step)\n",
            "I1208 00:43:51.256410 140583164200832 learning.py:507] global step 3603: loss = 1.6930 (1.094 sec/step)\n",
            "INFO:tensorflow:global step 3604: loss = 1.4747 (0.601 sec/step)\n",
            "I1208 00:43:51.947853 140583164200832 learning.py:507] global step 3604: loss = 1.4747 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 3605: loss = 2.2536 (1.270 sec/step)\n",
            "I1208 00:43:53.457633 140583164200832 learning.py:507] global step 3605: loss = 2.2536 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 3606: loss = 1.6977 (0.621 sec/step)\n",
            "I1208 00:43:54.262869 140583164200832 learning.py:507] global step 3606: loss = 1.6977 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 3607: loss = 2.0754 (1.195 sec/step)\n",
            "I1208 00:43:55.600309 140583164200832 learning.py:507] global step 3607: loss = 2.0754 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 3608: loss = 1.4759 (0.492 sec/step)\n",
            "I1208 00:43:56.094460 140583164200832 learning.py:507] global step 3608: loss = 1.4759 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3609: loss = 2.2549 (1.778 sec/step)\n",
            "I1208 00:43:57.873869 140583164200832 learning.py:507] global step 3609: loss = 2.2549 (1.778 sec/step)\n",
            "INFO:tensorflow:global step 3610: loss = 2.1541 (0.561 sec/step)\n",
            "I1208 00:43:58.835422 140583164200832 learning.py:507] global step 3610: loss = 2.1541 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3611: loss = 1.7568 (1.217 sec/step)\n",
            "I1208 00:44:00.148519 140583164200832 learning.py:507] global step 3611: loss = 1.7568 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 3612: loss = 1.5175 (0.550 sec/step)\n",
            "I1208 00:44:00.700698 140583164200832 learning.py:507] global step 3612: loss = 1.5175 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 3613: loss = 1.4973 (0.580 sec/step)\n",
            "I1208 00:44:01.282906 140583164200832 learning.py:507] global step 3613: loss = 1.4973 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 3614: loss = 2.2620 (0.638 sec/step)\n",
            "I1208 00:44:01.923561 140583164200832 learning.py:507] global step 3614: loss = 2.2620 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 3615: loss = 2.2858 (3.438 sec/step)\n",
            "I1208 00:44:05.632781 140583164200832 learning.py:507] global step 3615: loss = 2.2858 (3.438 sec/step)\n",
            "INFO:tensorflow:global step 3616: loss = 1.7597 (1.635 sec/step)\n",
            "I1208 00:44:07.285396 140583164200832 learning.py:507] global step 3616: loss = 1.7597 (1.635 sec/step)\n",
            "INFO:tensorflow:global step 3617: loss = 1.6704 (0.546 sec/step)\n",
            "I1208 00:44:07.833143 140583164200832 learning.py:507] global step 3617: loss = 1.6704 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 3618: loss = 2.0098 (0.965 sec/step)\n",
            "I1208 00:44:08.953037 140583164200832 learning.py:507] global step 3618: loss = 2.0098 (0.965 sec/step)\n",
            "INFO:tensorflow:global step 3619: loss = 1.8882 (2.187 sec/step)\n",
            "I1208 00:44:11.561086 140583164200832 learning.py:507] global step 3619: loss = 1.8882 (2.187 sec/step)\n",
            "INFO:tensorflow:global step 3620: loss = 2.3802 (0.587 sec/step)\n",
            "I1208 00:44:12.436058 140583164200832 learning.py:507] global step 3620: loss = 2.3802 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 3621: loss = 2.0657 (1.506 sec/step)\n",
            "I1208 00:44:14.193755 140583164200832 learning.py:507] global step 3621: loss = 2.0657 (1.506 sec/step)\n",
            "INFO:tensorflow:global step 3622: loss = 1.5418 (0.645 sec/step)\n",
            "I1208 00:44:14.841233 140583164200832 learning.py:507] global step 3622: loss = 1.5418 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 3623: loss = 1.1938 (1.980 sec/step)\n",
            "I1208 00:44:16.823616 140583164200832 learning.py:507] global step 3623: loss = 1.1938 (1.980 sec/step)\n",
            "INFO:tensorflow:global step 3624: loss = 1.9856 (0.621 sec/step)\n",
            "I1208 00:44:17.447016 140583164200832 learning.py:507] global step 3624: loss = 1.9856 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 3625: loss = 1.4616 (1.990 sec/step)\n",
            "I1208 00:44:19.439023 140583164200832 learning.py:507] global step 3625: loss = 1.4616 (1.990 sec/step)\n",
            "INFO:tensorflow:global step 3626: loss = 2.2594 (0.573 sec/step)\n",
            "I1208 00:44:20.311244 140583164200832 learning.py:507] global step 3626: loss = 2.2594 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 3627: loss = 1.5941 (0.611 sec/step)\n",
            "I1208 00:44:21.427671 140583164200832 learning.py:507] global step 3627: loss = 1.5941 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 3628: loss = 2.4511 (0.804 sec/step)\n",
            "I1208 00:44:22.684579 140583164200832 learning.py:507] global step 3628: loss = 2.4511 (0.804 sec/step)\n",
            "INFO:tensorflow:global step 3629: loss = 1.5800 (1.326 sec/step)\n",
            "I1208 00:44:24.056011 140583164200832 learning.py:507] global step 3629: loss = 1.5800 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 3630: loss = 1.2696 (0.638 sec/step)\n",
            "I1208 00:44:24.973150 140583164200832 learning.py:507] global step 3630: loss = 1.2696 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 3631: loss = 1.2821 (0.589 sec/step)\n",
            "I1208 00:44:26.042171 140583164200832 learning.py:507] global step 3631: loss = 1.2821 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 3632: loss = 2.3322 (0.650 sec/step)\n",
            "I1208 00:44:27.085982 140583164200832 learning.py:507] global step 3632: loss = 2.3322 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 3633: loss = 1.4323 (1.147 sec/step)\n",
            "I1208 00:44:28.416847 140583164200832 learning.py:507] global step 3633: loss = 1.4323 (1.147 sec/step)\n",
            "INFO:tensorflow:global step 3634: loss = 1.7105 (0.586 sec/step)\n",
            "I1208 00:44:29.299975 140583164200832 learning.py:507] global step 3634: loss = 1.7105 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 3635: loss = 1.7067 (1.296 sec/step)\n",
            "I1208 00:44:30.736611 140583164200832 learning.py:507] global step 3635: loss = 1.7067 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 3636: loss = 1.7637 (0.688 sec/step)\n",
            "I1208 00:44:31.512617 140583164200832 learning.py:507] global step 3636: loss = 1.7637 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 3637: loss = 1.6038 (1.299 sec/step)\n",
            "I1208 00:44:33.023269 140583164200832 learning.py:507] global step 3637: loss = 1.6038 (1.299 sec/step)\n",
            "INFO:tensorflow:global step 3638: loss = 2.2239 (0.661 sec/step)\n",
            "I1208 00:44:33.715184 140583164200832 learning.py:507] global step 3638: loss = 2.2239 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 3639: loss = 1.4522 (1.487 sec/step)\n",
            "I1208 00:44:35.219737 140583164200832 learning.py:507] global step 3639: loss = 1.4522 (1.487 sec/step)\n",
            "INFO:tensorflow:global step 3640: loss = 1.8073 (0.540 sec/step)\n",
            "I1208 00:44:35.761045 140583164200832 learning.py:507] global step 3640: loss = 1.8073 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 3641: loss = 2.2586 (1.767 sec/step)\n",
            "I1208 00:44:37.530280 140583164200832 learning.py:507] global step 3641: loss = 2.2586 (1.767 sec/step)\n",
            "INFO:tensorflow:global step 3642: loss = 1.9576 (0.492 sec/step)\n",
            "I1208 00:44:38.024409 140583164200832 learning.py:507] global step 3642: loss = 1.9576 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3643: loss = 2.1448 (1.064 sec/step)\n",
            "I1208 00:44:39.093361 140583164200832 learning.py:507] global step 3643: loss = 2.1448 (1.064 sec/step)\n",
            "INFO:tensorflow:global step 3644: loss = 1.8315 (2.044 sec/step)\n",
            "I1208 00:44:41.139075 140583164200832 learning.py:507] global step 3644: loss = 1.8315 (2.044 sec/step)\n",
            "INFO:tensorflow:global step 3645: loss = 2.2731 (0.719 sec/step)\n",
            "I1208 00:44:42.087976 140583164200832 learning.py:507] global step 3645: loss = 2.2731 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 3646: loss = 1.7910 (1.192 sec/step)\n",
            "I1208 00:44:43.426353 140583164200832 learning.py:507] global step 3646: loss = 1.7910 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 3647: loss = 1.4433 (0.616 sec/step)\n",
            "I1208 00:44:44.131284 140583164200832 learning.py:507] global step 3647: loss = 1.4433 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 3648: loss = 1.8804 (1.612 sec/step)\n",
            "I1208 00:44:45.781718 140583164200832 learning.py:507] global step 3648: loss = 1.8804 (1.612 sec/step)\n",
            "INFO:tensorflow:global step 3649: loss = 1.2517 (0.507 sec/step)\n",
            "I1208 00:44:46.291051 140583164200832 learning.py:507] global step 3649: loss = 1.2517 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 3650: loss = 1.9726 (1.844 sec/step)\n",
            "I1208 00:44:48.136445 140583164200832 learning.py:507] global step 3650: loss = 1.9726 (1.844 sec/step)\n",
            "INFO:tensorflow:global step 3651: loss = 1.3842 (0.560 sec/step)\n",
            "I1208 00:44:48.698303 140583164200832 learning.py:507] global step 3651: loss = 1.3842 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 3652: loss = 1.7246 (1.680 sec/step)\n",
            "I1208 00:44:50.380468 140583164200832 learning.py:507] global step 3652: loss = 1.7246 (1.680 sec/step)\n",
            "INFO:tensorflow:global step 3653: loss = 2.5745 (1.100 sec/step)\n",
            "I1208 00:44:51.482094 140583164200832 learning.py:507] global step 3653: loss = 2.5745 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 3654: loss = 2.0268 (1.124 sec/step)\n",
            "I1208 00:44:52.608113 140583164200832 learning.py:507] global step 3654: loss = 2.0268 (1.124 sec/step)\n",
            "INFO:tensorflow:global step 3655: loss = 2.0372 (0.767 sec/step)\n",
            "I1208 00:44:53.533756 140583164200832 learning.py:507] global step 3655: loss = 2.0372 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 3656: loss = 1.4077 (0.653 sec/step)\n",
            "I1208 00:44:54.359365 140583164200832 learning.py:507] global step 3656: loss = 1.4077 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 3657: loss = 1.8667 (1.760 sec/step)\n",
            "I1208 00:44:56.242867 140583164200832 learning.py:507] global step 3657: loss = 1.8667 (1.760 sec/step)\n",
            "INFO:tensorflow:global step 3658: loss = 1.8696 (0.647 sec/step)\n",
            "I1208 00:44:57.284724 140583164200832 learning.py:507] global step 3658: loss = 1.8696 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 3659: loss = 1.5996 (0.516 sec/step)\n",
            "I1208 00:44:57.852274 140583164200832 learning.py:507] global step 3659: loss = 1.5996 (0.516 sec/step)\n",
            "INFO:tensorflow:global step 3660: loss = 2.0311 (0.911 sec/step)\n",
            "I1208 00:44:59.189307 140583164200832 learning.py:507] global step 3660: loss = 2.0311 (0.911 sec/step)\n",
            "INFO:tensorflow:global step 3661: loss = 1.7370 (1.481 sec/step)\n",
            "I1208 00:45:00.775348 140583164200832 learning.py:507] global step 3661: loss = 1.7370 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 3662: loss = 2.1359 (0.594 sec/step)\n",
            "I1208 00:45:01.669127 140583164200832 learning.py:507] global step 3662: loss = 2.1359 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 3663: loss = 1.9663 (0.967 sec/step)\n",
            "I1208 00:45:02.909261 140583164200832 learning.py:507] global step 3663: loss = 1.9663 (0.967 sec/step)\n",
            "INFO:tensorflow:global step 3664: loss = 1.6561 (1.265 sec/step)\n",
            "I1208 00:45:04.176120 140583164200832 learning.py:507] global step 3664: loss = 1.6561 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 3665: loss = 1.7630 (0.576 sec/step)\n",
            "I1208 00:45:04.989038 140583164200832 learning.py:507] global step 3665: loss = 1.7630 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 3666: loss = 1.6236 (1.376 sec/step)\n",
            "I1208 00:45:06.449175 140583164200832 learning.py:507] global step 3666: loss = 1.6236 (1.376 sec/step)\n",
            "INFO:tensorflow:global step 3667: loss = 1.7609 (0.614 sec/step)\n",
            "I1208 00:45:07.278141 140583164200832 learning.py:507] global step 3667: loss = 1.7609 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 3668: loss = 2.0201 (0.571 sec/step)\n",
            "I1208 00:45:08.172565 140583164200832 learning.py:507] global step 3668: loss = 2.0201 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 3669: loss = 1.8340 (0.893 sec/step)\n",
            "I1208 00:45:09.203953 140583164200832 learning.py:507] global step 3669: loss = 1.8340 (0.893 sec/step)\n",
            "INFO:tensorflow:global step 3670: loss = 1.5199 (1.727 sec/step)\n",
            "I1208 00:45:10.974544 140583164200832 learning.py:507] global step 3670: loss = 1.5199 (1.727 sec/step)\n",
            "INFO:tensorflow:global step 3671: loss = 2.0545 (0.708 sec/step)\n",
            "I1208 00:45:11.818926 140583164200832 learning.py:507] global step 3671: loss = 2.0545 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 3672: loss = 1.2750 (1.390 sec/step)\n",
            "I1208 00:45:13.272454 140583164200832 learning.py:507] global step 3672: loss = 1.2750 (1.390 sec/step)\n",
            "INFO:tensorflow:global step 3673: loss = 2.0509 (0.503 sec/step)\n",
            "I1208 00:45:13.777740 140583164200832 learning.py:507] global step 3673: loss = 2.0509 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 3674: loss = 2.4312 (2.298 sec/step)\n",
            "I1208 00:45:16.116515 140583164200832 learning.py:507] global step 3674: loss = 2.4312 (2.298 sec/step)\n",
            "INFO:tensorflow:global step 3675: loss = 1.6832 (0.695 sec/step)\n",
            "I1208 00:45:17.066449 140583164200832 learning.py:507] global step 3675: loss = 1.6832 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 3676: loss = 1.3681 (0.578 sec/step)\n",
            "I1208 00:45:18.101643 140583164200832 learning.py:507] global step 3676: loss = 1.3681 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 3677: loss = 1.5822 (1.508 sec/step)\n",
            "I1208 00:45:19.650212 140583164200832 learning.py:507] global step 3677: loss = 1.5822 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 3678: loss = 2.0921 (0.620 sec/step)\n",
            "I1208 00:45:20.507755 140583164200832 learning.py:507] global step 3678: loss = 2.0921 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 3679: loss = 2.0314 (1.248 sec/step)\n",
            "I1208 00:45:21.907928 140583164200832 learning.py:507] global step 3679: loss = 2.0314 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 3680: loss = 2.1090 (0.612 sec/step)\n",
            "I1208 00:45:22.810168 140583164200832 learning.py:507] global step 3680: loss = 2.1090 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 3681: loss = 2.3026 (0.492 sec/step)\n",
            "I1208 00:45:23.654192 140583164200832 learning.py:507] global step 3681: loss = 2.3026 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3682: loss = 1.5540 (1.823 sec/step)\n",
            "I1208 00:45:25.632668 140583164200832 learning.py:507] global step 3682: loss = 1.5540 (1.823 sec/step)\n",
            "INFO:tensorflow:global step 3683: loss = 1.9907 (0.628 sec/step)\n",
            "I1208 00:45:26.518062 140583164200832 learning.py:507] global step 3683: loss = 1.9907 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 3684: loss = 2.3234 (0.613 sec/step)\n",
            "I1208 00:45:27.291271 140583164200832 learning.py:507] global step 3684: loss = 2.3234 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 3685: loss = 1.2611 (1.427 sec/step)\n",
            "I1208 00:45:28.836279 140583164200832 learning.py:507] global step 3685: loss = 1.2611 (1.427 sec/step)\n",
            "INFO:tensorflow:global step 3686: loss = 2.3780 (0.480 sec/step)\n",
            "I1208 00:45:29.318087 140583164200832 learning.py:507] global step 3686: loss = 2.3780 (0.480 sec/step)\n",
            "INFO:tensorflow:global step 3687: loss = 1.5109 (1.735 sec/step)\n",
            "I1208 00:45:31.054992 140583164200832 learning.py:507] global step 3687: loss = 1.5109 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 3688: loss = 1.6085 (0.576 sec/step)\n",
            "I1208 00:45:32.017604 140583164200832 learning.py:507] global step 3688: loss = 1.6085 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 3689: loss = 2.0171 (0.677 sec/step)\n",
            "I1208 00:45:32.738634 140583164200832 learning.py:507] global step 3689: loss = 2.0171 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 3690: loss = 2.5724 (1.392 sec/step)\n",
            "I1208 00:45:34.148901 140583164200832 learning.py:507] global step 3690: loss = 2.5724 (1.392 sec/step)\n",
            "INFO:tensorflow:global step 3691: loss = 1.6102 (0.623 sec/step)\n",
            "I1208 00:45:35.152108 140583164200832 learning.py:507] global step 3691: loss = 1.6102 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 3692: loss = 2.0956 (0.556 sec/step)\n",
            "I1208 00:45:36.035981 140583164200832 learning.py:507] global step 3692: loss = 2.0956 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 3693: loss = 1.9959 (1.379 sec/step)\n",
            "I1208 00:45:37.416467 140583164200832 learning.py:507] global step 3693: loss = 1.9959 (1.379 sec/step)\n",
            "INFO:tensorflow:global step 3694: loss = 2.1520 (1.074 sec/step)\n",
            "I1208 00:45:38.492504 140583164200832 learning.py:507] global step 3694: loss = 2.1520 (1.074 sec/step)\n",
            "INFO:tensorflow:global step 3695: loss = 1.9860 (0.652 sec/step)\n",
            "I1208 00:45:39.363181 140583164200832 learning.py:507] global step 3695: loss = 1.9860 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 3696: loss = 1.6452 (0.534 sec/step)\n",
            "I1208 00:45:40.246103 140583164200832 learning.py:507] global step 3696: loss = 1.6452 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 3697: loss = 1.6448 (1.340 sec/step)\n",
            "I1208 00:45:41.854704 140583164200832 learning.py:507] global step 3697: loss = 1.6448 (1.340 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3698.\n",
            "I1208 00:45:44.206163 140579459213056 supervisor.py:1050] Recording summary at step 3698.\n",
            "INFO:tensorflow:global step 3698: loss = 2.0023 (2.359 sec/step)\n",
            "I1208 00:45:44.216012 140583164200832 learning.py:507] global step 3698: loss = 2.0023 (2.359 sec/step)\n",
            "INFO:tensorflow:global step 3699: loss = 1.6652 (0.530 sec/step)\n",
            "I1208 00:45:44.918009 140583164200832 learning.py:507] global step 3699: loss = 1.6652 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 3700: loss = 1.9236 (1.337 sec/step)\n",
            "I1208 00:45:46.576173 140583164200832 learning.py:507] global step 3700: loss = 1.9236 (1.337 sec/step)\n",
            "INFO:tensorflow:global step 3701: loss = 1.4856 (0.493 sec/step)\n",
            "I1208 00:45:47.070770 140583164200832 learning.py:507] global step 3701: loss = 1.4856 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 3702: loss = 1.4731 (1.607 sec/step)\n",
            "I1208 00:45:48.681508 140583164200832 learning.py:507] global step 3702: loss = 1.4731 (1.607 sec/step)\n",
            "INFO:tensorflow:global step 3703: loss = 1.3354 (0.645 sec/step)\n",
            "I1208 00:45:49.501029 140583164200832 learning.py:507] global step 3703: loss = 1.3354 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 3704: loss = 1.7432 (1.248 sec/step)\n",
            "I1208 00:45:50.914957 140583164200832 learning.py:507] global step 3704: loss = 1.7432 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 3705: loss = 1.8798 (0.705 sec/step)\n",
            "I1208 00:45:52.013496 140583164200832 learning.py:507] global step 3705: loss = 1.8798 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 3706: loss = 1.8737 (0.562 sec/step)\n",
            "I1208 00:45:52.876854 140583164200832 learning.py:507] global step 3706: loss = 1.8737 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3707: loss = 1.5893 (0.546 sec/step)\n",
            "I1208 00:45:53.884071 140583164200832 learning.py:507] global step 3707: loss = 1.5893 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 3708: loss = 1.7654 (1.430 sec/step)\n",
            "I1208 00:45:55.448468 140583164200832 learning.py:507] global step 3708: loss = 1.7654 (1.430 sec/step)\n",
            "INFO:tensorflow:global step 3709: loss = 2.3490 (0.669 sec/step)\n",
            "I1208 00:45:56.425652 140583164200832 learning.py:507] global step 3709: loss = 2.3490 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 3710: loss = 1.7855 (0.678 sec/step)\n",
            "I1208 00:45:57.260028 140583164200832 learning.py:507] global step 3710: loss = 1.7855 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 3711: loss = 1.4544 (3.086 sec/step)\n",
            "I1208 00:46:00.652848 140583164200832 learning.py:507] global step 3711: loss = 1.4544 (3.086 sec/step)\n",
            "INFO:tensorflow:global step 3712: loss = 1.8447 (0.709 sec/step)\n",
            "I1208 00:46:01.438316 140583164200832 learning.py:507] global step 3712: loss = 1.8447 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 3713: loss = 1.5493 (0.571 sec/step)\n",
            "I1208 00:46:02.726579 140583164200832 learning.py:507] global step 3713: loss = 1.5493 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 3714: loss = 1.6258 (2.236 sec/step)\n",
            "I1208 00:46:05.025967 140583164200832 learning.py:507] global step 3714: loss = 1.6258 (2.236 sec/step)\n",
            "INFO:tensorflow:global step 3715: loss = 1.8165 (0.685 sec/step)\n",
            "I1208 00:46:06.128980 140583164200832 learning.py:507] global step 3715: loss = 1.8165 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 3716: loss = 1.4481 (0.691 sec/step)\n",
            "I1208 00:46:07.130698 140583164200832 learning.py:507] global step 3716: loss = 1.4481 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 3717: loss = 1.6939 (0.611 sec/step)\n",
            "I1208 00:46:07.874124 140583164200832 learning.py:507] global step 3717: loss = 1.6939 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 3718: loss = 1.3537 (1.830 sec/step)\n",
            "I1208 00:46:09.705704 140583164200832 learning.py:507] global step 3718: loss = 1.3537 (1.830 sec/step)\n",
            "INFO:tensorflow:global step 3719: loss = 1.5654 (0.541 sec/step)\n",
            "I1208 00:46:10.248569 140583164200832 learning.py:507] global step 3719: loss = 1.5654 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 3720: loss = 1.6131 (1.579 sec/step)\n",
            "I1208 00:46:12.006237 140583164200832 learning.py:507] global step 3720: loss = 1.6131 (1.579 sec/step)\n",
            "INFO:tensorflow:global step 3721: loss = 1.5654 (0.725 sec/step)\n",
            "I1208 00:46:13.002881 140583164200832 learning.py:507] global step 3721: loss = 1.5654 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 3722: loss = 1.5616 (1.137 sec/step)\n",
            "I1208 00:46:14.266875 140583164200832 learning.py:507] global step 3722: loss = 1.5616 (1.137 sec/step)\n",
            "INFO:tensorflow:global step 3723: loss = 1.9412 (0.557 sec/step)\n",
            "I1208 00:46:14.883658 140583164200832 learning.py:507] global step 3723: loss = 1.9412 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 3724: loss = 1.8611 (1.547 sec/step)\n",
            "I1208 00:46:16.432197 140583164200832 learning.py:507] global step 3724: loss = 1.8611 (1.547 sec/step)\n",
            "INFO:tensorflow:global step 3725: loss = 1.1953 (1.166 sec/step)\n",
            "I1208 00:46:17.599848 140583164200832 learning.py:507] global step 3725: loss = 1.1953 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 3726: loss = 1.5340 (0.514 sec/step)\n",
            "I1208 00:46:18.115490 140583164200832 learning.py:507] global step 3726: loss = 1.5340 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 3727: loss = 1.6738 (1.800 sec/step)\n",
            "I1208 00:46:19.918741 140583164200832 learning.py:507] global step 3727: loss = 1.6738 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 3728: loss = 1.9698 (0.452 sec/step)\n",
            "I1208 00:46:20.372565 140583164200832 learning.py:507] global step 3728: loss = 1.9698 (0.452 sec/step)\n",
            "INFO:tensorflow:global step 3729: loss = 2.0273 (1.788 sec/step)\n",
            "I1208 00:46:22.162364 140583164200832 learning.py:507] global step 3729: loss = 2.0273 (1.788 sec/step)\n",
            "INFO:tensorflow:global step 3730: loss = 1.5152 (0.636 sec/step)\n",
            "I1208 00:46:23.024870 140583164200832 learning.py:507] global step 3730: loss = 1.5152 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 3731: loss = 2.7976 (1.445 sec/step)\n",
            "I1208 00:46:24.481849 140583164200832 learning.py:507] global step 3731: loss = 2.7976 (1.445 sec/step)\n",
            "INFO:tensorflow:global step 3732: loss = 2.0323 (0.666 sec/step)\n",
            "I1208 00:46:25.174938 140583164200832 learning.py:507] global step 3732: loss = 2.0323 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 3733: loss = 2.2974 (0.697 sec/step)\n",
            "I1208 00:46:26.568675 140583164200832 learning.py:507] global step 3733: loss = 2.2974 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 3734: loss = 1.6982 (1.276 sec/step)\n",
            "I1208 00:46:27.858145 140583164200832 learning.py:507] global step 3734: loss = 1.6982 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 3735: loss = 1.7019 (0.630 sec/step)\n",
            "I1208 00:46:28.490051 140583164200832 learning.py:507] global step 3735: loss = 1.7019 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 3736: loss = 1.5325 (0.852 sec/step)\n",
            "I1208 00:46:29.536855 140583164200832 learning.py:507] global step 3736: loss = 1.5325 (0.852 sec/step)\n",
            "INFO:tensorflow:global step 3737: loss = 1.3992 (1.577 sec/step)\n",
            "I1208 00:46:31.558238 140583164200832 learning.py:507] global step 3737: loss = 1.3992 (1.577 sec/step)\n",
            "INFO:tensorflow:global step 3738: loss = 1.7464 (0.628 sec/step)\n",
            "I1208 00:46:32.362051 140583164200832 learning.py:507] global step 3738: loss = 1.7464 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 3739: loss = 1.6551 (1.474 sec/step)\n",
            "I1208 00:46:33.878392 140583164200832 learning.py:507] global step 3739: loss = 1.6551 (1.474 sec/step)\n",
            "INFO:tensorflow:global step 3740: loss = 1.6856 (0.646 sec/step)\n",
            "I1208 00:46:34.526418 140583164200832 learning.py:507] global step 3740: loss = 1.6856 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 3741: loss = 2.4195 (1.710 sec/step)\n",
            "I1208 00:46:36.238941 140583164200832 learning.py:507] global step 3741: loss = 2.4195 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 3742: loss = 1.6108 (0.648 sec/step)\n",
            "I1208 00:46:37.125740 140583164200832 learning.py:507] global step 3742: loss = 1.6108 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 3743: loss = 1.8280 (0.698 sec/step)\n",
            "I1208 00:46:38.236396 140583164200832 learning.py:507] global step 3743: loss = 1.8280 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 3744: loss = 1.7161 (0.566 sec/step)\n",
            "I1208 00:46:38.878727 140583164200832 learning.py:507] global step 3744: loss = 1.7161 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 3745: loss = 2.0841 (1.587 sec/step)\n",
            "I1208 00:46:40.642500 140583164200832 learning.py:507] global step 3745: loss = 2.0841 (1.587 sec/step)\n",
            "INFO:tensorflow:global step 3746: loss = 1.4707 (0.639 sec/step)\n",
            "I1208 00:46:41.510251 140583164200832 learning.py:507] global step 3746: loss = 1.4707 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 3747: loss = 1.8525 (1.077 sec/step)\n",
            "I1208 00:46:42.796840 140583164200832 learning.py:507] global step 3747: loss = 1.8525 (1.077 sec/step)\n",
            "INFO:tensorflow:global step 3748: loss = 1.8095 (0.637 sec/step)\n",
            "I1208 00:46:43.506848 140583164200832 learning.py:507] global step 3748: loss = 1.8095 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 3749: loss = 2.1790 (1.368 sec/step)\n",
            "I1208 00:46:45.204566 140583164200832 learning.py:507] global step 3749: loss = 2.1790 (1.368 sec/step)\n",
            "INFO:tensorflow:global step 3750: loss = 1.9304 (0.749 sec/step)\n",
            "I1208 00:46:46.211713 140583164200832 learning.py:507] global step 3750: loss = 1.9304 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 3751: loss = 1.7319 (0.526 sec/step)\n",
            "I1208 00:46:46.796240 140583164200832 learning.py:507] global step 3751: loss = 1.7319 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 3752: loss = 1.6328 (0.905 sec/step)\n",
            "I1208 00:46:47.723081 140583164200832 learning.py:507] global step 3752: loss = 1.6328 (0.905 sec/step)\n",
            "INFO:tensorflow:global step 3753: loss = 2.1495 (1.817 sec/step)\n",
            "I1208 00:46:49.567363 140583164200832 learning.py:507] global step 3753: loss = 2.1495 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 3754: loss = 1.6468 (0.744 sec/step)\n",
            "I1208 00:46:50.389037 140583164200832 learning.py:507] global step 3754: loss = 1.6468 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 3755: loss = 1.7004 (0.692 sec/step)\n",
            "I1208 00:46:51.486321 140583164200832 learning.py:507] global step 3755: loss = 1.7004 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 3756: loss = 1.7763 (0.766 sec/step)\n",
            "I1208 00:46:52.744986 140583164200832 learning.py:507] global step 3756: loss = 1.7763 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 3757: loss = 1.8201 (0.569 sec/step)\n",
            "I1208 00:46:53.701973 140583164200832 learning.py:507] global step 3757: loss = 1.8201 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 3758: loss = 1.9386 (1.737 sec/step)\n",
            "I1208 00:46:55.477330 140583164200832 learning.py:507] global step 3758: loss = 1.9386 (1.737 sec/step)\n",
            "INFO:tensorflow:global step 3759: loss = 2.0451 (0.559 sec/step)\n",
            "I1208 00:46:56.125427 140583164200832 learning.py:507] global step 3759: loss = 2.0451 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 3760: loss = 1.4709 (1.618 sec/step)\n",
            "I1208 00:46:57.749379 140583164200832 learning.py:507] global step 3760: loss = 1.4709 (1.618 sec/step)\n",
            "INFO:tensorflow:global step 3761: loss = 1.7144 (1.135 sec/step)\n",
            "I1208 00:46:58.885976 140583164200832 learning.py:507] global step 3761: loss = 1.7144 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 3762: loss = 2.1383 (0.575 sec/step)\n",
            "I1208 00:46:59.463452 140583164200832 learning.py:507] global step 3762: loss = 2.1383 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 3763: loss = 1.9913 (1.847 sec/step)\n",
            "I1208 00:47:01.346047 140583164200832 learning.py:507] global step 3763: loss = 1.9913 (1.847 sec/step)\n",
            "INFO:tensorflow:global step 3764: loss = 2.6603 (0.534 sec/step)\n",
            "I1208 00:47:02.023046 140583164200832 learning.py:507] global step 3764: loss = 2.6603 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 3765: loss = 1.4913 (1.437 sec/step)\n",
            "I1208 00:47:03.650423 140583164200832 learning.py:507] global step 3765: loss = 1.4913 (1.437 sec/step)\n",
            "INFO:tensorflow:global step 3766: loss = 2.0154 (0.600 sec/step)\n",
            "I1208 00:47:04.592710 140583164200832 learning.py:507] global step 3766: loss = 2.0154 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 3767: loss = 2.0495 (1.296 sec/step)\n",
            "I1208 00:47:06.021482 140583164200832 learning.py:507] global step 3767: loss = 2.0495 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 3768: loss = 2.4479 (0.712 sec/step)\n",
            "I1208 00:47:07.029995 140583164200832 learning.py:507] global step 3768: loss = 2.4479 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 3769: loss = 1.9411 (0.561 sec/step)\n",
            "I1208 00:47:07.694640 140583164200832 learning.py:507] global step 3769: loss = 1.9411 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3770: loss = 1.8148 (1.685 sec/step)\n",
            "I1208 00:47:09.381150 140583164200832 learning.py:507] global step 3770: loss = 1.8148 (1.685 sec/step)\n",
            "INFO:tensorflow:global step 3771: loss = 2.1996 (0.749 sec/step)\n",
            "I1208 00:47:10.496375 140583164200832 learning.py:507] global step 3771: loss = 2.1996 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 3772: loss = 1.8837 (0.575 sec/step)\n",
            "I1208 00:47:11.073873 140583164200832 learning.py:507] global step 3772: loss = 1.8837 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 3773: loss = 2.6539 (1.831 sec/step)\n",
            "I1208 00:47:12.906601 140583164200832 learning.py:507] global step 3773: loss = 2.6539 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 3774: loss = 1.6087 (0.815 sec/step)\n",
            "I1208 00:47:13.881856 140583164200832 learning.py:507] global step 3774: loss = 1.6087 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 3775: loss = 1.5418 (0.819 sec/step)\n",
            "I1208 00:47:14.968231 140583164200832 learning.py:507] global step 3775: loss = 1.5418 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 3776: loss = 2.0845 (0.743 sec/step)\n",
            "I1208 00:47:16.049421 140583164200832 learning.py:507] global step 3776: loss = 2.0845 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 3777: loss = 1.4159 (0.775 sec/step)\n",
            "I1208 00:47:17.040089 140583164200832 learning.py:507] global step 3777: loss = 1.4159 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 3778: loss = 1.8632 (1.559 sec/step)\n",
            "I1208 00:47:18.648501 140583164200832 learning.py:507] global step 3778: loss = 1.8632 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 3779: loss = 1.8536 (0.616 sec/step)\n",
            "I1208 00:47:19.377043 140583164200832 learning.py:507] global step 3779: loss = 1.8536 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 3780: loss = 1.4234 (1.526 sec/step)\n",
            "I1208 00:47:21.050285 140583164200832 learning.py:507] global step 3780: loss = 1.4234 (1.526 sec/step)\n",
            "INFO:tensorflow:global step 3781: loss = 2.4108 (0.667 sec/step)\n",
            "I1208 00:47:22.017139 140583164200832 learning.py:507] global step 3781: loss = 2.4108 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3782: loss = 1.7408 (0.550 sec/step)\n",
            "I1208 00:47:22.692452 140583164200832 learning.py:507] global step 3782: loss = 1.7408 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 3783: loss = 1.8513 (0.513 sec/step)\n",
            "I1208 00:47:23.207527 140583164200832 learning.py:507] global step 3783: loss = 1.8513 (0.513 sec/step)\n",
            "INFO:tensorflow:global step 3784: loss = 1.5045 (0.626 sec/step)\n",
            "I1208 00:47:23.835780 140583164200832 learning.py:507] global step 3784: loss = 1.5045 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 3785: loss = 1.3628 (3.430 sec/step)\n",
            "I1208 00:47:27.729801 140583164200832 learning.py:507] global step 3785: loss = 1.3628 (3.430 sec/step)\n",
            "INFO:tensorflow:global step 3786: loss = 1.6537 (1.515 sec/step)\n",
            "I1208 00:47:29.442375 140583164200832 learning.py:507] global step 3786: loss = 1.6537 (1.515 sec/step)\n",
            "INFO:tensorflow:global step 3787: loss = 1.4709 (0.741 sec/step)\n",
            "I1208 00:47:30.433609 140583164200832 learning.py:507] global step 3787: loss = 1.4709 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 3788: loss = 1.7432 (1.352 sec/step)\n",
            "I1208 00:47:31.806233 140583164200832 learning.py:507] global step 3788: loss = 1.7432 (1.352 sec/step)\n",
            "INFO:tensorflow:global step 3789: loss = 1.7654 (0.567 sec/step)\n",
            "I1208 00:47:32.374831 140583164200832 learning.py:507] global step 3789: loss = 1.7654 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 3790: loss = 1.5231 (0.978 sec/step)\n",
            "I1208 00:47:33.433430 140583164200832 learning.py:507] global step 3790: loss = 1.5231 (0.978 sec/step)\n",
            "INFO:tensorflow:global step 3791: loss = 1.3649 (1.848 sec/step)\n",
            "I1208 00:47:35.480934 140583164200832 learning.py:507] global step 3791: loss = 1.3649 (1.848 sec/step)\n",
            "INFO:tensorflow:global step 3792: loss = 2.2774 (0.527 sec/step)\n",
            "I1208 00:47:36.131949 140583164200832 learning.py:507] global step 3792: loss = 2.2774 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 3793: loss = 1.5289 (0.755 sec/step)\n",
            "I1208 00:47:37.215654 140583164200832 learning.py:507] global step 3793: loss = 1.5289 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 3794: loss = 1.6023 (0.844 sec/step)\n",
            "I1208 00:47:38.642737 140583164200832 learning.py:507] global step 3794: loss = 1.6023 (0.844 sec/step)\n",
            "INFO:tensorflow:global step 3795: loss = 2.2185 (1.241 sec/step)\n",
            "I1208 00:47:39.921606 140583164200832 learning.py:507] global step 3795: loss = 2.2185 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 3796: loss = 2.2246 (0.577 sec/step)\n",
            "I1208 00:47:40.731877 140583164200832 learning.py:507] global step 3796: loss = 2.2246 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 3797: loss = 1.7217 (1.228 sec/step)\n",
            "I1208 00:47:42.797102 140583164200832 learning.py:507] global step 3797: loss = 1.7217 (1.228 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3797.\n",
            "I1208 00:47:43.642917 140579459213056 supervisor.py:1050] Recording summary at step 3797.\n",
            "INFO:tensorflow:global step 3798: loss = 2.0477 (0.742 sec/step)\n",
            "I1208 00:47:43.954504 140583164200832 learning.py:507] global step 3798: loss = 2.0477 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 3799: loss = 1.9063 (0.728 sec/step)\n",
            "I1208 00:47:45.205939 140583164200832 learning.py:507] global step 3799: loss = 1.9063 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 3800: loss = 1.7236 (1.736 sec/step)\n",
            "I1208 00:47:47.258178 140583164200832 learning.py:507] global step 3800: loss = 1.7236 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 3801: loss = 1.7494 (0.506 sec/step)\n",
            "I1208 00:47:47.766892 140583164200832 learning.py:507] global step 3801: loss = 1.7494 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 3802: loss = 1.4169 (0.739 sec/step)\n",
            "I1208 00:47:48.776901 140583164200832 learning.py:507] global step 3802: loss = 1.4169 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 3803: loss = 1.8464 (1.700 sec/step)\n",
            "I1208 00:47:50.932406 140583164200832 learning.py:507] global step 3803: loss = 1.8464 (1.700 sec/step)\n",
            "INFO:tensorflow:global step 3804: loss = 2.4170 (1.689 sec/step)\n",
            "I1208 00:47:52.860344 140583164200832 learning.py:507] global step 3804: loss = 2.4170 (1.689 sec/step)\n",
            "INFO:tensorflow:global step 3805: loss = 1.3687 (0.619 sec/step)\n",
            "I1208 00:47:53.480926 140583164200832 learning.py:507] global step 3805: loss = 1.3687 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 3806: loss = 1.7025 (1.482 sec/step)\n",
            "I1208 00:47:55.205831 140583164200832 learning.py:507] global step 3806: loss = 1.7025 (1.482 sec/step)\n",
            "INFO:tensorflow:global step 3807: loss = 1.6648 (0.746 sec/step)\n",
            "I1208 00:47:56.323969 140583164200832 learning.py:507] global step 3807: loss = 1.6648 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 3808: loss = 2.0046 (1.435 sec/step)\n",
            "I1208 00:47:58.058369 140583164200832 learning.py:507] global step 3808: loss = 2.0046 (1.435 sec/step)\n",
            "INFO:tensorflow:global step 3809: loss = 1.4783 (0.717 sec/step)\n",
            "I1208 00:47:59.059878 140583164200832 learning.py:507] global step 3809: loss = 1.4783 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 3810: loss = 1.8261 (0.632 sec/step)\n",
            "I1208 00:47:59.975218 140583164200832 learning.py:507] global step 3810: loss = 1.8261 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 3811: loss = 1.8860 (2.119 sec/step)\n",
            "I1208 00:48:02.305022 140583164200832 learning.py:507] global step 3811: loss = 1.8860 (2.119 sec/step)\n",
            "INFO:tensorflow:global step 3812: loss = 1.7755 (0.475 sec/step)\n",
            "I1208 00:48:02.782171 140583164200832 learning.py:507] global step 3812: loss = 1.7755 (0.475 sec/step)\n",
            "INFO:tensorflow:global step 3813: loss = 1.7796 (1.695 sec/step)\n",
            "I1208 00:48:04.478501 140583164200832 learning.py:507] global step 3813: loss = 1.7796 (1.695 sec/step)\n",
            "INFO:tensorflow:global step 3814: loss = 1.9816 (0.564 sec/step)\n",
            "I1208 00:48:05.277849 140583164200832 learning.py:507] global step 3814: loss = 1.9816 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 3815: loss = 1.2001 (1.442 sec/step)\n",
            "I1208 00:48:06.802850 140583164200832 learning.py:507] global step 3815: loss = 1.2001 (1.442 sec/step)\n",
            "INFO:tensorflow:global step 3816: loss = 1.4542 (0.506 sec/step)\n",
            "I1208 00:48:07.310958 140583164200832 learning.py:507] global step 3816: loss = 1.4542 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 3817: loss = 1.9149 (0.964 sec/step)\n",
            "I1208 00:48:08.522621 140583164200832 learning.py:507] global step 3817: loss = 1.9149 (0.964 sec/step)\n",
            "INFO:tensorflow:global step 3818: loss = 1.3479 (1.460 sec/step)\n",
            "I1208 00:48:10.234268 140583164200832 learning.py:507] global step 3818: loss = 1.3479 (1.460 sec/step)\n",
            "INFO:tensorflow:global step 3819: loss = 1.4971 (0.483 sec/step)\n",
            "I1208 00:48:10.718683 140583164200832 learning.py:507] global step 3819: loss = 1.4971 (0.483 sec/step)\n",
            "INFO:tensorflow:global step 3820: loss = 1.7983 (1.710 sec/step)\n",
            "I1208 00:48:12.430584 140583164200832 learning.py:507] global step 3820: loss = 1.7983 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 3821: loss = 1.8618 (0.549 sec/step)\n",
            "I1208 00:48:13.286035 140583164200832 learning.py:507] global step 3821: loss = 1.8618 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 3822: loss = 1.2724 (2.412 sec/step)\n",
            "I1208 00:48:15.706895 140583164200832 learning.py:507] global step 3822: loss = 1.2724 (2.412 sec/step)\n",
            "INFO:tensorflow:global step 3823: loss = 1.5887 (0.658 sec/step)\n",
            "I1208 00:48:16.367191 140583164200832 learning.py:507] global step 3823: loss = 1.5887 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 3824: loss = 1.6857 (1.777 sec/step)\n",
            "I1208 00:48:18.145991 140583164200832 learning.py:507] global step 3824: loss = 1.6857 (1.777 sec/step)\n",
            "INFO:tensorflow:global step 3825: loss = 2.1095 (0.636 sec/step)\n",
            "I1208 00:48:18.784082 140583164200832 learning.py:507] global step 3825: loss = 2.1095 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 3826: loss = 1.6909 (1.865 sec/step)\n",
            "I1208 00:48:20.651583 140583164200832 learning.py:507] global step 3826: loss = 1.6909 (1.865 sec/step)\n",
            "INFO:tensorflow:global step 3827: loss = 2.2514 (0.763 sec/step)\n",
            "I1208 00:48:21.427353 140583164200832 learning.py:507] global step 3827: loss = 2.2514 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 3828: loss = 2.1068 (1.735 sec/step)\n",
            "I1208 00:48:23.343446 140583164200832 learning.py:507] global step 3828: loss = 2.1068 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 3829: loss = 1.8992 (0.590 sec/step)\n",
            "I1208 00:48:24.064325 140583164200832 learning.py:507] global step 3829: loss = 1.8992 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3830: loss = 1.7612 (2.120 sec/step)\n",
            "I1208 00:48:26.207076 140583164200832 learning.py:507] global step 3830: loss = 1.7612 (2.120 sec/step)\n",
            "INFO:tensorflow:global step 3831: loss = 2.1078 (0.592 sec/step)\n",
            "I1208 00:48:26.801006 140583164200832 learning.py:507] global step 3831: loss = 2.1078 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 3832: loss = 1.8848 (1.313 sec/step)\n",
            "I1208 00:48:28.219003 140583164200832 learning.py:507] global step 3832: loss = 1.8848 (1.313 sec/step)\n",
            "INFO:tensorflow:global step 3833: loss = 1.8403 (1.637 sec/step)\n",
            "I1208 00:48:30.058200 140583164200832 learning.py:507] global step 3833: loss = 1.8403 (1.637 sec/step)\n",
            "INFO:tensorflow:global step 3834: loss = 2.0246 (0.816 sec/step)\n",
            "I1208 00:48:30.888665 140583164200832 learning.py:507] global step 3834: loss = 2.0246 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 3835: loss = 2.1183 (1.769 sec/step)\n",
            "I1208 00:48:32.698137 140583164200832 learning.py:507] global step 3835: loss = 2.1183 (1.769 sec/step)\n",
            "INFO:tensorflow:global step 3836: loss = 1.9574 (0.751 sec/step)\n",
            "I1208 00:48:33.631714 140583164200832 learning.py:507] global step 3836: loss = 1.9574 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 3837: loss = 2.3597 (0.775 sec/step)\n",
            "I1208 00:48:34.789978 140583164200832 learning.py:507] global step 3837: loss = 2.3597 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 3838: loss = 1.7859 (0.520 sec/step)\n",
            "I1208 00:48:35.416599 140583164200832 learning.py:507] global step 3838: loss = 1.7859 (0.520 sec/step)\n",
            "INFO:tensorflow:global step 3839: loss = 1.4858 (1.812 sec/step)\n",
            "I1208 00:48:37.230531 140583164200832 learning.py:507] global step 3839: loss = 1.4858 (1.812 sec/step)\n",
            "INFO:tensorflow:global step 3840: loss = 1.6736 (0.563 sec/step)\n",
            "I1208 00:48:37.795098 140583164200832 learning.py:507] global step 3840: loss = 1.6736 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 3841: loss = 1.7484 (0.828 sec/step)\n",
            "I1208 00:48:38.849550 140583164200832 learning.py:507] global step 3841: loss = 1.7484 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 3842: loss = 1.9623 (1.526 sec/step)\n",
            "I1208 00:48:40.652803 140583164200832 learning.py:507] global step 3842: loss = 1.9623 (1.526 sec/step)\n",
            "INFO:tensorflow:global step 3843: loss = 1.9228 (0.644 sec/step)\n",
            "I1208 00:48:41.298760 140583164200832 learning.py:507] global step 3843: loss = 1.9228 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 3844: loss = 2.0226 (1.880 sec/step)\n",
            "I1208 00:48:43.180439 140583164200832 learning.py:507] global step 3844: loss = 2.0226 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 3845: loss = 1.6828 (0.517 sec/step)\n",
            "I1208 00:48:43.699396 140583164200832 learning.py:507] global step 3845: loss = 1.6828 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 3846: loss = 1.4884 (0.990 sec/step)\n",
            "I1208 00:48:44.718503 140583164200832 learning.py:507] global step 3846: loss = 1.4884 (0.990 sec/step)\n",
            "INFO:tensorflow:global step 3847: loss = 1.8237 (1.903 sec/step)\n",
            "I1208 00:48:46.731150 140583164200832 learning.py:507] global step 3847: loss = 1.8237 (1.903 sec/step)\n",
            "INFO:tensorflow:global step 3848: loss = 1.6425 (0.623 sec/step)\n",
            "I1208 00:48:47.592035 140583164200832 learning.py:507] global step 3848: loss = 1.6425 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 3849: loss = 2.1350 (0.815 sec/step)\n",
            "I1208 00:48:48.715970 140583164200832 learning.py:507] global step 3849: loss = 2.1350 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 3850: loss = 1.7188 (0.552 sec/step)\n",
            "I1208 00:48:49.434947 140583164200832 learning.py:507] global step 3850: loss = 1.7188 (0.552 sec/step)\n",
            "INFO:tensorflow:global step 3851: loss = 1.6407 (1.732 sec/step)\n",
            "I1208 00:48:51.169179 140583164200832 learning.py:507] global step 3851: loss = 1.6407 (1.732 sec/step)\n",
            "INFO:tensorflow:global step 3852: loss = 1.6029 (0.574 sec/step)\n",
            "I1208 00:48:51.955179 140583164200832 learning.py:507] global step 3852: loss = 1.6029 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 3853: loss = 2.0913 (0.703 sec/step)\n",
            "I1208 00:48:53.197022 140583164200832 learning.py:507] global step 3853: loss = 2.0913 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 3854: loss = 1.7278 (0.685 sec/step)\n",
            "I1208 00:48:54.280469 140583164200832 learning.py:507] global step 3854: loss = 1.7278 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 3855: loss = 1.5543 (0.529 sec/step)\n",
            "I1208 00:48:55.093868 140583164200832 learning.py:507] global step 3855: loss = 1.5543 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 3856: loss = 1.6647 (1.718 sec/step)\n",
            "I1208 00:48:56.814385 140583164200832 learning.py:507] global step 3856: loss = 1.6647 (1.718 sec/step)\n",
            "INFO:tensorflow:global step 3857: loss = 1.5560 (1.139 sec/step)\n",
            "I1208 00:48:57.955046 140583164200832 learning.py:507] global step 3857: loss = 1.5560 (1.139 sec/step)\n",
            "INFO:tensorflow:global step 3858: loss = 1.7024 (0.646 sec/step)\n",
            "I1208 00:48:58.907503 140583164200832 learning.py:507] global step 3858: loss = 1.7024 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 3859: loss = 1.9483 (0.506 sec/step)\n",
            "I1208 00:48:59.604307 140583164200832 learning.py:507] global step 3859: loss = 1.9483 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 3860: loss = 1.6867 (1.202 sec/step)\n",
            "I1208 00:49:00.873342 140583164200832 learning.py:507] global step 3860: loss = 1.6867 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 3861: loss = 2.0420 (1.708 sec/step)\n",
            "I1208 00:49:02.583764 140583164200832 learning.py:507] global step 3861: loss = 2.0420 (1.708 sec/step)\n",
            "INFO:tensorflow:global step 3862: loss = 1.3928 (0.615 sec/step)\n",
            "I1208 00:49:03.275479 140583164200832 learning.py:507] global step 3862: loss = 1.3928 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 3863: loss = 2.5213 (1.210 sec/step)\n",
            "I1208 00:49:04.802468 140583164200832 learning.py:507] global step 3863: loss = 2.5213 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 3864: loss = 1.5634 (0.654 sec/step)\n",
            "I1208 00:49:05.809587 140583164200832 learning.py:507] global step 3864: loss = 1.5634 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 3865: loss = 1.9077 (1.287 sec/step)\n",
            "I1208 00:49:07.119632 140583164200832 learning.py:507] global step 3865: loss = 1.9077 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 3866: loss = 1.9012 (0.506 sec/step)\n",
            "I1208 00:49:07.627281 140583164200832 learning.py:507] global step 3866: loss = 1.9012 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 3867: loss = 2.0351 (1.701 sec/step)\n",
            "I1208 00:49:09.329841 140583164200832 learning.py:507] global step 3867: loss = 2.0351 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 3868: loss = 1.7713 (0.559 sec/step)\n",
            "I1208 00:49:10.108336 140583164200832 learning.py:507] global step 3868: loss = 1.7713 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 3869: loss = 1.7078 (1.330 sec/step)\n",
            "I1208 00:49:11.600074 140583164200832 learning.py:507] global step 3869: loss = 1.7078 (1.330 sec/step)\n",
            "INFO:tensorflow:global step 3870: loss = 2.0264 (0.672 sec/step)\n",
            "I1208 00:49:12.505497 140583164200832 learning.py:507] global step 3870: loss = 2.0264 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 3871: loss = 1.7262 (1.245 sec/step)\n",
            "I1208 00:49:13.925555 140583164200832 learning.py:507] global step 3871: loss = 1.7262 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 3872: loss = 1.8579 (0.630 sec/step)\n",
            "I1208 00:49:14.698969 140583164200832 learning.py:507] global step 3872: loss = 1.8579 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 3873: loss = 1.6259 (1.246 sec/step)\n",
            "I1208 00:49:16.153471 140583164200832 learning.py:507] global step 3873: loss = 1.6259 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 3874: loss = 2.0634 (0.629 sec/step)\n",
            "I1208 00:49:16.806676 140583164200832 learning.py:507] global step 3874: loss = 2.0634 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 3875: loss = 1.9474 (1.631 sec/step)\n",
            "I1208 00:49:18.447051 140583164200832 learning.py:507] global step 3875: loss = 1.9474 (1.631 sec/step)\n",
            "INFO:tensorflow:global step 3876: loss = 2.3045 (0.619 sec/step)\n",
            "I1208 00:49:19.241705 140583164200832 learning.py:507] global step 3876: loss = 2.3045 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 3877: loss = 1.9981 (0.536 sec/step)\n",
            "I1208 00:49:20.475087 140583164200832 learning.py:507] global step 3877: loss = 1.9981 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 3878: loss = 1.8764 (0.629 sec/step)\n",
            "I1208 00:49:21.462138 140583164200832 learning.py:507] global step 3878: loss = 1.8764 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 3879: loss = 2.1495 (1.139 sec/step)\n",
            "I1208 00:49:22.765882 140583164200832 learning.py:507] global step 3879: loss = 2.1495 (1.139 sec/step)\n",
            "INFO:tensorflow:global step 3880: loss = 1.9480 (0.649 sec/step)\n",
            "I1208 00:49:23.611984 140583164200832 learning.py:507] global step 3880: loss = 1.9480 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 3881: loss = 1.7809 (1.301 sec/step)\n",
            "I1208 00:49:25.028390 140583164200832 learning.py:507] global step 3881: loss = 1.7809 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 3882: loss = 1.3750 (0.502 sec/step)\n",
            "I1208 00:49:25.726697 140583164200832 learning.py:507] global step 3882: loss = 1.3750 (0.502 sec/step)\n",
            "INFO:tensorflow:global step 3883: loss = 1.9005 (1.372 sec/step)\n",
            "I1208 00:49:27.292527 140583164200832 learning.py:507] global step 3883: loss = 1.9005 (1.372 sec/step)\n",
            "INFO:tensorflow:global step 3884: loss = 1.4284 (0.590 sec/step)\n",
            "I1208 00:49:28.185307 140583164200832 learning.py:507] global step 3884: loss = 1.4284 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3885: loss = 1.3323 (0.598 sec/step)\n",
            "I1208 00:49:29.160767 140583164200832 learning.py:507] global step 3885: loss = 1.3323 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 3886: loss = 1.8729 (0.665 sec/step)\n",
            "I1208 00:49:30.313099 140583164200832 learning.py:507] global step 3886: loss = 1.8729 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 3887: loss = 1.8727 (1.111 sec/step)\n",
            "I1208 00:49:31.440388 140583164200832 learning.py:507] global step 3887: loss = 1.8727 (1.111 sec/step)\n",
            "INFO:tensorflow:global step 3888: loss = 1.5139 (0.608 sec/step)\n",
            "I1208 00:49:32.285395 140583164200832 learning.py:507] global step 3888: loss = 1.5139 (0.608 sec/step)\n",
            "INFO:tensorflow:global step 3889: loss = 2.1888 (0.696 sec/step)\n",
            "I1208 00:49:33.422800 140583164200832 learning.py:507] global step 3889: loss = 2.1888 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 3890: loss = 1.4495 (0.751 sec/step)\n",
            "I1208 00:49:34.561692 140583164200832 learning.py:507] global step 3890: loss = 1.4495 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 3891: loss = 2.4178 (0.609 sec/step)\n",
            "I1208 00:49:35.409863 140583164200832 learning.py:507] global step 3891: loss = 2.4178 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 3892: loss = 1.9258 (1.346 sec/step)\n",
            "I1208 00:49:36.966499 140583164200832 learning.py:507] global step 3892: loss = 1.9258 (1.346 sec/step)\n",
            "INFO:tensorflow:global step 3893: loss = 1.8210 (0.704 sec/step)\n",
            "I1208 00:49:37.966101 140583164200832 learning.py:507] global step 3893: loss = 1.8210 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 3894: loss = 1.8779 (0.733 sec/step)\n",
            "I1208 00:49:38.918182 140583164200832 learning.py:507] global step 3894: loss = 1.8779 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 3895: loss = 1.8451 (0.675 sec/step)\n",
            "I1208 00:49:39.857494 140583164200832 learning.py:507] global step 3895: loss = 1.8451 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 3896: loss = 1.5634 (1.263 sec/step)\n",
            "I1208 00:49:41.334139 140583164200832 learning.py:507] global step 3896: loss = 1.5634 (1.263 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:49:42.076155 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 3897: loss = 1.4220 (0.669 sec/step)\n",
            "I1208 00:49:42.170540 140583164200832 learning.py:507] global step 3897: loss = 1.4220 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 3898: loss = 2.0123 (1.727 sec/step)\n",
            "I1208 00:49:44.901985 140583164200832 learning.py:507] global step 3898: loss = 2.0123 (1.727 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3898.\n",
            "I1208 00:49:44.902415 140579459213056 supervisor.py:1050] Recording summary at step 3898.\n",
            "INFO:tensorflow:global step 3899: loss = 2.3123 (1.603 sec/step)\n",
            "I1208 00:49:47.173212 140583164200832 learning.py:507] global step 3899: loss = 2.3123 (1.603 sec/step)\n",
            "INFO:tensorflow:global step 3900: loss = 1.5695 (0.561 sec/step)\n",
            "I1208 00:49:48.004857 140583164200832 learning.py:507] global step 3900: loss = 1.5695 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 3901: loss = 1.9137 (0.721 sec/step)\n",
            "I1208 00:49:48.957044 140583164200832 learning.py:507] global step 3901: loss = 1.9137 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 3902: loss = 1.4914 (0.677 sec/step)\n",
            "I1208 00:49:50.205085 140583164200832 learning.py:507] global step 3902: loss = 1.4914 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 3903: loss = 1.3967 (1.265 sec/step)\n",
            "I1208 00:49:51.564446 140583164200832 learning.py:507] global step 3903: loss = 1.3967 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 3904: loss = 1.9462 (1.133 sec/step)\n",
            "I1208 00:49:52.699552 140583164200832 learning.py:507] global step 3904: loss = 1.9462 (1.133 sec/step)\n",
            "INFO:tensorflow:global step 3905: loss = 1.9178 (0.579 sec/step)\n",
            "I1208 00:49:53.634711 140583164200832 learning.py:507] global step 3905: loss = 1.9178 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 3906: loss = 1.6060 (1.392 sec/step)\n",
            "I1208 00:49:55.094411 140583164200832 learning.py:507] global step 3906: loss = 1.6060 (1.392 sec/step)\n",
            "INFO:tensorflow:global step 3907: loss = 1.9861 (0.508 sec/step)\n",
            "I1208 00:49:55.604874 140583164200832 learning.py:507] global step 3907: loss = 1.9861 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 3908: loss = 2.0417 (1.568 sec/step)\n",
            "I1208 00:49:57.202713 140583164200832 learning.py:507] global step 3908: loss = 2.0417 (1.568 sec/step)\n",
            "INFO:tensorflow:global step 3909: loss = 1.8058 (1.145 sec/step)\n",
            "I1208 00:49:58.349013 140583164200832 learning.py:507] global step 3909: loss = 1.8058 (1.145 sec/step)\n",
            "INFO:tensorflow:global step 3910: loss = 1.3770 (0.667 sec/step)\n",
            "I1208 00:49:59.249255 140583164200832 learning.py:507] global step 3910: loss = 1.3770 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3911: loss = 1.9236 (0.709 sec/step)\n",
            "I1208 00:50:00.390545 140583164200832 learning.py:507] global step 3911: loss = 1.9236 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 3912: loss = 2.4989 (0.562 sec/step)\n",
            "I1208 00:50:01.271538 140583164200832 learning.py:507] global step 3912: loss = 2.4989 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 3913: loss = 1.7839 (1.430 sec/step)\n",
            "I1208 00:50:02.755194 140583164200832 learning.py:507] global step 3913: loss = 1.7839 (1.430 sec/step)\n",
            "INFO:tensorflow:global step 3914: loss = 1.4212 (0.564 sec/step)\n",
            "I1208 00:50:03.583759 140583164200832 learning.py:507] global step 3914: loss = 1.4212 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 3915: loss = 1.5803 (1.353 sec/step)\n",
            "I1208 00:50:05.071444 140583164200832 learning.py:507] global step 3915: loss = 1.5803 (1.353 sec/step)\n",
            "INFO:tensorflow:global step 3916: loss = 1.5181 (0.545 sec/step)\n",
            "I1208 00:50:05.843886 140583164200832 learning.py:507] global step 3916: loss = 1.5181 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 3917: loss = 1.6247 (1.185 sec/step)\n",
            "I1208 00:50:07.259011 140583164200832 learning.py:507] global step 3917: loss = 1.6247 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 3918: loss = 1.6500 (1.089 sec/step)\n",
            "I1208 00:50:08.349741 140583164200832 learning.py:507] global step 3918: loss = 1.6500 (1.089 sec/step)\n",
            "INFO:tensorflow:global step 3919: loss = 2.6175 (0.579 sec/step)\n",
            "I1208 00:50:09.041631 140583164200832 learning.py:507] global step 3919: loss = 2.6175 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 3920: loss = 2.4385 (2.597 sec/step)\n",
            "I1208 00:50:11.800314 140583164200832 learning.py:507] global step 3920: loss = 2.4385 (2.597 sec/step)\n",
            "INFO:tensorflow:global step 3921: loss = 1.6991 (0.802 sec/step)\n",
            "I1208 00:50:12.858875 140583164200832 learning.py:507] global step 3921: loss = 1.6991 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 3922: loss = 1.9064 (0.756 sec/step)\n",
            "I1208 00:50:13.910005 140583164200832 learning.py:507] global step 3922: loss = 1.9064 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 3923: loss = 1.7924 (0.564 sec/step)\n",
            "I1208 00:50:14.850111 140583164200832 learning.py:507] global step 3923: loss = 1.7924 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 3924: loss = 1.8863 (1.320 sec/step)\n",
            "I1208 00:50:16.212018 140583164200832 learning.py:507] global step 3924: loss = 1.8863 (1.320 sec/step)\n",
            "INFO:tensorflow:global step 3925: loss = 1.7357 (1.973 sec/step)\n",
            "I1208 00:50:18.522638 140583164200832 learning.py:507] global step 3925: loss = 1.7357 (1.973 sec/step)\n",
            "INFO:tensorflow:global step 3926: loss = 1.8670 (0.707 sec/step)\n",
            "I1208 00:50:19.531983 140583164200832 learning.py:507] global step 3926: loss = 1.8670 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 3927: loss = 1.7606 (0.674 sec/step)\n",
            "I1208 00:50:20.370908 140583164200832 learning.py:507] global step 3927: loss = 1.7606 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 3928: loss = 1.7366 (1.399 sec/step)\n",
            "I1208 00:50:21.914315 140583164200832 learning.py:507] global step 3928: loss = 1.7366 (1.399 sec/step)\n",
            "INFO:tensorflow:global step 3929: loss = 1.4020 (0.558 sec/step)\n",
            "I1208 00:50:22.474095 140583164200832 learning.py:507] global step 3929: loss = 1.4020 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 3930: loss = 1.9933 (0.813 sec/step)\n",
            "I1208 00:50:23.501054 140583164200832 learning.py:507] global step 3930: loss = 1.9933 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 3931: loss = 1.8905 (2.140 sec/step)\n",
            "I1208 00:50:25.914963 140583164200832 learning.py:507] global step 3931: loss = 1.8905 (2.140 sec/step)\n",
            "INFO:tensorflow:global step 3932: loss = 1.2988 (0.528 sec/step)\n",
            "I1208 00:50:26.444870 140583164200832 learning.py:507] global step 3932: loss = 1.2988 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 3933: loss = 2.0225 (0.837 sec/step)\n",
            "I1208 00:50:27.347419 140583164200832 learning.py:507] global step 3933: loss = 2.0225 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 3934: loss = 2.0332 (2.335 sec/step)\n",
            "I1208 00:50:29.684781 140583164200832 learning.py:507] global step 3934: loss = 2.0332 (2.335 sec/step)\n",
            "INFO:tensorflow:global step 3935: loss = 2.1527 (0.546 sec/step)\n",
            "I1208 00:50:30.232840 140583164200832 learning.py:507] global step 3935: loss = 2.1527 (0.546 sec/step)\n",
            "INFO:tensorflow:global step 3936: loss = 2.9371 (1.541 sec/step)\n",
            "I1208 00:50:31.904432 140583164200832 learning.py:507] global step 3936: loss = 2.9371 (1.541 sec/step)\n",
            "INFO:tensorflow:global step 3937: loss = 1.7610 (0.530 sec/step)\n",
            "I1208 00:50:32.480619 140583164200832 learning.py:507] global step 3937: loss = 1.7610 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 3938: loss = 1.5151 (1.763 sec/step)\n",
            "I1208 00:50:34.245912 140583164200832 learning.py:507] global step 3938: loss = 1.5151 (1.763 sec/step)\n",
            "INFO:tensorflow:global step 3939: loss = 2.0224 (0.518 sec/step)\n",
            "I1208 00:50:34.766104 140583164200832 learning.py:507] global step 3939: loss = 2.0224 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 3940: loss = 2.4300 (1.688 sec/step)\n",
            "I1208 00:50:36.456601 140583164200832 learning.py:507] global step 3940: loss = 2.4300 (1.688 sec/step)\n",
            "INFO:tensorflow:global step 3941: loss = 1.9357 (0.632 sec/step)\n",
            "I1208 00:50:37.237972 140583164200832 learning.py:507] global step 3941: loss = 1.9357 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 3942: loss = 1.7080 (1.167 sec/step)\n",
            "I1208 00:50:38.687401 140583164200832 learning.py:507] global step 3942: loss = 1.7080 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 3943: loss = 1.7908 (0.712 sec/step)\n",
            "I1208 00:50:39.601512 140583164200832 learning.py:507] global step 3943: loss = 1.7908 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 3944: loss = 1.7843 (0.773 sec/step)\n",
            "I1208 00:50:40.806772 140583164200832 learning.py:507] global step 3944: loss = 1.7843 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 3945: loss = 1.8335 (0.554 sec/step)\n",
            "I1208 00:50:41.404324 140583164200832 learning.py:507] global step 3945: loss = 1.8335 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 3946: loss = 1.5590 (1.489 sec/step)\n",
            "I1208 00:50:42.978883 140583164200832 learning.py:507] global step 3946: loss = 1.5590 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 3947: loss = 1.7904 (0.667 sec/step)\n",
            "I1208 00:50:43.909327 140583164200832 learning.py:507] global step 3947: loss = 1.7904 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3948: loss = 1.7294 (1.344 sec/step)\n",
            "I1208 00:50:45.396169 140583164200832 learning.py:507] global step 3948: loss = 1.7294 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 3949: loss = 1.4149 (0.702 sec/step)\n",
            "I1208 00:50:46.275726 140583164200832 learning.py:507] global step 3949: loss = 1.4149 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 3950: loss = 2.1198 (0.549 sec/step)\n",
            "I1208 00:50:47.325754 140583164200832 learning.py:507] global step 3950: loss = 2.1198 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 3951: loss = 1.4165 (0.612 sec/step)\n",
            "I1208 00:50:48.366020 140583164200832 learning.py:507] global step 3951: loss = 1.4165 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 3952: loss = 1.6302 (1.284 sec/step)\n",
            "I1208 00:50:49.938884 140583164200832 learning.py:507] global step 3952: loss = 1.6302 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 3953: loss = 1.7979 (0.712 sec/step)\n",
            "I1208 00:50:50.771487 140583164200832 learning.py:507] global step 3953: loss = 1.7979 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 3954: loss = 1.8835 (1.397 sec/step)\n",
            "I1208 00:50:52.358483 140583164200832 learning.py:507] global step 3954: loss = 1.8835 (1.397 sec/step)\n",
            "INFO:tensorflow:global step 3955: loss = 1.9166 (0.590 sec/step)\n",
            "I1208 00:50:53.164928 140583164200832 learning.py:507] global step 3955: loss = 1.9166 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3956: loss = 1.9570 (0.742 sec/step)\n",
            "I1208 00:50:54.363435 140583164200832 learning.py:507] global step 3956: loss = 1.9570 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 3957: loss = 1.7995 (1.416 sec/step)\n",
            "I1208 00:50:55.984559 140583164200832 learning.py:507] global step 3957: loss = 1.7995 (1.416 sec/step)\n",
            "INFO:tensorflow:global step 3958: loss = 1.4826 (0.682 sec/step)\n",
            "I1208 00:50:56.940335 140583164200832 learning.py:507] global step 3958: loss = 1.4826 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 3959: loss = 1.9161 (1.281 sec/step)\n",
            "I1208 00:50:58.307692 140583164200832 learning.py:507] global step 3959: loss = 1.9161 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 3960: loss = 2.6348 (0.659 sec/step)\n",
            "I1208 00:50:59.261927 140583164200832 learning.py:507] global step 3960: loss = 2.6348 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 3961: loss = 1.7652 (0.527 sec/step)\n",
            "I1208 00:51:00.302999 140583164200832 learning.py:507] global step 3961: loss = 1.7652 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 3962: loss = 1.3608 (0.580 sec/step)\n",
            "I1208 00:51:01.354425 140583164200832 learning.py:507] global step 3962: loss = 1.3608 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 3963: loss = 1.5432 (0.600 sec/step)\n",
            "I1208 00:51:02.265497 140583164200832 learning.py:507] global step 3963: loss = 1.5432 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 3964: loss = 1.7370 (0.662 sec/step)\n",
            "I1208 00:51:03.212544 140583164200832 learning.py:507] global step 3964: loss = 1.7370 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 3965: loss = 1.4089 (1.428 sec/step)\n",
            "I1208 00:51:04.927428 140583164200832 learning.py:507] global step 3965: loss = 1.4089 (1.428 sec/step)\n",
            "INFO:tensorflow:global step 3966: loss = 1.7606 (0.564 sec/step)\n",
            "I1208 00:51:05.753292 140583164200832 learning.py:507] global step 3966: loss = 1.7606 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 3967: loss = 1.4207 (1.440 sec/step)\n",
            "I1208 00:51:07.200428 140583164200832 learning.py:507] global step 3967: loss = 1.4207 (1.440 sec/step)\n",
            "INFO:tensorflow:global step 3968: loss = 2.4470 (0.633 sec/step)\n",
            "I1208 00:51:07.970757 140583164200832 learning.py:507] global step 3968: loss = 2.4470 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 3969: loss = 1.8934 (1.155 sec/step)\n",
            "I1208 00:51:09.315275 140583164200832 learning.py:507] global step 3969: loss = 1.8934 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 3970: loss = 1.7931 (0.648 sec/step)\n",
            "I1208 00:51:10.061352 140583164200832 learning.py:507] global step 3970: loss = 1.7931 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 3971: loss = 1.3507 (0.492 sec/step)\n",
            "I1208 00:51:10.815719 140583164200832 learning.py:507] global step 3971: loss = 1.3507 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 3972: loss = 1.7075 (1.744 sec/step)\n",
            "I1208 00:51:12.562254 140583164200832 learning.py:507] global step 3972: loss = 1.7075 (1.744 sec/step)\n",
            "INFO:tensorflow:global step 3973: loss = 2.5946 (0.704 sec/step)\n",
            "I1208 00:51:13.539502 140583164200832 learning.py:507] global step 3973: loss = 2.5946 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 3974: loss = 1.6502 (0.749 sec/step)\n",
            "I1208 00:51:14.729328 140583164200832 learning.py:507] global step 3974: loss = 1.6502 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 3975: loss = 2.4067 (0.670 sec/step)\n",
            "I1208 00:51:15.692229 140583164200832 learning.py:507] global step 3975: loss = 2.4067 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 3976: loss = 2.7079 (1.336 sec/step)\n",
            "I1208 00:51:17.105889 140583164200832 learning.py:507] global step 3976: loss = 2.7079 (1.336 sec/step)\n",
            "INFO:tensorflow:global step 3977: loss = 1.3552 (0.495 sec/step)\n",
            "I1208 00:51:17.831010 140583164200832 learning.py:507] global step 3977: loss = 1.3552 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 3978: loss = 1.6793 (0.763 sec/step)\n",
            "I1208 00:51:19.215478 140583164200832 learning.py:507] global step 3978: loss = 1.6793 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 3979: loss = 1.9539 (0.593 sec/step)\n",
            "I1208 00:51:20.160014 140583164200832 learning.py:507] global step 3979: loss = 1.9539 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 3980: loss = 1.4275 (1.203 sec/step)\n",
            "I1208 00:51:21.496953 140583164200832 learning.py:507] global step 3980: loss = 1.4275 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 3981: loss = 1.6915 (0.823 sec/step)\n",
            "I1208 00:51:22.329682 140583164200832 learning.py:507] global step 3981: loss = 1.6915 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 3982: loss = 1.7211 (1.463 sec/step)\n",
            "I1208 00:51:23.800959 140583164200832 learning.py:507] global step 3982: loss = 1.7211 (1.463 sec/step)\n",
            "INFO:tensorflow:global step 3983: loss = 1.7461 (0.604 sec/step)\n",
            "I1208 00:51:24.776976 140583164200832 learning.py:507] global step 3983: loss = 1.7461 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 3984: loss = 1.5206 (0.707 sec/step)\n",
            "I1208 00:51:25.723244 140583164200832 learning.py:507] global step 3984: loss = 1.5206 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 3985: loss = 2.1234 (0.717 sec/step)\n",
            "I1208 00:51:26.773852 140583164200832 learning.py:507] global step 3985: loss = 2.1234 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 3986: loss = 1.8616 (1.373 sec/step)\n",
            "I1208 00:51:28.271737 140583164200832 learning.py:507] global step 3986: loss = 1.8616 (1.373 sec/step)\n",
            "INFO:tensorflow:global step 3987: loss = 1.7116 (0.591 sec/step)\n",
            "I1208 00:51:28.865180 140583164200832 learning.py:507] global step 3987: loss = 1.7116 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 3988: loss = 1.7238 (1.837 sec/step)\n",
            "I1208 00:51:30.704336 140583164200832 learning.py:507] global step 3988: loss = 1.7238 (1.837 sec/step)\n",
            "INFO:tensorflow:global step 3989: loss = 1.4719 (0.744 sec/step)\n",
            "I1208 00:51:31.652083 140583164200832 learning.py:507] global step 3989: loss = 1.4719 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 3990: loss = 1.3833 (0.590 sec/step)\n",
            "I1208 00:51:32.493116 140583164200832 learning.py:507] global step 3990: loss = 1.3833 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 3991: loss = 1.8161 (1.494 sec/step)\n",
            "I1208 00:51:34.033488 140583164200832 learning.py:507] global step 3991: loss = 1.8161 (1.494 sec/step)\n",
            "INFO:tensorflow:global step 3992: loss = 1.8551 (0.666 sec/step)\n",
            "I1208 00:51:34.940756 140583164200832 learning.py:507] global step 3992: loss = 1.8551 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 3993: loss = 1.8919 (1.340 sec/step)\n",
            "I1208 00:51:36.424110 140583164200832 learning.py:507] global step 3993: loss = 1.8919 (1.340 sec/step)\n",
            "INFO:tensorflow:global step 3994: loss = 1.8825 (0.475 sec/step)\n",
            "I1208 00:51:36.901298 140583164200832 learning.py:507] global step 3994: loss = 1.8825 (0.475 sec/step)\n",
            "INFO:tensorflow:global step 3995: loss = 1.3582 (1.798 sec/step)\n",
            "I1208 00:51:38.700749 140583164200832 learning.py:507] global step 3995: loss = 1.3582 (1.798 sec/step)\n",
            "INFO:tensorflow:global step 3996: loss = 2.3213 (0.667 sec/step)\n",
            "I1208 00:51:39.497078 140583164200832 learning.py:507] global step 3996: loss = 2.3213 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 3997: loss = 1.6495 (0.654 sec/step)\n",
            "I1208 00:51:40.580636 140583164200832 learning.py:507] global step 3997: loss = 1.6495 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 3998: loss = 2.0843 (1.331 sec/step)\n",
            "I1208 00:51:42.095526 140583164200832 learning.py:507] global step 3998: loss = 2.0843 (1.331 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 3998.\n",
            "I1208 00:51:44.086953 140579459213056 supervisor.py:1050] Recording summary at step 3998.\n",
            "INFO:tensorflow:global step 3999: loss = 1.9561 (1.673 sec/step)\n",
            "I1208 00:51:44.333617 140583164200832 learning.py:507] global step 3999: loss = 1.9561 (1.673 sec/step)\n",
            "INFO:tensorflow:global step 4000: loss = 1.6983 (0.627 sec/step)\n",
            "I1208 00:51:44.962600 140583164200832 learning.py:507] global step 4000: loss = 1.6983 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 4001: loss = 1.6338 (1.803 sec/step)\n",
            "I1208 00:51:46.767594 140583164200832 learning.py:507] global step 4001: loss = 1.6338 (1.803 sec/step)\n",
            "INFO:tensorflow:global step 4002: loss = 2.1823 (1.109 sec/step)\n",
            "I1208 00:51:47.877931 140583164200832 learning.py:507] global step 4002: loss = 2.1823 (1.109 sec/step)\n",
            "INFO:tensorflow:global step 4003: loss = 1.6897 (0.682 sec/step)\n",
            "I1208 00:51:48.609982 140583164200832 learning.py:507] global step 4003: loss = 1.6897 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 4004: loss = 1.5129 (1.519 sec/step)\n",
            "I1208 00:51:50.138640 140583164200832 learning.py:507] global step 4004: loss = 1.5129 (1.519 sec/step)\n",
            "INFO:tensorflow:global step 4005: loss = 1.9401 (0.547 sec/step)\n",
            "I1208 00:51:51.045798 140583164200832 learning.py:507] global step 4005: loss = 1.9401 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 4006: loss = 1.7561 (1.217 sec/step)\n",
            "I1208 00:51:52.429445 140583164200832 learning.py:507] global step 4006: loss = 1.7561 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 4007: loss = 1.4278 (0.679 sec/step)\n",
            "I1208 00:51:53.250709 140583164200832 learning.py:507] global step 4007: loss = 1.4278 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 4008: loss = 2.3714 (0.616 sec/step)\n",
            "I1208 00:51:54.108652 140583164200832 learning.py:507] global step 4008: loss = 2.3714 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 4009: loss = 1.4565 (1.879 sec/step)\n",
            "I1208 00:51:55.989295 140583164200832 learning.py:507] global step 4009: loss = 1.4565 (1.879 sec/step)\n",
            "INFO:tensorflow:global step 4010: loss = 2.0712 (0.635 sec/step)\n",
            "I1208 00:51:56.722426 140583164200832 learning.py:507] global step 4010: loss = 2.0712 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 4011: loss = 1.6279 (1.282 sec/step)\n",
            "I1208 00:51:58.361378 140583164200832 learning.py:507] global step 4011: loss = 1.6279 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 4012: loss = 1.6505 (0.574 sec/step)\n",
            "I1208 00:51:59.261114 140583164200832 learning.py:507] global step 4012: loss = 1.6505 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 4013: loss = 1.6383 (0.527 sec/step)\n",
            "I1208 00:51:59.963052 140583164200832 learning.py:507] global step 4013: loss = 1.6383 (0.527 sec/step)\n",
            "INFO:tensorflow:global step 4014: loss = 1.7398 (0.794 sec/step)\n",
            "I1208 00:52:01.040436 140583164200832 learning.py:507] global step 4014: loss = 1.7398 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 4015: loss = 1.0970 (1.506 sec/step)\n",
            "I1208 00:52:02.775072 140583164200832 learning.py:507] global step 4015: loss = 1.0970 (1.506 sec/step)\n",
            "INFO:tensorflow:global step 4016: loss = 2.1381 (0.824 sec/step)\n",
            "I1208 00:52:03.762606 140583164200832 learning.py:507] global step 4016: loss = 2.1381 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 4017: loss = 1.6945 (1.221 sec/step)\n",
            "I1208 00:52:05.095226 140583164200832 learning.py:507] global step 4017: loss = 1.6945 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 4018: loss = 1.9451 (0.664 sec/step)\n",
            "I1208 00:52:06.061886 140583164200832 learning.py:507] global step 4018: loss = 1.9451 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 4019: loss = 1.8334 (0.760 sec/step)\n",
            "I1208 00:52:07.015112 140583164200832 learning.py:507] global step 4019: loss = 1.8334 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 4020: loss = 1.6348 (0.626 sec/step)\n",
            "I1208 00:52:08.087518 140583164200832 learning.py:507] global step 4020: loss = 1.6348 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 4021: loss = 2.0886 (0.677 sec/step)\n",
            "I1208 00:52:09.128686 140583164200832 learning.py:507] global step 4021: loss = 2.0886 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 4022: loss = 1.5416 (1.334 sec/step)\n",
            "I1208 00:52:10.652548 140583164200832 learning.py:507] global step 4022: loss = 1.5416 (1.334 sec/step)\n",
            "INFO:tensorflow:global step 4023: loss = 1.7767 (1.186 sec/step)\n",
            "I1208 00:52:11.840695 140583164200832 learning.py:507] global step 4023: loss = 1.7767 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 4024: loss = 1.6234 (0.803 sec/step)\n",
            "I1208 00:52:12.855982 140583164200832 learning.py:507] global step 4024: loss = 1.6234 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 4025: loss = 1.9378 (1.212 sec/step)\n",
            "I1208 00:52:14.096789 140583164200832 learning.py:507] global step 4025: loss = 1.9378 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 4026: loss = 1.4660 (0.699 sec/step)\n",
            "I1208 00:52:15.087915 140583164200832 learning.py:507] global step 4026: loss = 1.4660 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4027: loss = 1.6663 (0.619 sec/step)\n",
            "I1208 00:52:16.154473 140583164200832 learning.py:507] global step 4027: loss = 1.6663 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 4028: loss = 1.8975 (0.635 sec/step)\n",
            "I1208 00:52:17.023858 140583164200832 learning.py:507] global step 4028: loss = 1.8975 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 4029: loss = 1.7058 (2.290 sec/step)\n",
            "I1208 00:52:19.452921 140583164200832 learning.py:507] global step 4029: loss = 1.7058 (2.290 sec/step)\n",
            "INFO:tensorflow:global step 4030: loss = 2.2612 (0.586 sec/step)\n",
            "I1208 00:52:20.041388 140583164200832 learning.py:507] global step 4030: loss = 2.2612 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 4031: loss = 1.7663 (1.858 sec/step)\n",
            "I1208 00:52:21.901892 140583164200832 learning.py:507] global step 4031: loss = 1.7663 (1.858 sec/step)\n",
            "INFO:tensorflow:global step 4032: loss = 2.0326 (0.610 sec/step)\n",
            "I1208 00:52:22.514314 140583164200832 learning.py:507] global step 4032: loss = 2.0326 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 4033: loss = 1.6896 (1.722 sec/step)\n",
            "I1208 00:52:24.600233 140583164200832 learning.py:507] global step 4033: loss = 1.6896 (1.722 sec/step)\n",
            "INFO:tensorflow:global step 4034: loss = 1.5787 (0.573 sec/step)\n",
            "I1208 00:52:25.174841 140583164200832 learning.py:507] global step 4034: loss = 1.5787 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4035: loss = 1.9401 (1.517 sec/step)\n",
            "I1208 00:52:27.000252 140583164200832 learning.py:507] global step 4035: loss = 1.9401 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 4036: loss = 1.9109 (2.386 sec/step)\n",
            "I1208 00:52:29.575529 140583164200832 learning.py:507] global step 4036: loss = 1.9109 (2.386 sec/step)\n",
            "INFO:tensorflow:global step 4037: loss = 1.8685 (0.813 sec/step)\n",
            "I1208 00:52:30.408510 140583164200832 learning.py:507] global step 4037: loss = 1.8685 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 4038: loss = 1.8925 (2.025 sec/step)\n",
            "I1208 00:52:32.487509 140583164200832 learning.py:507] global step 4038: loss = 1.8925 (2.025 sec/step)\n",
            "INFO:tensorflow:global step 4039: loss = 1.6802 (1.262 sec/step)\n",
            "I1208 00:52:33.751320 140583164200832 learning.py:507] global step 4039: loss = 1.6802 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 4040: loss = 1.7172 (0.671 sec/step)\n",
            "I1208 00:52:34.746862 140583164200832 learning.py:507] global step 4040: loss = 1.7172 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 4041: loss = 1.5657 (0.592 sec/step)\n",
            "I1208 00:52:35.453842 140583164200832 learning.py:507] global step 4041: loss = 1.5657 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 4042: loss = 1.4672 (1.019 sec/step)\n",
            "I1208 00:52:36.673317 140583164200832 learning.py:507] global step 4042: loss = 1.4672 (1.019 sec/step)\n",
            "INFO:tensorflow:global step 4043: loss = 1.7751 (1.983 sec/step)\n",
            "I1208 00:52:38.924778 140583164200832 learning.py:507] global step 4043: loss = 1.7751 (1.983 sec/step)\n",
            "INFO:tensorflow:global step 4044: loss = 2.4778 (0.658 sec/step)\n",
            "I1208 00:52:39.804377 140583164200832 learning.py:507] global step 4044: loss = 2.4778 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 4045: loss = 1.6096 (0.601 sec/step)\n",
            "I1208 00:52:40.607256 140583164200832 learning.py:507] global step 4045: loss = 1.6096 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 4046: loss = 2.2677 (0.739 sec/step)\n",
            "I1208 00:52:41.347822 140583164200832 learning.py:507] global step 4046: loss = 2.2677 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 4047: loss = 2.4789 (2.292 sec/step)\n",
            "I1208 00:52:43.865137 140583164200832 learning.py:507] global step 4047: loss = 2.4789 (2.292 sec/step)\n",
            "INFO:tensorflow:global step 4048: loss = 1.9046 (2.114 sec/step)\n",
            "I1208 00:52:46.003898 140583164200832 learning.py:507] global step 4048: loss = 1.9046 (2.114 sec/step)\n",
            "INFO:tensorflow:global step 4049: loss = 1.6690 (0.572 sec/step)\n",
            "I1208 00:52:46.578456 140583164200832 learning.py:507] global step 4049: loss = 1.6690 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 4050: loss = 2.4944 (1.031 sec/step)\n",
            "I1208 00:52:47.782070 140583164200832 learning.py:507] global step 4050: loss = 2.4944 (1.031 sec/step)\n",
            "INFO:tensorflow:global step 4051: loss = 2.4642 (2.267 sec/step)\n",
            "I1208 00:52:50.337104 140583164200832 learning.py:507] global step 4051: loss = 2.4642 (2.267 sec/step)\n",
            "INFO:tensorflow:global step 4052: loss = 1.8944 (0.664 sec/step)\n",
            "I1208 00:52:51.200563 140583164200832 learning.py:507] global step 4052: loss = 1.8944 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 4053: loss = 2.0774 (1.433 sec/step)\n",
            "I1208 00:52:53.024152 140583164200832 learning.py:507] global step 4053: loss = 2.0774 (1.433 sec/step)\n",
            "INFO:tensorflow:global step 4054: loss = 1.6009 (1.277 sec/step)\n",
            "I1208 00:52:54.303498 140583164200832 learning.py:507] global step 4054: loss = 1.6009 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 4055: loss = 2.0536 (0.642 sec/step)\n",
            "I1208 00:52:54.947489 140583164200832 learning.py:507] global step 4055: loss = 2.0536 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 4056: loss = 1.6117 (0.642 sec/step)\n",
            "I1208 00:52:55.632863 140583164200832 learning.py:507] global step 4056: loss = 1.6117 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 4057: loss = 2.4319 (2.304 sec/step)\n",
            "I1208 00:52:58.324448 140583164200832 learning.py:507] global step 4057: loss = 2.4319 (2.304 sec/step)\n",
            "INFO:tensorflow:global step 4058: loss = 1.6816 (0.572 sec/step)\n",
            "I1208 00:52:58.898422 140583164200832 learning.py:507] global step 4058: loss = 1.6816 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 4059: loss = 1.9109 (1.946 sec/step)\n",
            "I1208 00:53:00.856448 140583164200832 learning.py:507] global step 4059: loss = 1.9109 (1.946 sec/step)\n",
            "INFO:tensorflow:global step 4060: loss = 2.0710 (1.208 sec/step)\n",
            "I1208 00:53:02.169888 140583164200832 learning.py:507] global step 4060: loss = 2.0710 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 4061: loss = 2.1431 (0.706 sec/step)\n",
            "I1208 00:53:02.909723 140583164200832 learning.py:507] global step 4061: loss = 2.1431 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 4062: loss = 1.7703 (2.302 sec/step)\n",
            "I1208 00:53:05.261681 140583164200832 learning.py:507] global step 4062: loss = 1.7703 (2.302 sec/step)\n",
            "INFO:tensorflow:global step 4063: loss = 1.9984 (0.630 sec/step)\n",
            "I1208 00:53:06.361172 140583164200832 learning.py:507] global step 4063: loss = 1.9984 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 4064: loss = 1.9437 (0.618 sec/step)\n",
            "I1208 00:53:07.045734 140583164200832 learning.py:507] global step 4064: loss = 1.9437 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 4065: loss = 1.3829 (1.933 sec/step)\n",
            "I1208 00:53:08.979800 140583164200832 learning.py:507] global step 4065: loss = 1.3829 (1.933 sec/step)\n",
            "INFO:tensorflow:global step 4066: loss = 1.6406 (0.558 sec/step)\n",
            "I1208 00:53:09.885192 140583164200832 learning.py:507] global step 4066: loss = 1.6406 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 4067: loss = 1.9701 (1.812 sec/step)\n",
            "I1208 00:53:11.898689 140583164200832 learning.py:507] global step 4067: loss = 1.9701 (1.812 sec/step)\n",
            "INFO:tensorflow:global step 4068: loss = 1.9483 (0.649 sec/step)\n",
            "I1208 00:53:12.549351 140583164200832 learning.py:507] global step 4068: loss = 1.9483 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 4069: loss = 1.8442 (1.688 sec/step)\n",
            "I1208 00:53:14.286041 140583164200832 learning.py:507] global step 4069: loss = 1.8442 (1.688 sec/step)\n",
            "INFO:tensorflow:global step 4070: loss = 1.2858 (0.702 sec/step)\n",
            "I1208 00:53:15.283686 140583164200832 learning.py:507] global step 4070: loss = 1.2858 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 4071: loss = 1.4024 (0.615 sec/step)\n",
            "I1208 00:53:16.057239 140583164200832 learning.py:507] global step 4071: loss = 1.4024 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 4072: loss = 1.1308 (1.712 sec/step)\n",
            "I1208 00:53:17.770789 140583164200832 learning.py:507] global step 4072: loss = 1.1308 (1.712 sec/step)\n",
            "INFO:tensorflow:global step 4073: loss = 2.0127 (0.572 sec/step)\n",
            "I1208 00:53:18.344710 140583164200832 learning.py:507] global step 4073: loss = 2.0127 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 4074: loss = 1.7527 (0.674 sec/step)\n",
            "I1208 00:53:19.021286 140583164200832 learning.py:507] global step 4074: loss = 1.7527 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 4075: loss = 1.8744 (2.546 sec/step)\n",
            "I1208 00:53:21.590725 140583164200832 learning.py:507] global step 4075: loss = 1.8744 (2.546 sec/step)\n",
            "INFO:tensorflow:global step 4076: loss = 1.8243 (2.041 sec/step)\n",
            "I1208 00:53:23.633377 140583164200832 learning.py:507] global step 4076: loss = 1.8243 (2.041 sec/step)\n",
            "INFO:tensorflow:global step 4077: loss = 1.6666 (0.618 sec/step)\n",
            "I1208 00:53:24.407592 140583164200832 learning.py:507] global step 4077: loss = 1.6666 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 4078: loss = 1.9561 (0.628 sec/step)\n",
            "I1208 00:53:25.715709 140583164200832 learning.py:507] global step 4078: loss = 1.9561 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 4079: loss = 1.8434 (0.663 sec/step)\n",
            "I1208 00:53:26.564802 140583164200832 learning.py:507] global step 4079: loss = 1.8434 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 4080: loss = 2.0390 (1.863 sec/step)\n",
            "I1208 00:53:28.429125 140583164200832 learning.py:507] global step 4080: loss = 2.0390 (1.863 sec/step)\n",
            "INFO:tensorflow:global step 4081: loss = 1.6645 (0.559 sec/step)\n",
            "I1208 00:53:28.990250 140583164200832 learning.py:507] global step 4081: loss = 1.6645 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 4082: loss = 1.7265 (0.540 sec/step)\n",
            "I1208 00:53:29.532707 140583164200832 learning.py:507] global step 4082: loss = 1.7265 (0.540 sec/step)\n",
            "INFO:tensorflow:global step 4083: loss = 1.7086 (1.054 sec/step)\n",
            "I1208 00:53:30.904623 140583164200832 learning.py:507] global step 4083: loss = 1.7086 (1.054 sec/step)\n",
            "INFO:tensorflow:global step 4084: loss = 1.5863 (2.205 sec/step)\n",
            "I1208 00:53:33.317696 140583164200832 learning.py:507] global step 4084: loss = 1.5863 (2.205 sec/step)\n",
            "INFO:tensorflow:global step 4085: loss = 1.7078 (0.683 sec/step)\n",
            "I1208 00:53:34.025663 140583164200832 learning.py:507] global step 4085: loss = 1.7078 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 4086: loss = 1.6927 (1.559 sec/step)\n",
            "I1208 00:53:35.643187 140583164200832 learning.py:507] global step 4086: loss = 1.6927 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 4087: loss = 1.5491 (0.687 sec/step)\n",
            "I1208 00:53:36.544062 140583164200832 learning.py:507] global step 4087: loss = 1.5491 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 4088: loss = 2.0873 (0.557 sec/step)\n",
            "I1208 00:53:37.273282 140583164200832 learning.py:507] global step 4088: loss = 2.0873 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 4089: loss = 1.7557 (1.605 sec/step)\n",
            "I1208 00:53:39.142744 140583164200832 learning.py:507] global step 4089: loss = 1.7557 (1.605 sec/step)\n",
            "INFO:tensorflow:global step 4090: loss = 1.7717 (1.748 sec/step)\n",
            "I1208 00:53:40.894588 140583164200832 learning.py:507] global step 4090: loss = 1.7717 (1.748 sec/step)\n",
            "INFO:tensorflow:global step 4091: loss = 1.3908 (0.729 sec/step)\n",
            "I1208 00:53:41.626526 140583164200832 learning.py:507] global step 4091: loss = 1.3908 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 4092: loss = 1.8686 (1.750 sec/step)\n",
            "I1208 00:53:43.663305 140583164200832 learning.py:507] global step 4092: loss = 1.8686 (1.750 sec/step)\n",
            "INFO:tensorflow:global step 4093: loss = 1.7034 (1.009 sec/step)\n",
            "I1208 00:53:44.910368 140583164200832 learning.py:507] global step 4093: loss = 1.7034 (1.009 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 4093.\n",
            "I1208 00:53:45.280029 140579459213056 supervisor.py:1050] Recording summary at step 4093.\n",
            "INFO:tensorflow:global step 4094: loss = 1.7585 (1.898 sec/step)\n",
            "I1208 00:53:47.109303 140583164200832 learning.py:507] global step 4094: loss = 1.7585 (1.898 sec/step)\n",
            "INFO:tensorflow:global step 4095: loss = 1.5831 (0.660 sec/step)\n",
            "I1208 00:53:48.084956 140583164200832 learning.py:507] global step 4095: loss = 1.5831 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 4096: loss = 2.6170 (0.711 sec/step)\n",
            "I1208 00:53:48.958013 140583164200832 learning.py:507] global step 4096: loss = 2.6170 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 4097: loss = 1.7292 (1.537 sec/step)\n",
            "I1208 00:53:50.866947 140583164200832 learning.py:507] global step 4097: loss = 1.7292 (1.537 sec/step)\n",
            "INFO:tensorflow:global step 4098: loss = 1.5091 (0.581 sec/step)\n",
            "I1208 00:53:51.449792 140583164200832 learning.py:507] global step 4098: loss = 1.5091 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 4099: loss = 1.8225 (1.996 sec/step)\n",
            "I1208 00:53:53.448117 140583164200832 learning.py:507] global step 4099: loss = 1.8225 (1.996 sec/step)\n",
            "INFO:tensorflow:global step 4100: loss = 1.9124 (0.534 sec/step)\n",
            "I1208 00:53:53.984037 140583164200832 learning.py:507] global step 4100: loss = 1.9124 (0.534 sec/step)\n",
            "INFO:tensorflow:global step 4101: loss = 2.1356 (0.532 sec/step)\n",
            "I1208 00:53:54.517735 140583164200832 learning.py:507] global step 4101: loss = 2.1356 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 4102: loss = 2.0528 (1.077 sec/step)\n",
            "I1208 00:53:55.784467 140583164200832 learning.py:507] global step 4102: loss = 2.0528 (1.077 sec/step)\n",
            "INFO:tensorflow:global step 4103: loss = 1.7149 (2.409 sec/step)\n",
            "I1208 00:53:58.288078 140583164200832 learning.py:507] global step 4103: loss = 1.7149 (2.409 sec/step)\n",
            "INFO:tensorflow:global step 4104: loss = 1.9248 (0.562 sec/step)\n",
            "I1208 00:53:59.144906 140583164200832 learning.py:507] global step 4104: loss = 1.9248 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 4105: loss = 1.6036 (0.622 sec/step)\n",
            "I1208 00:53:59.893278 140583164200832 learning.py:507] global step 4105: loss = 1.6036 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 4106: loss = 2.0102 (1.734 sec/step)\n",
            "I1208 00:54:01.844422 140583164200832 learning.py:507] global step 4106: loss = 2.0102 (1.734 sec/step)\n",
            "INFO:tensorflow:global step 4107: loss = 1.7876 (0.589 sec/step)\n",
            "I1208 00:54:02.806935 140583164200832 learning.py:507] global step 4107: loss = 1.7876 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 4108: loss = 1.8301 (0.579 sec/step)\n",
            "I1208 00:54:03.550402 140583164200832 learning.py:507] global step 4108: loss = 1.8301 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 4109: loss = 1.5818 (1.725 sec/step)\n",
            "I1208 00:54:05.276959 140583164200832 learning.py:507] global step 4109: loss = 1.5818 (1.725 sec/step)\n",
            "INFO:tensorflow:global step 4110: loss = 2.0224 (0.559 sec/step)\n",
            "I1208 00:54:06.119696 140583164200832 learning.py:507] global step 4110: loss = 2.0224 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 4111: loss = 1.9779 (0.528 sec/step)\n",
            "I1208 00:54:06.856321 140583164200832 learning.py:507] global step 4111: loss = 1.9779 (0.528 sec/step)\n",
            "INFO:tensorflow:global step 4112: loss = 1.8849 (1.692 sec/step)\n",
            "I1208 00:54:08.549615 140583164200832 learning.py:507] global step 4112: loss = 1.8849 (1.692 sec/step)\n",
            "INFO:tensorflow:global step 4113: loss = 1.5355 (0.697 sec/step)\n",
            "I1208 00:54:09.303987 140583164200832 learning.py:507] global step 4113: loss = 1.5355 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 4114: loss = 1.3621 (1.638 sec/step)\n",
            "I1208 00:54:10.953354 140583164200832 learning.py:507] global step 4114: loss = 1.3621 (1.638 sec/step)\n",
            "INFO:tensorflow:global step 4115: loss = 2.1069 (0.484 sec/step)\n",
            "I1208 00:54:11.439506 140583164200832 learning.py:507] global step 4115: loss = 2.1069 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 4116: loss = 1.7323 (1.701 sec/step)\n",
            "I1208 00:54:13.141970 140583164200832 learning.py:507] global step 4116: loss = 1.7323 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 4117: loss = 1.6238 (0.742 sec/step)\n",
            "I1208 00:54:13.952182 140583164200832 learning.py:507] global step 4117: loss = 1.6238 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 4118: loss = 1.9034 (0.618 sec/step)\n",
            "I1208 00:54:14.817275 140583164200832 learning.py:507] global step 4118: loss = 1.9034 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 4119: loss = 1.7056 (1.795 sec/step)\n",
            "I1208 00:54:16.617513 140583164200832 learning.py:507] global step 4119: loss = 1.7056 (1.795 sec/step)\n",
            "INFO:tensorflow:global step 4120: loss = 1.5075 (0.621 sec/step)\n",
            "I1208 00:54:17.538803 140583164200832 learning.py:507] global step 4120: loss = 1.5075 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 4121: loss = 1.7659 (1.243 sec/step)\n",
            "I1208 00:54:18.896209 140583164200832 learning.py:507] global step 4121: loss = 1.7659 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 4122: loss = 1.7209 (0.682 sec/step)\n",
            "I1208 00:54:19.818733 140583164200832 learning.py:507] global step 4122: loss = 1.7209 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 4123: loss = 1.5382 (0.746 sec/step)\n",
            "I1208 00:54:20.922291 140583164200832 learning.py:507] global step 4123: loss = 1.5382 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 4124: loss = 1.5652 (0.701 sec/step)\n",
            "I1208 00:54:21.950933 140583164200832 learning.py:507] global step 4124: loss = 1.5652 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 4125: loss = 1.9107 (1.335 sec/step)\n",
            "I1208 00:54:23.406644 140583164200832 learning.py:507] global step 4125: loss = 1.9107 (1.335 sec/step)\n",
            "INFO:tensorflow:global step 4126: loss = 1.4989 (0.569 sec/step)\n",
            "I1208 00:54:24.319340 140583164200832 learning.py:507] global step 4126: loss = 1.4989 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 4127: loss = 1.9623 (0.699 sec/step)\n",
            "I1208 00:54:25.261032 140583164200832 learning.py:507] global step 4127: loss = 1.9623 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4128: loss = 1.4157 (1.300 sec/step)\n",
            "I1208 00:54:26.629457 140583164200832 learning.py:507] global step 4128: loss = 1.4157 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 4129: loss = 1.2129 (0.721 sec/step)\n",
            "I1208 00:54:27.587140 140583164200832 learning.py:507] global step 4129: loss = 1.2129 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 4130: loss = 2.1405 (0.633 sec/step)\n",
            "I1208 00:54:28.511554 140583164200832 learning.py:507] global step 4130: loss = 2.1405 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 4131: loss = 2.0949 (1.320 sec/step)\n",
            "I1208 00:54:30.010154 140583164200832 learning.py:507] global step 4131: loss = 2.0949 (1.320 sec/step)\n",
            "INFO:tensorflow:global step 4132: loss = 1.2619 (0.493 sec/step)\n",
            "I1208 00:54:30.505171 140583164200832 learning.py:507] global step 4132: loss = 1.2619 (0.493 sec/step)\n",
            "INFO:tensorflow:global step 4133: loss = 1.7382 (1.364 sec/step)\n",
            "I1208 00:54:32.192423 140583164200832 learning.py:507] global step 4133: loss = 1.7382 (1.364 sec/step)\n",
            "INFO:tensorflow:global step 4134: loss = 1.7115 (2.026 sec/step)\n",
            "I1208 00:54:34.258563 140583164200832 learning.py:507] global step 4134: loss = 1.7115 (2.026 sec/step)\n",
            "INFO:tensorflow:global step 4135: loss = 1.8755 (0.754 sec/step)\n",
            "I1208 00:54:35.212892 140583164200832 learning.py:507] global step 4135: loss = 1.8755 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 4136: loss = 1.4062 (0.645 sec/step)\n",
            "I1208 00:54:36.360074 140583164200832 learning.py:507] global step 4136: loss = 1.4062 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 4137: loss = 1.7672 (1.981 sec/step)\n",
            "I1208 00:54:38.406351 140583164200832 learning.py:507] global step 4137: loss = 1.7672 (1.981 sec/step)\n",
            "INFO:tensorflow:global step 4138: loss = 2.3678 (0.594 sec/step)\n",
            "I1208 00:54:39.002732 140583164200832 learning.py:507] global step 4138: loss = 2.3678 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 4139: loss = 2.3881 (1.863 sec/step)\n",
            "I1208 00:54:40.867635 140583164200832 learning.py:507] global step 4139: loss = 2.3881 (1.863 sec/step)\n",
            "INFO:tensorflow:global step 4140: loss = 1.4073 (0.514 sec/step)\n",
            "I1208 00:54:41.383577 140583164200832 learning.py:507] global step 4140: loss = 1.4073 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 4141: loss = 1.2965 (2.213 sec/step)\n",
            "I1208 00:54:43.598826 140583164200832 learning.py:507] global step 4141: loss = 1.2965 (2.213 sec/step)\n",
            "INFO:tensorflow:global step 4142: loss = 1.5702 (0.573 sec/step)\n",
            "I1208 00:54:44.173779 140583164200832 learning.py:507] global step 4142: loss = 1.5702 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4143: loss = 1.4438 (1.831 sec/step)\n",
            "I1208 00:54:46.006550 140583164200832 learning.py:507] global step 4143: loss = 1.4438 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 4144: loss = 1.7952 (0.671 sec/step)\n",
            "I1208 00:54:46.913923 140583164200832 learning.py:507] global step 4144: loss = 1.7952 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 4145: loss = 1.4748 (1.122 sec/step)\n",
            "I1208 00:54:48.199974 140583164200832 learning.py:507] global step 4145: loss = 1.4748 (1.122 sec/step)\n",
            "INFO:tensorflow:global step 4146: loss = 1.9883 (0.655 sec/step)\n",
            "I1208 00:54:48.862976 140583164200832 learning.py:507] global step 4146: loss = 1.9883 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 4147: loss = 1.9468 (1.907 sec/step)\n",
            "I1208 00:54:50.772019 140583164200832 learning.py:507] global step 4147: loss = 1.9468 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 4148: loss = 1.4181 (1.189 sec/step)\n",
            "I1208 00:54:51.962718 140583164200832 learning.py:507] global step 4148: loss = 1.4181 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 4149: loss = 1.5311 (0.545 sec/step)\n",
            "I1208 00:54:52.509442 140583164200832 learning.py:507] global step 4149: loss = 1.5311 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 4150: loss = 1.3328 (1.864 sec/step)\n",
            "I1208 00:54:54.375478 140583164200832 learning.py:507] global step 4150: loss = 1.3328 (1.864 sec/step)\n",
            "INFO:tensorflow:global step 4151: loss = 1.3837 (0.651 sec/step)\n",
            "I1208 00:54:55.319535 140583164200832 learning.py:507] global step 4151: loss = 1.3837 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 4152: loss = 1.3388 (1.396 sec/step)\n",
            "I1208 00:54:56.842421 140583164200832 learning.py:507] global step 4152: loss = 1.3388 (1.396 sec/step)\n",
            "INFO:tensorflow:global step 4153: loss = 2.2348 (0.603 sec/step)\n",
            "I1208 00:54:57.447130 140583164200832 learning.py:507] global step 4153: loss = 2.2348 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 4154: loss = 2.1432 (1.615 sec/step)\n",
            "I1208 00:54:59.146969 140583164200832 learning.py:507] global step 4154: loss = 2.1432 (1.615 sec/step)\n",
            "INFO:tensorflow:global step 4155: loss = 1.9611 (0.625 sec/step)\n",
            "I1208 00:55:00.017649 140583164200832 learning.py:507] global step 4155: loss = 1.9611 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 4156: loss = 1.7791 (1.340 sec/step)\n",
            "I1208 00:55:01.635354 140583164200832 learning.py:507] global step 4156: loss = 1.7791 (1.340 sec/step)\n",
            "INFO:tensorflow:global step 4157: loss = 1.7630 (0.525 sec/step)\n",
            "I1208 00:55:02.161674 140583164200832 learning.py:507] global step 4157: loss = 1.7630 (0.525 sec/step)\n",
            "INFO:tensorflow:global step 4158: loss = 1.7433 (1.765 sec/step)\n",
            "I1208 00:55:03.928761 140583164200832 learning.py:507] global step 4158: loss = 1.7433 (1.765 sec/step)\n",
            "INFO:tensorflow:global step 4159: loss = 2.5067 (1.197 sec/step)\n",
            "I1208 00:55:05.127062 140583164200832 learning.py:507] global step 4159: loss = 2.5067 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 4160: loss = 2.1990 (0.500 sec/step)\n",
            "I1208 00:55:06.157273 140583164200832 learning.py:507] global step 4160: loss = 2.1990 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 4161: loss = 1.7640 (0.664 sec/step)\n",
            "I1208 00:55:06.837672 140583164200832 learning.py:507] global step 4161: loss = 1.7640 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 4162: loss = 1.5577 (1.808 sec/step)\n",
            "I1208 00:55:08.833592 140583164200832 learning.py:507] global step 4162: loss = 1.5577 (1.808 sec/step)\n",
            "INFO:tensorflow:global step 4163: loss = 1.6087 (0.779 sec/step)\n",
            "I1208 00:55:09.930524 140583164200832 learning.py:507] global step 4163: loss = 1.6087 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 4164: loss = 1.7607 (0.579 sec/step)\n",
            "I1208 00:55:10.575120 140583164200832 learning.py:507] global step 4164: loss = 1.7607 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 4165: loss = 1.7853 (1.574 sec/step)\n",
            "I1208 00:55:12.440191 140583164200832 learning.py:507] global step 4165: loss = 1.7853 (1.574 sec/step)\n",
            "INFO:tensorflow:global step 4166: loss = 1.7987 (2.136 sec/step)\n",
            "I1208 00:55:14.801584 140583164200832 learning.py:507] global step 4166: loss = 1.7987 (2.136 sec/step)\n",
            "INFO:tensorflow:global step 4167: loss = 1.2750 (0.699 sec/step)\n",
            "I1208 00:55:15.586329 140583164200832 learning.py:507] global step 4167: loss = 1.2750 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4168: loss = 1.2005 (1.740 sec/step)\n",
            "I1208 00:55:17.328356 140583164200832 learning.py:507] global step 4168: loss = 1.2005 (1.740 sec/step)\n",
            "INFO:tensorflow:global step 4169: loss = 1.5932 (0.649 sec/step)\n",
            "I1208 00:55:17.978784 140583164200832 learning.py:507] global step 4169: loss = 1.5932 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 4170: loss = 1.6078 (1.909 sec/step)\n",
            "I1208 00:55:19.889268 140583164200832 learning.py:507] global step 4170: loss = 1.6078 (1.909 sec/step)\n",
            "INFO:tensorflow:global step 4171: loss = 1.3379 (0.537 sec/step)\n",
            "I1208 00:55:20.428056 140583164200832 learning.py:507] global step 4171: loss = 1.3379 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 4172: loss = 1.4289 (0.819 sec/step)\n",
            "I1208 00:55:21.546733 140583164200832 learning.py:507] global step 4172: loss = 1.4289 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 4173: loss = 1.9808 (0.934 sec/step)\n",
            "I1208 00:55:22.706231 140583164200832 learning.py:507] global step 4173: loss = 1.9808 (0.934 sec/step)\n",
            "INFO:tensorflow:global step 4174: loss = 1.6338 (1.617 sec/step)\n",
            "I1208 00:55:24.630431 140583164200832 learning.py:507] global step 4174: loss = 1.6338 (1.617 sec/step)\n",
            "INFO:tensorflow:global step 4175: loss = 1.6320 (0.693 sec/step)\n",
            "I1208 00:55:25.377078 140583164200832 learning.py:507] global step 4175: loss = 1.6320 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 4176: loss = 1.8939 (2.108 sec/step)\n",
            "I1208 00:55:27.497662 140583164200832 learning.py:507] global step 4176: loss = 1.8939 (2.108 sec/step)\n",
            "INFO:tensorflow:global step 4177: loss = 1.4348 (0.557 sec/step)\n",
            "I1208 00:55:28.057580 140583164200832 learning.py:507] global step 4177: loss = 1.4348 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 4178: loss = 2.2599 (1.292 sec/step)\n",
            "I1208 00:55:29.696303 140583164200832 learning.py:507] global step 4178: loss = 2.2599 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 4179: loss = 1.5703 (1.833 sec/step)\n",
            "I1208 00:55:31.761918 140583164200832 learning.py:507] global step 4179: loss = 1.5703 (1.833 sec/step)\n",
            "INFO:tensorflow:global step 4180: loss = 1.6097 (0.571 sec/step)\n",
            "I1208 00:55:32.757227 140583164200832 learning.py:507] global step 4180: loss = 1.6097 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 4181: loss = 1.8832 (0.727 sec/step)\n",
            "I1208 00:55:33.609542 140583164200832 learning.py:507] global step 4181: loss = 1.8832 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 4182: loss = 2.0335 (2.145 sec/step)\n",
            "I1208 00:55:35.756653 140583164200832 learning.py:507] global step 4182: loss = 2.0335 (2.145 sec/step)\n",
            "INFO:tensorflow:global step 4183: loss = 1.6954 (0.541 sec/step)\n",
            "I1208 00:55:36.299446 140583164200832 learning.py:507] global step 4183: loss = 1.6954 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 4184: loss = 3.1936 (1.047 sec/step)\n",
            "I1208 00:55:37.577004 140583164200832 learning.py:507] global step 4184: loss = 3.1936 (1.047 sec/step)\n",
            "INFO:tensorflow:global step 4185: loss = 1.2975 (1.587 sec/step)\n",
            "I1208 00:55:39.580793 140583164200832 learning.py:507] global step 4185: loss = 1.2975 (1.587 sec/step)\n",
            "INFO:tensorflow:global step 4186: loss = 1.5856 (1.120 sec/step)\n",
            "I1208 00:55:40.702731 140583164200832 learning.py:507] global step 4186: loss = 1.5856 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 4187: loss = 1.6773 (0.710 sec/step)\n",
            "I1208 00:55:41.452821 140583164200832 learning.py:507] global step 4187: loss = 1.6773 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 4188: loss = 1.7664 (2.254 sec/step)\n",
            "I1208 00:55:44.013092 140583164200832 learning.py:507] global step 4188: loss = 1.7664 (2.254 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 4189.\n",
            "I1208 00:55:45.027612 140579459213056 supervisor.py:1050] Recording summary at step 4189.\n",
            "INFO:tensorflow:global step 4189: loss = 1.8668 (0.734 sec/step)\n",
            "I1208 00:55:45.027897 140583164200832 learning.py:507] global step 4189: loss = 1.8668 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 4190: loss = 2.0163 (0.715 sec/step)\n",
            "I1208 00:55:45.848698 140583164200832 learning.py:507] global step 4190: loss = 2.0163 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 4191: loss = 1.4266 (1.606 sec/step)\n",
            "I1208 00:55:47.472577 140583164200832 learning.py:507] global step 4191: loss = 1.4266 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 4192: loss = 1.4116 (0.557 sec/step)\n",
            "I1208 00:55:48.396267 140583164200832 learning.py:507] global step 4192: loss = 1.4116 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 4193: loss = 2.5436 (0.699 sec/step)\n",
            "I1208 00:55:49.479743 140583164200832 learning.py:507] global step 4193: loss = 2.5436 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4194: loss = 1.7425 (0.519 sec/step)\n",
            "I1208 00:55:50.171842 140583164200832 learning.py:507] global step 4194: loss = 1.7425 (0.519 sec/step)\n",
            "INFO:tensorflow:global step 4195: loss = 1.5134 (0.816 sec/step)\n",
            "I1208 00:55:51.080078 140583164200832 learning.py:507] global step 4195: loss = 1.5134 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 4196: loss = 2.1866 (1.601 sec/step)\n",
            "I1208 00:55:52.997689 140583164200832 learning.py:507] global step 4196: loss = 2.1866 (1.601 sec/step)\n",
            "INFO:tensorflow:global step 4197: loss = 1.9841 (1.068 sec/step)\n",
            "I1208 00:55:54.067198 140583164200832 learning.py:507] global step 4197: loss = 1.9841 (1.068 sec/step)\n",
            "INFO:tensorflow:global step 4198: loss = 1.8904 (0.718 sec/step)\n",
            "I1208 00:55:54.798829 140583164200832 learning.py:507] global step 4198: loss = 1.8904 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 4199: loss = 1.7892 (1.531 sec/step)\n",
            "I1208 00:55:56.331541 140583164200832 learning.py:507] global step 4199: loss = 1.7892 (1.531 sec/step)\n",
            "INFO:tensorflow:global step 4200: loss = 1.6777 (0.559 sec/step)\n",
            "I1208 00:55:57.130022 140583164200832 learning.py:507] global step 4200: loss = 1.6777 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 4201: loss = 1.7628 (0.539 sec/step)\n",
            "I1208 00:55:58.133610 140583164200832 learning.py:507] global step 4201: loss = 1.7628 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 4202: loss = 1.4638 (0.776 sec/step)\n",
            "I1208 00:55:59.249308 140583164200832 learning.py:507] global step 4202: loss = 1.4638 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 4203: loss = 2.2469 (0.565 sec/step)\n",
            "I1208 00:56:00.050472 140583164200832 learning.py:507] global step 4203: loss = 2.2469 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 4204: loss = 1.8127 (1.613 sec/step)\n",
            "I1208 00:56:01.665074 140583164200832 learning.py:507] global step 4204: loss = 1.8127 (1.613 sec/step)\n",
            "INFO:tensorflow:global step 4205: loss = 2.0568 (0.533 sec/step)\n",
            "I1208 00:56:02.200164 140583164200832 learning.py:507] global step 4205: loss = 2.0568 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 4206: loss = 1.4276 (1.626 sec/step)\n",
            "I1208 00:56:03.827365 140583164200832 learning.py:507] global step 4206: loss = 1.4276 (1.626 sec/step)\n",
            "INFO:tensorflow:global step 4207: loss = 1.5607 (0.495 sec/step)\n",
            "I1208 00:56:04.324094 140583164200832 learning.py:507] global step 4207: loss = 1.5607 (0.495 sec/step)\n",
            "INFO:tensorflow:global step 4208: loss = 1.7928 (0.656 sec/step)\n",
            "I1208 00:56:04.982877 140583164200832 learning.py:507] global step 4208: loss = 1.7928 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 4209: loss = 1.9355 (1.174 sec/step)\n",
            "I1208 00:56:06.471172 140583164200832 learning.py:507] global step 4209: loss = 1.9355 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 4210: loss = 1.2571 (2.051 sec/step)\n",
            "I1208 00:56:08.838234 140583164200832 learning.py:507] global step 4210: loss = 1.2571 (2.051 sec/step)\n",
            "INFO:tensorflow:global step 4211: loss = 1.6756 (0.592 sec/step)\n",
            "I1208 00:56:09.799992 140583164200832 learning.py:507] global step 4211: loss = 1.6756 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 4212: loss = 1.4468 (0.615 sec/step)\n",
            "I1208 00:56:10.604334 140583164200832 learning.py:507] global step 4212: loss = 1.4468 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 4213: loss = 2.2934 (1.034 sec/step)\n",
            "I1208 00:56:11.902855 140583164200832 learning.py:507] global step 4213: loss = 2.2934 (1.034 sec/step)\n",
            "INFO:tensorflow:global step 4214: loss = 1.2853 (1.641 sec/step)\n",
            "I1208 00:56:13.655314 140583164200832 learning.py:507] global step 4214: loss = 1.2853 (1.641 sec/step)\n",
            "INFO:tensorflow:global step 4215: loss = 1.5270 (0.492 sec/step)\n",
            "I1208 00:56:14.148858 140583164200832 learning.py:507] global step 4215: loss = 1.5270 (0.492 sec/step)\n",
            "INFO:tensorflow:global step 4216: loss = 1.5404 (1.797 sec/step)\n",
            "I1208 00:56:15.947695 140583164200832 learning.py:507] global step 4216: loss = 1.5404 (1.797 sec/step)\n",
            "INFO:tensorflow:global step 4217: loss = 1.6616 (0.573 sec/step)\n",
            "I1208 00:56:16.522638 140583164200832 learning.py:507] global step 4217: loss = 1.6616 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4218: loss = 1.5930 (1.672 sec/step)\n",
            "I1208 00:56:18.196642 140583164200832 learning.py:507] global step 4218: loss = 1.5930 (1.672 sec/step)\n",
            "INFO:tensorflow:global step 4219: loss = 2.3584 (0.723 sec/step)\n",
            "I1208 00:56:19.167141 140583164200832 learning.py:507] global step 4219: loss = 2.3584 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 4220: loss = 1.5370 (1.222 sec/step)\n",
            "I1208 00:56:20.475029 140583164200832 learning.py:507] global step 4220: loss = 1.5370 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 4221: loss = 1.8934 (0.675 sec/step)\n",
            "I1208 00:56:21.349939 140583164200832 learning.py:507] global step 4221: loss = 1.8934 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 4222: loss = 1.9093 (1.270 sec/step)\n",
            "I1208 00:56:22.781717 140583164200832 learning.py:507] global step 4222: loss = 1.9093 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 4223: loss = 1.7701 (0.581 sec/step)\n",
            "I1208 00:56:23.364791 140583164200832 learning.py:507] global step 4223: loss = 1.7701 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 4224: loss = 1.4445 (1.690 sec/step)\n",
            "I1208 00:56:25.056995 140583164200832 learning.py:507] global step 4224: loss = 1.4445 (1.690 sec/step)\n",
            "INFO:tensorflow:global step 4225: loss = 1.8719 (0.659 sec/step)\n",
            "I1208 00:56:25.745410 140583164200832 learning.py:507] global step 4225: loss = 1.8719 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 4226: loss = 1.7045 (1.287 sec/step)\n",
            "I1208 00:56:27.372614 140583164200832 learning.py:507] global step 4226: loss = 1.7045 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 4227: loss = 1.6021 (0.739 sec/step)\n",
            "I1208 00:56:28.160528 140583164200832 learning.py:507] global step 4227: loss = 1.6021 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 4228: loss = 1.4672 (1.526 sec/step)\n",
            "I1208 00:56:29.704938 140583164200832 learning.py:507] global step 4228: loss = 1.4672 (1.526 sec/step)\n",
            "INFO:tensorflow:global step 4229: loss = 1.3196 (0.679 sec/step)\n",
            "I1208 00:56:30.564045 140583164200832 learning.py:507] global step 4229: loss = 1.3196 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 4230: loss = 1.7347 (1.319 sec/step)\n",
            "I1208 00:56:31.984750 140583164200832 learning.py:507] global step 4230: loss = 1.7347 (1.319 sec/step)\n",
            "INFO:tensorflow:global step 4231: loss = 1.0638 (0.765 sec/step)\n",
            "I1208 00:56:32.777062 140583164200832 learning.py:507] global step 4231: loss = 1.0638 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 4232: loss = 2.0167 (1.401 sec/step)\n",
            "I1208 00:56:34.209480 140583164200832 learning.py:507] global step 4232: loss = 2.0167 (1.401 sec/step)\n",
            "INFO:tensorflow:global step 4233: loss = 1.5498 (0.674 sec/step)\n",
            "I1208 00:56:34.954411 140583164200832 learning.py:507] global step 4233: loss = 1.5498 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 4234: loss = 1.4907 (1.510 sec/step)\n",
            "I1208 00:56:36.465894 140583164200832 learning.py:507] global step 4234: loss = 1.4907 (1.510 sec/step)\n",
            "INFO:tensorflow:global step 4235: loss = 1.3706 (1.148 sec/step)\n",
            "I1208 00:56:37.615175 140583164200832 learning.py:507] global step 4235: loss = 1.3706 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 4236: loss = 1.7548 (0.506 sec/step)\n",
            "I1208 00:56:38.122863 140583164200832 learning.py:507] global step 4236: loss = 1.7548 (0.506 sec/step)\n",
            "INFO:tensorflow:global step 4237: loss = 1.6688 (2.221 sec/step)\n",
            "I1208 00:56:40.345694 140583164200832 learning.py:507] global step 4237: loss = 1.6688 (2.221 sec/step)\n",
            "INFO:tensorflow:global step 4238: loss = 2.1530 (0.521 sec/step)\n",
            "I1208 00:56:40.868333 140583164200832 learning.py:507] global step 4238: loss = 2.1530 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 4239: loss = 2.0219 (1.954 sec/step)\n",
            "I1208 00:56:42.824014 140583164200832 learning.py:507] global step 4239: loss = 2.0219 (1.954 sec/step)\n",
            "INFO:tensorflow:global step 4240: loss = 2.2069 (0.576 sec/step)\n",
            "I1208 00:56:43.401906 140583164200832 learning.py:507] global step 4240: loss = 2.2069 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 4241: loss = 1.7440 (1.878 sec/step)\n",
            "I1208 00:56:45.282356 140583164200832 learning.py:507] global step 4241: loss = 1.7440 (1.878 sec/step)\n",
            "INFO:tensorflow:global step 4242: loss = 1.2733 (0.553 sec/step)\n",
            "I1208 00:56:45.837721 140583164200832 learning.py:507] global step 4242: loss = 1.2733 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 4243: loss = 1.5337 (1.799 sec/step)\n",
            "I1208 00:56:47.638765 140583164200832 learning.py:507] global step 4243: loss = 1.5337 (1.799 sec/step)\n",
            "INFO:tensorflow:global step 4244: loss = 2.1534 (0.536 sec/step)\n",
            "I1208 00:56:48.176488 140583164200832 learning.py:507] global step 4244: loss = 2.1534 (0.536 sec/step)\n",
            "INFO:tensorflow:global step 4245: loss = 1.8822 (1.673 sec/step)\n",
            "I1208 00:56:49.851736 140583164200832 learning.py:507] global step 4245: loss = 1.8822 (1.673 sec/step)\n",
            "INFO:tensorflow:global step 4246: loss = 1.6299 (0.542 sec/step)\n",
            "I1208 00:56:50.741559 140583164200832 learning.py:507] global step 4246: loss = 1.6299 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 4247: loss = 1.2816 (1.373 sec/step)\n",
            "I1208 00:56:52.131135 140583164200832 learning.py:507] global step 4247: loss = 1.2816 (1.373 sec/step)\n",
            "INFO:tensorflow:global step 4248: loss = 1.7684 (0.696 sec/step)\n",
            "I1208 00:56:53.123504 140583164200832 learning.py:507] global step 4248: loss = 1.7684 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 4249: loss = 1.9148 (1.146 sec/step)\n",
            "I1208 00:56:54.390583 140583164200832 learning.py:507] global step 4249: loss = 1.9148 (1.146 sec/step)\n",
            "INFO:tensorflow:global step 4250: loss = 1.7353 (0.623 sec/step)\n",
            "I1208 00:56:55.015324 140583164200832 learning.py:507] global step 4250: loss = 1.7353 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 4251: loss = 1.7262 (1.550 sec/step)\n",
            "I1208 00:56:56.791830 140583164200832 learning.py:507] global step 4251: loss = 1.7262 (1.550 sec/step)\n",
            "INFO:tensorflow:global step 4252: loss = 1.4391 (1.414 sec/step)\n",
            "I1208 00:56:58.274757 140583164200832 learning.py:507] global step 4252: loss = 1.4391 (1.414 sec/step)\n",
            "INFO:tensorflow:global step 4253: loss = 1.9182 (0.554 sec/step)\n",
            "I1208 00:56:58.830613 140583164200832 learning.py:507] global step 4253: loss = 1.9182 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 4254: loss = 2.0822 (1.816 sec/step)\n",
            "I1208 00:57:00.648946 140583164200832 learning.py:507] global step 4254: loss = 2.0822 (1.816 sec/step)\n",
            "INFO:tensorflow:global step 4255: loss = 1.6421 (1.134 sec/step)\n",
            "I1208 00:57:01.784907 140583164200832 learning.py:507] global step 4255: loss = 1.6421 (1.134 sec/step)\n",
            "INFO:tensorflow:global step 4256: loss = 1.7075 (0.724 sec/step)\n",
            "I1208 00:57:02.653064 140583164200832 learning.py:507] global step 4256: loss = 1.7075 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 4257: loss = 2.6771 (1.316 sec/step)\n",
            "I1208 00:57:04.125222 140583164200832 learning.py:507] global step 4257: loss = 2.6771 (1.316 sec/step)\n",
            "INFO:tensorflow:global step 4258: loss = 1.5772 (0.561 sec/step)\n",
            "I1208 00:57:04.688274 140583164200832 learning.py:507] global step 4258: loss = 1.5772 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 4259: loss = 2.3214 (0.953 sec/step)\n",
            "I1208 00:57:05.707397 140583164200832 learning.py:507] global step 4259: loss = 2.3214 (0.953 sec/step)\n",
            "INFO:tensorflow:global step 4260: loss = 1.9594 (2.005 sec/step)\n",
            "I1208 00:57:07.809455 140583164200832 learning.py:507] global step 4260: loss = 1.9594 (2.005 sec/step)\n",
            "INFO:tensorflow:global step 4261: loss = 1.5336 (0.524 sec/step)\n",
            "I1208 00:57:08.334734 140583164200832 learning.py:507] global step 4261: loss = 1.5336 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 4262: loss = 1.5534 (1.557 sec/step)\n",
            "I1208 00:57:09.906386 140583164200832 learning.py:507] global step 4262: loss = 1.5534 (1.557 sec/step)\n",
            "INFO:tensorflow:global step 4263: loss = 1.9050 (0.563 sec/step)\n",
            "I1208 00:57:10.616619 140583164200832 learning.py:507] global step 4263: loss = 1.9050 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 4264: loss = 2.0743 (1.314 sec/step)\n",
            "I1208 00:57:12.262477 140583164200832 learning.py:507] global step 4264: loss = 2.0743 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 4265: loss = 1.6666 (0.573 sec/step)\n",
            "I1208 00:57:12.837798 140583164200832 learning.py:507] global step 4265: loss = 1.6666 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4266: loss = 2.1100 (0.934 sec/step)\n",
            "I1208 00:57:13.797161 140583164200832 learning.py:507] global step 4266: loss = 2.1100 (0.934 sec/step)\n",
            "INFO:tensorflow:global step 4267: loss = 2.7665 (1.903 sec/step)\n",
            "I1208 00:57:15.852761 140583164200832 learning.py:507] global step 4267: loss = 2.7665 (1.903 sec/step)\n",
            "INFO:tensorflow:global step 4268: loss = 1.5961 (0.735 sec/step)\n",
            "I1208 00:57:16.728434 140583164200832 learning.py:507] global step 4268: loss = 1.5961 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 4269: loss = 1.9506 (0.740 sec/step)\n",
            "I1208 00:57:17.625777 140583164200832 learning.py:507] global step 4269: loss = 1.9506 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 4270: loss = 1.6846 (1.445 sec/step)\n",
            "I1208 00:57:19.241272 140583164200832 learning.py:507] global step 4270: loss = 1.6846 (1.445 sec/step)\n",
            "INFO:tensorflow:global step 4271: loss = 1.8088 (1.039 sec/step)\n",
            "I1208 00:57:20.282273 140583164200832 learning.py:507] global step 4271: loss = 1.8088 (1.039 sec/step)\n",
            "INFO:tensorflow:global step 4272: loss = 1.8713 (1.139 sec/step)\n",
            "I1208 00:57:21.423196 140583164200832 learning.py:507] global step 4272: loss = 1.8713 (1.139 sec/step)\n",
            "INFO:tensorflow:global step 4273: loss = 1.5385 (0.566 sec/step)\n",
            "I1208 00:57:22.155320 140583164200832 learning.py:507] global step 4273: loss = 1.5385 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 4274: loss = 2.1856 (1.265 sec/step)\n",
            "I1208 00:57:23.671610 140583164200832 learning.py:507] global step 4274: loss = 2.1856 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 4275: loss = 1.7193 (1.096 sec/step)\n",
            "I1208 00:57:24.769645 140583164200832 learning.py:507] global step 4275: loss = 1.7193 (1.096 sec/step)\n",
            "INFO:tensorflow:global step 4276: loss = 1.4860 (0.562 sec/step)\n",
            "I1208 00:57:25.513955 140583164200832 learning.py:507] global step 4276: loss = 1.4860 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 4277: loss = 1.7749 (1.234 sec/step)\n",
            "I1208 00:57:27.075449 140583164200832 learning.py:507] global step 4277: loss = 1.7749 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 4278: loss = 1.9486 (1.149 sec/step)\n",
            "I1208 00:57:28.225835 140583164200832 learning.py:507] global step 4278: loss = 1.9486 (1.149 sec/step)\n",
            "INFO:tensorflow:global step 4279: loss = 1.4896 (0.718 sec/step)\n",
            "I1208 00:57:29.092123 140583164200832 learning.py:507] global step 4279: loss = 1.4896 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 4280: loss = 1.5810 (1.153 sec/step)\n",
            "I1208 00:57:30.356631 140583164200832 learning.py:507] global step 4280: loss = 1.5810 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 4281: loss = 1.7484 (0.505 sec/step)\n",
            "I1208 00:57:30.863280 140583164200832 learning.py:507] global step 4281: loss = 1.7484 (0.505 sec/step)\n",
            "INFO:tensorflow:global step 4282: loss = 1.8765 (1.751 sec/step)\n",
            "I1208 00:57:32.615935 140583164200832 learning.py:507] global step 4282: loss = 1.8765 (1.751 sec/step)\n",
            "INFO:tensorflow:global step 4283: loss = 1.7812 (0.537 sec/step)\n",
            "I1208 00:57:33.154958 140583164200832 learning.py:507] global step 4283: loss = 1.7812 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 4284: loss = 1.9274 (1.600 sec/step)\n",
            "I1208 00:57:34.774453 140583164200832 learning.py:507] global step 4284: loss = 1.9274 (1.600 sec/step)\n",
            "INFO:tensorflow:global step 4285: loss = 1.9329 (1.250 sec/step)\n",
            "I1208 00:57:36.025973 140583164200832 learning.py:507] global step 4285: loss = 1.9329 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 4286: loss = 1.9221 (1.016 sec/step)\n",
            "I1208 00:57:37.043778 140583164200832 learning.py:507] global step 4286: loss = 1.9221 (1.016 sec/step)\n",
            "INFO:tensorflow:global step 4287: loss = 1.4979 (0.507 sec/step)\n",
            "I1208 00:57:37.727981 140583164200832 learning.py:507] global step 4287: loss = 1.4979 (0.507 sec/step)\n",
            "INFO:tensorflow:global step 4288: loss = 1.6632 (1.344 sec/step)\n",
            "I1208 00:57:39.332460 140583164200832 learning.py:507] global step 4288: loss = 1.6632 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 4289: loss = 1.7324 (0.537 sec/step)\n",
            "I1208 00:57:40.208424 140583164200832 learning.py:507] global step 4289: loss = 1.7324 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 4290: loss = 1.6088 (1.236 sec/step)\n",
            "I1208 00:57:41.599521 140583164200832 learning.py:507] global step 4290: loss = 1.6088 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 4291: loss = 2.1241 (1.151 sec/step)\n",
            "I1208 00:57:43.331545 140583164200832 learning.py:507] global step 4291: loss = 2.1241 (1.151 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 4291.\n",
            "I1208 00:57:44.817958 140579459213056 supervisor.py:1050] Recording summary at step 4291.\n",
            "INFO:tensorflow:global step 4292: loss = 1.6155 (1.611 sec/step)\n",
            "I1208 00:57:45.054819 140583164200832 learning.py:507] global step 4292: loss = 1.6155 (1.611 sec/step)\n",
            "INFO:tensorflow:global step 4293: loss = 1.8829 (0.597 sec/step)\n",
            "I1208 00:57:45.838493 140583164200832 learning.py:507] global step 4293: loss = 1.8829 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 4294: loss = 1.3502 (1.059 sec/step)\n",
            "I1208 00:57:47.003249 140583164200832 learning.py:507] global step 4294: loss = 1.3502 (1.059 sec/step)\n",
            "INFO:tensorflow:global step 4295: loss = 1.4858 (1.320 sec/step)\n",
            "I1208 00:57:48.325653 140583164200832 learning.py:507] global step 4295: loss = 1.4858 (1.320 sec/step)\n",
            "INFO:tensorflow:global step 4296: loss = 1.7525 (0.480 sec/step)\n",
            "I1208 00:57:48.807220 140583164200832 learning.py:507] global step 4296: loss = 1.7525 (0.480 sec/step)\n",
            "INFO:tensorflow:global step 4297: loss = 2.0274 (1.735 sec/step)\n",
            "I1208 00:57:50.544334 140583164200832 learning.py:507] global step 4297: loss = 2.0274 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 4298: loss = 1.4366 (1.070 sec/step)\n",
            "I1208 00:57:51.615538 140583164200832 learning.py:507] global step 4298: loss = 1.4366 (1.070 sec/step)\n",
            "INFO:tensorflow:global step 4299: loss = 1.6641 (0.672 sec/step)\n",
            "I1208 00:57:52.470670 140583164200832 learning.py:507] global step 4299: loss = 1.6641 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 4300: loss = 1.5785 (1.280 sec/step)\n",
            "I1208 00:57:53.958501 140583164200832 learning.py:507] global step 4300: loss = 1.5785 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 4301: loss = 2.1633 (1.146 sec/step)\n",
            "I1208 00:57:55.106566 140583164200832 learning.py:507] global step 4301: loss = 2.1633 (1.146 sec/step)\n",
            "INFO:tensorflow:global step 4302: loss = 1.5814 (0.818 sec/step)\n",
            "I1208 00:57:56.152546 140583164200832 learning.py:507] global step 4302: loss = 1.5814 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 4303: loss = 1.5697 (0.701 sec/step)\n",
            "I1208 00:57:57.183242 140583164200832 learning.py:507] global step 4303: loss = 1.5697 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 4304: loss = 1.8574 (1.258 sec/step)\n",
            "I1208 00:57:58.560027 140583164200832 learning.py:507] global step 4304: loss = 1.8574 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 4305: loss = 1.6651 (0.530 sec/step)\n",
            "I1208 00:57:59.411591 140583164200832 learning.py:507] global step 4305: loss = 1.6651 (0.530 sec/step)\n",
            "INFO:tensorflow:global step 4306: loss = 2.1477 (1.399 sec/step)\n",
            "I1208 00:58:01.020447 140583164200832 learning.py:507] global step 4306: loss = 2.1477 (1.399 sec/step)\n",
            "INFO:tensorflow:global step 4307: loss = 1.6405 (0.706 sec/step)\n",
            "I1208 00:58:01.975102 140583164200832 learning.py:507] global step 4307: loss = 1.6405 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 4308: loss = 1.6334 (0.684 sec/step)\n",
            "I1208 00:58:02.749501 140583164200832 learning.py:507] global step 4308: loss = 1.6334 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 4309: loss = 1.8013 (1.620 sec/step)\n",
            "I1208 00:58:04.391526 140583164200832 learning.py:507] global step 4309: loss = 1.8013 (1.620 sec/step)\n",
            "INFO:tensorflow:global step 4310: loss = 1.8445 (0.491 sec/step)\n",
            "I1208 00:58:04.884705 140583164200832 learning.py:507] global step 4310: loss = 1.8445 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 4311: loss = 1.2750 (1.806 sec/step)\n",
            "I1208 00:58:06.693057 140583164200832 learning.py:507] global step 4311: loss = 1.2750 (1.806 sec/step)\n",
            "INFO:tensorflow:global step 4312: loss = 1.8790 (0.751 sec/step)\n",
            "I1208 00:58:07.681149 140583164200832 learning.py:507] global step 4312: loss = 1.8790 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 4313: loss = 1.6061 (1.187 sec/step)\n",
            "I1208 00:58:09.023625 140583164200832 learning.py:507] global step 4313: loss = 1.6061 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 4314: loss = 1.5848 (0.567 sec/step)\n",
            "I1208 00:58:09.842261 140583164200832 learning.py:507] global step 4314: loss = 1.5848 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 4315: loss = 1.4498 (0.711 sec/step)\n",
            "I1208 00:58:11.057099 140583164200832 learning.py:507] global step 4315: loss = 1.4498 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 4316: loss = 1.8296 (1.320 sec/step)\n",
            "I1208 00:58:12.458929 140583164200832 learning.py:507] global step 4316: loss = 1.8296 (1.320 sec/step)\n",
            "INFO:tensorflow:global step 4317: loss = 1.3586 (0.678 sec/step)\n",
            "I1208 00:58:13.277015 140583164200832 learning.py:507] global step 4317: loss = 1.3586 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 4318: loss = 2.1177 (0.541 sec/step)\n",
            "I1208 00:58:13.985453 140583164200832 learning.py:507] global step 4318: loss = 2.1177 (0.541 sec/step)\n",
            "INFO:tensorflow:global step 4319: loss = 1.7987 (0.773 sec/step)\n",
            "I1208 00:58:14.945365 140583164200832 learning.py:507] global step 4319: loss = 1.7987 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 4320: loss = 1.6740 (1.559 sec/step)\n",
            "I1208 00:58:16.825639 140583164200832 learning.py:507] global step 4320: loss = 1.6740 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 4321: loss = 2.2391 (0.654 sec/step)\n",
            "I1208 00:58:17.545977 140583164200832 learning.py:507] global step 4321: loss = 2.2391 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 4322: loss = 1.6373 (0.862 sec/step)\n",
            "I1208 00:58:18.929383 140583164200832 learning.py:507] global step 4322: loss = 1.6373 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 4323: loss = 1.5395 (0.595 sec/step)\n",
            "I1208 00:58:19.811330 140583164200832 learning.py:507] global step 4323: loss = 1.5395 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 4324: loss = 1.6322 (0.697 sec/step)\n",
            "I1208 00:58:20.890695 140583164200832 learning.py:507] global step 4324: loss = 1.6322 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 4325: loss = 1.8492 (0.500 sec/step)\n",
            "I1208 00:58:21.553885 140583164200832 learning.py:507] global step 4325: loss = 1.8492 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 4326: loss = 1.2982 (1.686 sec/step)\n",
            "I1208 00:58:23.242541 140583164200832 learning.py:507] global step 4326: loss = 1.2982 (1.686 sec/step)\n",
            "INFO:tensorflow:global step 4327: loss = 1.6677 (0.570 sec/step)\n",
            "I1208 00:58:23.917148 140583164200832 learning.py:507] global step 4327: loss = 1.6677 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 4328: loss = 2.3820 (1.510 sec/step)\n",
            "I1208 00:58:25.478938 140583164200832 learning.py:507] global step 4328: loss = 2.3820 (1.510 sec/step)\n",
            "INFO:tensorflow:global step 4329: loss = 2.0556 (0.472 sec/step)\n",
            "I1208 00:58:25.952790 140583164200832 learning.py:507] global step 4329: loss = 2.0556 (0.472 sec/step)\n",
            "INFO:tensorflow:global step 4330: loss = 1.2528 (1.667 sec/step)\n",
            "I1208 00:58:27.621545 140583164200832 learning.py:507] global step 4330: loss = 1.2528 (1.667 sec/step)\n",
            "INFO:tensorflow:global step 4331: loss = 1.1863 (0.521 sec/step)\n",
            "I1208 00:58:28.471131 140583164200832 learning.py:507] global step 4331: loss = 1.1863 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 4332: loss = 1.8068 (0.585 sec/step)\n",
            "I1208 00:58:29.504356 140583164200832 learning.py:507] global step 4332: loss = 1.8068 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 4333: loss = 1.4310 (1.420 sec/step)\n",
            "I1208 00:58:31.010854 140583164200832 learning.py:507] global step 4333: loss = 1.4310 (1.420 sec/step)\n",
            "INFO:tensorflow:global step 4334: loss = 1.6763 (0.574 sec/step)\n",
            "I1208 00:58:31.944751 140583164200832 learning.py:507] global step 4334: loss = 1.6763 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 4335: loss = 1.8707 (1.248 sec/step)\n",
            "I1208 00:58:33.322500 140583164200832 learning.py:507] global step 4335: loss = 1.8707 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 4336: loss = 1.7391 (0.764 sec/step)\n",
            "I1208 00:58:34.324216 140583164200832 learning.py:507] global step 4336: loss = 1.7391 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 4337: loss = 2.1142 (1.149 sec/step)\n",
            "I1208 00:58:35.546803 140583164200832 learning.py:507] global step 4337: loss = 2.1142 (1.149 sec/step)\n",
            "INFO:tensorflow:global step 4338: loss = 1.7137 (0.557 sec/step)\n",
            "I1208 00:58:36.273447 140583164200832 learning.py:507] global step 4338: loss = 1.7137 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 4339: loss = 1.8874 (1.451 sec/step)\n",
            "I1208 00:58:37.785683 140583164200832 learning.py:507] global step 4339: loss = 1.8874 (1.451 sec/step)\n",
            "INFO:tensorflow:global step 4340: loss = 1.6284 (1.093 sec/step)\n",
            "I1208 00:58:38.880864 140583164200832 learning.py:507] global step 4340: loss = 1.6284 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 4341: loss = 1.2449 (0.740 sec/step)\n",
            "I1208 00:58:39.822098 140583164200832 learning.py:507] global step 4341: loss = 1.2449 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 4342: loss = 1.6708 (1.368 sec/step)\n",
            "I1208 00:58:41.232386 140583164200832 learning.py:507] global step 4342: loss = 1.6708 (1.368 sec/step)\n",
            "INFO:tensorflow:global step 4343: loss = 1.7719 (0.569 sec/step)\n",
            "I1208 00:58:42.169707 140583164200832 learning.py:507] global step 4343: loss = 1.7719 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 4344: loss = 1.5129 (1.213 sec/step)\n",
            "I1208 00:58:43.485743 140583164200832 learning.py:507] global step 4344: loss = 1.5129 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 4345: loss = 1.7266 (0.600 sec/step)\n",
            "I1208 00:58:44.461702 140583164200832 learning.py:507] global step 4345: loss = 1.7266 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 4346: loss = 1.3095 (1.248 sec/step)\n",
            "I1208 00:58:45.753794 140583164200832 learning.py:507] global step 4346: loss = 1.3095 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 4347: loss = 2.4046 (0.724 sec/step)\n",
            "I1208 00:58:46.766623 140583164200832 learning.py:507] global step 4347: loss = 2.4046 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 4348: loss = 1.8672 (1.267 sec/step)\n",
            "I1208 00:58:48.098705 140583164200832 learning.py:507] global step 4348: loss = 1.8672 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 4349: loss = 1.5058 (0.576 sec/step)\n",
            "I1208 00:58:48.936939 140583164200832 learning.py:507] global step 4349: loss = 1.5058 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 4350: loss = 1.7600 (1.293 sec/step)\n",
            "I1208 00:58:50.331347 140583164200832 learning.py:507] global step 4350: loss = 1.7600 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 4351: loss = 1.8709 (0.613 sec/step)\n",
            "I1208 00:58:50.976527 140583164200832 learning.py:507] global step 4351: loss = 1.8709 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 4352: loss = 1.7792 (1.544 sec/step)\n",
            "I1208 00:58:52.545733 140583164200832 learning.py:507] global step 4352: loss = 1.7792 (1.544 sec/step)\n",
            "INFO:tensorflow:global step 4353: loss = 1.5921 (0.602 sec/step)\n",
            "I1208 00:58:53.327275 140583164200832 learning.py:507] global step 4353: loss = 1.5921 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 4354: loss = 1.9579 (1.096 sec/step)\n",
            "I1208 00:58:54.665662 140583164200832 learning.py:507] global step 4354: loss = 1.9579 (1.096 sec/step)\n",
            "INFO:tensorflow:global step 4355: loss = 1.7813 (0.556 sec/step)\n",
            "I1208 00:58:55.453751 140583164200832 learning.py:507] global step 4355: loss = 1.7813 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 4356: loss = 1.7795 (1.344 sec/step)\n",
            "I1208 00:58:56.959902 140583164200832 learning.py:507] global step 4356: loss = 1.7795 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 4357: loss = 2.1551 (0.605 sec/step)\n",
            "I1208 00:58:57.566910 140583164200832 learning.py:507] global step 4357: loss = 2.1551 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 4358: loss = 2.0453 (1.756 sec/step)\n",
            "I1208 00:58:59.325105 140583164200832 learning.py:507] global step 4358: loss = 2.0453 (1.756 sec/step)\n",
            "INFO:tensorflow:global step 4359: loss = 1.7556 (0.675 sec/step)\n",
            "I1208 00:59:00.279737 140583164200832 learning.py:507] global step 4359: loss = 1.7556 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 4360: loss = 2.0854 (0.617 sec/step)\n",
            "I1208 00:59:00.907255 140583164200832 learning.py:507] global step 4360: loss = 2.0854 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 4361: loss = 2.1161 (1.655 sec/step)\n",
            "I1208 00:59:02.564787 140583164200832 learning.py:507] global step 4361: loss = 2.1161 (1.655 sec/step)\n",
            "INFO:tensorflow:global step 4362: loss = 1.8193 (0.548 sec/step)\n",
            "I1208 00:59:03.244471 140583164200832 learning.py:507] global step 4362: loss = 1.8193 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 4363: loss = 2.5228 (1.692 sec/step)\n",
            "I1208 00:59:04.971123 140583164200832 learning.py:507] global step 4363: loss = 2.5228 (1.692 sec/step)\n",
            "INFO:tensorflow:global step 4364: loss = 1.3504 (0.491 sec/step)\n",
            "I1208 00:59:05.464520 140583164200832 learning.py:507] global step 4364: loss = 1.3504 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 4365: loss = 1.6246 (2.330 sec/step)\n",
            "I1208 00:59:07.797170 140583164200832 learning.py:507] global step 4365: loss = 1.6246 (2.330 sec/step)\n",
            "INFO:tensorflow:global step 4366: loss = 2.0648 (0.514 sec/step)\n",
            "I1208 00:59:08.313309 140583164200832 learning.py:507] global step 4366: loss = 2.0648 (0.514 sec/step)\n",
            "INFO:tensorflow:global step 4367: loss = 1.6273 (1.907 sec/step)\n",
            "I1208 00:59:10.222987 140583164200832 learning.py:507] global step 4367: loss = 1.6273 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 4368: loss = 1.4686 (0.543 sec/step)\n",
            "I1208 00:59:10.768332 140583164200832 learning.py:507] global step 4368: loss = 1.4686 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 4369: loss = 1.9123 (1.691 sec/step)\n",
            "I1208 00:59:12.481863 140583164200832 learning.py:507] global step 4369: loss = 1.9123 (1.691 sec/step)\n",
            "INFO:tensorflow:global step 4370: loss = 1.6433 (1.388 sec/step)\n",
            "I1208 00:59:13.919458 140583164200832 learning.py:507] global step 4370: loss = 1.6433 (1.388 sec/step)\n",
            "INFO:tensorflow:global step 4371: loss = 1.3790 (0.518 sec/step)\n",
            "I1208 00:59:14.439404 140583164200832 learning.py:507] global step 4371: loss = 1.3790 (0.518 sec/step)\n",
            "INFO:tensorflow:global step 4372: loss = 1.4242 (1.885 sec/step)\n",
            "I1208 00:59:16.325602 140583164200832 learning.py:507] global step 4372: loss = 1.4242 (1.885 sec/step)\n",
            "INFO:tensorflow:global step 4373: loss = 1.5478 (0.585 sec/step)\n",
            "I1208 00:59:17.062300 140583164200832 learning.py:507] global step 4373: loss = 1.5478 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 4374: loss = 2.1468 (1.952 sec/step)\n",
            "I1208 00:59:19.409898 140583164200832 learning.py:507] global step 4374: loss = 2.1468 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 4375: loss = 1.3568 (0.524 sec/step)\n",
            "I1208 00:59:19.935891 140583164200832 learning.py:507] global step 4375: loss = 1.3568 (0.524 sec/step)\n",
            "INFO:tensorflow:global step 4376: loss = 1.9651 (1.689 sec/step)\n",
            "I1208 00:59:21.626886 140583164200832 learning.py:507] global step 4376: loss = 1.9651 (1.689 sec/step)\n",
            "INFO:tensorflow:global step 4377: loss = 1.4147 (0.596 sec/step)\n",
            "I1208 00:59:22.224058 140583164200832 learning.py:507] global step 4377: loss = 1.4147 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 4378: loss = 1.7215 (1.491 sec/step)\n",
            "I1208 00:59:23.896303 140583164200832 learning.py:507] global step 4378: loss = 1.7215 (1.491 sec/step)\n",
            "INFO:tensorflow:global step 4379: loss = 1.9568 (1.433 sec/step)\n",
            "I1208 00:59:25.551642 140583164200832 learning.py:507] global step 4379: loss = 1.9568 (1.433 sec/step)\n",
            "INFO:tensorflow:global step 4380: loss = 1.6717 (0.723 sec/step)\n",
            "I1208 00:59:26.292417 140583164200832 learning.py:507] global step 4380: loss = 1.6717 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 4381: loss = 2.0622 (0.770 sec/step)\n",
            "I1208 00:59:27.537369 140583164200832 learning.py:507] global step 4381: loss = 2.0622 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 4382: loss = 1.7672 (1.102 sec/step)\n",
            "I1208 00:59:28.761193 140583164200832 learning.py:507] global step 4382: loss = 1.7672 (1.102 sec/step)\n",
            "INFO:tensorflow:global step 4383: loss = 1.6268 (0.566 sec/step)\n",
            "I1208 00:59:29.652932 140583164200832 learning.py:507] global step 4383: loss = 1.6268 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 4384: loss = 1.5946 (1.339 sec/step)\n",
            "I1208 00:59:31.010695 140583164200832 learning.py:507] global step 4384: loss = 1.5946 (1.339 sec/step)\n",
            "INFO:tensorflow:global step 4385: loss = 1.9218 (0.585 sec/step)\n",
            "I1208 00:59:31.631300 140583164200832 learning.py:507] global step 4385: loss = 1.9218 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 4386: loss = 1.6311 (1.596 sec/step)\n",
            "I1208 00:59:33.281705 140583164200832 learning.py:507] global step 4386: loss = 1.6311 (1.596 sec/step)\n",
            "INFO:tensorflow:global step 4387: loss = 2.1382 (0.484 sec/step)\n",
            "I1208 00:59:33.766850 140583164200832 learning.py:507] global step 4387: loss = 2.1382 (0.484 sec/step)\n",
            "INFO:tensorflow:global step 4388: loss = 1.7926 (0.641 sec/step)\n",
            "I1208 00:59:34.615667 140583164200832 learning.py:507] global step 4388: loss = 1.7926 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 4389: loss = 1.4381 (1.665 sec/step)\n",
            "I1208 00:59:36.536715 140583164200832 learning.py:507] global step 4389: loss = 1.4381 (1.665 sec/step)\n",
            "INFO:tensorflow:global step 4390: loss = 1.5105 (1.035 sec/step)\n",
            "I1208 00:59:37.573843 140583164200832 learning.py:507] global step 4390: loss = 1.5105 (1.035 sec/step)\n",
            "INFO:tensorflow:global step 4391: loss = 1.7794 (0.629 sec/step)\n",
            "I1208 00:59:38.241944 140583164200832 learning.py:507] global step 4391: loss = 1.7794 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 4392: loss = 1.9852 (1.355 sec/step)\n",
            "I1208 00:59:39.773779 140583164200832 learning.py:507] global step 4392: loss = 1.9852 (1.355 sec/step)\n",
            "INFO:tensorflow:global step 4393: loss = 1.2225 (0.637 sec/step)\n",
            "I1208 00:59:40.619185 140583164200832 learning.py:507] global step 4393: loss = 1.2225 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 4394: loss = 1.7505 (1.073 sec/step)\n",
            "I1208 00:59:41.880381 140583164200832 learning.py:507] global step 4394: loss = 1.7505 (1.073 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1208 00:59:42.077053 140579434034944 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 4394.\n",
            "I1208 00:59:44.501291 140579459213056 supervisor.py:1050] Recording summary at step 4394.\n",
            "INFO:tensorflow:global step 4395: loss = 1.8531 (2.787 sec/step)\n",
            "I1208 00:59:44.674211 140583164200832 learning.py:507] global step 4395: loss = 1.8531 (2.787 sec/step)\n",
            "INFO:tensorflow:global step 4396: loss = 2.1686 (0.797 sec/step)\n",
            "I1208 00:59:46.134829 140583164200832 learning.py:507] global step 4396: loss = 2.1686 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 4397: loss = 1.7208 (1.547 sec/step)\n",
            "I1208 00:59:47.913154 140583164200832 learning.py:507] global step 4397: loss = 1.7208 (1.547 sec/step)\n",
            "INFO:tensorflow:global step 4398: loss = 1.6563 (0.690 sec/step)\n",
            "I1208 00:59:48.927384 140583164200832 learning.py:507] global step 4398: loss = 1.6563 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 4399: loss = 1.7792 (0.545 sec/step)\n",
            "I1208 00:59:49.556912 140583164200832 learning.py:507] global step 4399: loss = 1.7792 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 4400: loss = 1.3631 (1.696 sec/step)\n",
            "I1208 00:59:51.255275 140583164200832 learning.py:507] global step 4400: loss = 1.3631 (1.696 sec/step)\n",
            "INFO:tensorflow:global step 4401: loss = 1.4439 (0.602 sec/step)\n",
            "I1208 00:59:51.859354 140583164200832 learning.py:507] global step 4401: loss = 1.4439 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 4402: loss = 1.8288 (1.672 sec/step)\n",
            "I1208 00:59:53.533313 140583164200832 learning.py:507] global step 4402: loss = 1.8288 (1.672 sec/step)\n",
            "INFO:tensorflow:global step 4403: loss = 1.5252 (0.531 sec/step)\n",
            "I1208 00:59:54.335831 140583164200832 learning.py:507] global step 4403: loss = 1.5252 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 4404: loss = 1.6796 (1.095 sec/step)\n",
            "I1208 00:59:55.622940 140583164200832 learning.py:507] global step 4404: loss = 1.6796 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 4405: loss = 1.5866 (0.579 sec/step)\n",
            "I1208 00:59:56.203615 140583164200832 learning.py:507] global step 4405: loss = 1.5866 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 4406: loss = 1.2407 (1.665 sec/step)\n",
            "I1208 00:59:57.871953 140583164200832 learning.py:507] global step 4406: loss = 1.2407 (1.665 sec/step)\n",
            "INFO:tensorflow:global step 4407: loss = 2.0180 (0.649 sec/step)\n",
            "I1208 00:59:58.772523 140583164200832 learning.py:507] global step 4407: loss = 2.0180 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 4408: loss = 1.6227 (1.270 sec/step)\n",
            "I1208 01:00:00.046587 140583164200832 learning.py:507] global step 4408: loss = 1.6227 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 4409: loss = 1.8354 (0.621 sec/step)\n",
            "I1208 01:00:00.709024 140583164200832 learning.py:507] global step 4409: loss = 1.8354 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 4410: loss = 1.6144 (1.546 sec/step)\n",
            "I1208 01:00:02.289171 140583164200832 learning.py:507] global step 4410: loss = 1.6144 (1.546 sec/step)\n",
            "INFO:tensorflow:global step 4411: loss = 1.2924 (0.787 sec/step)\n",
            "I1208 01:00:03.179322 140583164200832 learning.py:507] global step 4411: loss = 1.2924 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 4412: loss = 1.5381 (1.252 sec/step)\n",
            "I1208 01:00:04.576882 140583164200832 learning.py:507] global step 4412: loss = 1.5381 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 4413: loss = 1.7950 (0.508 sec/step)\n",
            "I1208 01:00:05.321991 140583164200832 learning.py:507] global step 4413: loss = 1.7950 (0.508 sec/step)\n",
            "INFO:tensorflow:global step 4414: loss = 1.2514 (1.386 sec/step)\n",
            "I1208 01:00:06.724490 140583164200832 learning.py:507] global step 4414: loss = 1.2514 (1.386 sec/step)\n",
            "INFO:tensorflow:global step 4415: loss = 1.2220 (0.559 sec/step)\n",
            "I1208 01:00:07.502951 140583164200832 learning.py:507] global step 4415: loss = 1.2220 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 4416: loss = 1.2421 (0.625 sec/step)\n",
            "I1208 01:00:08.421674 140583164200832 learning.py:507] global step 4416: loss = 1.2421 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 4417: loss = 1.7028 (1.369 sec/step)\n",
            "I1208 01:00:09.926903 140583164200832 learning.py:507] global step 4417: loss = 1.7028 (1.369 sec/step)\n",
            "INFO:tensorflow:global step 4418: loss = 1.6732 (0.621 sec/step)\n",
            "I1208 01:00:10.849214 140583164200832 learning.py:507] global step 4418: loss = 1.6732 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 4419: loss = 1.7147 (0.606 sec/step)\n",
            "I1208 01:00:12.057016 140583164200832 learning.py:507] global step 4419: loss = 1.7147 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 4420: loss = 1.4389 (1.268 sec/step)\n",
            "I1208 01:00:13.369899 140583164200832 learning.py:507] global step 4420: loss = 1.4389 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 4421: loss = 1.8094 (0.617 sec/step)\n",
            "I1208 01:00:14.187044 140583164200832 learning.py:507] global step 4421: loss = 1.8094 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 4422: loss = 1.3426 (0.510 sec/step)\n",
            "I1208 01:00:15.114727 140583164200832 learning.py:507] global step 4422: loss = 1.3426 (0.510 sec/step)\n",
            "INFO:tensorflow:global step 4423: loss = 1.4881 (1.356 sec/step)\n",
            "I1208 01:00:16.764123 140583164200832 learning.py:507] global step 4423: loss = 1.4881 (1.356 sec/step)\n",
            "INFO:tensorflow:global step 4424: loss = 1.5949 (0.706 sec/step)\n",
            "I1208 01:00:17.624011 140583164200832 learning.py:507] global step 4424: loss = 1.5949 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 4425: loss = 1.6252 (0.712 sec/step)\n",
            "I1208 01:00:18.747625 140583164200832 learning.py:507] global step 4425: loss = 1.6252 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 4426: loss = 1.4110 (1.072 sec/step)\n",
            "I1208 01:00:19.917414 140583164200832 learning.py:507] global step 4426: loss = 1.4110 (1.072 sec/step)\n",
            "INFO:tensorflow:global step 4427: loss = 1.9875 (0.627 sec/step)\n",
            "I1208 01:00:20.838017 140583164200832 learning.py:507] global step 4427: loss = 1.9875 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 4428: loss = 2.0688 (0.899 sec/step)\n",
            "I1208 01:00:22.028245 140583164200832 learning.py:507] global step 4428: loss = 2.0688 (0.899 sec/step)\n",
            "INFO:tensorflow:global step 4429: loss = 1.6776 (1.207 sec/step)\n",
            "I1208 01:00:23.289310 140583164200832 learning.py:507] global step 4429: loss = 1.6776 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 4430: loss = 1.6017 (0.573 sec/step)\n",
            "I1208 01:00:24.060908 140583164200832 learning.py:507] global step 4430: loss = 1.6017 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4431: loss = 1.7903 (0.724 sec/step)\n",
            "I1208 01:00:25.205884 140583164200832 learning.py:507] global step 4431: loss = 1.7903 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 4432: loss = 1.7592 (1.270 sec/step)\n",
            "I1208 01:00:26.642343 140583164200832 learning.py:507] global step 4432: loss = 1.7592 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 4433: loss = 1.5500 (0.677 sec/step)\n",
            "I1208 01:00:27.373198 140583164200832 learning.py:507] global step 4433: loss = 1.5500 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 4434: loss = 1.4923 (0.733 sec/step)\n",
            "I1208 01:00:28.584756 140583164200832 learning.py:507] global step 4434: loss = 1.4923 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 4435: loss = 1.6662 (0.646 sec/step)\n",
            "I1208 01:00:29.371705 140583164200832 learning.py:507] global step 4435: loss = 1.6662 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 4436: loss = 1.2677 (1.294 sec/step)\n",
            "I1208 01:00:30.944131 140583164200832 learning.py:507] global step 4436: loss = 1.2677 (1.294 sec/step)\n",
            "INFO:tensorflow:global step 4437: loss = 1.7842 (0.594 sec/step)\n",
            "I1208 01:00:31.836486 140583164200832 learning.py:507] global step 4437: loss = 1.7842 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 4438: loss = 1.3514 (0.613 sec/step)\n",
            "I1208 01:00:32.723129 140583164200832 learning.py:507] global step 4438: loss = 1.3514 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 4439: loss = 1.5764 (1.205 sec/step)\n",
            "I1208 01:00:34.198269 140583164200832 learning.py:507] global step 4439: loss = 1.5764 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 4440: loss = 1.6823 (0.570 sec/step)\n",
            "I1208 01:00:35.028305 140583164200832 learning.py:507] global step 4440: loss = 1.6823 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 4441: loss = 1.7208 (0.576 sec/step)\n",
            "I1208 01:00:36.210704 140583164200832 learning.py:507] global step 4441: loss = 1.7208 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 4442: loss = 2.0705 (0.621 sec/step)\n",
            "I1208 01:00:37.306976 140583164200832 learning.py:507] global step 4442: loss = 2.0705 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 4443: loss = 1.8579 (1.215 sec/step)\n",
            "I1208 01:00:38.523444 140583164200832 learning.py:507] global step 4443: loss = 1.8579 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 4444: loss = 1.9042 (0.675 sec/step)\n",
            "I1208 01:00:39.384413 140583164200832 learning.py:507] global step 4444: loss = 1.9042 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 4445: loss = 1.7663 (0.576 sec/step)\n",
            "I1208 01:00:40.382635 140583164200832 learning.py:507] global step 4445: loss = 1.7663 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 4446: loss = 1.2278 (0.707 sec/step)\n",
            "I1208 01:00:41.388455 140583164200832 learning.py:507] global step 4446: loss = 1.2278 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 4447: loss = 1.6254 (1.249 sec/step)\n",
            "I1208 01:00:42.757085 140583164200832 learning.py:507] global step 4447: loss = 1.6254 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 4448: loss = 1.4489 (1.092 sec/step)\n",
            "I1208 01:00:43.851340 140583164200832 learning.py:507] global step 4448: loss = 1.4489 (1.092 sec/step)\n",
            "INFO:tensorflow:global step 4449: loss = 1.5208 (0.698 sec/step)\n",
            "I1208 01:00:44.653058 140583164200832 learning.py:507] global step 4449: loss = 1.5208 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 4450: loss = 2.2091 (1.294 sec/step)\n",
            "I1208 01:00:46.126101 140583164200832 learning.py:507] global step 4450: loss = 2.2091 (1.294 sec/step)\n",
            "INFO:tensorflow:global step 4451: loss = 1.7913 (0.685 sec/step)\n",
            "I1208 01:00:47.065567 140583164200832 learning.py:507] global step 4451: loss = 1.7913 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 4452: loss = 1.6173 (1.269 sec/step)\n",
            "I1208 01:00:48.403095 140583164200832 learning.py:507] global step 4452: loss = 1.6173 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 4453: loss = 1.9663 (0.648 sec/step)\n",
            "I1208 01:00:49.217963 140583164200832 learning.py:507] global step 4453: loss = 1.9663 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 4454: loss = 1.4440 (1.384 sec/step)\n",
            "I1208 01:00:50.711032 140583164200832 learning.py:507] global step 4454: loss = 1.4440 (1.384 sec/step)\n",
            "INFO:tensorflow:global step 4455: loss = 2.1214 (1.130 sec/step)\n",
            "I1208 01:00:51.842971 140583164200832 learning.py:507] global step 4455: loss = 2.1214 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 4456: loss = 1.6143 (0.610 sec/step)\n",
            "I1208 01:00:52.720394 140583164200832 learning.py:507] global step 4456: loss = 1.6143 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 4457: loss = 1.6087 (0.494 sec/step)\n",
            "I1208 01:00:53.377738 140583164200832 learning.py:507] global step 4457: loss = 1.6087 (0.494 sec/step)\n",
            "INFO:tensorflow:global step 4458: loss = 1.4659 (1.831 sec/step)\n",
            "I1208 01:00:55.210612 140583164200832 learning.py:507] global step 4458: loss = 1.4659 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 4459: loss = 1.7235 (0.604 sec/step)\n",
            "I1208 01:00:55.980756 140583164200832 learning.py:507] global step 4459: loss = 1.7235 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 4460: loss = 1.2322 (1.570 sec/step)\n",
            "I1208 01:00:57.744682 140583164200832 learning.py:507] global step 4460: loss = 1.2322 (1.570 sec/step)\n",
            "INFO:tensorflow:global step 4461: loss = 1.9373 (0.639 sec/step)\n",
            "I1208 01:00:58.603980 140583164200832 learning.py:507] global step 4461: loss = 1.9373 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 4462: loss = 2.2984 (1.255 sec/step)\n",
            "I1208 01:01:00.166351 140583164200832 learning.py:507] global step 4462: loss = 2.2984 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 4463: loss = 2.4532 (0.571 sec/step)\n",
            "I1208 01:01:01.040126 140583164200832 learning.py:507] global step 4463: loss = 2.4532 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 4464: loss = 1.2731 (0.734 sec/step)\n",
            "I1208 01:01:02.102103 140583164200832 learning.py:507] global step 4464: loss = 1.2731 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 4465: loss = 1.8546 (1.367 sec/step)\n",
            "I1208 01:01:03.699136 140583164200832 learning.py:507] global step 4465: loss = 1.8546 (1.367 sec/step)\n",
            "INFO:tensorflow:global step 4466: loss = 1.3708 (0.658 sec/step)\n",
            "I1208 01:01:04.720306 140583164200832 learning.py:507] global step 4466: loss = 1.3708 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 4467: loss = 1.7952 (0.582 sec/step)\n",
            "I1208 01:01:05.344440 140583164200832 learning.py:507] global step 4467: loss = 1.7952 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 4468: loss = 1.7674 (1.778 sec/step)\n",
            "I1208 01:01:07.123980 140583164200832 learning.py:507] global step 4468: loss = 1.7674 (1.778 sec/step)\n",
            "INFO:tensorflow:global step 4469: loss = 1.1985 (0.602 sec/step)\n",
            "I1208 01:01:07.796129 140583164200832 learning.py:507] global step 4469: loss = 1.1985 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 4470: loss = 1.8143 (1.628 sec/step)\n",
            "I1208 01:01:09.426180 140583164200832 learning.py:507] global step 4470: loss = 1.8143 (1.628 sec/step)\n",
            "INFO:tensorflow:global step 4471: loss = 1.8755 (1.077 sec/step)\n",
            "I1208 01:01:10.504409 140583164200832 learning.py:507] global step 4471: loss = 1.8755 (1.077 sec/step)\n",
            "INFO:tensorflow:global step 4472: loss = 1.8477 (0.782 sec/step)\n",
            "I1208 01:01:11.597441 140583164200832 learning.py:507] global step 4472: loss = 1.8477 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 4473: loss = 1.6096 (1.188 sec/step)\n",
            "I1208 01:01:12.797608 140583164200832 learning.py:507] global step 4473: loss = 1.6096 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 4474: loss = 1.9490 (0.613 sec/step)\n",
            "I1208 01:01:13.667156 140583164200832 learning.py:507] global step 4474: loss = 1.9490 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 4475: loss = 2.2650 (0.662 sec/step)\n",
            "I1208 01:01:14.647938 140583164200832 learning.py:507] global step 4475: loss = 2.2650 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 4476: loss = 1.3224 (0.677 sec/step)\n",
            "I1208 01:01:16.021449 140583164200832 learning.py:507] global step 4476: loss = 1.3224 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 4477: loss = 2.0403 (0.633 sec/step)\n",
            "I1208 01:01:16.915016 140583164200832 learning.py:507] global step 4477: loss = 2.0403 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 4478: loss = 1.8384 (1.264 sec/step)\n",
            "I1208 01:01:18.309400 140583164200832 learning.py:507] global step 4478: loss = 1.8384 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 4479: loss = 2.3642 (0.643 sec/step)\n",
            "I1208 01:01:19.036207 140583164200832 learning.py:507] global step 4479: loss = 2.3642 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 4480: loss = 1.5028 (1.408 sec/step)\n",
            "I1208 01:01:20.582182 140583164200832 learning.py:507] global step 4480: loss = 1.5028 (1.408 sec/step)\n",
            "INFO:tensorflow:global step 4481: loss = 1.7744 (0.602 sec/step)\n",
            "I1208 01:01:21.571065 140583164200832 learning.py:507] global step 4481: loss = 1.7744 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 4482: loss = 2.0064 (0.645 sec/step)\n",
            "I1208 01:01:22.481017 140583164200832 learning.py:507] global step 4482: loss = 2.0064 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 4483: loss = 1.7853 (0.648 sec/step)\n",
            "I1208 01:01:23.594588 140583164200832 learning.py:507] global step 4483: loss = 1.7853 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 4484: loss = 1.4800 (0.566 sec/step)\n",
            "I1208 01:01:24.236095 140583164200832 learning.py:507] global step 4484: loss = 1.4800 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 4485: loss = 1.9926 (1.409 sec/step)\n",
            "I1208 01:01:25.729716 140583164200832 learning.py:507] global step 4485: loss = 1.9926 (1.409 sec/step)\n",
            "INFO:tensorflow:global step 4486: loss = 1.4718 (0.712 sec/step)\n",
            "I1208 01:01:26.666196 140583164200832 learning.py:507] global step 4486: loss = 1.4718 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 4487: loss = 1.6678 (1.345 sec/step)\n",
            "I1208 01:01:28.156575 140583164200832 learning.py:507] global step 4487: loss = 1.6678 (1.345 sec/step)\n",
            "INFO:tensorflow:global step 4488: loss = 1.8143 (0.652 sec/step)\n",
            "I1208 01:01:28.849160 140583164200832 learning.py:507] global step 4488: loss = 1.8143 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 4489: loss = 1.2943 (1.465 sec/step)\n",
            "I1208 01:01:30.367314 140583164200832 learning.py:507] global step 4489: loss = 1.2943 (1.465 sec/step)\n",
            "INFO:tensorflow:global step 4490: loss = 1.6681 (0.533 sec/step)\n",
            "I1208 01:01:30.902251 140583164200832 learning.py:507] global step 4490: loss = 1.6681 (0.533 sec/step)\n",
            "INFO:tensorflow:global step 4491: loss = 1.5720 (0.604 sec/step)\n",
            "I1208 01:01:31.508911 140583164200832 learning.py:507] global step 4491: loss = 1.5720 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 4492: loss = 2.0943 (0.771 sec/step)\n",
            "I1208 01:01:32.774212 140583164200832 learning.py:507] global step 4492: loss = 2.0943 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 4493: loss = 2.0994 (2.740 sec/step)\n",
            "I1208 01:01:35.692493 140583164200832 learning.py:507] global step 4493: loss = 2.0994 (2.740 sec/step)\n",
            "INFO:tensorflow:global step 4494: loss = 2.2626 (0.587 sec/step)\n",
            "I1208 01:01:36.432357 140583164200832 learning.py:507] global step 4494: loss = 2.2626 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 4495: loss = 1.4826 (0.570 sec/step)\n",
            "I1208 01:01:37.600303 140583164200832 learning.py:507] global step 4495: loss = 1.4826 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 4496: loss = 2.1940 (1.530 sec/step)\n",
            "I1208 01:01:39.159450 140583164200832 learning.py:507] global step 4496: loss = 2.1940 (1.530 sec/step)\n",
            "INFO:tensorflow:global step 4497: loss = 1.5442 (0.569 sec/step)\n",
            "I1208 01:01:39.974622 140583164200832 learning.py:507] global step 4497: loss = 1.5442 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 4498: loss = 1.8458 (0.739 sec/step)\n",
            "I1208 01:01:41.106677 140583164200832 learning.py:507] global step 4498: loss = 1.8458 (0.739 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 4498.\n",
            "I1208 01:01:44.058183 140579459213056 supervisor.py:1050] Recording summary at step 4498.\n",
            "INFO:tensorflow:global step 4499: loss = 1.7474 (3.034 sec/step)\n",
            "I1208 01:01:44.246067 140583164200832 learning.py:507] global step 4499: loss = 1.7474 (3.034 sec/step)\n",
            "INFO:tensorflow:global step 4500: loss = 1.1033 (0.655 sec/step)\n",
            "I1208 01:01:45.250471 140583164200832 learning.py:507] global step 4500: loss = 1.1033 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 4501: loss = 1.6025 (1.303 sec/step)\n",
            "I1208 01:01:46.566060 140583164200832 learning.py:507] global step 4501: loss = 1.6025 (1.303 sec/step)\n",
            "INFO:tensorflow:global step 4502: loss = 1.4784 (1.192 sec/step)\n",
            "I1208 01:01:47.759049 140583164200832 learning.py:507] global step 4502: loss = 1.4784 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 4503: loss = 1.5062 (0.678 sec/step)\n",
            "I1208 01:01:48.465455 140583164200832 learning.py:507] global step 4503: loss = 1.5062 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 4504: loss = 1.3380 (0.699 sec/step)\n",
            "I1208 01:01:49.755250 140583164200832 learning.py:507] global step 4504: loss = 1.3380 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4505: loss = 1.8903 (0.628 sec/step)\n",
            "I1208 01:01:50.482254 140583164200832 learning.py:507] global step 4505: loss = 1.8903 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 4506: loss = 1.2431 (2.203 sec/step)\n",
            "I1208 01:01:52.686735 140583164200832 learning.py:507] global step 4506: loss = 1.2431 (2.203 sec/step)\n",
            "INFO:tensorflow:global step 4507: loss = 1.7680 (0.497 sec/step)\n",
            "I1208 01:01:53.185914 140583164200832 learning.py:507] global step 4507: loss = 1.7680 (0.497 sec/step)\n",
            "INFO:tensorflow:global step 4508: loss = 1.3809 (0.957 sec/step)\n",
            "I1208 01:01:54.362258 140583164200832 learning.py:507] global step 4508: loss = 1.3809 (0.957 sec/step)\n",
            "INFO:tensorflow:global step 4509: loss = 1.5139 (1.417 sec/step)\n",
            "I1208 01:01:55.965096 140583164200832 learning.py:507] global step 4509: loss = 1.5139 (1.417 sec/step)\n",
            "INFO:tensorflow:global step 4510: loss = 1.2729 (0.722 sec/step)\n",
            "I1208 01:01:56.862928 140583164200832 learning.py:507] global step 4510: loss = 1.2729 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 4511: loss = 1.4566 (1.209 sec/step)\n",
            "I1208 01:01:58.204540 140583164200832 learning.py:507] global step 4511: loss = 1.4566 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 4512: loss = 1.8443 (0.535 sec/step)\n",
            "I1208 01:01:59.034778 140583164200832 learning.py:507] global step 4512: loss = 1.8443 (0.535 sec/step)\n",
            "INFO:tensorflow:global step 4513: loss = 1.2388 (1.798 sec/step)\n",
            "I1208 01:02:00.993720 140583164200832 learning.py:507] global step 4513: loss = 1.2388 (1.798 sec/step)\n",
            "INFO:tensorflow:global step 4514: loss = 1.5613 (0.632 sec/step)\n",
            "I1208 01:02:01.911658 140583164200832 learning.py:507] global step 4514: loss = 1.5613 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 4515: loss = 2.0184 (0.739 sec/step)\n",
            "I1208 01:02:02.943433 140583164200832 learning.py:507] global step 4515: loss = 2.0184 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 4516: loss = 1.6242 (0.572 sec/step)\n",
            "I1208 01:02:03.887697 140583164200832 learning.py:507] global step 4516: loss = 1.6242 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 4517: loss = 1.5247 (1.327 sec/step)\n",
            "I1208 01:02:05.286111 140583164200832 learning.py:507] global step 4517: loss = 1.5247 (1.327 sec/step)\n",
            "INFO:tensorflow:global step 4518: loss = 2.0120 (1.121 sec/step)\n",
            "I1208 01:02:06.408522 140583164200832 learning.py:507] global step 4518: loss = 2.0120 (1.121 sec/step)\n",
            "INFO:tensorflow:global step 4519: loss = 1.7706 (0.597 sec/step)\n",
            "I1208 01:02:07.287831 140583164200832 learning.py:507] global step 4519: loss = 1.7706 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 4520: loss = 1.6759 (0.690 sec/step)\n",
            "I1208 01:02:08.075718 140583164200832 learning.py:507] global step 4520: loss = 1.6759 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 4521: loss = 2.1589 (1.620 sec/step)\n",
            "I1208 01:02:09.767541 140583164200832 learning.py:507] global step 4521: loss = 2.1589 (1.620 sec/step)\n",
            "INFO:tensorflow:global step 4522: loss = 1.5304 (0.619 sec/step)\n",
            "I1208 01:02:10.412999 140583164200832 learning.py:507] global step 4522: loss = 1.5304 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 4523: loss = 1.7497 (0.626 sec/step)\n",
            "I1208 01:02:11.624243 140583164200832 learning.py:507] global step 4523: loss = 1.7497 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 4524: loss = 1.5636 (0.646 sec/step)\n",
            "I1208 01:02:12.776956 140583164200832 learning.py:507] global step 4524: loss = 1.5636 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 4525: loss = 1.7136 (1.267 sec/step)\n",
            "I1208 01:02:14.214361 140583164200832 learning.py:507] global step 4525: loss = 1.7136 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 4526: loss = 1.7575 (0.590 sec/step)\n",
            "I1208 01:02:14.806523 140583164200832 learning.py:507] global step 4526: loss = 1.7575 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 4527: loss = 1.5458 (1.771 sec/step)\n",
            "I1208 01:02:16.579766 140583164200832 learning.py:507] global step 4527: loss = 1.5458 (1.771 sec/step)\n",
            "INFO:tensorflow:global step 4528: loss = 1.4497 (0.773 sec/step)\n",
            "I1208 01:02:17.591688 140583164200832 learning.py:507] global step 4528: loss = 1.4497 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 4529: loss = 2.3100 (1.333 sec/step)\n",
            "I1208 01:02:18.972356 140583164200832 learning.py:507] global step 4529: loss = 2.3100 (1.333 sec/step)\n",
            "INFO:tensorflow:global step 4530: loss = 1.8942 (0.560 sec/step)\n",
            "I1208 01:02:19.534352 140583164200832 learning.py:507] global step 4530: loss = 1.8942 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 4531: loss = 2.1393 (1.525 sec/step)\n",
            "I1208 01:02:21.130677 140583164200832 learning.py:507] global step 4531: loss = 2.1393 (1.525 sec/step)\n",
            "INFO:tensorflow:global step 4532: loss = 1.7378 (0.576 sec/step)\n",
            "I1208 01:02:21.990619 140583164200832 learning.py:507] global step 4532: loss = 1.7378 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 4533: loss = 1.6913 (0.680 sec/step)\n",
            "I1208 01:02:22.709716 140583164200832 learning.py:507] global step 4533: loss = 1.6913 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 4534: loss = 1.5558 (0.859 sec/step)\n",
            "I1208 01:02:23.739790 140583164200832 learning.py:507] global step 4534: loss = 1.5558 (0.859 sec/step)\n",
            "INFO:tensorflow:global step 4535: loss = 1.7075 (1.863 sec/step)\n",
            "I1208 01:02:25.644197 140583164200832 learning.py:507] global step 4535: loss = 1.7075 (1.863 sec/step)\n",
            "INFO:tensorflow:global step 4536: loss = 1.8692 (0.652 sec/step)\n",
            "I1208 01:02:26.511929 140583164200832 learning.py:507] global step 4536: loss = 1.8692 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 4537: loss = 1.9382 (0.632 sec/step)\n",
            "I1208 01:02:27.632480 140583164200832 learning.py:507] global step 4537: loss = 1.9382 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 4538: loss = 2.0251 (0.691 sec/step)\n",
            "I1208 01:02:28.663999 140583164200832 learning.py:507] global step 4538: loss = 2.0251 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 4539: loss = 1.2713 (0.611 sec/step)\n",
            "I1208 01:02:29.676454 140583164200832 learning.py:507] global step 4539: loss = 1.2713 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 4540: loss = 1.7518 (1.366 sec/step)\n",
            "I1208 01:02:31.289719 140583164200832 learning.py:507] global step 4540: loss = 1.7518 (1.366 sec/step)\n",
            "INFO:tensorflow:global step 4541: loss = 2.6018 (0.645 sec/step)\n",
            "I1208 01:02:32.217053 140583164200832 learning.py:507] global step 4541: loss = 2.6018 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 4542: loss = 1.4793 (0.560 sec/step)\n",
            "I1208 01:02:33.017399 140583164200832 learning.py:507] global step 4542: loss = 1.4793 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 4543: loss = 2.0503 (1.296 sec/step)\n",
            "I1208 01:02:34.624760 140583164200832 learning.py:507] global step 4543: loss = 2.0503 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 4544: loss = 2.0648 (0.653 sec/step)\n",
            "I1208 01:02:35.554328 140583164200832 learning.py:507] global step 4544: loss = 2.0648 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 4545: loss = 1.8061 (0.596 sec/step)\n",
            "I1208 01:02:36.475581 140583164200832 learning.py:507] global step 4545: loss = 1.8061 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 4546: loss = 1.8336 (1.373 sec/step)\n",
            "I1208 01:02:38.036003 140583164200832 learning.py:507] global step 4546: loss = 1.8336 (1.373 sec/step)\n",
            "INFO:tensorflow:global step 4547: loss = 1.5647 (1.087 sec/step)\n",
            "I1208 01:02:39.124210 140583164200832 learning.py:507] global step 4547: loss = 1.5647 (1.087 sec/step)\n",
            "INFO:tensorflow:global step 4548: loss = 1.7864 (1.039 sec/step)\n",
            "I1208 01:02:40.164305 140583164200832 learning.py:507] global step 4548: loss = 1.7864 (1.039 sec/step)\n",
            "INFO:tensorflow:global step 4549: loss = 1.2186 (0.636 sec/step)\n",
            "I1208 01:02:40.943127 140583164200832 learning.py:507] global step 4549: loss = 1.2186 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 4550: loss = 1.6977 (1.383 sec/step)\n",
            "I1208 01:02:42.429193 140583164200832 learning.py:507] global step 4550: loss = 1.6977 (1.383 sec/step)\n",
            "INFO:tensorflow:global step 4551: loss = 1.8002 (0.521 sec/step)\n",
            "I1208 01:02:43.389619 140583164200832 learning.py:507] global step 4551: loss = 1.8002 (0.521 sec/step)\n",
            "INFO:tensorflow:global step 4552: loss = 1.3767 (1.187 sec/step)\n",
            "I1208 01:02:44.603911 140583164200832 learning.py:507] global step 4552: loss = 1.3767 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 4553: loss = 1.4883 (0.503 sec/step)\n",
            "I1208 01:02:45.475935 140583164200832 learning.py:507] global step 4553: loss = 1.4883 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 4554: loss = 1.5409 (0.485 sec/step)\n",
            "I1208 01:02:46.118577 140583164200832 learning.py:507] global step 4554: loss = 1.5409 (0.485 sec/step)\n",
            "INFO:tensorflow:global step 4555: loss = 1.4410 (1.747 sec/step)\n",
            "I1208 01:02:47.867421 140583164200832 learning.py:507] global step 4555: loss = 1.4410 (1.747 sec/step)\n",
            "INFO:tensorflow:global step 4556: loss = 1.4922 (0.500 sec/step)\n",
            "I1208 01:02:48.589111 140583164200832 learning.py:507] global step 4556: loss = 1.4922 (0.500 sec/step)\n",
            "INFO:tensorflow:global step 4557: loss = 1.7996 (0.504 sec/step)\n",
            "I1208 01:02:49.759927 140583164200832 learning.py:507] global step 4557: loss = 1.7996 (0.504 sec/step)\n",
            "INFO:tensorflow:global step 4558: loss = 1.9441 (0.697 sec/step)\n",
            "I1208 01:02:50.625319 140583164200832 learning.py:507] global step 4558: loss = 1.9441 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 4559: loss = 1.6309 (0.708 sec/step)\n",
            "I1208 01:02:51.793014 140583164200832 learning.py:507] global step 4559: loss = 1.6309 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 4560: loss = 1.5081 (0.517 sec/step)\n",
            "I1208 01:02:52.487032 140583164200832 learning.py:507] global step 4560: loss = 1.5081 (0.517 sec/step)\n",
            "INFO:tensorflow:global step 4561: loss = 2.0330 (1.800 sec/step)\n",
            "I1208 01:02:54.288832 140583164200832 learning.py:507] global step 4561: loss = 2.0330 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 4562: loss = 1.9162 (0.633 sec/step)\n",
            "I1208 01:02:55.048793 140583164200832 learning.py:507] global step 4562: loss = 1.9162 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 4563: loss = 1.6592 (1.482 sec/step)\n",
            "I1208 01:02:56.754385 140583164200832 learning.py:507] global step 4563: loss = 1.6592 (1.482 sec/step)\n",
            "INFO:tensorflow:global step 4564: loss = 1.1844 (0.503 sec/step)\n",
            "I1208 01:02:57.259074 140583164200832 learning.py:507] global step 4564: loss = 1.1844 (0.503 sec/step)\n",
            "INFO:tensorflow:global step 4565: loss = 1.7277 (1.687 sec/step)\n",
            "I1208 01:02:58.947328 140583164200832 learning.py:507] global step 4565: loss = 1.7277 (1.687 sec/step)\n",
            "INFO:tensorflow:global step 4566: loss = 1.8032 (0.667 sec/step)\n",
            "I1208 01:02:59.761989 140583164200832 learning.py:507] global step 4566: loss = 1.8032 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 4567: loss = 1.4638 (1.529 sec/step)\n",
            "I1208 01:03:01.341113 140583164200832 learning.py:507] global step 4567: loss = 1.4638 (1.529 sec/step)\n",
            "INFO:tensorflow:global step 4568: loss = 1.5143 (0.758 sec/step)\n",
            "I1208 01:03:02.253107 140583164200832 learning.py:507] global step 4568: loss = 1.5143 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 4569: loss = 1.7637 (0.491 sec/step)\n",
            "I1208 01:03:02.886995 140583164200832 learning.py:507] global step 4569: loss = 1.7637 (0.491 sec/step)\n",
            "INFO:tensorflow:global step 4570: loss = 1.3126 (2.507 sec/step)\n",
            "I1208 01:03:05.395633 140583164200832 learning.py:507] global step 4570: loss = 1.3126 (2.507 sec/step)\n",
            "INFO:tensorflow:global step 4571: loss = 2.0173 (0.559 sec/step)\n",
            "I1208 01:03:05.956762 140583164200832 learning.py:507] global step 4571: loss = 2.0173 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 4572: loss = 1.8911 (1.658 sec/step)\n",
            "I1208 01:03:07.641979 140583164200832 learning.py:507] global step 4572: loss = 1.8911 (1.658 sec/step)\n",
            "INFO:tensorflow:global step 4573: loss = 1.6284 (0.739 sec/step)\n",
            "I1208 01:03:08.674983 140583164200832 learning.py:507] global step 4573: loss = 1.6284 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 4574: loss = 1.4818 (0.561 sec/step)\n",
            "I1208 01:03:09.743927 140583164200832 learning.py:507] global step 4574: loss = 1.4818 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 4575: loss = 1.9389 (0.556 sec/step)\n",
            "I1208 01:03:10.398848 140583164200832 learning.py:507] global step 4575: loss = 1.9389 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 4576: loss = 1.4096 (1.898 sec/step)\n",
            "I1208 01:03:12.298150 140583164200832 learning.py:507] global step 4576: loss = 1.4096 (1.898 sec/step)\n",
            "INFO:tensorflow:global step 4577: loss = 2.0290 (0.595 sec/step)\n",
            "I1208 01:03:13.312067 140583164200832 learning.py:507] global step 4577: loss = 2.0290 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 4578: loss = 1.8224 (0.549 sec/step)\n",
            "I1208 01:03:13.980506 140583164200832 learning.py:507] global step 4578: loss = 1.8224 (0.549 sec/step)\n",
            "INFO:tensorflow:global step 4579: loss = 2.1330 (1.727 sec/step)\n",
            "I1208 01:03:15.709251 140583164200832 learning.py:507] global step 4579: loss = 2.1330 (1.727 sec/step)\n",
            "INFO:tensorflow:global step 4580: loss = 1.8220 (0.566 sec/step)\n",
            "I1208 01:03:16.277919 140583164200832 learning.py:507] global step 4580: loss = 1.8220 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 4581: loss = 1.5194 (1.068 sec/step)\n",
            "I1208 01:03:17.657146 140583164200832 learning.py:507] global step 4581: loss = 1.5194 (1.068 sec/step)\n",
            "INFO:tensorflow:global step 4582: loss = 2.0400 (1.068 sec/step)\n",
            "I1208 01:03:18.928221 140583164200832 learning.py:507] global step 4582: loss = 2.0400 (1.068 sec/step)\n",
            "INFO:tensorflow:global step 4583: loss = 1.8153 (0.657 sec/step)\n",
            "I1208 01:03:19.764422 140583164200832 learning.py:507] global step 4583: loss = 1.8153 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 4584: loss = 1.8197 (1.255 sec/step)\n",
            "I1208 01:03:21.171168 140583164200832 learning.py:507] global step 4584: loss = 1.8197 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 4585: loss = 1.7726 (0.653 sec/step)\n",
            "I1208 01:03:21.863394 140583164200832 learning.py:507] global step 4585: loss = 1.7726 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 4586: loss = 1.8950 (0.680 sec/step)\n",
            "I1208 01:03:22.741925 140583164200832 learning.py:507] global step 4586: loss = 1.8950 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 4587: loss = 1.7131 (1.554 sec/step)\n",
            "I1208 01:03:24.439590 140583164200832 learning.py:507] global step 4587: loss = 1.7131 (1.554 sec/step)\n",
            "INFO:tensorflow:global step 4588: loss = 1.6898 (0.699 sec/step)\n",
            "I1208 01:03:25.227092 140583164200832 learning.py:507] global step 4588: loss = 1.6898 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 4589: loss = 1.3430 (1.419 sec/step)\n",
            "I1208 01:03:26.718481 140583164200832 learning.py:507] global step 4589: loss = 1.3430 (1.419 sec/step)\n",
            "INFO:tensorflow:global step 4590: loss = 1.7703 (0.586 sec/step)\n",
            "I1208 01:03:27.306452 140583164200832 learning.py:507] global step 4590: loss = 1.7703 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 4591: loss = 1.5313 (1.596 sec/step)\n",
            "I1208 01:03:29.110343 140583164200832 learning.py:507] global step 4591: loss = 1.5313 (1.596 sec/step)\n",
            "INFO:tensorflow:global step 4592: loss = 2.1571 (2.184 sec/step)\n",
            "I1208 01:03:31.319885 140583164200832 learning.py:507] global step 4592: loss = 2.1571 (2.184 sec/step)\n",
            "INFO:tensorflow:global step 4593: loss = 1.1431 (0.550 sec/step)\n",
            "I1208 01:03:31.871239 140583164200832 learning.py:507] global step 4593: loss = 1.1431 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 4594: loss = 1.2645 (0.579 sec/step)\n",
            "I1208 01:03:32.452433 140583164200832 learning.py:507] global step 4594: loss = 1.2645 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 4595: loss = 1.7321 (1.976 sec/step)\n",
            "I1208 01:03:34.646338 140583164200832 learning.py:507] global step 4595: loss = 1.7321 (1.976 sec/step)\n",
            "INFO:tensorflow:global step 4596: loss = 1.8695 (1.904 sec/step)\n",
            "I1208 01:03:36.634125 140583164200832 learning.py:507] global step 4596: loss = 1.8695 (1.904 sec/step)\n",
            "INFO:tensorflow:global step 4597: loss = 1.4968 (0.768 sec/step)\n",
            "I1208 01:03:37.403992 140583164200832 learning.py:507] global step 4597: loss = 1.4968 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 4598: loss = 1.7113 (2.034 sec/step)\n",
            "I1208 01:03:39.439759 140583164200832 learning.py:507] global step 4598: loss = 1.7113 (2.034 sec/step)\n",
            "INFO:tensorflow:global step 4599: loss = 1.5591 (0.590 sec/step)\n",
            "I1208 01:03:40.398251 140583164200832 learning.py:507] global step 4599: loss = 1.5591 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 4600: loss = 1.6967 (0.539 sec/step)\n",
            "I1208 01:03:41.473009 140583164200832 learning.py:507] global step 4600: loss = 1.6967 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 4601: loss = 1.7577 (1.263 sec/step)\n",
            "I1208 01:03:44.074378 140583164200832 learning.py:507] global step 4601: loss = 1.7577 (1.263 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 4601.\n",
            "I1208 01:03:46.411620 140579459213056 supervisor.py:1050] Recording summary at step 4601.\n",
            "INFO:tensorflow:global step 4602: loss = 2.0924 (2.409 sec/step)\n",
            "I1208 01:03:46.705010 140583164200832 learning.py:507] global step 4602: loss = 2.0924 (2.409 sec/step)\n",
            "INFO:tensorflow:global step 4603: loss = 1.7982 (0.569 sec/step)\n",
            "I1208 01:03:47.600066 140583164200832 learning.py:507] global step 4603: loss = 1.7982 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 4604: loss = 1.8471 (0.697 sec/step)\n",
            "I1208 01:03:48.741125 140583164200832 learning.py:507] global step 4604: loss = 1.8471 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 4605: loss = 1.5575 (1.823 sec/step)\n",
            "I1208 01:03:50.791248 140583164200832 learning.py:507] global step 4605: loss = 1.5575 (1.823 sec/step)\n",
            "INFO:tensorflow:global step 4606: loss = 1.6097 (0.638 sec/step)\n",
            "I1208 01:03:51.711338 140583164200832 learning.py:507] global step 4606: loss = 1.6097 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 4607: loss = 1.4951 (1.408 sec/step)\n",
            "I1208 01:03:53.289352 140583164200832 learning.py:507] global step 4607: loss = 1.4951 (1.408 sec/step)\n",
            "INFO:tensorflow:global step 4608: loss = 1.3710 (0.547 sec/step)\n",
            "I1208 01:03:53.837989 140583164200832 learning.py:507] global step 4608: loss = 1.3710 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 4609: loss = 1.9129 (1.687 sec/step)\n",
            "I1208 01:03:55.527453 140583164200832 learning.py:507] global step 4609: loss = 1.9129 (1.687 sec/step)\n",
            "INFO:tensorflow:global step 4610: loss = 1.2898 (0.615 sec/step)\n",
            "I1208 01:03:56.144441 140583164200832 learning.py:507] global step 4610: loss = 1.2898 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 4611: loss = 2.5525 (0.573 sec/step)\n",
            "I1208 01:03:56.945260 140583164200832 learning.py:507] global step 4611: loss = 2.5525 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 4612: loss = 1.1244 (2.998 sec/step)\n",
            "I1208 01:03:59.945136 140583164200832 learning.py:507] global step 4612: loss = 1.1244 (2.998 sec/step)\n",
            "INFO:tensorflow:global step 4613: loss = 1.7155 (0.626 sec/step)\n",
            "I1208 01:04:00.657568 140583164200832 learning.py:507] global step 4613: loss = 1.7155 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 4614: loss = 1.7316 (1.875 sec/step)\n",
            "I1208 01:04:02.534865 140583164200832 learning.py:507] global step 4614: loss = 1.7316 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 4615: loss = 1.7520 (0.529 sec/step)\n",
            "I1208 01:04:03.065970 140583164200832 learning.py:507] global step 4615: loss = 1.7520 (0.529 sec/step)\n",
            "INFO:tensorflow:global step 4616: loss = 1.6441 (1.810 sec/step)\n",
            "I1208 01:04:04.906609 140583164200832 learning.py:507] global step 4616: loss = 1.6441 (1.810 sec/step)\n",
            "INFO:tensorflow:global step 4617: loss = 2.1400 (0.648 sec/step)\n",
            "I1208 01:04:05.871150 140583164200832 learning.py:507] global step 4617: loss = 2.1400 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 4618: loss = 1.6460 (0.769 sec/step)\n",
            "I1208 01:04:06.814050 140583164200832 learning.py:507] global step 4618: loss = 1.6460 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 4619: loss = 1.7915 (1.766 sec/step)\n",
            "I1208 01:04:08.582617 140583164200832 learning.py:507] global step 4619: loss = 1.7915 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 4620: loss = 1.6343 (0.532 sec/step)\n",
            "I1208 01:04:09.116376 140583164200832 learning.py:507] global step 4620: loss = 1.6343 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 4621: loss = 2.0370 (0.628 sec/step)\n",
            "I1208 01:04:09.991088 140583164200832 learning.py:507] global step 4621: loss = 2.0370 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 4622: loss = 1.1590 (2.520 sec/step)\n",
            "I1208 01:04:12.537721 140583164200832 learning.py:507] global step 4622: loss = 1.1590 (2.520 sec/step)\n",
            "INFO:tensorflow:global step 4623: loss = 1.6959 (0.650 sec/step)\n",
            "I1208 01:04:13.384065 140583164200832 learning.py:507] global step 4623: loss = 1.6959 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 4624: loss = 1.6687 (1.491 sec/step)\n",
            "I1208 01:04:14.934942 140583164200832 learning.py:507] global step 4624: loss = 1.6687 (1.491 sec/step)\n",
            "INFO:tensorflow:global step 4625: loss = 1.6445 (0.721 sec/step)\n",
            "I1208 01:04:15.717022 140583164200832 learning.py:507] global step 4625: loss = 1.6445 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 4626: loss = 1.8522 (1.368 sec/step)\n",
            "I1208 01:04:17.219628 140583164200832 learning.py:507] global step 4626: loss = 1.8522 (1.368 sec/step)\n",
            "INFO:tensorflow:global step 4627: loss = 1.2685 (0.620 sec/step)\n",
            "I1208 01:04:17.894092 140583164200832 learning.py:507] global step 4627: loss = 1.2685 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 4628: loss = 1.9005 (0.734 sec/step)\n",
            "I1208 01:04:18.970603 140583164200832 learning.py:507] global step 4628: loss = 1.9005 (0.734 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 417, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 505, in train_step\n",
            "    if sess.run(train_step_kwargs['should_log']):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "d681ccfb-387e-4af7-ec8e-249980b09483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt-4394 --output_directory inference_graph"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1208 01:05:03.252775 140189205202816 module_wrapper.py:139] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1208 01:05:03.258015 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1208 01:05:03.258215 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1208 01:05:03.288417 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1208 01:05:03.314672 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1208 01:05:03.316376 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1208 01:05:04.731018 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1208 01:05:04.742183 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.742354 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.780152 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.815066 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.850528 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.885552 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1208 01:05:04.920793 140189205202816 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1208 01:05:05.167759 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1208 01:05:05.584050 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W1208 01:05:05.584289 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1208 01:05:05.587237 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W1208 01:05:05.587411 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W1208 01:05:05.588291 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "108 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/5.49m params)\n",
            "  BoxPredictor_0 (--/9.23k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.16k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x512x6, 3.07k/3.07k params)\n",
            "  BoxPredictor_1 (--/36.90k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/24.60k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1024x24, 24.58k/24.58k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/12.30k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1024x12, 12.29k/12.29k params)\n",
            "  BoxPredictor_2 (--/18.47k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "  BoxPredictor_3 (--/9.25k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_4 (--/9.25k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_5 (--/4.64k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "  FeatureExtractor (--/5.41m params)\n",
            "    FeatureExtractor/MobilenetV1 (--/5.41m params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_0 (--/864 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_0/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_10_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_10_pointwise (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_11_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_11_pointwise (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_12_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_12_pointwise (--/524.29k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights (1x1x512x1024, 524.29k/524.29k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_depthwise (--/9.22k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights (3x3x1024x1, 9.22k/9.22k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise (--/1.05m params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights (1x1x1024x1024, 1.05m/1.05m params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256 (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_1_depthwise (--/288 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_1_pointwise (--/2.05k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights (1x1x32x64, 2.05k/2.05k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_2_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_2_pointwise (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_3_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_3_pointwise (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights (1x1x128x128, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_4_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_4_pointwise (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_5_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_5_pointwise (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights (1x1x256x256, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_6_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_6_pointwise (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_7_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_7_pointwise (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_8_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_8_pointwise (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_9_depthwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_9_pointwise (--/262.14k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "108 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/13.71k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1208 01:05:06.306744 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1208 01:05:06.846422 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-12-08 01:05:06.847940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-08 01:05:06.863588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.864220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-08 01:05:06.864519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-08 01:05:06.866208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-08 01:05:06.868088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-08 01:05:06.868484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-08 01:05:06.870629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-08 01:05:06.871980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-08 01:05:06.876426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-08 01:05:06.876546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.877171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.877721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-08 01:05:06.883486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-12-08 01:05:06.883679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d6b9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-08 01:05:06.883709: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-08 01:05:06.982030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.982769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d6b800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-08 01:05:06.982796: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-08 01:05:06.982986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.983553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-08 01:05:06.983611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-08 01:05:06.983628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-08 01:05:06.983643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-08 01:05:06.983657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-08 01:05:06.983672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-08 01:05:06.983685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-08 01:05:06.983700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-08 01:05:06.983752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.984347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.984915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-08 01:05:06.984992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-08 01:05:06.986343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-08 01:05:06.986370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-08 01:05:06.986381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-08 01:05:06.986490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.987151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:06.987726: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-08 01:05:06.987762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-4394\n",
            "I1208 01:05:06.989568 140189205202816 saver.py:1284] Restoring parameters from training/model.ckpt-4394\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W1208 01:05:10.009352 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-12-08 01:05:10.500070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:10.500637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-08 01:05:10.500722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-08 01:05:10.500747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-08 01:05:10.500768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-08 01:05:10.500791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-08 01:05:10.500837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-08 01:05:10.500859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-08 01:05:10.500880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-08 01:05:10.500962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:10.501501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:10.501987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-08 01:05:10.502027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-08 01:05:10.502040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-08 01:05:10.502050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-08 01:05:10.502148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:10.502689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:10.503282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-4394\n",
            "I1208 01:05:10.504664 140189205202816 saver.py:1284] Restoring parameters from training/model.ckpt-4394\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W1208 01:05:10.932011 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W1208 01:05:10.932284 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 199 variables.\n",
            "I1208 01:05:11.175102 140189205202816 graph_util_impl.py:334] Froze 199 variables.\n",
            "INFO:tensorflow:Converted 199 variables to const ops.\n",
            "I1208 01:05:11.248637 140189205202816 graph_util_impl.py:394] Converted 199 variables to const ops.\n",
            "2019-12-08 01:05:11.362386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:11.362978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-08 01:05:11.363079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-08 01:05:11.363102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-08 01:05:11.363124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-08 01:05:11.363143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-08 01:05:11.363167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-08 01:05:11.363186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-08 01:05:11.363205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-08 01:05:11.363288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:11.363831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:11.364306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-08 01:05:11.364348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-08 01:05:11.364362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-08 01:05:11.364378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-08 01:05:11.364466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:11.364999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-08 01:05:11.365492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W1208 01:05:11.652419 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W1208 01:05:11.654558 140189205202816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W1208 01:05:11.655072 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W1208 01:05:11.655270 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W1208 01:05:11.655499 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W1208 01:05:11.655638 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I1208 01:05:11.655920 140189205202816 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I1208 01:05:11.656018 140189205202816 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "I1208 01:05:11.875974 140189205202816 builder_impl.py:425] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1208 01:05:11.895377 140189205202816 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to inference_graph/pipeline.config\n",
            "I1208 01:05:11.895599 140189205202816 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -r /content/models/research/object_detection/inference_graph /gdrive/My\\ Drive/colabfiles/lektion31"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasenbilder190726_01/bild101.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8-djxitLZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasen.mov /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "outputId": "bfa15138-cb08-4976-95f5-47d75c52b129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = 'bild101.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEsCAIAAABi1XKVAAEAAElEQVR4nOz9SY90S5IliInodCcz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\nTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x300 at 0x7F0B617B5BA8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs2cfgcr1AZ",
        "colab_type": "code",
        "outputId": "a16b662a-6013-4f5d-f40f-7a21632333d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "######## Video Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/16/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on a video.\n",
        "# It draws boxes and scores around the objects of interest in each frame\n",
        "# of the video.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import imutils\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "VIDEO_NAME = 'rasen.mov'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "print(CWD_PATH)\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to video\n",
        "PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Open video file\n",
        "video = cv2.VideoCapture(PATH_TO_VIDEO)\n",
        "print(\"ok\")\n",
        "i = 0\n",
        "while(video.isOpened()):\n",
        "\n",
        "    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]\n",
        "    # i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "    ret, frame = video.read()\n",
        "    if ret==True:\n",
        "        \n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={image_tensor: frame_expanded})\n",
        "\n",
        "        # Draw the results of the detection (aka 'visulaize the results')\n",
        "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            frame,\n",
        "            np.squeeze(boxes),\n",
        "            np.squeeze(classes).astype(np.int32),\n",
        "            np.squeeze(scores),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.70)\n",
        "\n",
        "        # All the results have been drawn on the frame, so it's time to display it.\n",
        "        \n",
        "        frame = imutils.resize(frame, 400)\n",
        "        cv2_imshow(frame)\n",
        "        #print(boxes)\n",
        "        time.sleep(1)\n",
        "        clear_output()\n",
        "        \n",
        "\n",
        "        # Press 'q' to quit\n",
        "        #print(i)\n",
        "        #i = i + 1\n",
        "        #if cv2.waitKey(1) == ord('q'):\n",
        "        #    break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Clean up\n",
        "print(\"end1\")\n",
        "#video.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(\"end2\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAADhCAIAAABp1HRLAAEAAElEQVR4nDT9166sW5JmiU2tfuli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\n49IiYgCRdno+jucjJGgap/8D+p+ufo/48I4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x225 at 0x7F0B504B8320>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-89c51d19fa6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m#print(boxes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}