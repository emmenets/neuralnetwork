{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/190813objectdetectionclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "161aed3c-bd48-41ff-8afd-6be62be5a268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "0811119b-6c25-4331-80e9-cddf8d98a6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "89f16707-ddc3-4204-dad2-d85e986f2d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "de095b04-e4ac-408a-d258-1aafa47fdfc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-13 13:49:00--  http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 64.233.182.128, 2607:f8b0:4001:c03::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|64.233.182.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51025348 (49M) [application/x-tar]\n",
            "Saving to: ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’\n",
            "\n",
            "ssdlite_mobilenet_v 100%[===================>]  48.66M   103MB/s    in 0.5s    \n",
            "\n",
            "2019-08-13 13:49:01 (103 MB/s) - ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’ saved [51025348/51025348]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "f5084f96-2551-4dbd-f213-970afad51947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "!git clone https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/58)\u001b[K\rremote: Counting objects:   3% (2/58)\u001b[K\rremote: Counting objects:   5% (3/58)\u001b[K\rremote: Counting objects:   6% (4/58)\u001b[K\rremote: Counting objects:   8% (5/58)\u001b[K\rremote: Counting objects:  10% (6/58)\u001b[K\rremote: Counting objects:  12% (7/58)\u001b[K\rremote: Counting objects:  13% (8/58)\u001b[K\rremote: Counting objects:  15% (9/58)\u001b[K\rremote: Counting objects:  17% (10/58)\u001b[K\rremote: Counting objects:  18% (11/58)\u001b[K\rremote: Counting objects:  20% (12/58)\u001b[K\rremote: Counting objects:  22% (13/58)\u001b[K\rremote: Counting objects:  24% (14/58)\u001b[K\rremote: Counting objects:  25% (15/58)\u001b[K\rremote: Counting objects:  27% (16/58)\u001b[K\rremote: Counting objects:  29% (17/58)\u001b[K\rremote: Counting objects:  31% (18/58)\u001b[K\rremote: Counting objects:  32% (19/58)\u001b[K\rremote: Counting objects:  34% (20/58)\u001b[K\rremote: Counting objects:  36% (21/58)\u001b[K\rremote: Counting objects:  37% (22/58)\u001b[K\rremote: Counting objects:  39% (23/58)\u001b[K\rremote: Counting objects:  41% (24/58)\u001b[K\rremote: Counting objects:  43% (25/58)\u001b[K\rremote: Counting objects:  44% (26/58)\u001b[K\rremote: Counting objects:  46% (27/58)\u001b[K\rremote: Counting objects:  48% (28/58)\u001b[K\rremote: Counting objects:  50% (29/58)\u001b[K\rremote: Counting objects:  51% (30/58)\u001b[K\rremote: Counting objects:  53% (31/58)\u001b[K\rremote: Counting objects:  55% (32/58)\u001b[K\rremote: Counting objects:  56% (33/58)\u001b[K\rremote: Counting objects:  58% (34/58)\u001b[K\rremote: Counting objects:  60% (35/58)\u001b[K\rremote: Counting objects:  62% (36/58)\u001b[K\rremote: Counting objects:  63% (37/58)\u001b[K\rremote: Counting objects:  65% (38/58)\u001b[K\rremote: Counting objects:  67% (39/58)\u001b[K\rremote: Counting objects:  68% (40/58)\u001b[K\rremote: Counting objects:  70% (41/58)\u001b[K\rremote: Counting objects:  72% (42/58)\u001b[K\rremote: Counting objects:  74% (43/58)\u001b[K\rremote: Counting objects:  75% (44/58)\u001b[K\rremote: Counting objects:  77% (45/58)\u001b[K\rremote: Counting objects:  79% (46/58)\u001b[K\rremote: Counting objects:  81% (47/58)\u001b[K\rremote: Counting objects:  82% (48/58)\u001b[K\rremote: Counting objects:  84% (49/58)\u001b[K\rremote: Counting objects:  86% (50/58)\u001b[K\rremote: Counting objects:  87% (51/58)\u001b[K\rremote: Counting objects:  89% (52/58)\u001b[K\rremote: Counting objects:  91% (53/58)\u001b[K\rremote: Counting objects:  93% (54/58)\u001b[K\rremote: Counting objects:  94% (55/58)\u001b[K\rremote: Counting objects:  96% (56/58)\u001b[K\rremote: Counting objects:  98% (57/58)\u001b[K\rremote: Counting objects: 100% (58/58)\u001b[K\rremote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 1103 (delta 34), reused 3 (delta 1), pack-reused 1045\u001b[K\n",
            "Receiving objects: 100% (1103/1103), 57.62 MiB | 56.96 MiB/s, done.\n",
            "Resolving deltas: 100% (549/549), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/doc /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/inference_graph /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/training /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/translate /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_webcam.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/resizer.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test.mov /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test1.JPG /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/README.md /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "b8f71f9b-a2fb-495e-e0be-ba9102f99bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "outputId": "bc809635-1d45-4c26-bec4-b952c156be44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images\n",
        "!mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6n8GqZS-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98GmqrIhe3z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm /content/models/research/object_detection/images/test_labels.csv\n",
        "#!rm /content/models/research/object_detection/images/train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "outputId": "ba87296d-e365-4428-d048-91291208d707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!python3 xml_to_csv.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv99g_MdfhZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/models/research/object_detection/generate_tfrecord.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab_type": "code",
        "outputId": "d4c15f60-f57d-4afe-a994-881f539a75ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "1ef60de5-f307-4d93-ab7b-e851cc4a6291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 13:52:55.067945 139912869296000 deprecation_wrapper.py:119] From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0813 13:52:55.068580 139912869296000 deprecation_wrapper.py:119] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0813 13:52:55.163395 139912869296000 deprecation_wrapper.py:119] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "c6d2f68b-93b0-42d1-d675-8413143d73cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 13:53:01.907594 140064137394048 deprecation_wrapper.py:119] From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0813 13:53:01.908185 140064137394048 deprecation_wrapper.py:119] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0813 13:53:01.992487 140064137394048 deprecation_wrapper.py:119] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "46d91bff-1fdf-43e3-87cf-1fe739e410a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiYCKlkF2eOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7606775-4c5b-4818-d7a0-e295500df82c"
      },
      "source": [
        "!cat /content/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 90\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 3\n",
            "        use_depthwise: true\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      use_depthwise: true\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 24\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 200000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n",
            "  }\n",
            "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n",
            "  }\n",
            "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ayapmyjQt4W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "263fa6b7-a284-40ec-d409-1a58f6154865"
      },
      "source": [
        "%%writefile training/ssdlite_mobilenet_v2_coco.config\n",
        "\n",
        "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 3\n",
        "        use_depthwise: true\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      use_depthwise: true\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting training/ssdlite_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "a0ac724e-d816-43c0-b470-a33cde6a5e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "02fd28d8-549f-43de-8b56-598e763f231c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 14:24:36.824071 140596802582400 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0813 14:24:37.071317 140596802582400 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0813 14:24:37.102043 140596802582400 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0813 14:24:37.119957 140596802582400 deprecation_wrapper.py:119] From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0813 14:24:37.120173 140596802582400 deprecation_wrapper.py:119] From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0813 14:24:37.120955 140596802582400 deprecation_wrapper.py:119] From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0813 14:24:37.121541 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0813 14:24:37.121749 140596802582400 deprecation_wrapper.py:119] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0813 14:24:37.122114 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0813 14:24:37.127825 140596802582400 deprecation_wrapper.py:119] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0813 14:24:37.130167 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0813 14:24:37.134409 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0813 14:24:37.134607 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0813 14:24:37.147864 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0813 14:24:37.149507 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0813 14:24:37.149628 140596802582400 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "W0813 14:24:37.155502 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0813 14:24:37.155643 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0813 14:24:37.179636 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0813 14:24:37.360539 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0813 14:24:37.367700 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0813 14:24:37.371787 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0813 14:24:37.425373 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:196: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0813 14:24:37.437598 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0813 14:24:38.300944 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0813 14:24:38.305149 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0813 14:24:38.306195 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0813 14:24:38.315679 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0813 14:24:39.141885 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0813 14:24:43.343821 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0813 14:24:43.344010 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:24:43.458451 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:24:43.586251 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:24:43.702379 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:24:43.816128 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:24:43.931621 140596802582400 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0813 14:24:48.879716 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0813 14:24:48.881206 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0813 14:24:49.512377 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W0813 14:24:49.513189 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0813 14:24:49.513420 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0813 14:24:49.523012 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0813 14:24:52.102179 140596802582400 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0813 14:24:54.780429 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0813 14:25:00.383860 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0813 14:25:00.808873 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0813 14:25:00.811226 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0813 14:25:00.816515 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0813 14:25:00.828167 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0813 14:25:01.801878 140596802582400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0813 14:25:01.804526 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.804695 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.804767 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.804836 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.804897 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.804954 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805021 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805076 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805128 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805186 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805248 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805304 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805357 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805425 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805477 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805542 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805617 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805669 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805724 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805776 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805824 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.805877 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.805926 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.805974 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806035 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806085 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806133 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806188 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806236 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806308 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806363 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806416 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806467 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806532 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806586 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806655 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806713 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806766 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806818 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.806876 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.806928 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.806989 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807050 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.807099 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.807166 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807233 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.807287 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.807335 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807407 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.807459 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.807520 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807580 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.807650 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.807702 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807757 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.807828 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.807882 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.807940 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808013 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.808069 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.808138 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808196 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.808257 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.808321 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808378 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.808434 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.808514 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808574 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.808649 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.808722 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808783 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.808842 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.808906 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.808964 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.809022 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.809085 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.809144 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.809202 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.809309 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.809386 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.809447 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.809509 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.809567 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.809637 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.809699 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.809756 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.830811 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.830952 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.831032 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.831100 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.831180 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.831247 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.831322 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.831395 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.831466 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.831533 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.831634 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.831711 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.831799 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.831880 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.831972 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.832049 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.832133 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.832211 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.832301 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.832399 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.832478 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.832556 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.832660 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.832741 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.832818 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.832902 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.832982 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.833058 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.833181 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.833272 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.833351 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.833451 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.833525 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.833597 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.833698 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.833775 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.833847 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.833937 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.834013 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.834085 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.834182 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.834299 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.834389 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.834468 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.834542 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.834631 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.834728 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.834805 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.834880 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.834960 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.835035 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.835108 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.835186 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.835269 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.835355 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.835442 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.835513 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.835582 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.835696 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.835773 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.835855 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.835931 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.835999 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.836088 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.836180 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.836264 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.836348 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.836432 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.836507 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.836578 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.836675 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.836751 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.836826 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.836916 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.836992 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.837085 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.837170 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.837249 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.837336 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.837417 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.837495 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.837572 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.837707 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.837792 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.837873 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.837962 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.838043 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.838135 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.838220 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.838318 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.838401 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.838506 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.838593 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.838679 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.838756 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.838826 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.838893 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.838968 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.839058 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.839129 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.839219 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.839307 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.839382 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.839463 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.839536 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.839630 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.839729 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.839815 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.839881 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.839967 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.840039 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.840108 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.840184 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.840254 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.840352 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.840435 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.840507 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.840578 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.840687 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.840765 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.840837 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.840917 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.840991 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.841063 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.841143 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.841216 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.841297 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.841389 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.841476 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.841544 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.841634 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.841706 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.841794 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.841872 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.841944 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.842015 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.842108 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.842185 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.842279 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.842389 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.842473 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.842554 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.842659 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.842744 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.842824 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.842926 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.843009 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.843089 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.843178 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.843267 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.843349 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.843456 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.843532 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.843618 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.843739 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.843820 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.843898 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.843982 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.844061 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.844138 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.844222 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.844312 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.844391 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.844488 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.844569 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.844664 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.844762 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.844838 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.844908 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.844999 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.845070 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.845137 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.845252 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.845340 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.845413 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.845493 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.845567 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.845656 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.845738 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.845813 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.845885 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.845975 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.846051 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.846123 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.846204 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.846286 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.846358 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.846439 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.846512 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.846584 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.846691 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.846775 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.846841 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.846936 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.847011 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.847081 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.847162 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.847235 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.847317 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.847418 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.847490 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.847558 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.847650 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.847720 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.847806 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.847895 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.847965 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.848032 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.848117 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.848190 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.848265 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.848343 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.848414 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.848482 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.848557 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.848644 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.848713 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.848798 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.848888 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.848961 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.849046 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.849120 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.849192 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.849280 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.849357 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.849428 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.849518 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.849594 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.849685 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.849765 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.849840 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.849920 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.849994 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.850064 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.850131 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.850218 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.850299 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.850368 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.850453 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.850523 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.850629 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.850713 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.850786 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.850857 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.850943 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.851018 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.851090 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.851170 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.851245 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.851327 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.851408 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.851481 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.851552 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.851656 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.851736 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.851808 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.851896 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.851986 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.852056 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.852137 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.852208 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.852285 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.852375 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.852459 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.852525 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.852598 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.852687 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.852754 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.852850 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.852923 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.852994 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.853091 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.853162 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.853229 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.853314 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.853385 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.853454 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.853528 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.853597 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.853682 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.853768 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.853840 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.853907 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.853982 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.854051 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.854119 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.854194 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.854270 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.854340 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.854425 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.854497 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.854564 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.854655 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.854727 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.854794 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.854869 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.854939 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.855006 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.855089 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.855160 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.855227 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.855311 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.855381 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.855448 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.855524 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.855594 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.855679 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.855766 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.855838 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.855906 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.855982 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.856074 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.856145 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.856224 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.856307 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.856381 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.856472 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.856548 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.856633 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.856718 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.856792 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.856882 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.856976 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.857050 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.857121 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.857210 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.857293 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.857367 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.857459 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.857546 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.857643 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.857722 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.857793 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.857862 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.857947 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.858039 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.858120 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.858217 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.858309 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.858397 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.858477 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.858550 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.858633 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.858726 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.858802 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.858874 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.858954 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.859026 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.859097 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.859178 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.859282 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.859361 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.859457 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.859538 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.859633 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.859725 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.859804 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.859880 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.859974 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.860048 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.860119 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.860209 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.860294 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.860369 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.860449 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.860523 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.860587 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.860688 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.860763 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.860851 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.860940 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.861014 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.861085 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.861164 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.861237 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.861317 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.861396 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.861470 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.861540 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.861655 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.861761 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.861829 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.861929 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.862004 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.862074 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.862153 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.862228 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.862348 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.862450 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.862534 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.862631 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.862724 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.862806 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.862885 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.862974 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.863055 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.863135 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.863233 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.863327 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.863408 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.863511 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.863590 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.863686 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.863772 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.863851 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.863928 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.864024 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.864106 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.864184 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.864278 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.864357 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.864435 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.864521 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.864616 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.864696 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.864792 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.864873 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.864949 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.865044 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.865118 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.865211 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.865305 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.865386 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.865463 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.865557 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.865655 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.865734 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.865821 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.865900 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.865976 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.866061 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.866140 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.866216 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.866323 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.866408 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.866485 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.866569 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.866668 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.866747 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.866834 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.866912 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.866988 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.867094 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.867192 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.867282 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.867368 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.867448 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.867526 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.867626 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.867718 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.867790 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.867901 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.867983 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.868059 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.868145 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.868224 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.868324 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.868410 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.868490 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.868566 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.868702 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.868788 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.868869 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.868956 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.869038 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.869119 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.869206 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.869294 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.869375 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.869474 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.869568 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.869664 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.869752 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.869832 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.869909 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.869992 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.870070 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.870148 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.870254 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.870342 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.870435 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.870520 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.870599 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.870693 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.870778 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.870857 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.870936 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:01.871032 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0813 14:25:01.871112 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0813 14:25:01.871189 140596802582400 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0813 14:25:02.948505 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-08-13 14:25:04.924267: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-08-13 14:25:04.927441: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1793e680 executing computations on platform Host. Devices:\n",
            "2019-08-13 14:25:04.927494: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-08-13 14:25:04.934268: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-08-13 14:25:05.170966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:05.171557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1793df80 executing computations on platform CUDA. Devices:\n",
            "2019-08-13 14:25:05.171587: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-08-13 14:25:05.171886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:05.172260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-13 14:25:05.188739: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:25:05.380960: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-13 14:25:05.470504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-13 14:25:05.495892: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-13 14:25:05.686342: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-13 14:25:05.788234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-13 14:25:06.159343: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-13 14:25:06.159624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:06.160116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:06.160458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-13 14:25:06.164539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:25:06.166652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-13 14:25:06.166684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-13 14:25:06.166696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-13 14:25:06.170517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:06.171022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:25:06.171404: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-08-13 14:25:06.171448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-08-13 14:25:09.982858: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0813 14:25:10.343960 140596802582400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0813 14:25:10.345845 140596802582400 saver.py:1280] Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "I0813 14:25:10.978751 140596802582400 session_manager.py:500] Running local_init_op.\n",
            "I0813 14:25:11.476335 140596802582400 session_manager.py:502] Done running local_init_op.\n",
            "I0813 14:25:23.869056 140596802582400 learning.py:754] Starting Session.\n",
            "I0813 14:25:24.227114 140593739130624 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0813 14:25:24.230163 140596802582400 learning.py:768] Starting Queues.\n",
            "I0813 14:25:33.676040 140593730737920 supervisor.py:1099] global_step/sec: 0\n",
            "2019-08-13 14:25:43.344294: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "I0813 14:25:48.549094 140593722345216 supervisor.py:1050] Recording summary at step 0.\n",
            "I0813 14:25:50.076643 140596802582400 learning.py:507] global step 1: loss = 14.0752 (25.474 sec/step)\n",
            "I0813 14:25:51.287781 140596802582400 learning.py:507] global step 2: loss = 12.3882 (0.837 sec/step)\n",
            "I0813 14:25:52.155242 140596802582400 learning.py:507] global step 3: loss = 11.3237 (0.866 sec/step)\n",
            "I0813 14:25:53.008291 140596802582400 learning.py:507] global step 4: loss = 10.5131 (0.851 sec/step)\n",
            "I0813 14:25:53.846887 140596802582400 learning.py:507] global step 5: loss = 10.0569 (0.837 sec/step)\n",
            "I0813 14:25:54.694993 140596802582400 learning.py:507] global step 6: loss = 10.5577 (0.847 sec/step)\n",
            "I0813 14:25:55.548020 140596802582400 learning.py:507] global step 7: loss = 9.7806 (0.850 sec/step)\n",
            "I0813 14:25:56.377978 140596802582400 learning.py:507] global step 8: loss = 8.7728 (0.828 sec/step)\n",
            "I0813 14:25:57.209704 140596802582400 learning.py:507] global step 9: loss = 8.8736 (0.830 sec/step)\n",
            "I0813 14:25:58.053060 140596802582400 learning.py:507] global step 10: loss = 8.8078 (0.842 sec/step)\n",
            "I0813 14:25:58.890783 140596802582400 learning.py:507] global step 11: loss = 8.6493 (0.836 sec/step)\n",
            "I0813 14:25:59.745825 140596802582400 learning.py:507] global step 12: loss = 7.9907 (0.853 sec/step)\n",
            "I0813 14:26:00.588397 140596802582400 learning.py:507] global step 13: loss = 8.3905 (0.841 sec/step)\n",
            "I0813 14:26:01.427034 140596802582400 learning.py:507] global step 14: loss = 6.9499 (0.837 sec/step)\n",
            "I0813 14:26:02.293513 140596802582400 learning.py:507] global step 15: loss = 7.8498 (0.865 sec/step)\n",
            "I0813 14:26:03.149162 140596802582400 learning.py:507] global step 16: loss = 7.0004 (0.854 sec/step)\n",
            "I0813 14:26:04.021929 140596802582400 learning.py:507] global step 17: loss = 7.5724 (0.871 sec/step)\n",
            "I0813 14:26:04.870941 140596802582400 learning.py:507] global step 18: loss = 7.1258 (0.847 sec/step)\n",
            "I0813 14:26:05.701472 140596802582400 learning.py:507] global step 19: loss = 7.3721 (0.829 sec/step)\n",
            "I0813 14:26:06.522968 140596802582400 learning.py:507] global step 20: loss = 6.7647 (0.820 sec/step)\n",
            "I0813 14:26:07.384078 140596802582400 learning.py:507] global step 21: loss = 6.5012 (0.860 sec/step)\n",
            "I0813 14:26:08.247861 140596802582400 learning.py:507] global step 22: loss = 7.2277 (0.862 sec/step)\n",
            "I0813 14:26:09.066351 140596802582400 learning.py:507] global step 23: loss = 6.4236 (0.817 sec/step)\n",
            "I0813 14:26:09.910220 140596802582400 learning.py:507] global step 24: loss = 6.4965 (0.842 sec/step)\n",
            "I0813 14:26:10.755936 140596802582400 learning.py:507] global step 25: loss = 6.5326 (0.844 sec/step)\n",
            "I0813 14:26:11.578561 140596802582400 learning.py:507] global step 26: loss = 6.4202 (0.820 sec/step)\n",
            "I0813 14:26:12.428577 140596802582400 learning.py:507] global step 27: loss = 6.3884 (0.848 sec/step)\n",
            "I0813 14:26:13.267636 140596802582400 learning.py:507] global step 28: loss = 6.8161 (0.837 sec/step)\n",
            "I0813 14:26:14.116377 140596802582400 learning.py:507] global step 29: loss = 5.6361 (0.847 sec/step)\n",
            "I0813 14:26:14.963340 140596802582400 learning.py:507] global step 30: loss = 6.0919 (0.846 sec/step)\n",
            "I0813 14:26:15.824818 140596802582400 learning.py:507] global step 31: loss = 6.1598 (0.860 sec/step)\n",
            "I0813 14:26:16.665339 140596802582400 learning.py:507] global step 32: loss = 6.2413 (0.839 sec/step)\n",
            "I0813 14:26:17.547675 140596802582400 learning.py:507] global step 33: loss = 5.6696 (0.881 sec/step)\n",
            "I0813 14:26:18.396627 140596802582400 learning.py:507] global step 34: loss = 5.6815 (0.847 sec/step)\n",
            "I0813 14:26:19.249331 140596802582400 learning.py:507] global step 35: loss = 5.3912 (0.851 sec/step)\n",
            "I0813 14:26:20.089369 140596802582400 learning.py:507] global step 36: loss = 6.0443 (0.838 sec/step)\n",
            "I0813 14:26:20.953036 140596802582400 learning.py:507] global step 37: loss = 5.6433 (0.862 sec/step)\n",
            "I0813 14:26:21.783711 140596802582400 learning.py:507] global step 38: loss = 5.9586 (0.829 sec/step)\n",
            "I0813 14:26:22.633078 140596802582400 learning.py:507] global step 39: loss = 5.1000 (0.848 sec/step)\n",
            "I0813 14:26:23.473192 140596802582400 learning.py:507] global step 40: loss = 5.3149 (0.839 sec/step)\n",
            "I0813 14:26:24.305119 140596802582400 learning.py:507] global step 41: loss = 5.8493 (0.830 sec/step)\n",
            "I0813 14:26:25.177015 140596802582400 learning.py:507] global step 42: loss = 5.7289 (0.870 sec/step)\n",
            "I0813 14:26:26.034858 140596802582400 learning.py:507] global step 43: loss = 5.5104 (0.856 sec/step)\n",
            "I0813 14:26:26.887705 140596802582400 learning.py:507] global step 44: loss = 5.6207 (0.851 sec/step)\n",
            "I0813 14:26:27.758957 140596802582400 learning.py:507] global step 45: loss = 4.9828 (0.870 sec/step)\n",
            "I0813 14:26:28.658552 140596802582400 learning.py:507] global step 46: loss = 4.7185 (0.897 sec/step)\n",
            "I0813 14:26:29.495028 140596802582400 learning.py:507] global step 47: loss = 5.0567 (0.835 sec/step)\n",
            "I0813 14:26:30.340641 140596802582400 learning.py:507] global step 48: loss = 5.1994 (0.844 sec/step)\n",
            "I0813 14:26:31.189305 140596802582400 learning.py:507] global step 49: loss = 5.4527 (0.847 sec/step)\n",
            "I0813 14:26:32.030450 140596802582400 learning.py:507] global step 50: loss = 4.8123 (0.840 sec/step)\n",
            "I0813 14:26:32.893840 140596802582400 learning.py:507] global step 51: loss = 5.2400 (0.861 sec/step)\n",
            "I0813 14:26:33.747987 140596802582400 learning.py:507] global step 52: loss = 4.8720 (0.853 sec/step)\n",
            "I0813 14:26:34.618700 140596802582400 learning.py:507] global step 53: loss = 5.7114 (0.869 sec/step)\n",
            "I0813 14:26:35.446514 140596802582400 learning.py:507] global step 54: loss = 4.7305 (0.826 sec/step)\n",
            "I0813 14:26:36.287019 140596802582400 learning.py:507] global step 55: loss = 4.8287 (0.839 sec/step)\n",
            "I0813 14:26:37.137790 140596802582400 learning.py:507] global step 56: loss = 3.9160 (0.849 sec/step)\n",
            "I0813 14:26:37.977853 140596802582400 learning.py:507] global step 57: loss = 4.9474 (0.838 sec/step)\n",
            "I0813 14:26:38.821538 140596802582400 learning.py:507] global step 58: loss = 4.4228 (0.842 sec/step)\n",
            "I0813 14:26:39.666343 140596802582400 learning.py:507] global step 59: loss = 4.8674 (0.843 sec/step)\n",
            "I0813 14:26:40.518680 140596802582400 learning.py:507] global step 60: loss = 3.9828 (0.851 sec/step)\n",
            "I0813 14:26:41.359830 140596802582400 learning.py:507] global step 61: loss = 4.2197 (0.840 sec/step)\n",
            "I0813 14:26:42.234716 140596802582400 learning.py:507] global step 62: loss = 4.7379 (0.873 sec/step)\n",
            "I0813 14:26:43.075849 140596802582400 learning.py:507] global step 63: loss = 4.7987 (0.839 sec/step)\n",
            "I0813 14:26:43.927379 140596802582400 learning.py:507] global step 64: loss = 3.6955 (0.850 sec/step)\n",
            "I0813 14:26:44.777529 140596802582400 learning.py:507] global step 65: loss = 4.3187 (0.848 sec/step)\n",
            "I0813 14:26:45.666780 140596802582400 learning.py:507] global step 66: loss = 4.3573 (0.888 sec/step)\n",
            "I0813 14:26:46.507173 140596802582400 learning.py:507] global step 67: loss = 4.4098 (0.839 sec/step)\n",
            "I0813 14:26:47.360223 140596802582400 learning.py:507] global step 68: loss = 4.3587 (0.851 sec/step)\n",
            "I0813 14:26:48.220543 140596802582400 learning.py:507] global step 69: loss = 4.0620 (0.858 sec/step)\n",
            "I0813 14:26:49.072735 140596802582400 learning.py:507] global step 70: loss = 4.8616 (0.851 sec/step)\n",
            "I0813 14:26:49.912427 140596802582400 learning.py:507] global step 71: loss = 4.1798 (0.838 sec/step)\n",
            "I0813 14:26:50.774529 140596802582400 learning.py:507] global step 72: loss = 4.0291 (0.861 sec/step)\n",
            "I0813 14:26:51.629498 140596802582400 learning.py:507] global step 73: loss = 4.0566 (0.853 sec/step)\n",
            "I0813 14:26:52.489526 140596802582400 learning.py:507] global step 74: loss = 3.3633 (0.859 sec/step)\n",
            "I0813 14:26:53.350041 140596802582400 learning.py:507] global step 75: loss = 4.2676 (0.859 sec/step)\n",
            "I0813 14:26:54.181453 140596802582400 learning.py:507] global step 76: loss = 4.2543 (0.830 sec/step)\n",
            "I0813 14:26:55.031702 140596802582400 learning.py:507] global step 77: loss = 4.1422 (0.849 sec/step)\n",
            "I0813 14:26:55.878965 140596802582400 learning.py:507] global step 78: loss = 3.5805 (0.846 sec/step)\n",
            "I0813 14:26:56.721688 140596802582400 learning.py:507] global step 79: loss = 3.2172 (0.841 sec/step)\n",
            "I0813 14:26:57.568952 140596802582400 learning.py:507] global step 80: loss = 4.5377 (0.846 sec/step)\n",
            "I0813 14:26:58.409933 140596802582400 learning.py:507] global step 81: loss = 4.8885 (0.840 sec/step)\n",
            "I0813 14:26:59.271076 140596802582400 learning.py:507] global step 82: loss = 3.8615 (0.860 sec/step)\n",
            "I0813 14:27:00.108293 140596802582400 learning.py:507] global step 83: loss = 3.5360 (0.835 sec/step)\n",
            "I0813 14:27:00.940190 140596802582400 learning.py:507] global step 84: loss = 3.7364 (0.830 sec/step)\n",
            "I0813 14:27:01.801513 140596802582400 learning.py:507] global step 85: loss = 3.3623 (0.860 sec/step)\n",
            "I0813 14:27:02.664049 140596802582400 learning.py:507] global step 86: loss = 2.7607 (0.861 sec/step)\n",
            "I0813 14:27:03.531122 140596802582400 learning.py:507] global step 87: loss = 2.8234 (0.865 sec/step)\n",
            "I0813 14:27:04.389112 140596802582400 learning.py:507] global step 88: loss = 3.5606 (0.856 sec/step)\n",
            "I0813 14:27:05.254114 140596802582400 learning.py:507] global step 89: loss = 3.9381 (0.863 sec/step)\n",
            "I0813 14:27:06.112379 140596802582400 learning.py:507] global step 90: loss = 4.1244 (0.856 sec/step)\n",
            "I0813 14:27:06.946027 140596802582400 learning.py:507] global step 91: loss = 4.1573 (0.832 sec/step)\n",
            "I0813 14:27:07.815731 140596802582400 learning.py:507] global step 92: loss = 3.2320 (0.868 sec/step)\n",
            "I0813 14:27:08.678415 140596802582400 learning.py:507] global step 93: loss = 3.4505 (0.861 sec/step)\n",
            "I0813 14:27:09.524501 140596802582400 learning.py:507] global step 94: loss = 3.8160 (0.844 sec/step)\n",
            "I0813 14:27:10.369400 140596802582400 learning.py:507] global step 95: loss = 3.5575 (0.843 sec/step)\n",
            "I0813 14:27:11.239397 140596802582400 learning.py:507] global step 96: loss = 3.8803 (0.868 sec/step)\n",
            "I0813 14:27:12.095202 140596802582400 learning.py:507] global step 97: loss = 3.4028 (0.854 sec/step)\n",
            "I0813 14:27:12.959522 140596802582400 learning.py:507] global step 98: loss = 3.1394 (0.863 sec/step)\n",
            "I0813 14:27:13.833354 140596802582400 learning.py:507] global step 99: loss = 3.8488 (0.872 sec/step)\n",
            "I0813 14:27:14.658784 140596802582400 learning.py:507] global step 100: loss = 3.0224 (0.823 sec/step)\n",
            "I0813 14:27:15.486427 140596802582400 learning.py:507] global step 101: loss = 3.6597 (0.826 sec/step)\n",
            "I0813 14:27:16.311249 140596802582400 learning.py:507] global step 102: loss = 3.7015 (0.823 sec/step)\n",
            "I0813 14:27:17.143258 140596802582400 learning.py:507] global step 103: loss = 3.6763 (0.831 sec/step)\n",
            "I0813 14:27:17.994691 140596802582400 learning.py:507] global step 104: loss = 3.2790 (0.850 sec/step)\n",
            "I0813 14:27:18.821729 140596802582400 learning.py:507] global step 105: loss = 3.8244 (0.825 sec/step)\n",
            "I0813 14:27:19.693484 140596802582400 learning.py:507] global step 106: loss = 3.1869 (0.870 sec/step)\n",
            "I0813 14:27:20.526263 140596802582400 learning.py:507] global step 107: loss = 2.9999 (0.831 sec/step)\n",
            "I0813 14:27:21.377499 140596802582400 learning.py:507] global step 108: loss = 3.3969 (0.850 sec/step)\n",
            "I0813 14:27:22.224418 140596802582400 learning.py:507] global step 109: loss = 2.4099 (0.845 sec/step)\n",
            "I0813 14:27:23.064855 140596802582400 learning.py:507] global step 110: loss = 2.7295 (0.839 sec/step)\n",
            "I0813 14:27:23.899588 140596802582400 learning.py:507] global step 111: loss = 3.0333 (0.833 sec/step)\n",
            "I0813 14:27:24.780840 140596802582400 learning.py:507] global step 112: loss = 2.7242 (0.879 sec/step)\n",
            "I0813 14:27:26.026325 140593722345216 supervisor.py:1050] Recording summary at step 112.\n",
            "I0813 14:27:26.249148 140596802582400 learning.py:507] global step 113: loss = 3.0792 (1.462 sec/step)\n",
            "I0813 14:27:27.091885 140596802582400 learning.py:507] global step 114: loss = 3.1542 (0.841 sec/step)\n",
            "I0813 14:27:27.954272 140596802582400 learning.py:507] global step 115: loss = 3.1294 (0.861 sec/step)\n",
            "I0813 14:27:28.831097 140596802582400 learning.py:507] global step 116: loss = 2.5341 (0.875 sec/step)\n",
            "I0813 14:27:29.350469 140593730737920 supervisor.py:1099] global_step/sec: 1.00281\n",
            "I0813 14:27:29.691133 140596802582400 learning.py:507] global step 117: loss = 2.9076 (0.859 sec/step)\n",
            "I0813 14:27:30.529059 140596802582400 learning.py:507] global step 118: loss = 2.3979 (0.836 sec/step)\n",
            "I0813 14:27:31.375777 140596802582400 learning.py:507] global step 119: loss = 3.2912 (0.845 sec/step)\n",
            "I0813 14:27:32.220860 140596802582400 learning.py:507] global step 120: loss = 2.2426 (0.844 sec/step)\n",
            "I0813 14:27:33.055501 140596802582400 learning.py:507] global step 121: loss = 4.9421 (0.833 sec/step)\n",
            "I0813 14:27:33.899400 140596802582400 learning.py:507] global step 122: loss = 2.8289 (0.842 sec/step)\n",
            "I0813 14:27:34.751437 140596802582400 learning.py:507] global step 123: loss = 2.7820 (0.851 sec/step)\n",
            "I0813 14:27:35.607725 140596802582400 learning.py:507] global step 124: loss = 2.7244 (0.855 sec/step)\n",
            "I0813 14:27:36.445375 140596802582400 learning.py:507] global step 125: loss = 3.3682 (0.836 sec/step)\n",
            "I0813 14:27:37.286808 140596802582400 learning.py:507] global step 126: loss = 3.1471 (0.840 sec/step)\n",
            "I0813 14:27:38.140743 140596802582400 learning.py:507] global step 127: loss = 3.0480 (0.852 sec/step)\n",
            "I0813 14:27:38.994998 140596802582400 learning.py:507] global step 128: loss = 3.3158 (0.853 sec/step)\n",
            "I0813 14:27:39.850519 140596802582400 learning.py:507] global step 129: loss = 2.5131 (0.854 sec/step)\n",
            "I0813 14:27:40.706722 140596802582400 learning.py:507] global step 130: loss = 2.9551 (0.855 sec/step)\n",
            "I0813 14:27:41.557763 140596802582400 learning.py:507] global step 131: loss = 4.3048 (0.849 sec/step)\n",
            "I0813 14:27:42.392668 140596802582400 learning.py:507] global step 132: loss = 3.5861 (0.833 sec/step)\n",
            "I0813 14:27:43.219029 140596802582400 learning.py:507] global step 133: loss = 2.1960 (0.825 sec/step)\n",
            "I0813 14:27:44.083436 140596802582400 learning.py:507] global step 134: loss = 3.0897 (0.863 sec/step)\n",
            "I0813 14:27:44.938692 140596802582400 learning.py:507] global step 135: loss = 2.3708 (0.853 sec/step)\n",
            "I0813 14:27:45.788408 140596802582400 learning.py:507] global step 136: loss = 3.4443 (0.848 sec/step)\n",
            "I0813 14:27:46.638014 140596802582400 learning.py:507] global step 137: loss = 3.0551 (0.848 sec/step)\n",
            "I0813 14:27:47.484917 140596802582400 learning.py:507] global step 138: loss = 2.8798 (0.845 sec/step)\n",
            "I0813 14:27:48.315509 140596802582400 learning.py:507] global step 139: loss = 2.2295 (0.829 sec/step)\n",
            "I0813 14:27:49.150348 140596802582400 learning.py:507] global step 140: loss = 2.3957 (0.833 sec/step)\n",
            "I0813 14:27:50.003972 140596802582400 learning.py:507] global step 141: loss = 2.3452 (0.852 sec/step)\n",
            "I0813 14:27:50.856693 140596802582400 learning.py:507] global step 142: loss = 2.5267 (0.851 sec/step)\n",
            "I0813 14:27:51.695839 140596802582400 learning.py:507] global step 143: loss = 3.0998 (0.838 sec/step)\n",
            "I0813 14:27:52.539646 140596802582400 learning.py:507] global step 144: loss = 2.5538 (0.842 sec/step)\n",
            "I0813 14:27:53.371487 140596802582400 learning.py:507] global step 145: loss = 3.1444 (0.830 sec/step)\n",
            "I0813 14:27:54.212912 140596802582400 learning.py:507] global step 146: loss = 2.3924 (0.840 sec/step)\n",
            "I0813 14:27:55.075080 140596802582400 learning.py:507] global step 147: loss = 2.5422 (0.861 sec/step)\n",
            "I0813 14:27:55.922584 140596802582400 learning.py:507] global step 148: loss = 2.9844 (0.846 sec/step)\n",
            "I0813 14:27:56.774197 140596802582400 learning.py:507] global step 149: loss = 4.0887 (0.850 sec/step)\n",
            "I0813 14:27:57.614907 140596802582400 learning.py:507] global step 150: loss = 2.9777 (0.839 sec/step)\n",
            "I0813 14:27:58.445147 140596802582400 learning.py:507] global step 151: loss = 2.5464 (0.829 sec/step)\n",
            "I0813 14:27:59.294476 140596802582400 learning.py:507] global step 152: loss = 2.9634 (0.848 sec/step)\n",
            "I0813 14:28:00.144422 140596802582400 learning.py:507] global step 153: loss = 3.5014 (0.848 sec/step)\n",
            "I0813 14:28:01.009230 140596802582400 learning.py:507] global step 154: loss = 3.3318 (0.863 sec/step)\n",
            "I0813 14:28:01.853730 140596802582400 learning.py:507] global step 155: loss = 2.5136 (0.843 sec/step)\n",
            "I0813 14:28:02.706448 140596802582400 learning.py:507] global step 156: loss = 3.3356 (0.851 sec/step)\n",
            "I0813 14:28:03.557362 140596802582400 learning.py:507] global step 157: loss = 2.8142 (0.849 sec/step)\n",
            "I0813 14:28:04.404049 140596802582400 learning.py:507] global step 158: loss = 2.9136 (0.845 sec/step)\n",
            "I0813 14:28:05.262269 140596802582400 learning.py:507] global step 159: loss = 3.7158 (0.857 sec/step)\n",
            "I0813 14:28:06.111654 140596802582400 learning.py:507] global step 160: loss = 3.5352 (0.848 sec/step)\n",
            "I0813 14:28:06.949717 140596802582400 learning.py:507] global step 161: loss = 3.1203 (0.836 sec/step)\n",
            "I0813 14:28:07.802950 140596802582400 learning.py:507] global step 162: loss = 2.8814 (0.852 sec/step)\n",
            "I0813 14:28:08.650489 140596802582400 learning.py:507] global step 163: loss = 3.3049 (0.846 sec/step)\n",
            "I0813 14:28:09.485147 140596802582400 learning.py:507] global step 164: loss = 4.1131 (0.833 sec/step)\n",
            "I0813 14:28:10.320119 140596802582400 learning.py:507] global step 165: loss = 3.0400 (0.833 sec/step)\n",
            "I0813 14:28:11.188804 140596802582400 learning.py:507] global step 166: loss = 2.6849 (0.867 sec/step)\n",
            "I0813 14:28:12.026417 140596802582400 learning.py:507] global step 167: loss = 2.7109 (0.836 sec/step)\n",
            "I0813 14:28:12.877224 140596802582400 learning.py:507] global step 168: loss = 2.4804 (0.849 sec/step)\n",
            "I0813 14:28:13.730182 140596802582400 learning.py:507] global step 169: loss = 2.5382 (0.851 sec/step)\n",
            "I0813 14:28:14.565726 140596802582400 learning.py:507] global step 170: loss = 2.9374 (0.834 sec/step)\n",
            "I0813 14:28:15.415737 140596802582400 learning.py:507] global step 171: loss = 2.2718 (0.848 sec/step)\n",
            "I0813 14:28:16.277757 140596802582400 learning.py:507] global step 172: loss = 2.5515 (0.860 sec/step)\n",
            "I0813 14:28:17.122672 140596802582400 learning.py:507] global step 173: loss = 2.7244 (0.843 sec/step)\n",
            "I0813 14:28:17.960960 140596802582400 learning.py:507] global step 174: loss = 3.1008 (0.837 sec/step)\n",
            "I0813 14:28:18.807469 140596802582400 learning.py:507] global step 175: loss = 3.0345 (0.845 sec/step)\n",
            "I0813 14:28:19.655242 140596802582400 learning.py:507] global step 176: loss = 2.9223 (0.846 sec/step)\n",
            "I0813 14:28:20.502283 140596802582400 learning.py:507] global step 177: loss = 2.7308 (0.846 sec/step)\n",
            "I0813 14:28:21.380559 140596802582400 learning.py:507] global step 178: loss = 2.7474 (0.877 sec/step)\n",
            "I0813 14:28:22.228821 140596802582400 learning.py:507] global step 179: loss = 2.7614 (0.845 sec/step)\n",
            "I0813 14:28:23.102462 140596802582400 learning.py:507] global step 180: loss = 3.7789 (0.872 sec/step)\n",
            "I0813 14:28:23.948541 140596802582400 learning.py:507] global step 181: loss = 3.4337 (0.845 sec/step)\n",
            "I0813 14:28:24.801828 140596802582400 learning.py:507] global step 182: loss = 3.1178 (0.851 sec/step)\n",
            "I0813 14:28:25.657704 140596802582400 learning.py:507] global step 183: loss = 3.5310 (0.854 sec/step)\n",
            "I0813 14:28:26.493498 140596802582400 learning.py:507] global step 184: loss = 2.9176 (0.834 sec/step)\n",
            "I0813 14:28:27.339776 140596802582400 learning.py:507] global step 185: loss = 2.1850 (0.845 sec/step)\n",
            "I0813 14:28:28.210650 140596802582400 learning.py:507] global step 186: loss = 2.4462 (0.869 sec/step)\n",
            "I0813 14:28:29.067315 140596802582400 learning.py:507] global step 187: loss = 2.5010 (0.855 sec/step)\n",
            "I0813 14:28:29.917587 140596802582400 learning.py:507] global step 188: loss = 3.2147 (0.849 sec/step)\n",
            "I0813 14:28:30.769516 140596802582400 learning.py:507] global step 189: loss = 2.2106 (0.850 sec/step)\n",
            "I0813 14:28:31.616079 140596802582400 learning.py:507] global step 190: loss = 2.9835 (0.845 sec/step)\n",
            "I0813 14:28:32.452351 140596802582400 learning.py:507] global step 191: loss = 3.0740 (0.835 sec/step)\n",
            "I0813 14:28:33.287894 140596802582400 learning.py:507] global step 192: loss = 2.4869 (0.834 sec/step)\n",
            "I0813 14:28:34.130182 140596802582400 learning.py:507] global step 193: loss = 2.5024 (0.840 sec/step)\n",
            "I0813 14:28:34.983440 140596802582400 learning.py:507] global step 194: loss = 2.4646 (0.852 sec/step)\n",
            "I0813 14:28:35.857342 140596802582400 learning.py:507] global step 195: loss = 3.2534 (0.872 sec/step)\n",
            "I0813 14:28:36.724036 140596802582400 learning.py:507] global step 196: loss = 2.2088 (0.865 sec/step)\n",
            "I0813 14:28:37.564474 140596802582400 learning.py:507] global step 197: loss = 2.6716 (0.839 sec/step)\n",
            "I0813 14:28:38.412105 140596802582400 learning.py:507] global step 198: loss = 2.9650 (0.846 sec/step)\n",
            "I0813 14:28:39.266103 140596802582400 learning.py:507] global step 199: loss = 1.9435 (0.852 sec/step)\n",
            "I0813 14:28:40.134119 140596802582400 learning.py:507] global step 200: loss = 2.0541 (0.866 sec/step)\n",
            "I0813 14:28:40.992368 140596802582400 learning.py:507] global step 201: loss = 2.0616 (0.857 sec/step)\n",
            "I0813 14:28:41.828325 140596802582400 learning.py:507] global step 202: loss = 3.5276 (0.834 sec/step)\n",
            "I0813 14:28:42.697049 140596802582400 learning.py:507] global step 203: loss = 2.3759 (0.867 sec/step)\n",
            "I0813 14:28:43.547652 140596802582400 learning.py:507] global step 204: loss = 1.9402 (0.849 sec/step)\n",
            "I0813 14:28:44.397617 140596802582400 learning.py:507] global step 205: loss = 2.7956 (0.848 sec/step)\n",
            "I0813 14:28:45.236128 140596802582400 learning.py:507] global step 206: loss = 2.3132 (0.837 sec/step)\n",
            "I0813 14:28:46.078273 140596802582400 learning.py:507] global step 207: loss = 3.0156 (0.841 sec/step)\n",
            "I0813 14:28:46.929565 140596802582400 learning.py:507] global step 208: loss = 2.7533 (0.850 sec/step)\n",
            "I0813 14:28:47.752091 140596802582400 learning.py:507] global step 209: loss = 2.5337 (0.821 sec/step)\n",
            "I0813 14:28:48.601303 140596802582400 learning.py:507] global step 210: loss = 3.2325 (0.848 sec/step)\n",
            "I0813 14:28:49.453452 140596802582400 learning.py:507] global step 211: loss = 2.8481 (0.850 sec/step)\n",
            "I0813 14:28:50.325543 140596802582400 learning.py:507] global step 212: loss = 2.4145 (0.871 sec/step)\n",
            "I0813 14:28:51.159205 140596802582400 learning.py:507] global step 213: loss = 2.1380 (0.832 sec/step)\n",
            "I0813 14:28:52.015231 140596802582400 learning.py:507] global step 214: loss = 2.3000 (0.854 sec/step)\n",
            "I0813 14:28:52.867534 140596802582400 learning.py:507] global step 215: loss = 2.3925 (0.851 sec/step)\n",
            "I0813 14:28:53.698556 140596802582400 learning.py:507] global step 216: loss = 3.5087 (0.829 sec/step)\n",
            "I0813 14:28:54.546282 140596802582400 learning.py:507] global step 217: loss = 2.2582 (0.846 sec/step)\n",
            "I0813 14:28:55.395736 140596802582400 learning.py:507] global step 218: loss = 2.8437 (0.848 sec/step)\n",
            "I0813 14:28:56.237335 140596802582400 learning.py:507] global step 219: loss = 3.6370 (0.840 sec/step)\n",
            "I0813 14:28:57.095767 140596802582400 learning.py:507] global step 220: loss = 2.8578 (0.857 sec/step)\n",
            "I0813 14:28:57.952207 140596802582400 learning.py:507] global step 221: loss = 2.4597 (0.855 sec/step)\n",
            "I0813 14:28:58.832985 140596802582400 learning.py:507] global step 222: loss = 2.5267 (0.879 sec/step)\n",
            "I0813 14:28:59.696566 140596802582400 learning.py:507] global step 223: loss = 3.7267 (0.862 sec/step)\n",
            "I0813 14:29:00.548198 140596802582400 learning.py:507] global step 224: loss = 3.1220 (0.850 sec/step)\n",
            "I0813 14:29:01.393839 140596802582400 learning.py:507] global step 225: loss = 2.1915 (0.844 sec/step)\n",
            "I0813 14:29:02.242930 140596802582400 learning.py:507] global step 226: loss = 2.8677 (0.847 sec/step)\n",
            "I0813 14:29:03.142008 140596802582400 learning.py:507] global step 227: loss = 2.7290 (0.898 sec/step)\n",
            "I0813 14:29:04.037309 140596802582400 learning.py:507] global step 228: loss = 2.6613 (0.894 sec/step)\n",
            "I0813 14:29:04.917687 140596802582400 learning.py:507] global step 229: loss = 2.5419 (0.879 sec/step)\n",
            "I0813 14:29:05.800111 140596802582400 learning.py:507] global step 230: loss = 3.0869 (0.881 sec/step)\n",
            "I0813 14:29:06.653849 140596802582400 learning.py:507] global step 231: loss = 3.4447 (0.852 sec/step)\n",
            "I0813 14:29:07.507767 140596802582400 learning.py:507] global step 232: loss = 3.3379 (0.852 sec/step)\n",
            "I0813 14:29:08.379937 140596802582400 learning.py:507] global step 233: loss = 2.9361 (0.870 sec/step)\n",
            "I0813 14:29:09.273015 140596802582400 learning.py:507] global step 234: loss = 2.5541 (0.892 sec/step)\n",
            "I0813 14:29:10.124200 140596802582400 learning.py:507] global step 235: loss = 1.6236 (0.850 sec/step)\n",
            "I0813 14:29:10.993651 140596802582400 learning.py:507] global step 236: loss = 2.5278 (0.868 sec/step)\n",
            "I0813 14:29:11.872580 140596802582400 learning.py:507] global step 237: loss = 2.2892 (0.877 sec/step)\n",
            "I0813 14:29:12.739511 140596802582400 learning.py:507] global step 238: loss = 2.2727 (0.865 sec/step)\n",
            "I0813 14:29:13.634909 140596802582400 learning.py:507] global step 239: loss = 4.2463 (0.894 sec/step)\n",
            "I0813 14:29:14.467146 140596802582400 learning.py:507] global step 240: loss = 2.9189 (0.831 sec/step)\n",
            "I0813 14:29:15.328973 140596802582400 learning.py:507] global step 241: loss = 2.1284 (0.860 sec/step)\n",
            "I0813 14:29:16.189813 140596802582400 learning.py:507] global step 242: loss = 3.0433 (0.859 sec/step)\n",
            "I0813 14:29:17.062312 140596802582400 learning.py:507] global step 243: loss = 2.6702 (0.871 sec/step)\n",
            "I0813 14:29:17.913752 140596802582400 learning.py:507] global step 244: loss = 2.0526 (0.850 sec/step)\n",
            "I0813 14:29:18.763981 140596802582400 learning.py:507] global step 245: loss = 3.0818 (0.849 sec/step)\n",
            "I0813 14:29:19.624515 140596802582400 learning.py:507] global step 246: loss = 3.3763 (0.859 sec/step)\n",
            "I0813 14:29:20.467715 140596802582400 learning.py:507] global step 247: loss = 2.2385 (0.842 sec/step)\n",
            "I0813 14:29:21.321978 140596802582400 learning.py:507] global step 248: loss = 2.8022 (0.852 sec/step)\n",
            "I0813 14:29:22.194077 140596802582400 learning.py:507] global step 249: loss = 2.2230 (0.871 sec/step)\n",
            "I0813 14:29:23.045932 140596802582400 learning.py:507] global step 250: loss = 2.4339 (0.850 sec/step)\n",
            "I0813 14:29:23.901381 140596802582400 learning.py:507] global step 251: loss = 2.7906 (0.854 sec/step)\n",
            "I0813 14:29:24.840235 140596802582400 learning.py:507] global step 252: loss = 2.8479 (0.937 sec/step)\n",
            "I0813 14:29:25.987468 140593722345216 supervisor.py:1050] Recording summary at step 252.\n",
            "I0813 14:29:26.278388 140596802582400 learning.py:507] global step 253: loss = 2.6900 (1.369 sec/step)\n",
            "I0813 14:29:27.119994 140596802582400 learning.py:507] global step 254: loss = 2.8175 (0.840 sec/step)\n",
            "I0813 14:29:27.959242 140596802582400 learning.py:507] global step 255: loss = 3.6064 (0.838 sec/step)\n",
            "I0813 14:29:28.908905 140596802582400 learning.py:507] global step 256: loss = 3.2902 (0.948 sec/step)\n",
            "I0813 14:29:29.525009 140593730737920 supervisor.py:1099] global_step/sec: 1.16497\n",
            "I0813 14:29:29.798193 140596802582400 learning.py:507] global step 257: loss = 3.0174 (0.887 sec/step)\n",
            "I0813 14:29:30.662311 140596802582400 learning.py:507] global step 258: loss = 2.8230 (0.862 sec/step)\n",
            "I0813 14:29:31.518573 140596802582400 learning.py:507] global step 259: loss = 2.2569 (0.855 sec/step)\n",
            "I0813 14:29:32.387030 140596802582400 learning.py:507] global step 260: loss = 2.9179 (0.867 sec/step)\n",
            "I0813 14:29:33.242775 140596802582400 learning.py:507] global step 261: loss = 2.5094 (0.854 sec/step)\n",
            "I0813 14:29:34.117048 140596802582400 learning.py:507] global step 262: loss = 2.4988 (0.873 sec/step)\n",
            "I0813 14:29:34.986650 140596802582400 learning.py:507] global step 263: loss = 3.0690 (0.868 sec/step)\n",
            "I0813 14:29:35.839967 140596802582400 learning.py:507] global step 264: loss = 2.3759 (0.852 sec/step)\n",
            "I0813 14:29:36.693206 140596802582400 learning.py:507] global step 265: loss = 3.3184 (0.851 sec/step)\n",
            "I0813 14:29:37.565666 140596802582400 learning.py:507] global step 266: loss = 1.8768 (0.871 sec/step)\n",
            "I0813 14:29:38.400400 140596802582400 learning.py:507] global step 267: loss = 2.1643 (0.833 sec/step)\n",
            "I0813 14:29:39.246368 140596802582400 learning.py:507] global step 268: loss = 2.7035 (0.845 sec/step)\n",
            "I0813 14:29:40.084868 140596802582400 learning.py:507] global step 269: loss = 2.3969 (0.837 sec/step)\n",
            "I0813 14:29:40.958411 140596802582400 learning.py:507] global step 270: loss = 2.1431 (0.872 sec/step)\n",
            "I0813 14:29:41.818044 140596802582400 learning.py:507] global step 271: loss = 2.6618 (0.858 sec/step)\n",
            "I0813 14:29:42.675431 140596802582400 learning.py:507] global step 272: loss = 2.4545 (0.856 sec/step)\n",
            "I0813 14:29:43.508643 140596802582400 learning.py:507] global step 273: loss = 2.9095 (0.832 sec/step)\n",
            "I0813 14:29:44.369973 140596802582400 learning.py:507] global step 274: loss = 3.4166 (0.859 sec/step)\n",
            "I0813 14:29:45.214571 140596802582400 learning.py:507] global step 275: loss = 2.7838 (0.843 sec/step)\n",
            "I0813 14:29:46.063593 140596802582400 learning.py:507] global step 276: loss = 2.3072 (0.847 sec/step)\n",
            "I0813 14:29:46.935941 140596802582400 learning.py:507] global step 277: loss = 2.2867 (0.871 sec/step)\n",
            "I0813 14:29:47.786292 140596802582400 learning.py:507] global step 278: loss = 2.1130 (0.849 sec/step)\n",
            "I0813 14:29:48.617787 140596802582400 learning.py:507] global step 279: loss = 3.1834 (0.830 sec/step)\n",
            "I0813 14:29:49.469624 140596802582400 learning.py:507] global step 280: loss = 4.5145 (0.850 sec/step)\n",
            "I0813 14:29:50.323497 140596802582400 learning.py:507] global step 281: loss = 2.4605 (0.852 sec/step)\n",
            "I0813 14:29:51.175251 140596802582400 learning.py:507] global step 282: loss = 2.2852 (0.850 sec/step)\n",
            "I0813 14:29:52.026291 140596802582400 learning.py:507] global step 283: loss = 2.7578 (0.849 sec/step)\n",
            "I0813 14:29:52.897636 140596802582400 learning.py:507] global step 284: loss = 2.6115 (0.870 sec/step)\n",
            "I0813 14:29:53.730481 140596802582400 learning.py:507] global step 285: loss = 3.2205 (0.831 sec/step)\n",
            "I0813 14:29:54.564547 140596802582400 learning.py:507] global step 286: loss = 2.8209 (0.832 sec/step)\n",
            "I0813 14:29:55.428188 140596802582400 learning.py:507] global step 287: loss = 2.4269 (0.862 sec/step)\n",
            "I0813 14:29:56.283564 140596802582400 learning.py:507] global step 288: loss = 2.8688 (0.853 sec/step)\n",
            "I0813 14:29:57.137264 140596802582400 learning.py:507] global step 289: loss = 2.4767 (0.852 sec/step)\n",
            "I0813 14:29:57.986722 140596802582400 learning.py:507] global step 290: loss = 2.4067 (0.848 sec/step)\n",
            "I0813 14:29:58.824358 140596802582400 learning.py:507] global step 291: loss = 1.7193 (0.836 sec/step)\n",
            "I0813 14:29:59.673133 140596802582400 learning.py:507] global step 292: loss = 1.6054 (0.847 sec/step)\n",
            "I0813 14:30:00.520434 140596802582400 learning.py:507] global step 293: loss = 2.7689 (0.846 sec/step)\n",
            "I0813 14:30:01.376285 140596802582400 learning.py:507] global step 294: loss = 3.1316 (0.854 sec/step)\n",
            "I0813 14:30:02.289059 140596802582400 learning.py:507] global step 295: loss = 3.1577 (0.911 sec/step)\n",
            "I0813 14:30:03.159255 140596802582400 learning.py:507] global step 296: loss = 2.7539 (0.868 sec/step)\n",
            "I0813 14:30:04.033992 140596802582400 learning.py:507] global step 297: loss = 1.9975 (0.873 sec/step)\n",
            "I0813 14:30:04.908891 140596802582400 learning.py:507] global step 298: loss = 2.3787 (0.873 sec/step)\n",
            "I0813 14:30:05.755975 140596802582400 learning.py:507] global step 299: loss = 2.7995 (0.845 sec/step)\n",
            "I0813 14:30:06.609795 140596802582400 learning.py:507] global step 300: loss = 2.1443 (0.852 sec/step)\n",
            "I0813 14:30:07.470965 140596802582400 learning.py:507] global step 301: loss = 3.0818 (0.860 sec/step)\n",
            "I0813 14:30:08.354213 140596802582400 learning.py:507] global step 302: loss = 2.6139 (0.882 sec/step)\n",
            "I0813 14:30:09.203808 140596802582400 learning.py:507] global step 303: loss = 2.4239 (0.848 sec/step)\n",
            "I0813 14:30:10.055932 140596802582400 learning.py:507] global step 304: loss = 2.8347 (0.850 sec/step)\n",
            "I0813 14:30:10.919410 140596802582400 learning.py:507] global step 305: loss = 2.3169 (0.862 sec/step)\n",
            "I0813 14:30:11.780248 140596802582400 learning.py:507] global step 306: loss = 2.3468 (0.859 sec/step)\n",
            "I0813 14:30:12.651553 140596802582400 learning.py:507] global step 307: loss = 2.6404 (0.870 sec/step)\n",
            "I0813 14:30:13.501650 140596802582400 learning.py:507] global step 308: loss = 2.4606 (0.849 sec/step)\n",
            "I0813 14:30:14.337259 140596802582400 learning.py:507] global step 309: loss = 2.7999 (0.834 sec/step)\n",
            "I0813 14:30:15.187225 140596802582400 learning.py:507] global step 310: loss = 2.3785 (0.848 sec/step)\n",
            "I0813 14:30:16.043134 140596802582400 learning.py:507] global step 311: loss = 2.5136 (0.854 sec/step)\n",
            "I0813 14:30:16.907962 140596802582400 learning.py:507] global step 312: loss = 2.0985 (0.863 sec/step)\n",
            "I0813 14:30:17.767825 140596802582400 learning.py:507] global step 313: loss = 2.5671 (0.858 sec/step)\n",
            "I0813 14:30:18.628353 140596802582400 learning.py:507] global step 314: loss = 2.4660 (0.859 sec/step)\n",
            "I0813 14:30:19.489244 140596802582400 learning.py:507] global step 315: loss = 1.6968 (0.859 sec/step)\n",
            "I0813 14:30:20.358050 140596802582400 learning.py:507] global step 316: loss = 2.8366 (0.867 sec/step)\n",
            "I0813 14:30:21.221632 140596802582400 learning.py:507] global step 317: loss = 3.5750 (0.862 sec/step)\n",
            "I0813 14:30:22.078229 140596802582400 learning.py:507] global step 318: loss = 2.4998 (0.855 sec/step)\n",
            "I0813 14:30:22.942816 140596802582400 learning.py:507] global step 319: loss = 2.4186 (0.863 sec/step)\n",
            "I0813 14:30:23.815552 140596802582400 learning.py:507] global step 320: loss = 3.1427 (0.871 sec/step)\n",
            "I0813 14:30:24.671647 140596802582400 learning.py:507] global step 321: loss = 2.1646 (0.854 sec/step)\n",
            "I0813 14:30:25.509062 140596802582400 learning.py:507] global step 322: loss = 2.8196 (0.836 sec/step)\n",
            "I0813 14:30:26.338490 140596802582400 learning.py:507] global step 323: loss = 2.8551 (0.828 sec/step)\n",
            "I0813 14:30:27.189771 140596802582400 learning.py:507] global step 324: loss = 2.3704 (0.850 sec/step)\n",
            "I0813 14:30:28.043866 140596802582400 learning.py:507] global step 325: loss = 3.0818 (0.852 sec/step)\n",
            "I0813 14:30:28.895714 140596802582400 learning.py:507] global step 326: loss = 2.8971 (0.850 sec/step)\n",
            "I0813 14:30:29.787844 140596802582400 learning.py:507] global step 327: loss = 2.4526 (0.890 sec/step)\n",
            "I0813 14:30:30.676965 140596802582400 learning.py:507] global step 328: loss = 1.8459 (0.887 sec/step)\n",
            "I0813 14:30:31.532838 140596802582400 learning.py:507] global step 329: loss = 2.1233 (0.854 sec/step)\n",
            "I0813 14:30:32.380484 140596802582400 learning.py:507] global step 330: loss = 2.4787 (0.846 sec/step)\n",
            "I0813 14:30:33.250897 140596802582400 learning.py:507] global step 331: loss = 2.7022 (0.869 sec/step)\n",
            "I0813 14:30:34.089020 140596802582400 learning.py:507] global step 332: loss = 2.3065 (0.837 sec/step)\n",
            "I0813 14:30:34.939448 140596802582400 learning.py:507] global step 333: loss = 2.1032 (0.849 sec/step)\n",
            "I0813 14:30:35.788506 140596802582400 learning.py:507] global step 334: loss = 3.0313 (0.847 sec/step)\n",
            "I0813 14:30:36.643354 140596802582400 learning.py:507] global step 335: loss = 2.2672 (0.853 sec/step)\n",
            "I0813 14:30:37.481927 140596802582400 learning.py:507] global step 336: loss = 2.1596 (0.837 sec/step)\n",
            "I0813 14:30:38.328545 140596802582400 learning.py:507] global step 337: loss = 2.6347 (0.845 sec/step)\n",
            "I0813 14:30:39.178953 140596802582400 learning.py:507] global step 338: loss = 2.4742 (0.849 sec/step)\n",
            "I0813 14:30:40.027868 140596802582400 learning.py:507] global step 339: loss = 1.9174 (0.847 sec/step)\n",
            "I0813 14:30:40.880658 140596802582400 learning.py:507] global step 340: loss = 1.9564 (0.851 sec/step)\n",
            "I0813 14:30:41.727804 140596802582400 learning.py:507] global step 341: loss = 3.0870 (0.846 sec/step)\n",
            "I0813 14:30:42.584684 140596802582400 learning.py:507] global step 342: loss = 2.8796 (0.855 sec/step)\n",
            "I0813 14:30:43.418567 140596802582400 learning.py:507] global step 343: loss = 2.4712 (0.832 sec/step)\n",
            "I0813 14:30:44.285941 140596802582400 learning.py:507] global step 344: loss = 2.4153 (0.866 sec/step)\n",
            "I0813 14:30:45.124363 140596802582400 learning.py:507] global step 345: loss = 2.3821 (0.837 sec/step)\n",
            "I0813 14:30:45.972231 140596802582400 learning.py:507] global step 346: loss = 2.6309 (0.846 sec/step)\n",
            "I0813 14:30:46.821931 140596802582400 learning.py:507] global step 347: loss = 2.6311 (0.848 sec/step)\n",
            "I0813 14:30:47.674407 140596802582400 learning.py:507] global step 348: loss = 2.4229 (0.851 sec/step)\n",
            "I0813 14:30:48.523963 140596802582400 learning.py:507] global step 349: loss = 2.2246 (0.848 sec/step)\n",
            "I0813 14:30:49.373963 140596802582400 learning.py:507] global step 350: loss = 2.0982 (0.848 sec/step)\n",
            "I0813 14:30:50.223485 140596802582400 learning.py:507] global step 351: loss = 2.3242 (0.848 sec/step)\n",
            "I0813 14:30:51.065818 140596802582400 learning.py:507] global step 352: loss = 2.1766 (0.840 sec/step)\n",
            "I0813 14:30:51.903029 140596802582400 learning.py:507] global step 353: loss = 1.9297 (0.836 sec/step)\n",
            "I0813 14:30:52.754293 140596802582400 learning.py:507] global step 354: loss = 2.8777 (0.850 sec/step)\n",
            "I0813 14:30:53.616616 140596802582400 learning.py:507] global step 355: loss = 2.8937 (0.861 sec/step)\n",
            "I0813 14:30:54.485769 140596802582400 learning.py:507] global step 356: loss = 2.3357 (0.868 sec/step)\n",
            "I0813 14:30:55.320801 140596802582400 learning.py:507] global step 357: loss = 2.7632 (0.834 sec/step)\n",
            "I0813 14:30:56.154991 140596802582400 learning.py:507] global step 358: loss = 2.3941 (0.833 sec/step)\n",
            "I0813 14:30:57.018847 140596802582400 learning.py:507] global step 359: loss = 2.0214 (0.862 sec/step)\n",
            "I0813 14:30:57.859744 140596802582400 learning.py:507] global step 360: loss = 3.1043 (0.839 sec/step)\n",
            "I0813 14:30:58.705688 140596802582400 learning.py:507] global step 361: loss = 2.3791 (0.844 sec/step)\n",
            "I0813 14:30:59.557496 140596802582400 learning.py:507] global step 362: loss = 3.1232 (0.850 sec/step)\n",
            "I0813 14:31:00.407046 140596802582400 learning.py:507] global step 363: loss = 3.7608 (0.848 sec/step)\n",
            "I0813 14:31:01.243486 140596802582400 learning.py:507] global step 364: loss = 2.7891 (0.835 sec/step)\n",
            "I0813 14:31:02.078141 140596802582400 learning.py:507] global step 365: loss = 2.0227 (0.833 sec/step)\n",
            "I0813 14:31:02.949489 140596802582400 learning.py:507] global step 366: loss = 2.9041 (0.870 sec/step)\n",
            "I0813 14:31:03.806802 140596802582400 learning.py:507] global step 367: loss = 1.9457 (0.856 sec/step)\n",
            "I0813 14:31:04.704227 140596802582400 learning.py:507] global step 368: loss = 3.0391 (0.896 sec/step)\n",
            "I0813 14:31:05.560756 140596802582400 learning.py:507] global step 369: loss = 2.9224 (0.855 sec/step)\n",
            "I0813 14:31:06.414256 140596802582400 learning.py:507] global step 370: loss = 2.3222 (0.852 sec/step)\n",
            "I0813 14:31:07.280827 140596802582400 learning.py:507] global step 371: loss = 2.4320 (0.865 sec/step)\n",
            "I0813 14:31:08.157973 140596802582400 learning.py:507] global step 372: loss = 2.0595 (0.876 sec/step)\n",
            "I0813 14:31:09.045806 140596802582400 learning.py:507] global step 373: loss = 2.1156 (0.886 sec/step)\n",
            "I0813 14:31:09.883882 140596802582400 learning.py:507] global step 374: loss = 1.9139 (0.837 sec/step)\n",
            "I0813 14:31:10.733850 140596802582400 learning.py:507] global step 375: loss = 2.0130 (0.848 sec/step)\n",
            "I0813 14:31:11.581314 140596802582400 learning.py:507] global step 376: loss = 1.7408 (0.846 sec/step)\n",
            "I0813 14:31:12.465310 140596802582400 learning.py:507] global step 377: loss = 2.6753 (0.881 sec/step)\n",
            "I0813 14:31:13.311847 140596802582400 learning.py:507] global step 378: loss = 2.9140 (0.845 sec/step)\n",
            "I0813 14:31:14.160875 140596802582400 learning.py:507] global step 379: loss = 2.8127 (0.847 sec/step)\n",
            "I0813 14:31:15.019457 140596802582400 learning.py:507] global step 380: loss = 1.8991 (0.856 sec/step)\n",
            "I0813 14:31:15.862896 140596802582400 learning.py:507] global step 381: loss = 2.5740 (0.842 sec/step)\n",
            "I0813 14:31:16.719716 140596802582400 learning.py:507] global step 382: loss = 1.7651 (0.855 sec/step)\n",
            "I0813 14:31:17.569435 140596802582400 learning.py:507] global step 383: loss = 2.1837 (0.848 sec/step)\n",
            "I0813 14:31:18.404217 140596802582400 learning.py:507] global step 384: loss = 2.1163 (0.833 sec/step)\n",
            "I0813 14:31:19.264835 140596802582400 learning.py:507] global step 385: loss = 1.1357 (0.859 sec/step)\n",
            "I0813 14:31:20.127182 140596802582400 learning.py:507] global step 386: loss = 2.2289 (0.861 sec/step)\n",
            "I0813 14:31:20.979697 140596802582400 learning.py:507] global step 387: loss = 2.1195 (0.851 sec/step)\n",
            "I0813 14:31:21.824954 140596802582400 learning.py:507] global step 388: loss = 3.1445 (0.844 sec/step)\n",
            "I0813 14:31:22.668322 140596802582400 learning.py:507] global step 389: loss = 2.3886 (0.842 sec/step)\n",
            "I0813 14:31:23.515898 140596802582400 learning.py:507] global step 390: loss = 3.0655 (0.846 sec/step)\n",
            "I0813 14:31:24.401591 140596802582400 learning.py:507] global step 391: loss = 1.8176 (0.884 sec/step)\n",
            "I0813 14:31:25.681679 140593722345216 supervisor.py:1050] Recording summary at step 391.\n",
            "I0813 14:31:25.924450 140596802582400 learning.py:507] global step 392: loss = 2.3458 (1.521 sec/step)\n",
            "I0813 14:31:26.797424 140596802582400 learning.py:507] global step 393: loss = 2.8973 (0.871 sec/step)\n",
            "I0813 14:31:27.635203 140596802582400 learning.py:507] global step 394: loss = 2.0001 (0.836 sec/step)\n",
            "I0813 14:31:28.568313 140596802582400 learning.py:507] global step 395: loss = 1.7071 (0.931 sec/step)\n",
            "I0813 14:31:29.117033 140593730737920 supervisor.py:1099] global_step/sec: 1.16229\n",
            "I0813 14:31:29.472014 140596802582400 learning.py:507] global step 396: loss = 2.2408 (0.902 sec/step)\n",
            "I0813 14:31:30.397700 140596802582400 learning.py:507] global step 397: loss = 2.2092 (0.919 sec/step)\n",
            "I0813 14:31:31.336016 140596802582400 learning.py:507] global step 398: loss = 1.9550 (0.937 sec/step)\n",
            "I0813 14:31:32.232405 140596802582400 learning.py:507] global step 399: loss = 2.3926 (0.894 sec/step)\n",
            "I0813 14:31:33.104682 140596802582400 learning.py:507] global step 400: loss = 3.1539 (0.871 sec/step)\n",
            "I0813 14:31:33.986064 140596802582400 learning.py:507] global step 401: loss = 1.6520 (0.880 sec/step)\n",
            "I0813 14:31:34.869762 140596802582400 learning.py:507] global step 402: loss = 2.0520 (0.882 sec/step)\n",
            "I0813 14:31:35.758428 140596802582400 learning.py:507] global step 403: loss = 2.0922 (0.887 sec/step)\n",
            "I0813 14:31:36.654783 140596802582400 learning.py:507] global step 404: loss = 2.1334 (0.895 sec/step)\n",
            "I0813 14:31:37.567227 140596802582400 learning.py:507] global step 405: loss = 1.7934 (0.910 sec/step)\n",
            "I0813 14:31:38.478500 140596802582400 learning.py:507] global step 406: loss = 2.8545 (0.909 sec/step)\n",
            "I0813 14:31:39.386360 140596802582400 learning.py:507] global step 407: loss = 2.2615 (0.906 sec/step)\n",
            "I0813 14:31:40.249340 140596802582400 learning.py:507] global step 408: loss = 2.5228 (0.861 sec/step)\n",
            "I0813 14:31:41.153589 140596802582400 learning.py:507] global step 409: loss = 2.1921 (0.902 sec/step)\n",
            "I0813 14:31:42.052938 140596802582400 learning.py:507] global step 410: loss = 2.6327 (0.898 sec/step)\n",
            "I0813 14:31:42.940859 140596802582400 learning.py:507] global step 411: loss = 2.4764 (0.886 sec/step)\n",
            "I0813 14:31:43.831782 140596802582400 learning.py:507] global step 412: loss = 2.1130 (0.889 sec/step)\n",
            "I0813 14:31:44.720706 140596802582400 learning.py:507] global step 413: loss = 2.4795 (0.887 sec/step)\n",
            "I0813 14:31:45.596354 140596802582400 learning.py:507] global step 414: loss = 2.1558 (0.874 sec/step)\n",
            "I0813 14:31:46.448460 140596802582400 learning.py:507] global step 415: loss = 2.4814 (0.851 sec/step)\n",
            "I0813 14:31:47.315756 140596802582400 learning.py:507] global step 416: loss = 2.4684 (0.866 sec/step)\n",
            "I0813 14:31:48.166325 140596802582400 learning.py:507] global step 417: loss = 1.9395 (0.849 sec/step)\n",
            "I0813 14:31:49.066616 140596802582400 learning.py:507] global step 418: loss = 1.8626 (0.899 sec/step)\n",
            "I0813 14:31:49.896514 140596802582400 learning.py:507] global step 419: loss = 2.2010 (0.828 sec/step)\n",
            "I0813 14:31:50.730336 140596802582400 learning.py:507] global step 420: loss = 1.5552 (0.832 sec/step)\n",
            "I0813 14:31:51.568306 140596802582400 learning.py:507] global step 421: loss = 2.0867 (0.836 sec/step)\n",
            "I0813 14:31:52.420937 140596802582400 learning.py:507] global step 422: loss = 2.0970 (0.851 sec/step)\n",
            "I0813 14:31:53.249369 140596802582400 learning.py:507] global step 423: loss = 2.4159 (0.826 sec/step)\n",
            "I0813 14:31:54.130437 140596802582400 learning.py:507] global step 424: loss = 3.3935 (0.879 sec/step)\n",
            "I0813 14:31:54.968326 140596802582400 learning.py:507] global step 425: loss = 2.2028 (0.836 sec/step)\n",
            "I0813 14:31:55.822289 140596802582400 learning.py:507] global step 426: loss = 2.4921 (0.852 sec/step)\n",
            "I0813 14:31:56.685070 140596802582400 learning.py:507] global step 427: loss = 1.2683 (0.861 sec/step)\n",
            "I0813 14:31:57.569203 140596802582400 learning.py:507] global step 428: loss = 2.4683 (0.883 sec/step)\n",
            "I0813 14:31:58.416883 140596802582400 learning.py:507] global step 429: loss = 2.5117 (0.846 sec/step)\n",
            "I0813 14:31:59.285075 140596802582400 learning.py:507] global step 430: loss = 2.0222 (0.866 sec/step)\n",
            "I0813 14:32:00.137660 140596802582400 learning.py:507] global step 431: loss = 3.2972 (0.851 sec/step)\n",
            "I0813 14:32:00.955966 140596802582400 learning.py:507] global step 432: loss = 2.5028 (0.817 sec/step)\n",
            "I0813 14:32:01.807226 140596802582400 learning.py:507] global step 433: loss = 2.6376 (0.850 sec/step)\n",
            "I0813 14:32:02.681388 140596802582400 learning.py:507] global step 434: loss = 1.8998 (0.873 sec/step)\n",
            "I0813 14:32:03.548593 140596802582400 learning.py:507] global step 435: loss = 1.4945 (0.866 sec/step)\n",
            "I0813 14:32:04.435940 140596802582400 learning.py:507] global step 436: loss = 1.7334 (0.886 sec/step)\n",
            "I0813 14:32:05.279349 140596802582400 learning.py:507] global step 437: loss = 2.4012 (0.842 sec/step)\n",
            "I0813 14:32:06.126959 140596802582400 learning.py:507] global step 438: loss = 1.7589 (0.846 sec/step)\n",
            "I0813 14:32:06.971582 140596802582400 learning.py:507] global step 439: loss = 3.3900 (0.843 sec/step)\n",
            "I0813 14:32:07.828554 140596802582400 learning.py:507] global step 440: loss = 3.0279 (0.855 sec/step)\n",
            "I0813 14:32:08.682371 140596802582400 learning.py:507] global step 441: loss = 2.3242 (0.852 sec/step)\n",
            "I0813 14:32:09.565940 140596802582400 learning.py:507] global step 442: loss = 2.1236 (0.882 sec/step)\n",
            "I0813 14:32:10.420180 140596802582400 learning.py:507] global step 443: loss = 2.3578 (0.852 sec/step)\n",
            "I0813 14:32:11.270355 140596802582400 learning.py:507] global step 444: loss = 2.3425 (0.849 sec/step)\n",
            "I0813 14:32:12.118222 140596802582400 learning.py:507] global step 445: loss = 1.8069 (0.846 sec/step)\n",
            "I0813 14:32:12.973530 140596802582400 learning.py:507] global step 446: loss = 2.4066 (0.854 sec/step)\n",
            "I0813 14:32:13.826304 140596802582400 learning.py:507] global step 447: loss = 2.2047 (0.851 sec/step)\n",
            "I0813 14:32:14.675526 140596802582400 learning.py:507] global step 448: loss = 1.9334 (0.848 sec/step)\n",
            "I0813 14:32:15.530108 140596802582400 learning.py:507] global step 449: loss = 2.3733 (0.853 sec/step)\n",
            "I0813 14:32:16.380139 140596802582400 learning.py:507] global step 450: loss = 2.6236 (0.848 sec/step)\n",
            "I0813 14:32:17.230675 140596802582400 learning.py:507] global step 451: loss = 1.7612 (0.849 sec/step)\n",
            "I0813 14:32:18.067620 140596802582400 learning.py:507] global step 452: loss = 2.6954 (0.835 sec/step)\n",
            "I0813 14:32:18.910160 140596802582400 learning.py:507] global step 453: loss = 3.0478 (0.841 sec/step)\n",
            "I0813 14:32:19.806228 140596802582400 learning.py:507] global step 454: loss = 1.9526 (0.894 sec/step)\n",
            "I0813 14:32:20.677821 140596802582400 learning.py:507] global step 455: loss = 3.4681 (0.870 sec/step)\n",
            "I0813 14:32:21.528737 140596802582400 learning.py:507] global step 456: loss = 2.1473 (0.849 sec/step)\n",
            "I0813 14:32:22.384417 140596802582400 learning.py:507] global step 457: loss = 2.5358 (0.854 sec/step)\n",
            "I0813 14:32:23.232039 140596802582400 learning.py:507] global step 458: loss = 1.3230 (0.846 sec/step)\n",
            "I0813 14:32:24.104019 140596802582400 learning.py:507] global step 459: loss = 1.4546 (0.870 sec/step)\n",
            "I0813 14:32:24.955748 140596802582400 learning.py:507] global step 460: loss = 2.5255 (0.850 sec/step)\n",
            "I0813 14:32:25.794059 140596802582400 learning.py:507] global step 461: loss = 3.3191 (0.837 sec/step)\n",
            "I0813 14:32:26.655710 140596802582400 learning.py:507] global step 462: loss = 2.7508 (0.859 sec/step)\n",
            "I0813 14:32:27.507775 140596802582400 learning.py:507] global step 463: loss = 2.5856 (0.850 sec/step)\n",
            "I0813 14:32:28.361616 140596802582400 learning.py:507] global step 464: loss = 2.4261 (0.852 sec/step)\n",
            "I0813 14:32:29.213985 140596802582400 learning.py:507] global step 465: loss = 2.4006 (0.851 sec/step)\n",
            "I0813 14:32:30.130110 140596802582400 learning.py:507] global step 466: loss = 3.0601 (0.915 sec/step)\n",
            "I0813 14:32:31.012139 140596802582400 learning.py:507] global step 467: loss = 1.5956 (0.880 sec/step)\n",
            "I0813 14:32:31.855986 140596802582400 learning.py:507] global step 468: loss = 2.3114 (0.842 sec/step)\n",
            "I0813 14:32:32.704354 140596802582400 learning.py:507] global step 469: loss = 2.2797 (0.847 sec/step)\n",
            "I0813 14:32:33.557416 140596802582400 learning.py:507] global step 470: loss = 1.9984 (0.851 sec/step)\n",
            "I0813 14:32:34.408920 140596802582400 learning.py:507] global step 471: loss = 2.5476 (0.850 sec/step)\n",
            "I0813 14:32:35.260839 140596802582400 learning.py:507] global step 472: loss = 2.1211 (0.851 sec/step)\n",
            "I0813 14:32:36.113984 140596802582400 learning.py:507] global step 473: loss = 2.1745 (0.851 sec/step)\n",
            "I0813 14:32:36.969653 140596802582400 learning.py:507] global step 474: loss = 2.1908 (0.854 sec/step)\n",
            "I0813 14:32:37.822647 140596802582400 learning.py:507] global step 475: loss = 2.1046 (0.852 sec/step)\n",
            "I0813 14:32:38.656854 140596802582400 learning.py:507] global step 476: loss = 1.9986 (0.833 sec/step)\n",
            "I0813 14:32:39.506383 140596802582400 learning.py:507] global step 477: loss = 2.5561 (0.848 sec/step)\n",
            "I0813 14:32:40.380639 140596802582400 learning.py:507] global step 478: loss = 2.7243 (0.873 sec/step)\n",
            "I0813 14:32:41.217908 140596802582400 learning.py:507] global step 479: loss = 2.6363 (0.836 sec/step)\n",
            "I0813 14:32:42.062745 140596802582400 learning.py:507] global step 480: loss = 2.0326 (0.843 sec/step)\n",
            "I0813 14:32:42.900719 140596802582400 learning.py:507] global step 481: loss = 2.0685 (0.836 sec/step)\n",
            "I0813 14:32:43.766421 140596802582400 learning.py:507] global step 482: loss = 3.1888 (0.864 sec/step)\n",
            "I0813 14:32:44.604809 140596802582400 learning.py:507] global step 483: loss = 3.0712 (0.837 sec/step)\n",
            "I0813 14:32:45.488855 140596802582400 learning.py:507] global step 484: loss = 2.5326 (0.882 sec/step)\n",
            "I0813 14:32:46.355726 140596802582400 learning.py:507] global step 485: loss = 2.2358 (0.865 sec/step)\n",
            "I0813 14:32:47.196841 140596802582400 learning.py:507] global step 486: loss = 2.1465 (0.839 sec/step)\n",
            "I0813 14:32:48.050724 140596802582400 learning.py:507] global step 487: loss = 1.5849 (0.852 sec/step)\n",
            "I0813 14:32:48.891068 140596802582400 learning.py:507] global step 488: loss = 3.1292 (0.839 sec/step)\n",
            "I0813 14:32:49.719171 140596802582400 learning.py:507] global step 489: loss = 2.1714 (0.827 sec/step)\n",
            "I0813 14:32:50.582847 140596802582400 learning.py:507] global step 490: loss = 2.1160 (0.862 sec/step)\n",
            "I0813 14:32:51.438900 140596802582400 learning.py:507] global step 491: loss = 1.9069 (0.855 sec/step)\n",
            "I0813 14:32:52.270401 140596802582400 learning.py:507] global step 492: loss = 2.1764 (0.830 sec/step)\n",
            "I0813 14:32:53.106641 140596802582400 learning.py:507] global step 493: loss = 1.4551 (0.835 sec/step)\n",
            "I0813 14:32:53.951124 140596802582400 learning.py:507] global step 494: loss = 1.9111 (0.843 sec/step)\n",
            "I0813 14:32:54.785853 140596802582400 learning.py:507] global step 495: loss = 2.7353 (0.833 sec/step)\n",
            "I0813 14:32:55.638552 140596802582400 learning.py:507] global step 496: loss = 2.6308 (0.851 sec/step)\n",
            "I0813 14:32:56.505008 140596802582400 learning.py:507] global step 497: loss = 1.7490 (0.865 sec/step)\n",
            "I0813 14:32:57.379372 140596802582400 learning.py:507] global step 498: loss = 2.4197 (0.873 sec/step)\n",
            "I0813 14:32:58.230021 140596802582400 learning.py:507] global step 499: loss = 1.3124 (0.849 sec/step)\n",
            "I0813 14:32:59.094209 140596802582400 learning.py:507] global step 500: loss = 1.6347 (0.863 sec/step)\n",
            "I0813 14:32:59.956104 140596802582400 learning.py:507] global step 501: loss = 1.8496 (0.860 sec/step)\n",
            "I0813 14:33:00.807119 140596802582400 learning.py:507] global step 502: loss = 1.6339 (0.849 sec/step)\n",
            "I0813 14:33:01.666926 140596802582400 learning.py:507] global step 503: loss = 1.5690 (0.858 sec/step)\n",
            "I0813 14:33:02.532270 140596802582400 learning.py:507] global step 504: loss = 2.3441 (0.864 sec/step)\n",
            "I0813 14:33:03.399144 140596802582400 learning.py:507] global step 505: loss = 2.7455 (0.865 sec/step)\n",
            "I0813 14:33:04.272001 140596802582400 learning.py:507] global step 506: loss = 1.8058 (0.871 sec/step)\n",
            "I0813 14:33:05.152411 140596802582400 learning.py:507] global step 507: loss = 2.3027 (0.879 sec/step)\n",
            "I0813 14:33:06.014496 140596802582400 learning.py:507] global step 508: loss = 1.8760 (0.860 sec/step)\n",
            "I0813 14:33:06.895005 140596802582400 learning.py:507] global step 509: loss = 1.7614 (0.879 sec/step)\n",
            "I0813 14:33:07.757827 140596802582400 learning.py:507] global step 510: loss = 2.2728 (0.861 sec/step)\n",
            "I0813 14:33:08.588459 140596802582400 learning.py:507] global step 511: loss = 1.3927 (0.829 sec/step)\n",
            "I0813 14:33:09.440619 140596802582400 learning.py:507] global step 512: loss = 1.9857 (0.851 sec/step)\n",
            "I0813 14:33:10.294065 140596802582400 learning.py:507] global step 513: loss = 2.6498 (0.852 sec/step)\n",
            "I0813 14:33:11.147596 140596802582400 learning.py:507] global step 514: loss = 2.7177 (0.852 sec/step)\n",
            "I0813 14:33:11.995279 140596802582400 learning.py:507] global step 515: loss = 1.4072 (0.846 sec/step)\n",
            "I0813 14:33:12.846199 140596802582400 learning.py:507] global step 516: loss = 2.4952 (0.849 sec/step)\n",
            "I0813 14:33:13.703400 140596802582400 learning.py:507] global step 517: loss = 2.5433 (0.856 sec/step)\n",
            "I0813 14:33:14.567023 140596802582400 learning.py:507] global step 518: loss = 2.3839 (0.862 sec/step)\n",
            "I0813 14:33:15.419161 140596802582400 learning.py:507] global step 519: loss = 1.6616 (0.851 sec/step)\n",
            "I0813 14:33:16.272842 140596802582400 learning.py:507] global step 520: loss = 2.5367 (0.852 sec/step)\n",
            "I0813 14:33:17.134094 140596802582400 learning.py:507] global step 521: loss = 2.0476 (0.859 sec/step)\n",
            "I0813 14:33:17.977640 140596802582400 learning.py:507] global step 522: loss = 1.8794 (0.842 sec/step)\n",
            "I0813 14:33:18.810125 140596802582400 learning.py:507] global step 523: loss = 1.5017 (0.831 sec/step)\n",
            "I0813 14:33:19.662363 140596802582400 learning.py:507] global step 524: loss = 2.0572 (0.851 sec/step)\n",
            "I0813 14:33:20.514194 140596802582400 learning.py:507] global step 525: loss = 2.5883 (0.850 sec/step)\n",
            "I0813 14:33:21.369426 140596802582400 learning.py:507] global step 526: loss = 2.7600 (0.854 sec/step)\n",
            "I0813 14:33:22.252234 140596802582400 learning.py:507] global step 527: loss = 2.9400 (0.881 sec/step)\n",
            "I0813 14:33:23.105390 140596802582400 learning.py:507] global step 528: loss = 1.6589 (0.851 sec/step)\n",
            "I0813 14:33:23.928722 140596802582400 learning.py:507] global step 529: loss = 2.8408 (0.822 sec/step)\n",
            "I0813 14:33:24.916919 140596802582400 learning.py:507] global step 530: loss = 1.9237 (0.932 sec/step)\n",
            "I0813 14:33:26.067075 140593722345216 supervisor.py:1050] Recording summary at step 530.\n",
            "I0813 14:33:26.333931 140596802582400 learning.py:507] global step 531: loss = 1.7724 (1.414 sec/step)\n",
            "I0813 14:33:27.202340 140596802582400 learning.py:507] global step 532: loss = 1.7706 (0.867 sec/step)\n",
            "I0813 14:33:28.070741 140596802582400 learning.py:507] global step 533: loss = 1.9938 (0.867 sec/step)\n",
            "I0813 14:33:28.909099 140596802582400 learning.py:507] global step 534: loss = 2.0070 (0.837 sec/step)\n",
            "I0813 14:33:29.299670 140593730737920 supervisor.py:1099] global_step/sec: 1.15657\n",
            "I0813 14:33:29.781349 140596802582400 learning.py:507] global step 535: loss = 2.8279 (0.871 sec/step)\n",
            "I0813 14:33:30.676463 140596802582400 learning.py:507] global step 536: loss = 1.8473 (0.893 sec/step)\n",
            "I0813 14:33:31.556037 140596802582400 learning.py:507] global step 537: loss = 2.4471 (0.878 sec/step)\n",
            "I0813 14:33:32.435734 140596802582400 learning.py:507] global step 538: loss = 1.8868 (0.878 sec/step)\n",
            "I0813 14:33:33.269677 140596802582400 learning.py:507] global step 539: loss = 2.5765 (0.832 sec/step)\n",
            "I0813 14:33:34.121517 140596802582400 learning.py:507] global step 540: loss = 2.3902 (0.850 sec/step)\n",
            "I0813 14:33:34.974099 140596802582400 learning.py:507] global step 541: loss = 3.1998 (0.851 sec/step)\n",
            "I0813 14:33:35.823424 140596802582400 learning.py:507] global step 542: loss = 2.0785 (0.848 sec/step)\n",
            "I0813 14:33:36.671477 140596802582400 learning.py:507] global step 543: loss = 1.8333 (0.847 sec/step)\n",
            "I0813 14:33:37.510473 140596802582400 learning.py:507] global step 544: loss = 1.9514 (0.837 sec/step)\n",
            "I0813 14:33:38.361235 140596802582400 learning.py:507] global step 545: loss = 2.8345 (0.849 sec/step)\n",
            "I0813 14:33:39.195304 140596802582400 learning.py:507] global step 546: loss = 2.6365 (0.832 sec/step)\n",
            "I0813 14:33:40.028923 140596802582400 learning.py:507] global step 547: loss = 1.5660 (0.832 sec/step)\n",
            "I0813 14:33:40.895196 140596802582400 learning.py:507] global step 548: loss = 2.2897 (0.865 sec/step)\n",
            "I0813 14:33:41.747134 140596802582400 learning.py:507] global step 549: loss = 2.1048 (0.851 sec/step)\n",
            "I0813 14:33:42.605795 140596802582400 learning.py:507] global step 550: loss = 2.1518 (0.857 sec/step)\n",
            "I0813 14:33:43.447967 140596802582400 learning.py:507] global step 551: loss = 1.5269 (0.840 sec/step)\n",
            "I0813 14:33:44.289307 140596802582400 learning.py:507] global step 552: loss = 2.0114 (0.840 sec/step)\n",
            "I0813 14:33:45.138240 140596802582400 learning.py:507] global step 553: loss = 2.0286 (0.847 sec/step)\n",
            "I0813 14:33:45.996921 140596802582400 learning.py:507] global step 554: loss = 2.7607 (0.857 sec/step)\n",
            "I0813 14:33:46.811617 140596802582400 learning.py:507] global step 555: loss = 1.5800 (0.813 sec/step)\n",
            "I0813 14:33:47.657129 140596802582400 learning.py:507] global step 556: loss = 1.8041 (0.844 sec/step)\n",
            "I0813 14:33:48.491495 140596802582400 learning.py:507] global step 557: loss = 2.7146 (0.833 sec/step)\n",
            "I0813 14:33:49.325235 140596802582400 learning.py:507] global step 558: loss = 2.5409 (0.832 sec/step)\n",
            "I0813 14:33:50.170009 140596802582400 learning.py:507] global step 559: loss = 2.9076 (0.843 sec/step)\n",
            "I0813 14:33:51.024892 140596802582400 learning.py:507] global step 560: loss = 3.2110 (0.853 sec/step)\n",
            "I0813 14:33:51.876562 140596802582400 learning.py:507] global step 561: loss = 2.9889 (0.850 sec/step)\n",
            "I0813 14:33:52.728638 140596802582400 learning.py:507] global step 562: loss = 2.1757 (0.850 sec/step)\n",
            "I0813 14:33:53.575209 140596802582400 learning.py:507] global step 563: loss = 2.0572 (0.845 sec/step)\n",
            "I0813 14:33:54.418826 140596802582400 learning.py:507] global step 564: loss = 2.3236 (0.841 sec/step)\n",
            "I0813 14:33:55.262259 140596802582400 learning.py:507] global step 565: loss = 1.7114 (0.842 sec/step)\n",
            "I0813 14:33:56.084802 140596802582400 learning.py:507] global step 566: loss = 1.3270 (0.821 sec/step)\n",
            "I0813 14:33:56.908132 140596802582400 learning.py:507] global step 567: loss = 2.7874 (0.822 sec/step)\n",
            "I0813 14:33:57.742871 140596802582400 learning.py:507] global step 568: loss = 2.4491 (0.833 sec/step)\n",
            "I0813 14:33:58.608647 140596802582400 learning.py:507] global step 569: loss = 1.6630 (0.864 sec/step)\n",
            "I0813 14:33:59.440736 140596802582400 learning.py:507] global step 570: loss = 2.3604 (0.831 sec/step)\n",
            "I0813 14:34:00.289586 140596802582400 learning.py:507] global step 571: loss = 2.5275 (0.847 sec/step)\n",
            "I0813 14:34:01.105236 140596802582400 learning.py:507] global step 572: loss = 1.4833 (0.814 sec/step)\n",
            "I0813 14:34:01.927395 140596802582400 learning.py:507] global step 573: loss = 2.5423 (0.820 sec/step)\n",
            "I0813 14:34:02.775902 140596802582400 learning.py:507] global step 574: loss = 1.5339 (0.847 sec/step)\n",
            "I0813 14:34:03.638583 140596802582400 learning.py:507] global step 575: loss = 2.4680 (0.861 sec/step)\n",
            "I0813 14:34:04.495017 140596802582400 learning.py:507] global step 576: loss = 1.9855 (0.855 sec/step)\n",
            "I0813 14:34:05.323155 140596802582400 learning.py:507] global step 577: loss = 2.5421 (0.827 sec/step)\n",
            "I0813 14:34:06.155450 140596802582400 learning.py:507] global step 578: loss = 1.6967 (0.831 sec/step)\n",
            "I0813 14:34:06.988859 140596802582400 learning.py:507] global step 579: loss = 2.0116 (0.832 sec/step)\n",
            "I0813 14:34:07.840339 140596802582400 learning.py:507] global step 580: loss = 1.6925 (0.850 sec/step)\n",
            "I0813 14:34:08.691681 140596802582400 learning.py:507] global step 581: loss = 1.6284 (0.850 sec/step)\n",
            "I0813 14:34:09.541809 140596802582400 learning.py:507] global step 582: loss = 1.9108 (0.849 sec/step)\n",
            "I0813 14:34:10.412251 140596802582400 learning.py:507] global step 583: loss = 2.0785 (0.869 sec/step)\n",
            "I0813 14:34:11.249320 140596802582400 learning.py:507] global step 584: loss = 2.8543 (0.835 sec/step)\n",
            "I0813 14:34:12.097909 140596802582400 learning.py:507] global step 585: loss = 1.8946 (0.847 sec/step)\n",
            "I0813 14:34:12.931326 140596802582400 learning.py:507] global step 586: loss = 1.8267 (0.832 sec/step)\n",
            "I0813 14:34:13.799085 140596802582400 learning.py:507] global step 587: loss = 2.2868 (0.866 sec/step)\n",
            "I0813 14:34:14.635664 140596802582400 learning.py:507] global step 588: loss = 2.2289 (0.835 sec/step)\n",
            "I0813 14:34:15.468714 140596802582400 learning.py:507] global step 589: loss = 2.4551 (0.832 sec/step)\n",
            "I0813 14:34:16.303385 140596802582400 learning.py:507] global step 590: loss = 1.8234 (0.833 sec/step)\n",
            "I0813 14:34:17.152852 140596802582400 learning.py:507] global step 591: loss = 2.5133 (0.848 sec/step)\n",
            "I0813 14:34:18.006123 140596802582400 learning.py:507] global step 592: loss = 2.4906 (0.852 sec/step)\n",
            "I0813 14:34:18.858713 140596802582400 learning.py:507] global step 593: loss = 1.7619 (0.851 sec/step)\n",
            "I0813 14:34:19.686953 140596802582400 learning.py:507] global step 594: loss = 2.3667 (0.827 sec/step)\n",
            "I0813 14:34:20.536459 140596802582400 learning.py:507] global step 595: loss = 2.3848 (0.848 sec/step)\n",
            "I0813 14:34:21.386581 140596802582400 learning.py:507] global step 596: loss = 2.4155 (0.848 sec/step)\n",
            "I0813 14:34:22.242219 140596802582400 learning.py:507] global step 597: loss = 3.1365 (0.854 sec/step)\n",
            "I0813 14:34:23.092942 140596802582400 learning.py:507] global step 598: loss = 1.6182 (0.849 sec/step)\n",
            "I0813 14:34:23.964471 140596802582400 learning.py:507] global step 599: loss = 1.9651 (0.870 sec/step)\n",
            "I0813 14:34:24.781255 140596802582400 learning.py:507] global step 600: loss = 2.1839 (0.815 sec/step)\n",
            "I0813 14:34:25.634732 140596802582400 learning.py:507] global step 601: loss = 2.0592 (0.851 sec/step)\n",
            "I0813 14:34:26.495356 140596802582400 learning.py:507] global step 602: loss = 3.0947 (0.859 sec/step)\n",
            "I0813 14:34:27.333817 140596802582400 learning.py:507] global step 603: loss = 2.6479 (0.837 sec/step)\n",
            "I0813 14:34:28.170318 140596802582400 learning.py:507] global step 604: loss = 2.5887 (0.835 sec/step)\n",
            "I0813 14:34:29.072930 140596802582400 learning.py:507] global step 605: loss = 2.1023 (0.901 sec/step)\n",
            "I0813 14:34:29.918226 140596802582400 learning.py:507] global step 606: loss = 2.7986 (0.844 sec/step)\n",
            "I0813 14:34:30.757232 140596802582400 learning.py:507] global step 607: loss = 1.5310 (0.837 sec/step)\n",
            "I0813 14:34:31.648355 140596802582400 learning.py:507] global step 608: loss = 2.0404 (0.890 sec/step)\n",
            "I0813 14:34:32.495730 140596802582400 learning.py:507] global step 609: loss = 1.6453 (0.846 sec/step)\n",
            "I0813 14:34:33.319066 140596802582400 learning.py:507] global step 610: loss = 3.0220 (0.822 sec/step)\n",
            "I0813 14:34:34.177990 140596802582400 learning.py:507] global step 611: loss = 2.0658 (0.857 sec/step)\n",
            "I0813 14:34:35.022848 140596802582400 learning.py:507] global step 612: loss = 2.6363 (0.843 sec/step)\n",
            "I0813 14:34:35.863719 140596802582400 learning.py:507] global step 613: loss = 2.6038 (0.839 sec/step)\n",
            "I0813 14:34:36.698384 140596802582400 learning.py:507] global step 614: loss = 0.7979 (0.833 sec/step)\n",
            "I0813 14:34:37.542915 140596802582400 learning.py:507] global step 615: loss = 1.7655 (0.843 sec/step)\n",
            "I0813 14:34:38.387059 140596802582400 learning.py:507] global step 616: loss = 2.2833 (0.842 sec/step)\n",
            "I0813 14:34:39.219866 140596802582400 learning.py:507] global step 617: loss = 1.2479 (0.831 sec/step)\n",
            "I0813 14:34:40.043327 140596802582400 learning.py:507] global step 618: loss = 2.0580 (0.822 sec/step)\n",
            "I0813 14:34:40.878066 140596802582400 learning.py:507] global step 619: loss = 2.2316 (0.833 sec/step)\n",
            "I0813 14:34:41.728583 140596802582400 learning.py:507] global step 620: loss = 1.6314 (0.849 sec/step)\n",
            "I0813 14:34:42.582847 140596802582400 learning.py:507] global step 621: loss = 2.5865 (0.853 sec/step)\n",
            "I0813 14:34:43.438236 140596802582400 learning.py:507] global step 622: loss = 2.0230 (0.854 sec/step)\n",
            "I0813 14:34:44.266719 140596802582400 learning.py:507] global step 623: loss = 1.8252 (0.827 sec/step)\n",
            "I0813 14:34:45.120226 140596802582400 learning.py:507] global step 624: loss = 2.5946 (0.852 sec/step)\n",
            "I0813 14:34:45.951904 140596802582400 learning.py:507] global step 625: loss = 1.9375 (0.830 sec/step)\n",
            "I0813 14:34:46.802562 140596802582400 learning.py:507] global step 626: loss = 2.1235 (0.849 sec/step)\n",
            "I0813 14:34:47.636921 140596802582400 learning.py:507] global step 627: loss = 1.4691 (0.833 sec/step)\n",
            "I0813 14:34:48.487402 140596802582400 learning.py:507] global step 628: loss = 2.1084 (0.849 sec/step)\n",
            "I0813 14:34:49.308176 140596802582400 learning.py:507] global step 629: loss = 1.4024 (0.819 sec/step)\n",
            "I0813 14:34:50.154020 140596802582400 learning.py:507] global step 630: loss = 2.0722 (0.844 sec/step)\n",
            "I0813 14:34:51.004781 140596802582400 learning.py:507] global step 631: loss = 1.4178 (0.849 sec/step)\n",
            "I0813 14:34:51.855857 140596802582400 learning.py:507] global step 632: loss = 2.3295 (0.849 sec/step)\n",
            "I0813 14:34:52.689691 140596802582400 learning.py:507] global step 633: loss = 2.1928 (0.832 sec/step)\n",
            "I0813 14:34:53.522336 140596802582400 learning.py:507] global step 634: loss = 2.5928 (0.831 sec/step)\n",
            "I0813 14:34:54.355527 140596802582400 learning.py:507] global step 635: loss = 1.4352 (0.832 sec/step)\n",
            "I0813 14:34:55.211947 140596802582400 learning.py:507] global step 636: loss = 2.7109 (0.855 sec/step)\n",
            "I0813 14:34:56.079929 140596802582400 learning.py:507] global step 637: loss = 2.0946 (0.866 sec/step)\n",
            "I0813 14:34:56.916946 140596802582400 learning.py:507] global step 638: loss = 2.4827 (0.835 sec/step)\n",
            "I0813 14:34:57.765253 140596802582400 learning.py:507] global step 639: loss = 1.6407 (0.847 sec/step)\n",
            "I0813 14:34:58.609496 140596802582400 learning.py:507] global step 640: loss = 2.7725 (0.843 sec/step)\n",
            "I0813 14:34:59.472911 140596802582400 learning.py:507] global step 641: loss = 1.9352 (0.862 sec/step)\n",
            "I0813 14:35:00.326272 140596802582400 learning.py:507] global step 642: loss = 1.2880 (0.852 sec/step)\n",
            "I0813 14:35:01.178924 140596802582400 learning.py:507] global step 643: loss = 1.6463 (0.851 sec/step)\n",
            "I0813 14:35:02.030732 140596802582400 learning.py:507] global step 644: loss = 1.3636 (0.850 sec/step)\n",
            "I0813 14:35:02.885990 140596802582400 learning.py:507] global step 645: loss = 2.6108 (0.854 sec/step)\n",
            "I0813 14:35:03.737998 140596802582400 learning.py:507] global step 646: loss = 2.4344 (0.851 sec/step)\n",
            "I0813 14:35:04.591188 140596802582400 learning.py:507] global step 647: loss = 1.8822 (0.852 sec/step)\n",
            "I0813 14:35:05.470184 140596802582400 learning.py:507] global step 648: loss = 1.4622 (0.877 sec/step)\n",
            "I0813 14:35:06.318294 140596802582400 learning.py:507] global step 649: loss = 2.5608 (0.847 sec/step)\n",
            "I0813 14:35:07.161675 140596802582400 learning.py:507] global step 650: loss = 2.0349 (0.842 sec/step)\n",
            "I0813 14:35:08.037029 140596802582400 learning.py:507] global step 651: loss = 2.0447 (0.874 sec/step)\n",
            "I0813 14:35:08.866209 140596802582400 learning.py:507] global step 652: loss = 1.7663 (0.828 sec/step)\n",
            "I0813 14:35:09.704968 140596802582400 learning.py:507] global step 653: loss = 3.2247 (0.837 sec/step)\n",
            "I0813 14:35:10.565384 140596802582400 learning.py:507] global step 654: loss = 2.8634 (0.859 sec/step)\n",
            "I0813 14:35:11.421637 140596802582400 learning.py:507] global step 655: loss = 2.4142 (0.855 sec/step)\n",
            "I0813 14:35:12.274305 140596802582400 learning.py:507] global step 656: loss = 1.8445 (0.851 sec/step)\n",
            "I0813 14:35:13.126559 140596802582400 learning.py:507] global step 657: loss = 3.1567 (0.850 sec/step)\n",
            "I0813 14:35:13.977407 140596802582400 learning.py:507] global step 658: loss = 2.6213 (0.849 sec/step)\n",
            "I0813 14:35:14.808442 140596802582400 learning.py:507] global step 659: loss = 3.9750 (0.829 sec/step)\n",
            "I0813 14:35:15.659838 140596802582400 learning.py:507] global step 660: loss = 3.2628 (0.850 sec/step)\n",
            "I0813 14:35:16.508254 140596802582400 learning.py:507] global step 661: loss = 2.2176 (0.847 sec/step)\n",
            "I0813 14:35:17.361860 140596802582400 learning.py:507] global step 662: loss = 1.6360 (0.852 sec/step)\n",
            "I0813 14:35:18.193735 140596802582400 learning.py:507] global step 663: loss = 2.4974 (0.830 sec/step)\n",
            "I0813 14:35:19.016156 140596802582400 learning.py:507] global step 664: loss = 1.7742 (0.820 sec/step)\n",
            "I0813 14:35:19.859209 140596802582400 learning.py:507] global step 665: loss = 2.2574 (0.841 sec/step)\n",
            "I0813 14:35:20.709464 140596802582400 learning.py:507] global step 666: loss = 1.2427 (0.849 sec/step)\n",
            "I0813 14:35:21.578263 140596802582400 learning.py:507] global step 667: loss = 2.1462 (0.867 sec/step)\n",
            "I0813 14:35:22.413435 140596802582400 learning.py:507] global step 668: loss = 2.4036 (0.834 sec/step)\n",
            "I0813 14:35:23.244569 140596802582400 learning.py:507] global step 669: loss = 1.8002 (0.830 sec/step)\n",
            "I0813 14:35:24.073394 140596802582400 learning.py:507] global step 670: loss = 1.6919 (0.827 sec/step)\n",
            "I0813 14:35:24.227394 140593739130624 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0813 14:35:25.211712 140596802582400 learning.py:507] global step 671: loss = 1.3571 (1.017 sec/step)\n",
            "I0813 14:35:26.137655 140593722345216 supervisor.py:1050] Recording summary at step 671.\n",
            "I0813 14:35:27.164914 140596802582400 learning.py:507] global step 672: loss = 2.8292 (1.915 sec/step)\n",
            "I0813 14:35:28.611896 140596802582400 learning.py:507] global step 673: loss = 1.7213 (1.302 sec/step)\n",
            "I0813 14:35:29.096105 140593730737920 supervisor.py:1099] global_step/sec: 1.1603\n",
            "I0813 14:35:29.457512 140596802582400 learning.py:507] global step 674: loss = 1.9991 (0.843 sec/step)\n",
            "I0813 14:35:30.368921 140596802582400 learning.py:507] global step 675: loss = 1.4199 (0.910 sec/step)\n",
            "I0813 14:35:31.224961 140596802582400 learning.py:507] global step 676: loss = 2.0916 (0.854 sec/step)\n",
            "I0813 14:35:32.057689 140596802582400 learning.py:507] global step 677: loss = 2.6589 (0.831 sec/step)\n",
            "I0813 14:35:32.902796 140596802582400 learning.py:507] global step 678: loss = 2.8502 (0.843 sec/step)\n",
            "I0813 14:35:33.756819 140596802582400 learning.py:507] global step 679: loss = 2.0980 (0.852 sec/step)\n",
            "I0813 14:35:34.593209 140596802582400 learning.py:507] global step 680: loss = 1.9552 (0.835 sec/step)\n",
            "I0813 14:35:35.436444 140596802582400 learning.py:507] global step 681: loss = 2.0331 (0.841 sec/step)\n",
            "I0813 14:35:36.273449 140596802582400 learning.py:507] global step 682: loss = 2.2797 (0.835 sec/step)\n",
            "I0813 14:35:37.118684 140596802582400 learning.py:507] global step 683: loss = 2.1168 (0.844 sec/step)\n",
            "I0813 14:35:37.977278 140596802582400 learning.py:507] global step 684: loss = 1.8448 (0.857 sec/step)\n",
            "I0813 14:35:38.850738 140596802582400 learning.py:507] global step 685: loss = 1.8688 (0.871 sec/step)\n",
            "I0813 14:35:39.692796 140596802582400 learning.py:507] global step 686: loss = 2.5544 (0.841 sec/step)\n",
            "I0813 14:35:40.545396 140596802582400 learning.py:507] global step 687: loss = 2.0819 (0.851 sec/step)\n",
            "I0813 14:35:41.404199 140596802582400 learning.py:507] global step 688: loss = 2.5555 (0.857 sec/step)\n",
            "I0813 14:35:42.264460 140596802582400 learning.py:507] global step 689: loss = 1.5163 (0.859 sec/step)\n",
            "I0813 14:35:43.123492 140596802582400 learning.py:507] global step 690: loss = 1.8150 (0.858 sec/step)\n",
            "I0813 14:35:43.979805 140596802582400 learning.py:507] global step 691: loss = 2.6179 (0.855 sec/step)\n",
            "I0813 14:35:44.828776 140596802582400 learning.py:507] global step 692: loss = 1.5673 (0.847 sec/step)\n",
            "I0813 14:35:45.723483 140596802582400 learning.py:507] global step 693: loss = 2.1965 (0.893 sec/step)\n",
            "I0813 14:35:46.578399 140596802582400 learning.py:507] global step 694: loss = 1.9655 (0.853 sec/step)\n",
            "I0813 14:35:47.430704 140596802582400 learning.py:507] global step 695: loss = 2.3659 (0.851 sec/step)\n",
            "I0813 14:35:48.301938 140596802582400 learning.py:507] global step 696: loss = 2.7762 (0.870 sec/step)\n",
            "I0813 14:35:49.137031 140596802582400 learning.py:507] global step 697: loss = 1.6265 (0.833 sec/step)\n",
            "I0813 14:35:49.982640 140596802582400 learning.py:507] global step 698: loss = 2.0229 (0.844 sec/step)\n",
            "I0813 14:35:50.834921 140596802582400 learning.py:507] global step 699: loss = 2.5624 (0.851 sec/step)\n",
            "I0813 14:35:51.695543 140596802582400 learning.py:507] global step 700: loss = 1.8339 (0.859 sec/step)\n",
            "I0813 14:35:52.547269 140596802582400 learning.py:507] global step 701: loss = 2.3312 (0.850 sec/step)\n",
            "I0813 14:35:53.396662 140596802582400 learning.py:507] global step 702: loss = 2.7456 (0.848 sec/step)\n",
            "I0813 14:35:54.243468 140596802582400 learning.py:507] global step 703: loss = 1.8628 (0.845 sec/step)\n",
            "I0813 14:35:55.079761 140596802582400 learning.py:507] global step 704: loss = 1.7161 (0.835 sec/step)\n",
            "I0813 14:35:55.914670 140596802582400 learning.py:507] global step 705: loss = 2.2442 (0.833 sec/step)\n",
            "I0813 14:35:56.769370 140596802582400 learning.py:507] global step 706: loss = 1.3240 (0.853 sec/step)\n",
            "I0813 14:35:57.619416 140596802582400 learning.py:507] global step 707: loss = 2.6644 (0.848 sec/step)\n",
            "I0813 14:35:58.439474 140596802582400 learning.py:507] global step 708: loss = 1.8170 (0.819 sec/step)\n",
            "I0813 14:35:59.262660 140596802582400 learning.py:507] global step 709: loss = 3.2930 (0.822 sec/step)\n",
            "I0813 14:36:00.095022 140596802582400 learning.py:507] global step 710: loss = 3.5578 (0.831 sec/step)\n",
            "I0813 14:36:00.965682 140596802582400 learning.py:507] global step 711: loss = 1.6084 (0.869 sec/step)\n",
            "I0813 14:36:01.784388 140596802582400 learning.py:507] global step 712: loss = 2.4571 (0.817 sec/step)\n",
            "I0813 14:36:02.651721 140596802582400 learning.py:507] global step 713: loss = 1.9183 (0.866 sec/step)\n",
            "I0813 14:36:03.521974 140596802582400 learning.py:507] global step 714: loss = 2.0576 (0.869 sec/step)\n",
            "I0813 14:36:04.376288 140596802582400 learning.py:507] global step 715: loss = 1.5385 (0.853 sec/step)\n",
            "I0813 14:36:05.208464 140596802582400 learning.py:507] global step 716: loss = 2.3451 (0.831 sec/step)\n",
            "I0813 14:36:06.039318 140596802582400 learning.py:507] global step 717: loss = 2.3654 (0.829 sec/step)\n",
            "I0813 14:36:06.857509 140596802582400 learning.py:507] global step 718: loss = 2.1909 (0.817 sec/step)\n",
            "I0813 14:36:07.704379 140596802582400 learning.py:507] global step 719: loss = 2.3156 (0.845 sec/step)\n",
            "I0813 14:36:08.534802 140596802582400 learning.py:507] global step 720: loss = 2.6346 (0.829 sec/step)\n",
            "I0813 14:36:09.372209 140596802582400 learning.py:507] global step 721: loss = 1.2665 (0.836 sec/step)\n",
            "I0813 14:36:10.201992 140596802582400 learning.py:507] global step 722: loss = 1.8701 (0.828 sec/step)\n",
            "I0813 14:36:11.155428 140596802582400 learning.py:507] global step 723: loss = 2.1994 (0.952 sec/step)\n",
            "I0813 14:36:11.989485 140596802582400 learning.py:507] global step 724: loss = 2.2110 (0.833 sec/step)\n",
            "I0813 14:36:12.821068 140596802582400 learning.py:507] global step 725: loss = 1.9340 (0.830 sec/step)\n",
            "I0813 14:36:13.649445 140596802582400 learning.py:507] global step 726: loss = 2.4882 (0.827 sec/step)\n",
            "I0813 14:36:14.475534 140596802582400 learning.py:507] global step 727: loss = 2.3406 (0.824 sec/step)\n",
            "I0813 14:36:15.321347 140596802582400 learning.py:507] global step 728: loss = 1.9685 (0.841 sec/step)\n",
            "I0813 14:36:16.247140 140596802582400 learning.py:507] global step 729: loss = 1.6443 (0.924 sec/step)\n",
            "I0813 14:36:17.094722 140596802582400 learning.py:507] global step 730: loss = 2.8486 (0.846 sec/step)\n",
            "I0813 14:36:17.968375 140596802582400 learning.py:507] global step 731: loss = 2.2868 (0.872 sec/step)\n",
            "I0813 14:36:18.799620 140596802582400 learning.py:507] global step 732: loss = 1.4237 (0.830 sec/step)\n",
            "I0813 14:36:19.635071 140596802582400 learning.py:507] global step 733: loss = 2.2313 (0.834 sec/step)\n",
            "I0813 14:36:20.477533 140596802582400 learning.py:507] global step 734: loss = 1.7581 (0.841 sec/step)\n",
            "I0813 14:36:21.316878 140596802582400 learning.py:507] global step 735: loss = 1.6950 (0.838 sec/step)\n",
            "I0813 14:36:22.153318 140596802582400 learning.py:507] global step 736: loss = 1.6718 (0.835 sec/step)\n",
            "I0813 14:36:22.987204 140596802582400 learning.py:507] global step 737: loss = 2.2838 (0.832 sec/step)\n",
            "I0813 14:36:23.811462 140596802582400 learning.py:507] global step 738: loss = 1.4836 (0.823 sec/step)\n",
            "I0813 14:36:24.643056 140596802582400 learning.py:507] global step 739: loss = 2.4046 (0.830 sec/step)\n",
            "I0813 14:36:25.492439 140596802582400 learning.py:507] global step 740: loss = 2.4233 (0.848 sec/step)\n",
            "I0813 14:36:26.360615 140596802582400 learning.py:507] global step 741: loss = 2.2778 (0.867 sec/step)\n",
            "I0813 14:36:27.237458 140596802582400 learning.py:507] global step 742: loss = 2.2548 (0.875 sec/step)\n",
            "I0813 14:36:28.112509 140596802582400 learning.py:507] global step 743: loss = 2.6108 (0.873 sec/step)\n",
            "I0813 14:36:28.960863 140596802582400 learning.py:507] global step 744: loss = 1.7022 (0.847 sec/step)\n",
            "I0813 14:36:29.814863 140596802582400 learning.py:507] global step 745: loss = 2.2608 (0.852 sec/step)\n",
            "I0813 14:36:30.669311 140596802582400 learning.py:507] global step 746: loss = 1.6733 (0.853 sec/step)\n",
            "I0813 14:36:31.581466 140596802582400 learning.py:507] global step 747: loss = 2.1037 (0.910 sec/step)\n",
            "I0813 14:36:32.417249 140596802582400 learning.py:507] global step 748: loss = 2.5607 (0.834 sec/step)\n",
            "I0813 14:36:33.263520 140596802582400 learning.py:507] global step 749: loss = 2.6792 (0.845 sec/step)\n",
            "I0813 14:36:34.084158 140596802582400 learning.py:507] global step 750: loss = 2.1114 (0.819 sec/step)\n",
            "I0813 14:36:34.892436 140596802582400 learning.py:507] global step 751: loss = 2.2820 (0.807 sec/step)\n",
            "I0813 14:36:35.736935 140596802582400 learning.py:507] global step 752: loss = 3.1675 (0.843 sec/step)\n",
            "I0813 14:36:36.590952 140596802582400 learning.py:507] global step 753: loss = 1.6362 (0.852 sec/step)\n",
            "I0813 14:36:37.441160 140596802582400 learning.py:507] global step 754: loss = 2.3042 (0.849 sec/step)\n",
            "I0813 14:36:38.273677 140596802582400 learning.py:507] global step 755: loss = 1.7586 (0.831 sec/step)\n",
            "I0813 14:36:39.106402 140596802582400 learning.py:507] global step 756: loss = 2.1583 (0.831 sec/step)\n",
            "I0813 14:36:39.955036 140596802582400 learning.py:507] global step 757: loss = 2.3349 (0.847 sec/step)\n",
            "I0813 14:36:40.777181 140596802582400 learning.py:507] global step 758: loss = 1.8601 (0.821 sec/step)\n",
            "I0813 14:36:41.638841 140596802582400 learning.py:507] global step 759: loss = 1.6838 (0.860 sec/step)\n",
            "I0813 14:36:42.508148 140596802582400 learning.py:507] global step 760: loss = 1.3190 (0.868 sec/step)\n",
            "I0813 14:36:43.341849 140596802582400 learning.py:507] global step 761: loss = 1.5993 (0.832 sec/step)\n",
            "I0813 14:36:44.173053 140596802582400 learning.py:507] global step 762: loss = 1.4423 (0.830 sec/step)\n",
            "I0813 14:36:45.005278 140596802582400 learning.py:507] global step 763: loss = 1.6885 (0.831 sec/step)\n",
            "I0813 14:36:45.855882 140596802582400 learning.py:507] global step 764: loss = 1.5018 (0.849 sec/step)\n",
            "I0813 14:36:46.702592 140596802582400 learning.py:507] global step 765: loss = 2.3797 (0.845 sec/step)\n",
            "I0813 14:36:47.534412 140596802582400 learning.py:507] global step 766: loss = 2.6203 (0.830 sec/step)\n",
            "I0813 14:36:48.352378 140596802582400 learning.py:507] global step 767: loss = 2.3393 (0.816 sec/step)\n",
            "I0813 14:36:49.222250 140596802582400 learning.py:507] global step 768: loss = 1.7766 (0.868 sec/step)\n",
            "I0813 14:36:50.057028 140596802582400 learning.py:507] global step 769: loss = 0.8692 (0.833 sec/step)\n",
            "I0813 14:36:50.901736 140596802582400 learning.py:507] global step 770: loss = 2.3499 (0.843 sec/step)\n",
            "I0813 14:36:51.736680 140596802582400 learning.py:507] global step 771: loss = 2.0669 (0.834 sec/step)\n",
            "I0813 14:36:52.568692 140596802582400 learning.py:507] global step 772: loss = 2.2797 (0.830 sec/step)\n",
            "I0813 14:36:53.385120 140596802582400 learning.py:507] global step 773: loss = 1.9405 (0.815 sec/step)\n",
            "I0813 14:36:54.227702 140596802582400 learning.py:507] global step 774: loss = 1.6262 (0.841 sec/step)\n",
            "I0813 14:36:55.063703 140596802582400 learning.py:507] global step 775: loss = 1.7859 (0.834 sec/step)\n",
            "I0813 14:36:55.895013 140596802582400 learning.py:507] global step 776: loss = 2.1852 (0.830 sec/step)\n",
            "I0813 14:36:56.732832 140596802582400 learning.py:507] global step 777: loss = 2.7451 (0.836 sec/step)\n",
            "I0813 14:36:57.563740 140596802582400 learning.py:507] global step 778: loss = 2.8488 (0.829 sec/step)\n",
            "I0813 14:36:58.408864 140596802582400 learning.py:507] global step 779: loss = 1.7027 (0.843 sec/step)\n",
            "I0813 14:36:59.281736 140596802582400 learning.py:507] global step 780: loss = 1.8684 (0.871 sec/step)\n",
            "I0813 14:37:00.152111 140596802582400 learning.py:507] global step 781: loss = 1.9846 (0.869 sec/step)\n",
            "I0813 14:37:01.008069 140596802582400 learning.py:507] global step 782: loss = 2.3101 (0.854 sec/step)\n",
            "I0813 14:37:01.873594 140596802582400 learning.py:507] global step 783: loss = 2.4015 (0.864 sec/step)\n",
            "I0813 14:37:02.718828 140596802582400 learning.py:507] global step 784: loss = 1.1517 (0.844 sec/step)\n",
            "I0813 14:37:03.584673 140596802582400 learning.py:507] global step 785: loss = 1.7742 (0.864 sec/step)\n",
            "I0813 14:37:04.449086 140596802582400 learning.py:507] global step 786: loss = 3.0372 (0.863 sec/step)\n",
            "I0813 14:37:05.324199 140596802582400 learning.py:507] global step 787: loss = 1.5299 (0.874 sec/step)\n",
            "I0813 14:37:06.184282 140596802582400 learning.py:507] global step 788: loss = 1.6520 (0.859 sec/step)\n",
            "I0813 14:37:07.032968 140596802582400 learning.py:507] global step 789: loss = 1.5483 (0.847 sec/step)\n",
            "I0813 14:37:07.912192 140596802582400 learning.py:507] global step 790: loss = 1.3855 (0.877 sec/step)\n",
            "I0813 14:37:08.740974 140596802582400 learning.py:507] global step 791: loss = 1.6107 (0.827 sec/step)\n",
            "I0813 14:37:09.593680 140596802582400 learning.py:507] global step 792: loss = 2.3987 (0.851 sec/step)\n",
            "I0813 14:37:10.438909 140596802582400 learning.py:507] global step 793: loss = 1.9813 (0.844 sec/step)\n",
            "I0813 14:37:11.296683 140596802582400 learning.py:507] global step 794: loss = 2.0426 (0.856 sec/step)\n",
            "I0813 14:37:12.132556 140596802582400 learning.py:507] global step 795: loss = 2.1461 (0.834 sec/step)\n",
            "I0813 14:37:12.970134 140596802582400 learning.py:507] global step 796: loss = 1.8055 (0.836 sec/step)\n",
            "I0813 14:37:13.817019 140596802582400 learning.py:507] global step 797: loss = 2.2260 (0.845 sec/step)\n",
            "I0813 14:37:14.677354 140596802582400 learning.py:507] global step 798: loss = 2.3753 (0.858 sec/step)\n",
            "I0813 14:37:15.538961 140596802582400 learning.py:507] global step 799: loss = 2.2744 (0.860 sec/step)\n",
            "I0813 14:37:16.381222 140596802582400 learning.py:507] global step 800: loss = 1.5440 (0.841 sec/step)\n",
            "I0813 14:37:17.238256 140596802582400 learning.py:507] global step 801: loss = 2.9731 (0.856 sec/step)\n",
            "I0813 14:37:18.074374 140596802582400 learning.py:507] global step 802: loss = 2.3981 (0.835 sec/step)\n",
            "I0813 14:37:18.911906 140596802582400 learning.py:507] global step 803: loss = 2.4276 (0.836 sec/step)\n",
            "I0813 14:37:19.772963 140596802582400 learning.py:507] global step 804: loss = 1.8830 (0.859 sec/step)\n",
            "I0813 14:37:20.630721 140596802582400 learning.py:507] global step 805: loss = 1.6346 (0.856 sec/step)\n",
            "I0813 14:37:21.509228 140596802582400 learning.py:507] global step 806: loss = 1.5092 (0.877 sec/step)\n",
            "I0813 14:37:22.341651 140596802582400 learning.py:507] global step 807: loss = 1.6537 (0.831 sec/step)\n",
            "I0813 14:37:23.188970 140596802582400 learning.py:507] global step 808: loss = 1.4504 (0.846 sec/step)\n",
            "I0813 14:37:24.040445 140596802582400 learning.py:507] global step 809: loss = 1.6708 (0.850 sec/step)\n",
            "I0813 14:37:25.465237 140596802582400 learning.py:507] global step 810: loss = 2.0271 (1.104 sec/step)\n",
            "I0813 14:37:26.178526 140593722345216 supervisor.py:1050] Recording summary at step 810.\n",
            "I0813 14:37:26.437159 140596802582400 learning.py:507] global step 811: loss = 1.9675 (0.970 sec/step)\n",
            "I0813 14:37:27.278428 140596802582400 learning.py:507] global step 812: loss = 2.7254 (0.840 sec/step)\n",
            "I0813 14:37:28.129839 140596802582400 learning.py:507] global step 813: loss = 2.2947 (0.850 sec/step)\n",
            "I0813 14:37:28.974761 140596802582400 learning.py:507] global step 814: loss = 2.2050 (0.843 sec/step)\n",
            "I0813 14:37:29.110908 140593730737920 supervisor.py:1099] global_step/sec: 1.17486\n",
            "I0813 14:37:29.838408 140596802582400 learning.py:507] global step 815: loss = 1.8211 (0.862 sec/step)\n",
            "I0813 14:37:30.705187 140596802582400 learning.py:507] global step 816: loss = 1.5962 (0.864 sec/step)\n",
            "I0813 14:37:31.575912 140596802582400 learning.py:507] global step 817: loss = 2.4251 (0.869 sec/step)\n",
            "I0813 14:37:32.468360 140596802582400 learning.py:507] global step 818: loss = 4.5800 (0.891 sec/step)\n",
            "I0813 14:37:33.312426 140596802582400 learning.py:507] global step 819: loss = 1.2745 (0.842 sec/step)\n",
            "I0813 14:37:34.164724 140596802582400 learning.py:507] global step 820: loss = 2.7038 (0.851 sec/step)\n",
            "I0813 14:37:35.022781 140596802582400 learning.py:507] global step 821: loss = 2.7766 (0.856 sec/step)\n",
            "I0813 14:37:35.872206 140596802582400 learning.py:507] global step 822: loss = 2.9011 (0.848 sec/step)\n",
            "I0813 14:37:36.703423 140596802582400 learning.py:507] global step 823: loss = 2.7660 (0.830 sec/step)\n",
            "I0813 14:37:37.536137 140596802582400 learning.py:507] global step 824: loss = 1.9278 (0.831 sec/step)\n",
            "I0813 14:37:38.364661 140596802582400 learning.py:507] global step 825: loss = 2.4966 (0.827 sec/step)\n",
            "I0813 14:37:39.219507 140596802582400 learning.py:507] global step 826: loss = 1.8743 (0.853 sec/step)\n",
            "I0813 14:37:40.067170 140596802582400 learning.py:507] global step 827: loss = 2.4162 (0.846 sec/step)\n",
            "I0813 14:37:40.938926 140596802582400 learning.py:507] global step 828: loss = 2.8462 (0.870 sec/step)\n",
            "I0813 14:37:41.784992 140596802582400 learning.py:507] global step 829: loss = 2.8117 (0.845 sec/step)\n",
            "I0813 14:37:42.626778 140596802582400 learning.py:507] global step 830: loss = 2.5809 (0.840 sec/step)\n",
            "I0813 14:37:43.467409 140596802582400 learning.py:507] global step 831: loss = 2.2015 (0.839 sec/step)\n",
            "I0813 14:37:44.305797 140596802582400 learning.py:507] global step 832: loss = 1.9164 (0.837 sec/step)\n",
            "I0813 14:37:45.136340 140596802582400 learning.py:507] global step 833: loss = 1.4890 (0.829 sec/step)\n",
            "I0813 14:37:45.967075 140596802582400 learning.py:507] global step 834: loss = 1.6574 (0.829 sec/step)\n",
            "I0813 14:37:46.796046 140596802582400 learning.py:507] global step 835: loss = 2.0697 (0.827 sec/step)\n",
            "I0813 14:37:47.640419 140596802582400 learning.py:507] global step 836: loss = 2.4120 (0.843 sec/step)\n",
            "I0813 14:37:48.480299 140596802582400 learning.py:507] global step 837: loss = 2.2967 (0.838 sec/step)\n",
            "I0813 14:37:49.350557 140596802582400 learning.py:507] global step 838: loss = 2.6870 (0.869 sec/step)\n",
            "I0813 14:37:50.187567 140596802582400 learning.py:507] global step 839: loss = 1.4794 (0.835 sec/step)\n",
            "I0813 14:37:51.054230 140596802582400 learning.py:507] global step 840: loss = 1.3492 (0.865 sec/step)\n",
            "I0813 14:37:51.890279 140596802582400 learning.py:507] global step 841: loss = 1.3541 (0.834 sec/step)\n",
            "I0813 14:37:52.741995 140596802582400 learning.py:507] global step 842: loss = 1.7836 (0.850 sec/step)\n",
            "I0813 14:37:53.594387 140596802582400 learning.py:507] global step 843: loss = 2.6086 (0.851 sec/step)\n",
            "I0813 14:37:54.449497 140596802582400 learning.py:507] global step 844: loss = 1.7610 (0.854 sec/step)\n",
            "I0813 14:37:55.288155 140596802582400 learning.py:507] global step 845: loss = 2.1684 (0.837 sec/step)\n",
            "I0813 14:37:56.136349 140596802582400 learning.py:507] global step 846: loss = 2.0486 (0.845 sec/step)\n",
            "I0813 14:37:56.982448 140596802582400 learning.py:507] global step 847: loss = 1.8828 (0.842 sec/step)\n",
            "I0813 14:37:57.833825 140596802582400 learning.py:507] global step 848: loss = 2.8293 (0.850 sec/step)\n",
            "I0813 14:37:58.683256 140596802582400 learning.py:507] global step 849: loss = 1.8556 (0.848 sec/step)\n",
            "I0813 14:37:59.530323 140596802582400 learning.py:507] global step 850: loss = 1.7909 (0.845 sec/step)\n",
            "I0813 14:38:00.391684 140596802582400 learning.py:507] global step 851: loss = 1.6293 (0.859 sec/step)\n",
            "I0813 14:38:01.242910 140596802582400 learning.py:507] global step 852: loss = 2.1074 (0.850 sec/step)\n",
            "I0813 14:38:02.097593 140596802582400 learning.py:507] global step 853: loss = 1.2947 (0.853 sec/step)\n",
            "I0813 14:38:02.991346 140596802582400 learning.py:507] global step 854: loss = 1.9244 (0.892 sec/step)\n",
            "I0813 14:38:03.881015 140596802582400 learning.py:507] global step 855: loss = 1.0541 (0.888 sec/step)\n",
            "I0813 14:38:04.748811 140596802582400 learning.py:507] global step 856: loss = 2.0110 (0.866 sec/step)\n",
            "I0813 14:38:05.584633 140596802582400 learning.py:507] global step 857: loss = 1.6778 (0.834 sec/step)\n",
            "I0813 14:38:06.449802 140596802582400 learning.py:507] global step 858: loss = 2.3256 (0.863 sec/step)\n",
            "I0813 14:38:07.289777 140596802582400 learning.py:507] global step 859: loss = 2.1413 (0.838 sec/step)\n",
            "I0813 14:38:08.170686 140596802582400 learning.py:507] global step 860: loss = 2.3788 (0.879 sec/step)\n",
            "I0813 14:38:09.014407 140596802582400 learning.py:507] global step 861: loss = 1.7438 (0.842 sec/step)\n",
            "I0813 14:38:09.862672 140596802582400 learning.py:507] global step 862: loss = 2.2939 (0.847 sec/step)\n",
            "I0813 14:38:10.695899 140596802582400 learning.py:507] global step 863: loss = 2.4303 (0.832 sec/step)\n",
            "I0813 14:38:11.569149 140596802582400 learning.py:507] global step 864: loss = 2.4290 (0.872 sec/step)\n",
            "I0813 14:38:12.409411 140596802582400 learning.py:507] global step 865: loss = 2.2520 (0.839 sec/step)\n",
            "I0813 14:38:13.256666 140596802582400 learning.py:507] global step 866: loss = 1.5736 (0.846 sec/step)\n",
            "I0813 14:38:14.107348 140596802582400 learning.py:507] global step 867: loss = 1.5347 (0.849 sec/step)\n",
            "I0813 14:38:14.974081 140596802582400 learning.py:507] global step 868: loss = 1.8568 (0.865 sec/step)\n",
            "I0813 14:38:15.812953 140596802582400 learning.py:507] global step 869: loss = 2.1427 (0.837 sec/step)\n",
            "I0813 14:38:16.642812 140596802582400 learning.py:507] global step 870: loss = 1.3696 (0.828 sec/step)\n",
            "I0813 14:38:17.475582 140596802582400 learning.py:507] global step 871: loss = 2.2544 (0.831 sec/step)\n",
            "I0813 14:38:18.327999 140596802582400 learning.py:507] global step 872: loss = 1.6751 (0.851 sec/step)\n",
            "I0813 14:38:19.181268 140596802582400 learning.py:507] global step 873: loss = 2.1181 (0.852 sec/step)\n",
            "I0813 14:38:20.041464 140596802582400 learning.py:507] global step 874: loss = 2.0642 (0.859 sec/step)\n",
            "I0813 14:38:20.895013 140596802582400 learning.py:507] global step 875: loss = 2.4233 (0.852 sec/step)\n",
            "I0813 14:38:21.755896 140596802582400 learning.py:507] global step 876: loss = 1.2092 (0.859 sec/step)\n",
            "I0813 14:38:22.627329 140596802582400 learning.py:507] global step 877: loss = 1.4027 (0.870 sec/step)\n",
            "I0813 14:38:23.481012 140596802582400 learning.py:507] global step 878: loss = 1.6652 (0.852 sec/step)\n",
            "I0813 14:38:24.332773 140596802582400 learning.py:507] global step 879: loss = 1.7712 (0.850 sec/step)\n",
            "I0813 14:38:25.209957 140596802582400 learning.py:507] global step 880: loss = 2.3213 (0.876 sec/step)\n",
            "I0813 14:38:26.066063 140596802582400 learning.py:507] global step 881: loss = 1.9026 (0.855 sec/step)\n",
            "I0813 14:38:26.911447 140596802582400 learning.py:507] global step 882: loss = 2.5380 (0.844 sec/step)\n",
            "I0813 14:38:27.777072 140596802582400 learning.py:507] global step 883: loss = 1.3693 (0.864 sec/step)\n",
            "I0813 14:38:28.615705 140596802582400 learning.py:507] global step 884: loss = 3.2408 (0.836 sec/step)\n",
            "I0813 14:38:29.483967 140596802582400 learning.py:507] global step 885: loss = 2.1844 (0.867 sec/step)\n",
            "I0813 14:38:30.336716 140596802582400 learning.py:507] global step 886: loss = 1.2402 (0.851 sec/step)\n",
            "I0813 14:38:31.168444 140596802582400 learning.py:507] global step 887: loss = 2.1936 (0.830 sec/step)\n",
            "I0813 14:38:32.061832 140596802582400 learning.py:507] global step 888: loss = 2.0381 (0.891 sec/step)\n",
            "I0813 14:38:32.931670 140596802582400 learning.py:507] global step 889: loss = 2.9286 (0.868 sec/step)\n",
            "I0813 14:38:33.799675 140596802582400 learning.py:507] global step 890: loss = 1.8030 (0.866 sec/step)\n",
            "I0813 14:38:34.646680 140596802582400 learning.py:507] global step 891: loss = 1.9605 (0.845 sec/step)\n",
            "I0813 14:38:35.486942 140596802582400 learning.py:507] global step 892: loss = 2.8180 (0.839 sec/step)\n",
            "I0813 14:38:36.336876 140596802582400 learning.py:507] global step 893: loss = 1.5867 (0.848 sec/step)\n",
            "I0813 14:38:37.171933 140596802582400 learning.py:507] global step 894: loss = 2.4248 (0.834 sec/step)\n",
            "I0813 14:38:38.027270 140596802582400 learning.py:507] global step 895: loss = 1.5025 (0.854 sec/step)\n",
            "I0813 14:38:38.875873 140596802582400 learning.py:507] global step 896: loss = 1.7803 (0.847 sec/step)\n",
            "I0813 14:38:39.708526 140596802582400 learning.py:507] global step 897: loss = 1.8108 (0.831 sec/step)\n",
            "I0813 14:38:40.559659 140596802582400 learning.py:507] global step 898: loss = 2.0472 (0.849 sec/step)\n",
            "I0813 14:38:41.390429 140596802582400 learning.py:507] global step 899: loss = 1.5364 (0.829 sec/step)\n",
            "I0813 14:38:42.240650 140596802582400 learning.py:507] global step 900: loss = 2.1549 (0.849 sec/step)\n",
            "I0813 14:38:43.110974 140596802582400 learning.py:507] global step 901: loss = 1.5913 (0.869 sec/step)\n",
            "I0813 14:38:43.957664 140596802582400 learning.py:507] global step 902: loss = 3.1134 (0.845 sec/step)\n",
            "I0813 14:38:44.797442 140596802582400 learning.py:507] global step 903: loss = 2.4459 (0.838 sec/step)\n",
            "I0813 14:38:45.631628 140596802582400 learning.py:507] global step 904: loss = 2.1014 (0.833 sec/step)\n",
            "I0813 14:38:46.458121 140596802582400 learning.py:507] global step 905: loss = 1.7819 (0.821 sec/step)\n",
            "I0813 14:38:47.294308 140596802582400 learning.py:507] global step 906: loss = 1.8462 (0.833 sec/step)\n",
            "I0813 14:38:48.146313 140596802582400 learning.py:507] global step 907: loss = 2.3724 (0.850 sec/step)\n",
            "I0813 14:38:48.976819 140596802582400 learning.py:507] global step 908: loss = 3.7046 (0.829 sec/step)\n",
            "I0813 14:38:49.829022 140596802582400 learning.py:507] global step 909: loss = 1.6083 (0.851 sec/step)\n",
            "I0813 14:38:50.671922 140596802582400 learning.py:507] global step 910: loss = 2.7214 (0.841 sec/step)\n",
            "I0813 14:38:51.514146 140596802582400 learning.py:507] global step 911: loss = 1.3878 (0.841 sec/step)\n",
            "I0813 14:38:52.354314 140596802582400 learning.py:507] global step 912: loss = 3.1160 (0.839 sec/step)\n",
            "I0813 14:38:53.215417 140596802582400 learning.py:507] global step 913: loss = 1.6615 (0.860 sec/step)\n",
            "I0813 14:38:54.033807 140596802582400 learning.py:507] global step 914: loss = 1.7306 (0.817 sec/step)\n",
            "I0813 14:38:54.859474 140596802582400 learning.py:507] global step 915: loss = 1.3340 (0.824 sec/step)\n",
            "I0813 14:38:55.703692 140596802582400 learning.py:507] global step 916: loss = 2.0287 (0.843 sec/step)\n",
            "I0813 14:38:56.550732 140596802582400 learning.py:507] global step 917: loss = 2.5064 (0.846 sec/step)\n",
            "I0813 14:38:57.421483 140596802582400 learning.py:507] global step 918: loss = 3.1674 (0.869 sec/step)\n",
            "I0813 14:38:58.266206 140596802582400 learning.py:507] global step 919: loss = 1.9858 (0.843 sec/step)\n",
            "I0813 14:38:59.102673 140596802582400 learning.py:507] global step 920: loss = 1.4244 (0.835 sec/step)\n",
            "I0813 14:38:59.948250 140596802582400 learning.py:507] global step 921: loss = 1.3440 (0.844 sec/step)\n",
            "I0813 14:39:00.801921 140596802582400 learning.py:507] global step 922: loss = 1.6072 (0.849 sec/step)\n",
            "I0813 14:39:01.652558 140596802582400 learning.py:507] global step 923: loss = 1.8707 (0.849 sec/step)\n",
            "I0813 14:39:02.504633 140596802582400 learning.py:507] global step 924: loss = 1.4168 (0.850 sec/step)\n",
            "I0813 14:39:03.374751 140596802582400 learning.py:507] global step 925: loss = 3.0138 (0.868 sec/step)\n",
            "I0813 14:39:04.286545 140596802582400 learning.py:507] global step 926: loss = 2.4278 (0.910 sec/step)\n",
            "I0813 14:39:05.177299 140596802582400 learning.py:507] global step 927: loss = 2.2791 (0.889 sec/step)\n",
            "I0813 14:39:06.008957 140596802582400 learning.py:507] global step 928: loss = 2.2592 (0.830 sec/step)\n",
            "I0813 14:39:06.875264 140596802582400 learning.py:507] global step 929: loss = 1.3281 (0.865 sec/step)\n",
            "I0813 14:39:07.731729 140596802582400 learning.py:507] global step 930: loss = 1.4462 (0.855 sec/step)\n",
            "I0813 14:39:08.581031 140596802582400 learning.py:507] global step 931: loss = 2.7978 (0.848 sec/step)\n",
            "I0813 14:39:09.420718 140596802582400 learning.py:507] global step 932: loss = 2.7982 (0.838 sec/step)\n",
            "I0813 14:39:10.260034 140596802582400 learning.py:507] global step 933: loss = 1.4213 (0.838 sec/step)\n",
            "I0813 14:39:11.114719 140596802582400 learning.py:507] global step 934: loss = 1.9252 (0.853 sec/step)\n",
            "I0813 14:39:11.938592 140596802582400 learning.py:507] global step 935: loss = 1.8544 (0.822 sec/step)\n",
            "I0813 14:39:12.783456 140596802582400 learning.py:507] global step 936: loss = 2.0880 (0.843 sec/step)\n",
            "I0813 14:39:13.622043 140596802582400 learning.py:507] global step 937: loss = 2.1179 (0.837 sec/step)\n",
            "I0813 14:39:14.488267 140596802582400 learning.py:507] global step 938: loss = 2.0079 (0.865 sec/step)\n",
            "I0813 14:39:15.348266 140596802582400 learning.py:507] global step 939: loss = 2.4084 (0.858 sec/step)\n",
            "I0813 14:39:16.197786 140596802582400 learning.py:507] global step 940: loss = 2.3003 (0.847 sec/step)\n",
            "I0813 14:39:17.028861 140596802582400 learning.py:507] global step 941: loss = 2.0875 (0.829 sec/step)\n",
            "I0813 14:39:17.875047 140596802582400 learning.py:507] global step 942: loss = 1.6595 (0.845 sec/step)\n",
            "I0813 14:39:18.727970 140596802582400 learning.py:507] global step 943: loss = 2.2317 (0.851 sec/step)\n",
            "I0813 14:39:19.580156 140596802582400 learning.py:507] global step 944: loss = 1.9994 (0.851 sec/step)\n",
            "I0813 14:39:20.413925 140596802582400 learning.py:507] global step 945: loss = 2.6471 (0.832 sec/step)\n",
            "I0813 14:39:21.265699 140596802582400 learning.py:507] global step 946: loss = 2.2766 (0.850 sec/step)\n",
            "I0813 14:39:22.120727 140596802582400 learning.py:507] global step 947: loss = 1.5095 (0.853 sec/step)\n",
            "I0813 14:39:22.972481 140596802582400 learning.py:507] global step 948: loss = 2.9655 (0.850 sec/step)\n",
            "I0813 14:39:23.832397 140596802582400 learning.py:507] global step 949: loss = 1.6453 (0.858 sec/step)\n",
            "I0813 14:39:24.752754 140596802582400 learning.py:507] global step 950: loss = 1.8037 (0.919 sec/step)\n",
            "I0813 14:39:25.715799 140593722345216 supervisor.py:1050] Recording summary at step 950.\n",
            "I0813 14:39:26.235947 140596802582400 learning.py:507] global step 951: loss = 1.2914 (1.482 sec/step)\n",
            "I0813 14:39:27.091350 140596802582400 learning.py:507] global step 952: loss = 1.9854 (0.854 sec/step)\n",
            "I0813 14:39:27.923284 140596802582400 learning.py:507] global step 953: loss = 1.8615 (0.830 sec/step)\n",
            "I0813 14:39:28.835945 140596802582400 learning.py:507] global step 954: loss = 2.3633 (0.911 sec/step)\n",
            "I0813 14:39:29.427065 140593730737920 supervisor.py:1099] global_step/sec: 1.1636\n",
            "I0813 14:39:29.692994 140596802582400 learning.py:507] global step 955: loss = 2.4461 (0.855 sec/step)\n",
            "I0813 14:39:30.539397 140596802582400 learning.py:507] global step 956: loss = 2.0839 (0.845 sec/step)\n",
            "I0813 14:39:31.378287 140596802582400 learning.py:507] global step 957: loss = 1.8569 (0.837 sec/step)\n",
            "I0813 14:39:32.244461 140596802582400 learning.py:507] global step 958: loss = 2.5506 (0.865 sec/step)\n",
            "I0813 14:39:33.097924 140596802582400 learning.py:507] global step 959: loss = 1.3746 (0.852 sec/step)\n",
            "I0813 14:39:33.934118 140596802582400 learning.py:507] global step 960: loss = 1.5489 (0.835 sec/step)\n",
            "I0813 14:39:34.802775 140596802582400 learning.py:507] global step 961: loss = 1.4004 (0.867 sec/step)\n",
            "I0813 14:39:35.631129 140596802582400 learning.py:507] global step 962: loss = 1.4811 (0.827 sec/step)\n",
            "I0813 14:39:36.477989 140596802582400 learning.py:507] global step 963: loss = 0.7325 (0.845 sec/step)\n",
            "I0813 14:39:37.316976 140596802582400 learning.py:507] global step 964: loss = 1.7622 (0.837 sec/step)\n",
            "I0813 14:39:38.150317 140596802582400 learning.py:507] global step 965: loss = 2.9160 (0.832 sec/step)\n",
            "I0813 14:39:38.994359 140596802582400 learning.py:507] global step 966: loss = 2.1736 (0.843 sec/step)\n",
            "I0813 14:39:39.830443 140596802582400 learning.py:507] global step 967: loss = 1.3670 (0.834 sec/step)\n",
            "I0813 14:39:40.681526 140596802582400 learning.py:507] global step 968: loss = 1.2435 (0.849 sec/step)\n",
            "I0813 14:39:41.568359 140596802582400 learning.py:507] global step 969: loss = 1.3635 (0.885 sec/step)\n",
            "I0813 14:39:42.420809 140596802582400 learning.py:507] global step 970: loss = 2.3121 (0.850 sec/step)\n",
            "I0813 14:39:43.289017 140596802582400 learning.py:507] global step 971: loss = 2.2606 (0.867 sec/step)\n",
            "I0813 14:39:44.147484 140596802582400 learning.py:507] global step 972: loss = 1.8193 (0.857 sec/step)\n",
            "I0813 14:39:45.012913 140596802582400 learning.py:507] global step 973: loss = 1.6249 (0.864 sec/step)\n",
            "I0813 14:39:45.852661 140596802582400 learning.py:507] global step 974: loss = 1.7514 (0.838 sec/step)\n",
            "I0813 14:39:46.702203 140596802582400 learning.py:507] global step 975: loss = 1.7405 (0.848 sec/step)\n",
            "I0813 14:39:47.555191 140596802582400 learning.py:507] global step 976: loss = 1.4045 (0.851 sec/step)\n",
            "I0813 14:39:48.390772 140596802582400 learning.py:507] global step 977: loss = 1.8204 (0.834 sec/step)\n",
            "I0813 14:39:49.238103 140596802582400 learning.py:507] global step 978: loss = 2.1681 (0.846 sec/step)\n",
            "I0813 14:39:50.078563 140596802582400 learning.py:507] global step 979: loss = 2.0756 (0.839 sec/step)\n",
            "I0813 14:39:50.920291 140596802582400 learning.py:507] global step 980: loss = 1.3492 (0.839 sec/step)\n",
            "I0813 14:39:51.771912 140596802582400 learning.py:507] global step 981: loss = 1.0131 (0.850 sec/step)\n",
            "I0813 14:39:52.640307 140596802582400 learning.py:507] global step 982: loss = 2.0371 (0.867 sec/step)\n",
            "I0813 14:39:53.536372 140596802582400 learning.py:507] global step 983: loss = 2.0023 (0.894 sec/step)\n",
            "I0813 14:39:54.426869 140596802582400 learning.py:507] global step 984: loss = 0.8615 (0.889 sec/step)\n",
            "I0813 14:39:55.314734 140596802582400 learning.py:507] global step 985: loss = 1.4972 (0.886 sec/step)\n",
            "I0813 14:39:56.171432 140596802582400 learning.py:507] global step 986: loss = 1.0626 (0.855 sec/step)\n",
            "I0813 14:39:57.062950 140596802582400 learning.py:507] global step 987: loss = 2.1066 (0.890 sec/step)\n",
            "I0813 14:39:57.955720 140596802582400 learning.py:507] global step 988: loss = 2.0899 (0.891 sec/step)\n",
            "I0813 14:39:58.864928 140596802582400 learning.py:507] global step 989: loss = 1.6053 (0.908 sec/step)\n",
            "I0813 14:39:59.723512 140596802582400 learning.py:507] global step 990: loss = 1.3682 (0.857 sec/step)\n",
            "I0813 14:40:00.611266 140596802582400 learning.py:507] global step 991: loss = 1.2785 (0.886 sec/step)\n",
            "I0813 14:40:01.483263 140596802582400 learning.py:507] global step 992: loss = 1.0230 (0.870 sec/step)\n",
            "I0813 14:40:02.356395 140596802582400 learning.py:507] global step 993: loss = 3.6960 (0.871 sec/step)\n",
            "I0813 14:40:03.267776 140596802582400 learning.py:507] global step 994: loss = 1.6409 (0.910 sec/step)\n",
            "I0813 14:40:04.162250 140596802582400 learning.py:507] global step 995: loss = 2.3522 (0.893 sec/step)\n",
            "I0813 14:40:05.102134 140596802582400 learning.py:507] global step 996: loss = 1.4157 (0.938 sec/step)\n",
            "I0813 14:40:05.994213 140596802582400 learning.py:507] global step 997: loss = 2.2877 (0.890 sec/step)\n",
            "I0813 14:40:06.889837 140596802582400 learning.py:507] global step 998: loss = 2.1832 (0.894 sec/step)\n",
            "I0813 14:40:07.807582 140596802582400 learning.py:507] global step 999: loss = 2.0769 (0.916 sec/step)\n",
            "I0813 14:40:08.699709 140596802582400 learning.py:507] global step 1000: loss = 2.1429 (0.890 sec/step)\n",
            "I0813 14:40:09.590521 140596802582400 learning.py:507] global step 1001: loss = 1.1227 (0.889 sec/step)\n",
            "I0813 14:40:10.443197 140596802582400 learning.py:507] global step 1002: loss = 3.0373 (0.851 sec/step)\n",
            "I0813 14:40:11.316454 140596802582400 learning.py:507] global step 1003: loss = 2.2114 (0.872 sec/step)\n",
            "I0813 14:40:12.195049 140596802582400 learning.py:507] global step 1004: loss = 1.8655 (0.877 sec/step)\n",
            "I0813 14:40:13.040419 140596802582400 learning.py:507] global step 1005: loss = 2.3598 (0.844 sec/step)\n",
            "I0813 14:40:13.883103 140596802582400 learning.py:507] global step 1006: loss = 1.5110 (0.841 sec/step)\n",
            "I0813 14:40:14.726291 140596802582400 learning.py:507] global step 1007: loss = 1.5322 (0.842 sec/step)\n",
            "I0813 14:40:15.570528 140596802582400 learning.py:507] global step 1008: loss = 2.0362 (0.843 sec/step)\n",
            "I0813 14:40:16.417078 140596802582400 learning.py:507] global step 1009: loss = 1.4482 (0.845 sec/step)\n",
            "I0813 14:40:17.257757 140596802582400 learning.py:507] global step 1010: loss = 2.0823 (0.839 sec/step)\n",
            "I0813 14:40:18.107567 140596802582400 learning.py:507] global step 1011: loss = 2.0857 (0.848 sec/step)\n",
            "I0813 14:40:18.941639 140596802582400 learning.py:507] global step 1012: loss = 2.3594 (0.832 sec/step)\n",
            "I0813 14:40:19.774996 140596802582400 learning.py:507] global step 1013: loss = 1.6127 (0.832 sec/step)\n",
            "I0813 14:40:20.627633 140596802582400 learning.py:507] global step 1014: loss = 1.8827 (0.851 sec/step)\n",
            "I0813 14:40:21.478788 140596802582400 learning.py:507] global step 1015: loss = 1.2203 (0.850 sec/step)\n",
            "I0813 14:40:22.351401 140596802582400 learning.py:507] global step 1016: loss = 1.4384 (0.871 sec/step)\n",
            "I0813 14:40:23.183622 140596802582400 learning.py:507] global step 1017: loss = 1.4522 (0.831 sec/step)\n",
            "I0813 14:40:24.030124 140596802582400 learning.py:507] global step 1018: loss = 2.0631 (0.845 sec/step)\n",
            "I0813 14:40:24.862414 140596802582400 learning.py:507] global step 1019: loss = 2.0898 (0.830 sec/step)\n",
            "I0813 14:40:25.715837 140596802582400 learning.py:507] global step 1020: loss = 1.4515 (0.852 sec/step)\n",
            "I0813 14:40:26.570350 140596802582400 learning.py:507] global step 1021: loss = 2.4612 (0.853 sec/step)\n",
            "I0813 14:40:27.422069 140596802582400 learning.py:507] global step 1022: loss = 1.7657 (0.850 sec/step)\n",
            "I0813 14:40:28.256823 140596802582400 learning.py:507] global step 1023: loss = 1.7697 (0.833 sec/step)\n",
            "I0813 14:40:29.108774 140596802582400 learning.py:507] global step 1024: loss = 1.6716 (0.850 sec/step)\n",
            "I0813 14:40:29.954912 140596802582400 learning.py:507] global step 1025: loss = 1.9926 (0.845 sec/step)\n",
            "I0813 14:40:30.811146 140596802582400 learning.py:507] global step 1026: loss = 1.6845 (0.855 sec/step)\n",
            "I0813 14:40:31.661713 140596802582400 learning.py:507] global step 1027: loss = 1.2866 (0.849 sec/step)\n",
            "I0813 14:40:32.556382 140596802582400 learning.py:507] global step 1028: loss = 1.4349 (0.893 sec/step)\n",
            "I0813 14:40:33.428214 140596802582400 learning.py:507] global step 1029: loss = 1.5318 (0.870 sec/step)\n",
            "I0813 14:40:34.278718 140596802582400 learning.py:507] global step 1030: loss = 2.7004 (0.849 sec/step)\n",
            "I0813 14:40:35.132619 140596802582400 learning.py:507] global step 1031: loss = 1.8020 (0.852 sec/step)\n",
            "I0813 14:40:36.004147 140596802582400 learning.py:507] global step 1032: loss = 1.6881 (0.870 sec/step)\n",
            "I0813 14:40:36.856133 140596802582400 learning.py:507] global step 1033: loss = 1.9267 (0.850 sec/step)\n",
            "I0813 14:40:37.721994 140596802582400 learning.py:507] global step 1034: loss = 1.7876 (0.864 sec/step)\n",
            "I0813 14:40:38.581997 140596802582400 learning.py:507] global step 1035: loss = 1.8162 (0.858 sec/step)\n",
            "I0813 14:40:39.444391 140596802582400 learning.py:507] global step 1036: loss = 1.9429 (0.861 sec/step)\n",
            "I0813 14:40:40.272255 140596802582400 learning.py:507] global step 1037: loss = 2.1443 (0.826 sec/step)\n",
            "I0813 14:40:41.137845 140596802582400 learning.py:507] global step 1038: loss = 1.8460 (0.864 sec/step)\n",
            "I0813 14:40:41.976021 140596802582400 learning.py:507] global step 1039: loss = 1.3578 (0.837 sec/step)\n",
            "I0813 14:40:42.822552 140596802582400 learning.py:507] global step 1040: loss = 1.6801 (0.845 sec/step)\n",
            "I0813 14:40:43.660670 140596802582400 learning.py:507] global step 1041: loss = 2.0724 (0.836 sec/step)\n",
            "I0813 14:40:44.490953 140596802582400 learning.py:507] global step 1042: loss = 1.7264 (0.829 sec/step)\n",
            "I0813 14:40:45.333003 140596802582400 learning.py:507] global step 1043: loss = 2.1713 (0.840 sec/step)\n",
            "I0813 14:40:46.169812 140596802582400 learning.py:507] global step 1044: loss = 2.0163 (0.835 sec/step)\n",
            "I0813 14:40:47.040351 140596802582400 learning.py:507] global step 1045: loss = 1.8126 (0.869 sec/step)\n",
            "I0813 14:40:47.893227 140596802582400 learning.py:507] global step 1046: loss = 1.8600 (0.851 sec/step)\n",
            "I0813 14:40:48.743411 140596802582400 learning.py:507] global step 1047: loss = 1.7019 (0.849 sec/step)\n",
            "I0813 14:40:49.588881 140596802582400 learning.py:507] global step 1048: loss = 2.6555 (0.844 sec/step)\n",
            "I0813 14:40:50.427032 140596802582400 learning.py:507] global step 1049: loss = 1.8075 (0.837 sec/step)\n",
            "I0813 14:40:51.275444 140596802582400 learning.py:507] global step 1050: loss = 1.5128 (0.847 sec/step)\n",
            "I0813 14:40:52.115174 140596802582400 learning.py:507] global step 1051: loss = 1.1174 (0.838 sec/step)\n",
            "I0813 14:40:52.985406 140596802582400 learning.py:507] global step 1052: loss = 1.9374 (0.868 sec/step)\n",
            "I0813 14:40:53.836708 140596802582400 learning.py:507] global step 1053: loss = 2.4064 (0.850 sec/step)\n",
            "I0813 14:40:54.685801 140596802582400 learning.py:507] global step 1054: loss = 2.6691 (0.848 sec/step)\n",
            "I0813 14:40:55.538281 140596802582400 learning.py:507] global step 1055: loss = 2.2319 (0.851 sec/step)\n",
            "I0813 14:40:56.410258 140596802582400 learning.py:507] global step 1056: loss = 1.9388 (0.870 sec/step)\n",
            "I0813 14:40:57.280498 140596802582400 learning.py:507] global step 1057: loss = 2.4662 (0.869 sec/step)\n",
            "I0813 14:40:58.124672 140596802582400 learning.py:507] global step 1058: loss = 1.6173 (0.842 sec/step)\n",
            "I0813 14:40:58.970061 140596802582400 learning.py:507] global step 1059: loss = 2.0765 (0.844 sec/step)\n",
            "I0813 14:40:59.821689 140596802582400 learning.py:507] global step 1060: loss = 2.6087 (0.850 sec/step)\n",
            "I0813 14:41:00.673969 140596802582400 learning.py:507] global step 1061: loss = 2.4366 (0.851 sec/step)\n",
            "I0813 14:41:01.544559 140596802582400 learning.py:507] global step 1062: loss = 2.0757 (0.869 sec/step)\n",
            "I0813 14:41:02.432916 140596802582400 learning.py:507] global step 1063: loss = 1.1618 (0.887 sec/step)\n",
            "I0813 14:41:03.302977 140596802582400 learning.py:507] global step 1064: loss = 1.4670 (0.868 sec/step)\n",
            "I0813 14:41:04.167805 140596802582400 learning.py:507] global step 1065: loss = 2.0681 (0.863 sec/step)\n",
            "I0813 14:41:05.038980 140596802582400 learning.py:507] global step 1066: loss = 1.5161 (0.869 sec/step)\n",
            "I0813 14:41:05.906520 140596802582400 learning.py:507] global step 1067: loss = 1.9206 (0.866 sec/step)\n",
            "I0813 14:41:06.758498 140596802582400 learning.py:507] global step 1068: loss = 1.6173 (0.850 sec/step)\n",
            "I0813 14:41:07.640697 140596802582400 learning.py:507] global step 1069: loss = 2.0178 (0.881 sec/step)\n",
            "I0813 14:41:08.502286 140596802582400 learning.py:507] global step 1070: loss = 2.0705 (0.860 sec/step)\n",
            "I0813 14:41:09.352599 140596802582400 learning.py:507] global step 1071: loss = 2.3160 (0.849 sec/step)\n",
            "I0813 14:41:10.193410 140596802582400 learning.py:507] global step 1072: loss = 2.1828 (0.839 sec/step)\n",
            "I0813 14:41:11.037333 140596802582400 learning.py:507] global step 1073: loss = 2.4626 (0.842 sec/step)\n",
            "I0813 14:41:11.888594 140596802582400 learning.py:507] global step 1074: loss = 1.5899 (0.850 sec/step)\n",
            "I0813 14:41:12.723983 140596802582400 learning.py:507] global step 1075: loss = 1.8555 (0.834 sec/step)\n",
            "I0813 14:41:13.574895 140596802582400 learning.py:507] global step 1076: loss = 1.7456 (0.849 sec/step)\n",
            "I0813 14:41:14.420664 140596802582400 learning.py:507] global step 1077: loss = 1.2025 (0.844 sec/step)\n",
            "I0813 14:41:15.272501 140596802582400 learning.py:507] global step 1078: loss = 1.6349 (0.850 sec/step)\n",
            "I0813 14:41:16.134089 140596802582400 learning.py:507] global step 1079: loss = 1.8472 (0.860 sec/step)\n",
            "I0813 14:41:16.990499 140596802582400 learning.py:507] global step 1080: loss = 2.0403 (0.855 sec/step)\n",
            "I0813 14:41:17.867635 140596802582400 learning.py:507] global step 1081: loss = 1.9040 (0.875 sec/step)\n",
            "I0813 14:41:18.730826 140596802582400 learning.py:507] global step 1082: loss = 1.9076 (0.862 sec/step)\n",
            "I0813 14:41:19.569381 140596802582400 learning.py:507] global step 1083: loss = 1.2755 (0.837 sec/step)\n",
            "I0813 14:41:20.411277 140596802582400 learning.py:507] global step 1084: loss = 1.0575 (0.840 sec/step)\n",
            "I0813 14:41:21.253760 140596802582400 learning.py:507] global step 1085: loss = 2.4288 (0.841 sec/step)\n",
            "I0813 14:41:22.099003 140596802582400 learning.py:507] global step 1086: loss = 1.4364 (0.844 sec/step)\n",
            "I0813 14:41:22.953508 140596802582400 learning.py:507] global step 1087: loss = 1.1098 (0.853 sec/step)\n",
            "I0813 14:41:23.822954 140596802582400 learning.py:507] global step 1088: loss = 2.2602 (0.868 sec/step)\n",
            "I0813 14:41:24.693935 140596802582400 learning.py:507] global step 1089: loss = 1.4437 (0.869 sec/step)\n",
            "I0813 14:41:25.891725 140593722345216 supervisor.py:1050] Recording summary at step 1089.\n",
            "I0813 14:41:26.174556 140596802582400 learning.py:507] global step 1090: loss = 2.2854 (1.352 sec/step)\n",
            "I0813 14:41:27.024173 140596802582400 learning.py:507] global step 1091: loss = 1.7456 (0.848 sec/step)\n",
            "I0813 14:41:27.876159 140596802582400 learning.py:507] global step 1092: loss = 1.5735 (0.850 sec/step)\n",
            "I0813 14:41:28.710983 140596802582400 learning.py:507] global step 1093: loss = 2.1959 (0.833 sec/step)\n",
            "I0813 14:41:29.279537 140593730737920 supervisor.py:1099] global_step/sec: 1.15976\n",
            "I0813 14:41:29.544459 140596802582400 learning.py:507] global step 1094: loss = 2.4726 (0.832 sec/step)\n",
            "I0813 14:41:30.394423 140596802582400 learning.py:507] global step 1095: loss = 1.5019 (0.848 sec/step)\n",
            "I0813 14:41:31.244743 140596802582400 learning.py:507] global step 1096: loss = 2.3005 (0.849 sec/step)\n",
            "I0813 14:41:32.116246 140596802582400 learning.py:507] global step 1097: loss = 1.8962 (0.870 sec/step)\n",
            "I0813 14:41:33.010285 140596802582400 learning.py:507] global step 1098: loss = 3.5944 (0.892 sec/step)\n",
            "I0813 14:41:33.885161 140596802582400 learning.py:507] global step 1099: loss = 1.3031 (0.873 sec/step)\n",
            "I0813 14:41:34.738386 140596802582400 learning.py:507] global step 1100: loss = 2.0580 (0.852 sec/step)\n",
            "I0813 14:41:35.610726 140596802582400 learning.py:507] global step 1101: loss = 2.3291 (0.871 sec/step)\n",
            "I0813 14:41:36.465454 140596802582400 learning.py:507] global step 1102: loss = 1.4000 (0.853 sec/step)\n",
            "I0813 14:41:37.314931 140596802582400 learning.py:507] global step 1103: loss = 2.5391 (0.848 sec/step)\n",
            "I0813 14:41:38.153812 140596802582400 learning.py:507] global step 1104: loss = 1.1759 (0.837 sec/step)\n",
            "I0813 14:41:38.996770 140596802582400 learning.py:507] global step 1105: loss = 1.7528 (0.841 sec/step)\n",
            "I0813 14:41:39.817973 140596802582400 learning.py:507] global step 1106: loss = 2.4705 (0.819 sec/step)\n",
            "I0813 14:41:40.697867 140596802582400 learning.py:507] global step 1107: loss = 1.2843 (0.878 sec/step)\n",
            "I0813 14:41:41.574442 140596802582400 learning.py:507] global step 1108: loss = 1.3449 (0.875 sec/step)\n",
            "I0813 14:41:42.435773 140596802582400 learning.py:507] global step 1109: loss = 2.4409 (0.859 sec/step)\n",
            "I0813 14:41:43.282941 140596802582400 learning.py:507] global step 1110: loss = 1.6239 (0.846 sec/step)\n",
            "I0813 14:41:44.114562 140596802582400 learning.py:507] global step 1111: loss = 2.7324 (0.830 sec/step)\n",
            "I0813 14:41:44.947757 140596802582400 learning.py:507] global step 1112: loss = 2.3257 (0.832 sec/step)\n",
            "I0813 14:41:45.799074 140596802582400 learning.py:507] global step 1113: loss = 2.2905 (0.850 sec/step)\n",
            "I0813 14:41:46.666958 140596802582400 learning.py:507] global step 1114: loss = 1.9077 (0.866 sec/step)\n",
            "I0813 14:41:47.502238 140596802582400 learning.py:507] global step 1115: loss = 1.5657 (0.834 sec/step)\n",
            "I0813 14:41:48.323504 140596802582400 learning.py:507] global step 1116: loss = 3.0045 (0.819 sec/step)\n",
            "I0813 14:41:49.189461 140596802582400 learning.py:507] global step 1117: loss = 1.6181 (0.864 sec/step)\n",
            "I0813 14:41:50.026097 140596802582400 learning.py:507] global step 1118: loss = 2.7324 (0.835 sec/step)\n",
            "I0813 14:41:50.873894 140596802582400 learning.py:507] global step 1119: loss = 2.0992 (0.846 sec/step)\n",
            "I0813 14:41:51.737291 140596802582400 learning.py:507] global step 1120: loss = 2.5214 (0.862 sec/step)\n",
            "I0813 14:41:52.586274 140596802582400 learning.py:507] global step 1121: loss = 2.4632 (0.847 sec/step)\n",
            "I0813 14:41:53.424523 140596802582400 learning.py:507] global step 1122: loss = 1.5207 (0.837 sec/step)\n",
            "I0813 14:41:54.258838 140596802582400 learning.py:507] global step 1123: loss = 1.9044 (0.833 sec/step)\n",
            "I0813 14:41:55.106918 140596802582400 learning.py:507] global step 1124: loss = 1.7886 (0.846 sec/step)\n",
            "I0813 14:41:55.963271 140596802582400 learning.py:507] global step 1125: loss = 2.0310 (0.855 sec/step)\n",
            "I0813 14:41:56.815032 140596802582400 learning.py:507] global step 1126: loss = 2.1755 (0.850 sec/step)\n",
            "I0813 14:41:57.682928 140596802582400 learning.py:507] global step 1127: loss = 1.0793 (0.866 sec/step)\n",
            "I0813 14:41:58.539026 140596802582400 learning.py:507] global step 1128: loss = 1.8498 (0.854 sec/step)\n",
            "I0813 14:41:59.408366 140596802582400 learning.py:507] global step 1129: loss = 1.2293 (0.868 sec/step)\n",
            "I0813 14:42:00.260094 140596802582400 learning.py:507] global step 1130: loss = 1.2132 (0.850 sec/step)\n",
            "I0813 14:42:01.112237 140596802582400 learning.py:507] global step 1131: loss = 2.3204 (0.850 sec/step)\n",
            "I0813 14:42:02.001689 140596802582400 learning.py:507] global step 1132: loss = 1.8162 (0.888 sec/step)\n",
            "I0813 14:42:02.871433 140596802582400 learning.py:507] global step 1133: loss = 1.6204 (0.868 sec/step)\n",
            "I0813 14:42:03.743024 140596802582400 learning.py:507] global step 1134: loss = 1.6141 (0.870 sec/step)\n",
            "I0813 14:42:04.616713 140596802582400 learning.py:507] global step 1135: loss = 1.7361 (0.872 sec/step)\n",
            "I0813 14:42:05.453393 140596802582400 learning.py:507] global step 1136: loss = 1.2313 (0.835 sec/step)\n",
            "I0813 14:42:06.315304 140596802582400 learning.py:507] global step 1137: loss = 2.1715 (0.860 sec/step)\n",
            "I0813 14:42:07.169766 140596802582400 learning.py:507] global step 1138: loss = 1.7749 (0.853 sec/step)\n",
            "I0813 14:42:08.055903 140596802582400 learning.py:507] global step 1139: loss = 1.5751 (0.885 sec/step)\n",
            "I0813 14:42:08.894715 140596802582400 learning.py:507] global step 1140: loss = 1.9948 (0.837 sec/step)\n",
            "I0813 14:42:09.764724 140596802582400 learning.py:507] global step 1141: loss = 1.6382 (0.868 sec/step)\n",
            "I0813 14:42:10.598839 140596802582400 learning.py:507] global step 1142: loss = 1.8912 (0.832 sec/step)\n",
            "I0813 14:42:11.444868 140596802582400 learning.py:507] global step 1143: loss = 0.9944 (0.844 sec/step)\n",
            "I0813 14:42:12.297696 140596802582400 learning.py:507] global step 1144: loss = 1.6019 (0.851 sec/step)\n",
            "I0813 14:42:13.148517 140596802582400 learning.py:507] global step 1145: loss = 2.2431 (0.849 sec/step)\n",
            "I0813 14:42:13.979453 140596802582400 learning.py:507] global step 1146: loss = 1.7674 (0.829 sec/step)\n",
            "I0813 14:42:14.813513 140596802582400 learning.py:507] global step 1147: loss = 2.0248 (0.832 sec/step)\n",
            "I0813 14:42:15.663694 140596802582400 learning.py:507] global step 1148: loss = 1.5760 (0.849 sec/step)\n",
            "I0813 14:42:16.534370 140596802582400 learning.py:507] global step 1149: loss = 1.4811 (0.869 sec/step)\n",
            "I0813 14:42:17.382638 140596802582400 learning.py:507] global step 1150: loss = 1.7260 (0.847 sec/step)\n",
            "I0813 14:42:18.240886 140596802582400 learning.py:507] global step 1151: loss = 1.6008 (0.857 sec/step)\n",
            "I0813 14:42:19.060267 140596802582400 learning.py:507] global step 1152: loss = 1.7423 (0.818 sec/step)\n",
            "I0813 14:42:19.926969 140596802582400 learning.py:507] global step 1153: loss = 1.5644 (0.865 sec/step)\n",
            "I0813 14:42:20.780630 140596802582400 learning.py:507] global step 1154: loss = 1.7695 (0.852 sec/step)\n",
            "I0813 14:42:21.646468 140596802582400 learning.py:507] global step 1155: loss = 2.1277 (0.864 sec/step)\n",
            "I0813 14:42:22.487332 140596802582400 learning.py:507] global step 1156: loss = 1.3223 (0.838 sec/step)\n",
            "I0813 14:42:23.336726 140596802582400 learning.py:507] global step 1157: loss = 2.1030 (0.848 sec/step)\n",
            "I0813 14:42:24.185563 140596802582400 learning.py:507] global step 1158: loss = 2.4549 (0.847 sec/step)\n",
            "I0813 14:42:25.034293 140596802582400 learning.py:507] global step 1159: loss = 2.6617 (0.847 sec/step)\n",
            "I0813 14:42:25.893216 140596802582400 learning.py:507] global step 1160: loss = 1.3667 (0.857 sec/step)\n",
            "I0813 14:42:26.744580 140596802582400 learning.py:507] global step 1161: loss = 2.2870 (0.850 sec/step)\n",
            "I0813 14:42:27.612245 140596802582400 learning.py:507] global step 1162: loss = 2.5962 (0.866 sec/step)\n",
            "I0813 14:42:28.466030 140596802582400 learning.py:507] global step 1163: loss = 1.8539 (0.852 sec/step)\n",
            "I0813 14:42:29.306541 140596802582400 learning.py:507] global step 1164: loss = 2.4758 (0.839 sec/step)\n",
            "I0813 14:42:30.172325 140596802582400 learning.py:507] global step 1165: loss = 2.2381 (0.864 sec/step)\n",
            "I0813 14:42:31.020931 140596802582400 learning.py:507] global step 1166: loss = 2.5691 (0.847 sec/step)\n",
            "I0813 14:42:31.873291 140596802582400 learning.py:507] global step 1167: loss = 1.4114 (0.851 sec/step)\n",
            "I0813 14:42:32.753083 140596802582400 learning.py:507] global step 1168: loss = 1.5688 (0.878 sec/step)\n",
            "I0813 14:42:33.637647 140596802582400 learning.py:507] global step 1169: loss = 0.9385 (0.883 sec/step)\n",
            "I0813 14:42:34.470834 140596802582400 learning.py:507] global step 1170: loss = 1.6570 (0.831 sec/step)\n",
            "I0813 14:42:35.362658 140596802582400 learning.py:507] global step 1171: loss = 2.8212 (0.890 sec/step)\n",
            "I0813 14:42:36.239725 140596802582400 learning.py:507] global step 1172: loss = 2.9122 (0.875 sec/step)\n",
            "I0813 14:42:37.103589 140596802582400 learning.py:507] global step 1173: loss = 1.5614 (0.862 sec/step)\n",
            "I0813 14:42:37.991532 140596802582400 learning.py:507] global step 1174: loss = 2.4647 (0.886 sec/step)\n",
            "I0813 14:42:38.856988 140596802582400 learning.py:507] global step 1175: loss = 2.1527 (0.864 sec/step)\n",
            "I0813 14:42:39.706283 140596802582400 learning.py:507] global step 1176: loss = 1.5862 (0.848 sec/step)\n",
            "I0813 14:42:40.574214 140596802582400 learning.py:507] global step 1177: loss = 2.1184 (0.866 sec/step)\n",
            "I0813 14:42:41.461932 140596802582400 learning.py:507] global step 1178: loss = 1.6663 (0.886 sec/step)\n",
            "I0813 14:42:42.345148 140596802582400 learning.py:507] global step 1179: loss = 2.3530 (0.882 sec/step)\n",
            "I0813 14:42:43.211932 140596802582400 learning.py:507] global step 1180: loss = 1.4321 (0.865 sec/step)\n",
            "I0813 14:42:44.085823 140596802582400 learning.py:507] global step 1181: loss = 2.3364 (0.872 sec/step)\n",
            "I0813 14:42:44.940794 140596802582400 learning.py:507] global step 1182: loss = 1.5080 (0.853 sec/step)\n",
            "I0813 14:42:45.794086 140596802582400 learning.py:507] global step 1183: loss = 1.4758 (0.852 sec/step)\n",
            "I0813 14:42:46.625536 140596802582400 learning.py:507] global step 1184: loss = 1.9870 (0.830 sec/step)\n",
            "I0813 14:42:47.476326 140596802582400 learning.py:507] global step 1185: loss = 1.4595 (0.849 sec/step)\n",
            "I0813 14:42:48.310548 140596802582400 learning.py:507] global step 1186: loss = 1.4609 (0.833 sec/step)\n",
            "I0813 14:42:49.161951 140596802582400 learning.py:507] global step 1187: loss = 1.2551 (0.850 sec/step)\n",
            "I0813 14:42:50.011106 140596802582400 learning.py:507] global step 1188: loss = 1.6275 (0.848 sec/step)\n",
            "I0813 14:42:50.908658 140596802582400 learning.py:507] global step 1189: loss = 2.1857 (0.896 sec/step)\n",
            "I0813 14:42:51.740302 140596802582400 learning.py:507] global step 1190: loss = 1.4395 (0.830 sec/step)\n",
            "I0813 14:42:52.594589 140596802582400 learning.py:507] global step 1191: loss = 1.2895 (0.853 sec/step)\n",
            "I0813 14:42:53.440942 140596802582400 learning.py:507] global step 1192: loss = 1.8154 (0.845 sec/step)\n",
            "I0813 14:42:54.288615 140596802582400 learning.py:507] global step 1193: loss = 1.5740 (0.845 sec/step)\n",
            "I0813 14:42:55.148816 140596802582400 learning.py:507] global step 1194: loss = 2.2209 (0.858 sec/step)\n",
            "I0813 14:42:55.982707 140596802582400 learning.py:507] global step 1195: loss = 2.1208 (0.832 sec/step)\n",
            "I0813 14:42:56.832537 140596802582400 learning.py:507] global step 1196: loss = 2.0904 (0.848 sec/step)\n",
            "I0813 14:42:57.689374 140596802582400 learning.py:507] global step 1197: loss = 1.7004 (0.855 sec/step)\n",
            "I0813 14:42:58.536534 140596802582400 learning.py:507] global step 1198: loss = 3.0339 (0.845 sec/step)\n",
            "I0813 14:42:59.372587 140596802582400 learning.py:507] global step 1199: loss = 1.8440 (0.835 sec/step)\n",
            "I0813 14:43:00.203715 140596802582400 learning.py:507] global step 1200: loss = 1.7655 (0.829 sec/step)\n",
            "I0813 14:43:01.029471 140596802582400 learning.py:507] global step 1201: loss = 3.0261 (0.824 sec/step)\n",
            "I0813 14:43:01.902438 140596802582400 learning.py:507] global step 1202: loss = 2.1361 (0.871 sec/step)\n",
            "I0813 14:43:02.758759 140596802582400 learning.py:507] global step 1203: loss = 1.7992 (0.855 sec/step)\n",
            "I0813 14:43:03.612317 140596802582400 learning.py:507] global step 1204: loss = 2.0884 (0.852 sec/step)\n",
            "I0813 14:43:04.485026 140596802582400 learning.py:507] global step 1205: loss = 1.8354 (0.871 sec/step)\n",
            "I0813 14:43:05.335848 140596802582400 learning.py:507] global step 1206: loss = 1.4038 (0.849 sec/step)\n",
            "I0813 14:43:06.168999 140596802582400 learning.py:507] global step 1207: loss = 1.5535 (0.832 sec/step)\n",
            "I0813 14:43:07.035965 140596802582400 learning.py:507] global step 1208: loss = 1.7572 (0.865 sec/step)\n",
            "I0813 14:43:07.914231 140596802582400 learning.py:507] global step 1209: loss = 1.3030 (0.877 sec/step)\n",
            "I0813 14:43:08.788781 140596802582400 learning.py:507] global step 1210: loss = 1.7951 (0.873 sec/step)\n",
            "I0813 14:43:09.637928 140596802582400 learning.py:507] global step 1211: loss = 1.7611 (0.847 sec/step)\n",
            "I0813 14:43:10.479547 140596802582400 learning.py:507] global step 1212: loss = 2.1884 (0.840 sec/step)\n",
            "I0813 14:43:11.322689 140596802582400 learning.py:507] global step 1213: loss = 2.8140 (0.842 sec/step)\n",
            "I0813 14:43:12.185130 140596802582400 learning.py:507] global step 1214: loss = 1.0502 (0.861 sec/step)\n",
            "I0813 14:43:13.045153 140596802582400 learning.py:507] global step 1215: loss = 1.4846 (0.858 sec/step)\n",
            "I0813 14:43:13.894593 140596802582400 learning.py:507] global step 1216: loss = 1.5487 (0.848 sec/step)\n",
            "I0813 14:43:14.732151 140596802582400 learning.py:507] global step 1217: loss = 1.6658 (0.836 sec/step)\n",
            "I0813 14:43:15.583940 140596802582400 learning.py:507] global step 1218: loss = 1.2789 (0.850 sec/step)\n",
            "I0813 14:43:16.432375 140596802582400 learning.py:507] global step 1219: loss = 1.4698 (0.847 sec/step)\n",
            "I0813 14:43:17.318943 140596802582400 learning.py:507] global step 1220: loss = 1.4451 (0.885 sec/step)\n",
            "I0813 14:43:18.158644 140596802582400 learning.py:507] global step 1221: loss = 1.7465 (0.838 sec/step)\n",
            "I0813 14:43:19.009263 140596802582400 learning.py:507] global step 1222: loss = 2.1846 (0.849 sec/step)\n",
            "I0813 14:43:19.862798 140596802582400 learning.py:507] global step 1223: loss = 1.9665 (0.852 sec/step)\n",
            "I0813 14:43:20.704671 140596802582400 learning.py:507] global step 1224: loss = 1.3970 (0.840 sec/step)\n",
            "I0813 14:43:21.549247 140596802582400 learning.py:507] global step 1225: loss = 1.6827 (0.843 sec/step)\n",
            "I0813 14:43:22.418078 140596802582400 learning.py:507] global step 1226: loss = 1.9444 (0.867 sec/step)\n",
            "I0813 14:43:23.248881 140596802582400 learning.py:507] global step 1227: loss = 1.9679 (0.829 sec/step)\n",
            "I0813 14:43:24.099551 140596802582400 learning.py:507] global step 1228: loss = 1.9038 (0.849 sec/step)\n",
            "I0813 14:43:25.619845 140596802582400 learning.py:507] global step 1229: loss = 1.5656 (1.518 sec/step)\n",
            "I0813 14:43:25.624407 140593722345216 supervisor.py:1050] Recording summary at step 1229.\n",
            "I0813 14:43:26.484901 140596802582400 learning.py:507] global step 1230: loss = 2.0232 (0.863 sec/step)\n",
            "I0813 14:43:27.341357 140596802582400 learning.py:507] global step 1231: loss = 1.3688 (0.855 sec/step)\n",
            "I0813 14:43:28.201458 140596802582400 learning.py:507] global step 1232: loss = 1.8205 (0.859 sec/step)\n",
            "I0813 14:43:29.027206 140596802582400 learning.py:507] global step 1233: loss = 3.0790 (0.824 sec/step)\n",
            "I0813 14:43:29.100866 140593730737920 supervisor.py:1099] global_step/sec: 1.16841\n",
            "I0813 14:43:29.871104 140596802582400 learning.py:507] global step 1234: loss = 0.9665 (0.842 sec/step)\n",
            "I0813 14:43:30.724408 140596802582400 learning.py:507] global step 1235: loss = 1.4366 (0.852 sec/step)\n",
            "I0813 14:43:31.573529 140596802582400 learning.py:507] global step 1236: loss = 1.0478 (0.847 sec/step)\n",
            "I0813 14:43:32.448523 140596802582400 learning.py:507] global step 1237: loss = 1.5194 (0.873 sec/step)\n",
            "I0813 14:43:33.320025 140596802582400 learning.py:507] global step 1238: loss = 1.5290 (0.870 sec/step)\n",
            "I0813 14:43:34.171913 140596802582400 learning.py:507] global step 1239: loss = 1.2763 (0.850 sec/step)\n",
            "I0813 14:43:35.016938 140596802582400 learning.py:507] global step 1240: loss = 2.8029 (0.844 sec/step)\n",
            "I0813 14:43:35.857536 140596802582400 learning.py:507] global step 1241: loss = 1.8453 (0.839 sec/step)\n",
            "I0813 14:43:36.727400 140596802582400 learning.py:507] global step 1242: loss = 1.6593 (0.868 sec/step)\n",
            "I0813 14:43:37.577840 140596802582400 learning.py:507] global step 1243: loss = 2.4301 (0.849 sec/step)\n",
            "I0813 14:43:38.424993 140596802582400 learning.py:507] global step 1244: loss = 2.0336 (0.845 sec/step)\n",
            "I0813 14:43:39.280681 140596802582400 learning.py:507] global step 1245: loss = 1.3123 (0.854 sec/step)\n",
            "I0813 14:43:40.150175 140596802582400 learning.py:507] global step 1246: loss = 1.6660 (0.868 sec/step)\n",
            "I0813 14:43:41.019576 140596802582400 learning.py:507] global step 1247: loss = 3.6916 (0.868 sec/step)\n",
            "I0813 14:43:41.898083 140596802582400 learning.py:507] global step 1248: loss = 1.2965 (0.877 sec/step)\n",
            "I0813 14:43:42.782555 140596802582400 learning.py:507] global step 1249: loss = 2.5370 (0.883 sec/step)\n",
            "I0813 14:43:43.641400 140596802582400 learning.py:507] global step 1250: loss = 2.7551 (0.857 sec/step)\n",
            "I0813 14:43:44.494371 140596802582400 learning.py:507] global step 1251: loss = 2.1210 (0.851 sec/step)\n",
            "I0813 14:43:45.327191 140596802582400 learning.py:507] global step 1252: loss = 1.7301 (0.831 sec/step)\n",
            "I0813 14:43:46.178810 140596802582400 learning.py:507] global step 1253: loss = 1.6909 (0.850 sec/step)\n",
            "I0813 14:43:47.047724 140596802582400 learning.py:507] global step 1254: loss = 2.2271 (0.867 sec/step)\n",
            "I0813 14:43:47.904701 140596802582400 learning.py:507] global step 1255: loss = 1.6635 (0.856 sec/step)\n",
            "I0813 14:43:48.765598 140596802582400 learning.py:507] global step 1256: loss = 2.1030 (0.860 sec/step)\n",
            "I0813 14:43:49.629353 140596802582400 learning.py:507] global step 1257: loss = 1.7830 (0.862 sec/step)\n",
            "I0813 14:43:50.516081 140596802582400 learning.py:507] global step 1258: loss = 2.3314 (0.885 sec/step)\n",
            "I0813 14:43:51.385403 140596802582400 learning.py:507] global step 1259: loss = 1.8759 (0.867 sec/step)\n",
            "I0813 14:43:52.248065 140596802582400 learning.py:507] global step 1260: loss = 1.6079 (0.861 sec/step)\n",
            "I0813 14:43:53.135886 140596802582400 learning.py:507] global step 1261: loss = 1.6707 (0.886 sec/step)\n",
            "I0813 14:43:53.992824 140596802582400 learning.py:507] global step 1262: loss = 1.5431 (0.855 sec/step)\n",
            "I0813 14:43:54.862643 140596802582400 learning.py:507] global step 1263: loss = 1.4988 (0.868 sec/step)\n",
            "I0813 14:43:55.717164 140596802582400 learning.py:507] global step 1264: loss = 1.3675 (0.853 sec/step)\n",
            "I0813 14:43:56.575467 140596802582400 learning.py:507] global step 1265: loss = 1.6868 (0.857 sec/step)\n",
            "I0813 14:43:57.436090 140596802582400 learning.py:507] global step 1266: loss = 1.5690 (0.859 sec/step)\n",
            "I0813 14:43:58.276437 140596802582400 learning.py:507] global step 1267: loss = 1.2793 (0.839 sec/step)\n",
            "I0813 14:43:59.131589 140596802582400 learning.py:507] global step 1268: loss = 2.0164 (0.854 sec/step)\n",
            "I0813 14:43:59.991838 140596802582400 learning.py:507] global step 1269: loss = 1.9806 (0.859 sec/step)\n",
            "I0813 14:44:00.835407 140596802582400 learning.py:507] global step 1270: loss = 2.0115 (0.842 sec/step)\n",
            "I0813 14:44:01.670224 140596802582400 learning.py:507] global step 1271: loss = 2.2788 (0.833 sec/step)\n",
            "I0813 14:44:02.557988 140596802582400 learning.py:507] global step 1272: loss = 1.8336 (0.885 sec/step)\n",
            "I0813 14:44:03.452427 140596802582400 learning.py:507] global step 1273: loss = 2.2284 (0.893 sec/step)\n",
            "I0813 14:44:04.325948 140596802582400 learning.py:507] global step 1274: loss = 1.3406 (0.872 sec/step)\n",
            "I0813 14:44:05.213930 140596802582400 learning.py:507] global step 1275: loss = 1.5097 (0.886 sec/step)\n",
            "I0813 14:44:06.086254 140596802582400 learning.py:507] global step 1276: loss = 1.1449 (0.871 sec/step)\n",
            "I0813 14:44:06.920404 140596802582400 learning.py:507] global step 1277: loss = 2.1214 (0.832 sec/step)\n",
            "I0813 14:44:07.777769 140596802582400 learning.py:507] global step 1278: loss = 1.6450 (0.856 sec/step)\n",
            "I0813 14:44:08.640309 140596802582400 learning.py:507] global step 1279: loss = 1.4165 (0.861 sec/step)\n",
            "I0813 14:44:09.513436 140596802582400 learning.py:507] global step 1280: loss = 1.7509 (0.871 sec/step)\n",
            "I0813 14:44:10.381518 140596802582400 learning.py:507] global step 1281: loss = 2.5864 (0.866 sec/step)\n",
            "I0813 14:44:11.273492 140596802582400 learning.py:507] global step 1282: loss = 1.5497 (0.890 sec/step)\n",
            "I0813 14:44:12.116451 140596802582400 learning.py:507] global step 1283: loss = 2.1044 (0.841 sec/step)\n",
            "I0813 14:44:12.950875 140596802582400 learning.py:507] global step 1284: loss = 1.0427 (0.833 sec/step)\n",
            "I0813 14:44:13.831921 140596802582400 learning.py:507] global step 1285: loss = 1.9627 (0.880 sec/step)\n",
            "I0813 14:44:14.704451 140596802582400 learning.py:507] global step 1286: loss = 1.5274 (0.871 sec/step)\n",
            "I0813 14:44:15.559710 140596802582400 learning.py:507] global step 1287: loss = 2.1928 (0.854 sec/step)\n",
            "I0813 14:44:16.428586 140596802582400 learning.py:507] global step 1288: loss = 2.4266 (0.867 sec/step)\n",
            "I0813 14:44:17.275594 140596802582400 learning.py:507] global step 1289: loss = 2.2497 (0.845 sec/step)\n",
            "I0813 14:44:18.118251 140596802582400 learning.py:507] global step 1290: loss = 1.3461 (0.840 sec/step)\n",
            "I0813 14:44:18.978670 140596802582400 learning.py:507] global step 1291: loss = 2.8686 (0.859 sec/step)\n",
            "I0813 14:44:19.835070 140596802582400 learning.py:507] global step 1292: loss = 1.3468 (0.855 sec/step)\n",
            "I0813 14:44:20.700591 140596802582400 learning.py:507] global step 1293: loss = 1.9706 (0.864 sec/step)\n",
            "I0813 14:44:21.575700 140596802582400 learning.py:507] global step 1294: loss = 1.4261 (0.873 sec/step)\n",
            "I0813 14:44:22.448354 140596802582400 learning.py:507] global step 1295: loss = 1.7527 (0.871 sec/step)\n",
            "I0813 14:44:23.281112 140596802582400 learning.py:507] global step 1296: loss = 1.1296 (0.831 sec/step)\n",
            "I0813 14:44:24.152187 140596802582400 learning.py:507] global step 1297: loss = 1.0345 (0.869 sec/step)\n",
            "I0813 14:44:25.021189 140596802582400 learning.py:507] global step 1298: loss = 1.0191 (0.867 sec/step)\n",
            "I0813 14:44:25.877350 140596802582400 learning.py:507] global step 1299: loss = 2.5761 (0.854 sec/step)\n",
            "I0813 14:44:26.747783 140596802582400 learning.py:507] global step 1300: loss = 1.4891 (0.869 sec/step)\n",
            "I0813 14:44:27.602729 140596802582400 learning.py:507] global step 1301: loss = 1.1328 (0.853 sec/step)\n",
            "I0813 14:44:28.467180 140596802582400 learning.py:507] global step 1302: loss = 1.3801 (0.863 sec/step)\n",
            "I0813 14:44:29.326498 140596802582400 learning.py:507] global step 1303: loss = 1.9993 (0.858 sec/step)\n",
            "I0813 14:44:30.171270 140596802582400 learning.py:507] global step 1304: loss = 1.9029 (0.843 sec/step)\n",
            "I0813 14:44:31.026549 140596802582400 learning.py:507] global step 1305: loss = 1.9556 (0.854 sec/step)\n",
            "I0813 14:44:31.897276 140596802582400 learning.py:507] global step 1306: loss = 2.2674 (0.869 sec/step)\n",
            "I0813 14:44:32.751155 140596802582400 learning.py:507] global step 1307: loss = 1.9679 (0.852 sec/step)\n",
            "I0813 14:44:33.623002 140596802582400 learning.py:507] global step 1308: loss = 1.3867 (0.870 sec/step)\n",
            "I0813 14:44:34.511766 140596802582400 learning.py:507] global step 1309: loss = 2.6795 (0.887 sec/step)\n",
            "I0813 14:44:35.356234 140596802582400 learning.py:507] global step 1310: loss = 1.3385 (0.843 sec/step)\n",
            "I0813 14:44:36.201518 140596802582400 learning.py:507] global step 1311: loss = 2.1951 (0.844 sec/step)\n",
            "I0813 14:44:37.051167 140596802582400 learning.py:507] global step 1312: loss = 2.0520 (0.848 sec/step)\n",
            "I0813 14:44:37.916501 140596802582400 learning.py:507] global step 1313: loss = 1.4527 (0.864 sec/step)\n",
            "I0813 14:44:38.766739 140596802582400 learning.py:507] global step 1314: loss = 1.9389 (0.849 sec/step)\n",
            "I0813 14:44:39.590795 140596802582400 learning.py:507] global step 1315: loss = 1.5843 (0.822 sec/step)\n",
            "I0813 14:44:40.481478 140596802582400 learning.py:507] global step 1316: loss = 1.9526 (0.889 sec/step)\n",
            "I0813 14:44:41.340497 140596802582400 learning.py:507] global step 1317: loss = 1.7048 (0.857 sec/step)\n",
            "I0813 14:44:42.182335 140596802582400 learning.py:507] global step 1318: loss = 2.5700 (0.840 sec/step)\n",
            "I0813 14:44:43.045242 140596802582400 learning.py:507] global step 1319: loss = 2.6240 (0.861 sec/step)\n",
            "I0813 14:44:43.906461 140596802582400 learning.py:507] global step 1320: loss = 1.0372 (0.860 sec/step)\n",
            "I0813 14:44:44.747971 140596802582400 learning.py:507] global step 1321: loss = 1.5217 (0.840 sec/step)\n",
            "I0813 14:44:45.600006 140596802582400 learning.py:507] global step 1322: loss = 1.6397 (0.850 sec/step)\n",
            "I0813 14:44:46.440911 140596802582400 learning.py:507] global step 1323: loss = 1.9489 (0.839 sec/step)\n",
            "I0813 14:44:47.290948 140596802582400 learning.py:507] global step 1324: loss = 2.7018 (0.848 sec/step)\n",
            "I0813 14:44:48.123856 140596802582400 learning.py:507] global step 1325: loss = 2.1859 (0.831 sec/step)\n",
            "I0813 14:44:48.958091 140596802582400 learning.py:507] global step 1326: loss = 2.3569 (0.833 sec/step)\n",
            "I0813 14:44:49.786870 140596802582400 learning.py:507] global step 1327: loss = 2.0554 (0.827 sec/step)\n",
            "I0813 14:44:50.639301 140596802582400 learning.py:507] global step 1328: loss = 2.1562 (0.851 sec/step)\n",
            "I0813 14:44:51.503579 140596802582400 learning.py:507] global step 1329: loss = 1.8024 (0.863 sec/step)\n",
            "I0813 14:44:52.380932 140596802582400 learning.py:507] global step 1330: loss = 1.5620 (0.876 sec/step)\n",
            "I0813 14:44:53.229519 140596802582400 learning.py:507] global step 1331: loss = 1.9770 (0.847 sec/step)\n",
            "I0813 14:44:54.072090 140596802582400 learning.py:507] global step 1332: loss = 1.3115 (0.840 sec/step)\n",
            "I0813 14:44:54.935830 140596802582400 learning.py:507] global step 1333: loss = 2.0412 (0.862 sec/step)\n",
            "I0813 14:44:55.787436 140596802582400 learning.py:507] global step 1334: loss = 2.2079 (0.850 sec/step)\n",
            "I0813 14:44:56.655878 140596802582400 learning.py:507] global step 1335: loss = 2.5838 (0.867 sec/step)\n",
            "I0813 14:44:57.503834 140596802582400 learning.py:507] global step 1336: loss = 2.0971 (0.846 sec/step)\n",
            "I0813 14:44:58.347668 140596802582400 learning.py:507] global step 1337: loss = 1.0039 (0.842 sec/step)\n",
            "I0813 14:44:59.165036 140596802582400 learning.py:507] global step 1338: loss = 1.5497 (0.816 sec/step)\n",
            "I0813 14:45:00.010317 140596802582400 learning.py:507] global step 1339: loss = 1.2736 (0.844 sec/step)\n",
            "I0813 14:45:00.866468 140596802582400 learning.py:507] global step 1340: loss = 1.7015 (0.855 sec/step)\n",
            "I0813 14:45:01.730385 140596802582400 learning.py:507] global step 1341: loss = 2.0836 (0.862 sec/step)\n",
            "I0813 14:45:02.646252 140596802582400 learning.py:507] global step 1342: loss = 1.8577 (0.914 sec/step)\n",
            "I0813 14:45:03.521447 140596802582400 learning.py:507] global step 1343: loss = 1.9346 (0.873 sec/step)\n",
            "I0813 14:45:04.394746 140596802582400 learning.py:507] global step 1344: loss = 1.6971 (0.872 sec/step)\n",
            "I0813 14:45:05.266575 140596802582400 learning.py:507] global step 1345: loss = 1.9849 (0.870 sec/step)\n",
            "I0813 14:45:06.129150 140596802582400 learning.py:507] global step 1346: loss = 1.9691 (0.861 sec/step)\n",
            "I0813 14:45:06.983742 140596802582400 learning.py:507] global step 1347: loss = 2.0238 (0.853 sec/step)\n",
            "I0813 14:45:07.864804 140596802582400 learning.py:507] global step 1348: loss = 2.2184 (0.880 sec/step)\n",
            "I0813 14:45:08.717557 140596802582400 learning.py:507] global step 1349: loss = 2.1661 (0.851 sec/step)\n",
            "I0813 14:45:09.588474 140596802582400 learning.py:507] global step 1350: loss = 1.8170 (0.869 sec/step)\n",
            "I0813 14:45:10.439050 140596802582400 learning.py:507] global step 1351: loss = 1.9429 (0.849 sec/step)\n",
            "I0813 14:45:11.300379 140596802582400 learning.py:507] global step 1352: loss = 1.6101 (0.860 sec/step)\n",
            "I0813 14:45:12.182720 140596802582400 learning.py:507] global step 1353: loss = 2.1484 (0.881 sec/step)\n",
            "I0813 14:45:13.040059 140596802582400 learning.py:507] global step 1354: loss = 1.9072 (0.856 sec/step)\n",
            "I0813 14:45:13.884676 140596802582400 learning.py:507] global step 1355: loss = 1.7786 (0.843 sec/step)\n",
            "I0813 14:45:14.737390 140596802582400 learning.py:507] global step 1356: loss = 1.6978 (0.851 sec/step)\n",
            "I0813 14:45:15.591061 140596802582400 learning.py:507] global step 1357: loss = 2.2975 (0.851 sec/step)\n",
            "I0813 14:45:16.451644 140596802582400 learning.py:507] global step 1358: loss = 1.5228 (0.859 sec/step)\n",
            "I0813 14:45:17.315037 140596802582400 learning.py:507] global step 1359: loss = 2.0331 (0.862 sec/step)\n",
            "I0813 14:45:18.184046 140596802582400 learning.py:507] global step 1360: loss = 1.3053 (0.867 sec/step)\n",
            "I0813 14:45:19.092655 140596802582400 learning.py:507] global step 1361: loss = 1.8760 (0.907 sec/step)\n",
            "I0813 14:45:19.929641 140596802582400 learning.py:507] global step 1362: loss = 2.1270 (0.835 sec/step)\n",
            "I0813 14:45:20.770160 140596802582400 learning.py:507] global step 1363: loss = 2.0962 (0.838 sec/step)\n",
            "I0813 14:45:21.612443 140596802582400 learning.py:507] global step 1364: loss = 2.2830 (0.840 sec/step)\n",
            "I0813 14:45:22.464671 140596802582400 learning.py:507] global step 1365: loss = 1.1568 (0.851 sec/step)\n",
            "I0813 14:45:23.318942 140596802582400 learning.py:507] global step 1366: loss = 2.1770 (0.853 sec/step)\n",
            "I0813 14:45:24.160002 140596802582400 learning.py:507] global step 1367: loss = 1.5638 (0.839 sec/step)\n",
            "I0813 14:45:24.227721 140593739130624 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0813 14:45:26.444705 140596802582400 learning.py:507] global step 1368: loss = 2.1533 (2.278 sec/step)\n",
            "I0813 14:45:26.445202 140593722345216 supervisor.py:1050] Recording summary at step 1368.\n",
            "I0813 14:45:27.836736 140596802582400 learning.py:507] global step 1369: loss = 2.0182 (1.245 sec/step)\n",
            "I0813 14:45:28.681720 140596802582400 learning.py:507] global step 1370: loss = 1.5046 (0.842 sec/step)\n",
            "I0813 14:45:29.532267 140596802582400 learning.py:507] global step 1371: loss = 2.8173 (0.849 sec/step)\n",
            "I0813 14:45:30.380221 140596802582400 learning.py:507] global step 1372: loss = 1.9852 (0.846 sec/step)\n",
            "I0813 14:45:31.224662 140596802582400 learning.py:507] global step 1373: loss = 1.3834 (0.843 sec/step)\n",
            "I0813 14:45:32.072658 140596802582400 learning.py:507] global step 1374: loss = 1.6843 (0.846 sec/step)\n",
            "I0813 14:45:32.904581 140596802582400 learning.py:507] global step 1375: loss = 2.3003 (0.830 sec/step)\n",
            "I0813 14:45:33.761143 140596802582400 learning.py:507] global step 1376: loss = 1.5628 (0.855 sec/step)\n",
            "I0813 14:45:34.644476 140596802582400 learning.py:507] global step 1377: loss = 1.5631 (0.882 sec/step)\n",
            "I0813 14:45:35.493144 140596802582400 learning.py:507] global step 1378: loss = 0.9918 (0.847 sec/step)\n",
            "I0813 14:45:36.325289 140596802582400 learning.py:507] global step 1379: loss = 1.1516 (0.831 sec/step)\n",
            "I0813 14:45:37.155796 140596802582400 learning.py:507] global step 1380: loss = 1.6997 (0.829 sec/step)\n",
            "I0813 14:45:38.024559 140596802582400 learning.py:507] global step 1381: loss = 1.0063 (0.867 sec/step)\n",
            "I0813 14:45:38.899774 140596802582400 learning.py:507] global step 1382: loss = 1.0876 (0.874 sec/step)\n",
            "I0813 14:45:39.751323 140596802582400 learning.py:507] global step 1383: loss = 2.0189 (0.850 sec/step)\n",
            "I0813 14:45:40.588177 140596802582400 learning.py:507] global step 1384: loss = 1.9467 (0.835 sec/step)\n",
            "I0813 14:45:41.445802 140596802582400 learning.py:507] global step 1385: loss = 2.4209 (0.856 sec/step)\n",
            "I0813 14:45:42.290290 140596802582400 learning.py:507] global step 1386: loss = 1.0109 (0.843 sec/step)\n",
            "I0813 14:45:43.156259 140596802582400 learning.py:507] global step 1387: loss = 1.7669 (0.864 sec/step)\n",
            "I0813 14:45:44.007525 140596802582400 learning.py:507] global step 1388: loss = 3.0033 (0.850 sec/step)\n",
            "I0813 14:45:44.859281 140596802582400 learning.py:507] global step 1389: loss = 1.4928 (0.850 sec/step)\n",
            "I0813 14:45:45.716212 140596802582400 learning.py:507] global step 1390: loss = 2.0488 (0.855 sec/step)\n",
            "I0813 14:45:46.562575 140596802582400 learning.py:507] global step 1391: loss = 1.9367 (0.845 sec/step)\n",
            "I0813 14:45:47.394885 140596802582400 learning.py:507] global step 1392: loss = 1.6153 (0.831 sec/step)\n",
            "I0813 14:45:48.239598 140596802582400 learning.py:507] global step 1393: loss = 2.2760 (0.843 sec/step)\n",
            "I0813 14:45:49.091586 140596802582400 learning.py:507] global step 1394: loss = 2.4554 (0.851 sec/step)\n",
            "I0813 14:45:49.932063 140596802582400 learning.py:507] global step 1395: loss = 1.8966 (0.838 sec/step)\n",
            "I0813 14:45:50.776737 140596802582400 learning.py:507] global step 1396: loss = 1.7316 (0.843 sec/step)\n",
            "I0813 14:45:51.628675 140596802582400 learning.py:507] global step 1397: loss = 1.3525 (0.850 sec/step)\n",
            "I0813 14:45:52.492373 140596802582400 learning.py:507] global step 1398: loss = 1.2661 (0.862 sec/step)\n",
            "I0813 14:45:53.332634 140596802582400 learning.py:507] global step 1399: loss = 2.3150 (0.839 sec/step)\n",
            "I0813 14:45:54.183562 140596802582400 learning.py:507] global step 1400: loss = 1.4504 (0.850 sec/step)\n",
            "I0813 14:45:55.051264 140596802582400 learning.py:507] global step 1401: loss = 1.1607 (0.866 sec/step)\n",
            "I0813 14:45:55.909525 140596802582400 learning.py:507] global step 1402: loss = 1.5916 (0.857 sec/step)\n",
            "I0813 14:45:56.764806 140596802582400 learning.py:507] global step 1403: loss = 1.2606 (0.854 sec/step)\n",
            "I0813 14:45:57.631743 140596802582400 learning.py:507] global step 1404: loss = 2.2826 (0.865 sec/step)\n",
            "I0813 14:45:58.486640 140596802582400 learning.py:507] global step 1405: loss = 1.4776 (0.853 sec/step)\n",
            "I0813 14:45:59.321648 140596802582400 learning.py:507] global step 1406: loss = 1.6867 (0.833 sec/step)\n",
            "I0813 14:46:00.168302 140596802582400 learning.py:507] global step 1407: loss = 2.1130 (0.845 sec/step)\n",
            "I0813 14:46:01.037023 140596802582400 learning.py:507] global step 1408: loss = 1.1273 (0.867 sec/step)\n",
            "I0813 14:46:01.863236 140596802582400 learning.py:507] global step 1409: loss = 1.6839 (0.825 sec/step)\n",
            "I0813 14:46:02.725089 140596802582400 learning.py:507] global step 1410: loss = 1.9534 (0.860 sec/step)\n",
            "I0813 14:46:03.596711 140596802582400 learning.py:507] global step 1411: loss = 3.0092 (0.870 sec/step)\n",
            "I0813 14:46:04.471712 140596802582400 learning.py:507] global step 1412: loss = 1.7404 (0.873 sec/step)\n",
            "I0813 14:46:05.344547 140596802582400 learning.py:507] global step 1413: loss = 2.9059 (0.871 sec/step)\n",
            "I0813 14:46:06.199001 140596802582400 learning.py:507] global step 1414: loss = 0.9413 (0.853 sec/step)\n",
            "I0813 14:46:07.050590 140596802582400 learning.py:507] global step 1415: loss = 1.2138 (0.850 sec/step)\n",
            "I0813 14:46:07.959505 140596802582400 learning.py:507] global step 1416: loss = 2.0777 (0.907 sec/step)\n",
            "I0813 14:46:08.814696 140596802582400 learning.py:507] global step 1417: loss = 1.8264 (0.854 sec/step)\n",
            "I0813 14:46:09.665626 140596802582400 learning.py:507] global step 1418: loss = 1.8257 (0.849 sec/step)\n",
            "I0813 14:46:10.524328 140596802582400 learning.py:507] global step 1419: loss = 1.6225 (0.857 sec/step)\n",
            "I0813 14:46:11.361749 140596802582400 learning.py:507] global step 1420: loss = 1.6121 (0.835 sec/step)\n",
            "I0813 14:46:12.224012 140596802582400 learning.py:507] global step 1421: loss = 2.2744 (0.860 sec/step)\n",
            "I0813 14:46:13.055428 140596802582400 learning.py:507] global step 1422: loss = 2.3546 (0.830 sec/step)\n",
            "I0813 14:46:13.893207 140596802582400 learning.py:507] global step 1423: loss = 2.2899 (0.836 sec/step)\n",
            "I0813 14:46:14.733528 140596802582400 learning.py:507] global step 1424: loss = 2.2668 (0.839 sec/step)\n",
            "I0813 14:46:15.576885 140596802582400 learning.py:507] global step 1425: loss = 1.4167 (0.842 sec/step)\n",
            "I0813 14:46:16.426271 140596802582400 learning.py:507] global step 1426: loss = 2.2208 (0.848 sec/step)\n",
            "I0813 14:46:17.264334 140596802582400 learning.py:507] global step 1427: loss = 2.4451 (0.836 sec/step)\n",
            "I0813 14:46:18.116098 140596802582400 learning.py:507] global step 1428: loss = 1.2111 (0.850 sec/step)\n",
            "I0813 14:46:18.947009 140596802582400 learning.py:507] global step 1429: loss = 1.9074 (0.829 sec/step)\n",
            "I0813 14:46:19.773276 140596802582400 learning.py:507] global step 1430: loss = 2.1084 (0.824 sec/step)\n",
            "I0813 14:46:20.624291 140596802582400 learning.py:507] global step 1431: loss = 1.8446 (0.849 sec/step)\n",
            "I0813 14:46:21.463481 140596802582400 learning.py:507] global step 1432: loss = 1.9259 (0.838 sec/step)\n",
            "I0813 14:46:22.324939 140596802582400 learning.py:507] global step 1433: loss = 2.3447 (0.860 sec/step)\n",
            "I0813 14:46:23.184070 140596802582400 learning.py:507] global step 1434: loss = 1.2421 (0.858 sec/step)\n",
            "I0813 14:46:24.037243 140596802582400 learning.py:507] global step 1435: loss = 1.3345 (0.852 sec/step)\n",
            "I0813 14:46:24.905473 140596802582400 learning.py:507] global step 1436: loss = 1.9070 (0.867 sec/step)\n",
            "I0813 14:46:25.761620 140596802582400 learning.py:507] global step 1437: loss = 2.3411 (0.854 sec/step)\n",
            "I0813 14:46:26.610324 140596802582400 learning.py:507] global step 1438: loss = 1.9474 (0.847 sec/step)\n",
            "I0813 14:46:27.454762 140596802582400 learning.py:507] global step 1439: loss = 2.2634 (0.843 sec/step)\n",
            "I0813 14:46:28.298984 140596802582400 learning.py:507] global step 1440: loss = 2.2072 (0.843 sec/step)\n",
            "I0813 14:46:29.168413 140596802582400 learning.py:507] global step 1441: loss = 1.8884 (0.868 sec/step)\n",
            "I0813 14:46:30.020810 140596802582400 learning.py:507] global step 1442: loss = 2.7594 (0.851 sec/step)\n",
            "I0813 14:46:30.869657 140596802582400 learning.py:507] global step 1443: loss = 2.6109 (0.847 sec/step)\n",
            "I0813 14:46:31.738410 140596802582400 learning.py:507] global step 1444: loss = 2.4326 (0.867 sec/step)\n",
            "I0813 14:46:32.591964 140596802582400 learning.py:507] global step 1445: loss = 1.5657 (0.852 sec/step)\n",
            "I0813 14:46:33.445968 140596802582400 learning.py:507] global step 1446: loss = 1.4633 (0.852 sec/step)\n",
            "I0813 14:46:34.297450 140596802582400 learning.py:507] global step 1447: loss = 1.9831 (0.850 sec/step)\n",
            "I0813 14:46:35.135646 140596802582400 learning.py:507] global step 1448: loss = 2.0093 (0.837 sec/step)\n",
            "I0813 14:46:36.039551 140596802582400 learning.py:507] global step 1449: loss = 2.0410 (0.903 sec/step)\n",
            "I0813 14:46:36.930978 140596802582400 learning.py:507] global step 1450: loss = 1.0267 (0.890 sec/step)\n",
            "I0813 14:46:37.804981 140596802582400 learning.py:507] global step 1451: loss = 1.6073 (0.872 sec/step)\n",
            "I0813 14:46:38.653635 140596802582400 learning.py:507] global step 1452: loss = 1.6078 (0.847 sec/step)\n",
            "I0813 14:46:39.527247 140596802582400 learning.py:507] global step 1453: loss = 1.6903 (0.872 sec/step)\n",
            "I0813 14:46:40.363813 140596802582400 learning.py:507] global step 1454: loss = 1.1661 (0.835 sec/step)\n",
            "I0813 14:46:41.217423 140596802582400 learning.py:507] global step 1455: loss = 2.1634 (0.852 sec/step)\n",
            "I0813 14:46:42.080963 140596802582400 learning.py:507] global step 1456: loss = 1.8348 (0.862 sec/step)\n",
            "I0813 14:46:42.935628 140596802582400 learning.py:507] global step 1457: loss = 1.4153 (0.853 sec/step)\n",
            "I0813 14:46:43.779869 140596802582400 learning.py:507] global step 1458: loss = 1.7610 (0.843 sec/step)\n",
            "I0813 14:46:44.619717 140596802582400 learning.py:507] global step 1459: loss = 1.6533 (0.838 sec/step)\n",
            "I0813 14:46:45.475100 140596802582400 learning.py:507] global step 1460: loss = 1.6119 (0.854 sec/step)\n",
            "I0813 14:46:46.320269 140596802582400 learning.py:507] global step 1461: loss = 2.2300 (0.844 sec/step)\n",
            "I0813 14:46:47.149441 140596802582400 learning.py:507] global step 1462: loss = 2.1058 (0.828 sec/step)\n",
            "I0813 14:46:47.997617 140596802582400 learning.py:507] global step 1463: loss = 1.8465 (0.847 sec/step)\n",
            "I0813 14:46:48.835927 140596802582400 learning.py:507] global step 1464: loss = 1.5291 (0.837 sec/step)\n",
            "I0813 14:46:49.708515 140596802582400 learning.py:507] global step 1465: loss = 1.9930 (0.871 sec/step)\n",
            "I0813 14:46:50.554084 140596802582400 learning.py:507] global step 1466: loss = 0.9367 (0.844 sec/step)\n",
            "I0813 14:46:51.416946 140596802582400 learning.py:507] global step 1467: loss = 2.2596 (0.861 sec/step)\n",
            "I0813 14:46:52.265066 140596802582400 learning.py:507] global step 1468: loss = 1.1162 (0.846 sec/step)\n",
            "I0813 14:46:53.116279 140596802582400 learning.py:507] global step 1469: loss = 1.5181 (0.850 sec/step)\n",
            "I0813 14:46:53.964589 140596802582400 learning.py:507] global step 1470: loss = 1.2520 (0.847 sec/step)\n",
            "I0813 14:46:54.815655 140596802582400 learning.py:507] global step 1471: loss = 2.4866 (0.849 sec/step)\n",
            "I0813 14:46:55.671706 140596802582400 learning.py:507] global step 1472: loss = 2.5537 (0.854 sec/step)\n",
            "I0813 14:46:56.523965 140596802582400 learning.py:507] global step 1473: loss = 1.4891 (0.851 sec/step)\n",
            "I0813 14:46:57.355095 140596802582400 learning.py:507] global step 1474: loss = 1.3892 (0.830 sec/step)\n",
            "I0813 14:46:58.195929 140596802582400 learning.py:507] global step 1475: loss = 2.2501 (0.839 sec/step)\n",
            "I0813 14:46:59.024278 140596802582400 learning.py:507] global step 1476: loss = 1.4962 (0.827 sec/step)\n",
            "I0813 14:46:59.869711 140596802582400 learning.py:507] global step 1477: loss = 1.6363 (0.844 sec/step)\n",
            "I0813 14:47:00.722145 140596802582400 learning.py:507] global step 1478: loss = 2.1342 (0.851 sec/step)\n",
            "I0813 14:47:01.569567 140596802582400 learning.py:507] global step 1479: loss = 2.1654 (0.846 sec/step)\n",
            "I0813 14:47:02.425934 140596802582400 learning.py:507] global step 1480: loss = 1.4245 (0.855 sec/step)\n",
            "I0813 14:47:03.294295 140596802582400 learning.py:507] global step 1481: loss = 1.7932 (0.867 sec/step)\n",
            "I0813 14:47:04.167233 140596802582400 learning.py:507] global step 1482: loss = 2.5635 (0.871 sec/step)\n",
            "I0813 14:47:05.021713 140596802582400 learning.py:507] global step 1483: loss = 2.1737 (0.853 sec/step)\n",
            "I0813 14:47:05.872204 140596802582400 learning.py:507] global step 1484: loss = 1.9823 (0.849 sec/step)\n",
            "I0813 14:47:06.707595 140596802582400 learning.py:507] global step 1485: loss = 1.7073 (0.834 sec/step)\n",
            "I0813 14:47:07.559946 140596802582400 learning.py:507] global step 1486: loss = 1.5597 (0.851 sec/step)\n",
            "I0813 14:47:08.412710 140596802582400 learning.py:507] global step 1487: loss = 1.5513 (0.851 sec/step)\n",
            "I0813 14:47:09.260890 140596802582400 learning.py:507] global step 1488: loss = 2.0502 (0.847 sec/step)\n",
            "I0813 14:47:10.096927 140596802582400 learning.py:507] global step 1489: loss = 1.4634 (0.834 sec/step)\n",
            "I0813 14:47:10.960052 140596802582400 learning.py:507] global step 1490: loss = 1.2242 (0.862 sec/step)\n",
            "I0813 14:47:11.807901 140596802582400 learning.py:507] global step 1491: loss = 1.3972 (0.846 sec/step)\n",
            "I0813 14:47:12.670588 140596802582400 learning.py:507] global step 1492: loss = 2.6643 (0.861 sec/step)\n",
            "I0813 14:47:13.547401 140596802582400 learning.py:507] global step 1493: loss = 1.1884 (0.875 sec/step)\n",
            "I0813 14:47:14.389241 140596802582400 learning.py:507] global step 1494: loss = 1.6875 (0.840 sec/step)\n",
            "I0813 14:47:15.251108 140596802582400 learning.py:507] global step 1495: loss = 1.8663 (0.860 sec/step)\n",
            "I0813 14:47:16.120291 140596802582400 learning.py:507] global step 1496: loss = 2.1782 (0.867 sec/step)\n",
            "I0813 14:47:16.950472 140596802582400 learning.py:507] global step 1497: loss = 1.6301 (0.829 sec/step)\n",
            "I0813 14:47:17.808758 140596802582400 learning.py:507] global step 1498: loss = 2.2525 (0.856 sec/step)\n",
            "I0813 14:47:18.676701 140596802582400 learning.py:507] global step 1499: loss = 1.4356 (0.866 sec/step)\n",
            "I0813 14:47:19.532992 140596802582400 learning.py:507] global step 1500: loss = 1.8349 (0.855 sec/step)\n",
            "I0813 14:47:20.404690 140596802582400 learning.py:507] global step 1501: loss = 1.8986 (0.870 sec/step)\n",
            "I0813 14:47:21.252307 140596802582400 learning.py:507] global step 1502: loss = 1.4084 (0.846 sec/step)\n",
            "I0813 14:47:22.108983 140596802582400 learning.py:507] global step 1503: loss = 2.3148 (0.855 sec/step)\n",
            "I0813 14:47:22.955045 140596802582400 learning.py:507] global step 1504: loss = 1.3922 (0.844 sec/step)\n",
            "I0813 14:47:23.800325 140596802582400 learning.py:507] global step 1505: loss = 1.2886 (0.844 sec/step)\n",
            "I0813 14:47:25.128322 140593722345216 supervisor.py:1050] Recording summary at step 1505.\n",
            "I0813 14:47:25.333464 140596802582400 learning.py:507] global step 1506: loss = 1.5207 (1.532 sec/step)\n",
            "I0813 14:47:26.182166 140596802582400 learning.py:507] global step 1507: loss = 1.6467 (0.847 sec/step)\n",
            "I0813 14:47:27.015023 140596802582400 learning.py:507] global step 1508: loss = 1.4079 (0.831 sec/step)\n",
            "I0813 14:47:27.869520 140596802582400 learning.py:507] global step 1509: loss = 1.4345 (0.853 sec/step)\n",
            "I0813 14:47:28.720658 140596802582400 learning.py:507] global step 1510: loss = 2.0308 (0.850 sec/step)\n",
            "I0813 14:47:29.570558 140596802582400 learning.py:507] global step 1511: loss = 1.2822 (0.848 sec/step)\n",
            "I0813 14:47:30.404889 140596802582400 learning.py:507] global step 1512: loss = 2.0553 (0.833 sec/step)\n",
            "I0813 14:47:31.256420 140596802582400 learning.py:507] global step 1513: loss = 1.0561 (0.850 sec/step)\n",
            "I0813 14:47:32.094416 140596802582400 learning.py:507] global step 1514: loss = 1.4789 (0.836 sec/step)\n",
            "I0813 14:47:32.939348 140596802582400 learning.py:507] global step 1515: loss = 2.2047 (0.843 sec/step)\n",
            "I0813 14:47:33.791071 140596802582400 learning.py:507] global step 1516: loss = 2.2647 (0.850 sec/step)\n",
            "I0813 14:47:34.640009 140596802582400 learning.py:507] global step 1517: loss = 1.9645 (0.847 sec/step)\n",
            "I0813 14:47:35.504112 140596802582400 learning.py:507] global step 1518: loss = 3.1353 (0.863 sec/step)\n",
            "I0813 14:47:36.351179 140596802582400 learning.py:507] global step 1519: loss = 1.2435 (0.846 sec/step)\n",
            "I0813 14:47:37.193683 140596802582400 learning.py:507] global step 1520: loss = 1.1965 (0.841 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 416, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "1e937486-372f-4f84-e86c-ffc6477d0bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-1367 --output_directory inference_graph"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 14:48:11.425951 140199918995328 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0813 14:48:11.436256 140199918995328 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0813 14:48:11.446425 140199918995328 deprecation_wrapper.py:119] From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0813 14:48:11.446977 140199918995328 deprecation_wrapper.py:119] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0813 14:48:11.453061 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0813 14:48:11.453237 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0813 14:48:11.488866 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0813 14:48:11.519469 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0813 14:48:14.587922 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0813 14:48:14.601207 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0813 14:48:14.601360 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:48:14.705378 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:48:14.822546 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:48:14.924320 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:48:15.031990 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0813 14:48:15.136343 140199918995328 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0813 14:48:15.624186 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:567: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0813 14:48:15.956912 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:260: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0813 14:48:15.957164 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0813 14:48:15.960823 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0813 14:48:15.961953 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "162 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/2.99m params)\n",
            "  BoxPredictor_0 (--/20.75k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/69.16k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/27.68k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/13.86k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/13.86k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/6.95k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "162 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/17.63k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/add_8 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/add_11 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/add (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_1 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_4 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_3 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_6 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_7 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_9 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_10 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "W0813 14:48:17.260725 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-08-13 14:48:18.478721: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-08-13 14:48:18.524134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.524559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-13 14:48:18.524869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:48:18.526064: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-13 14:48:18.527198: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-13 14:48:18.527561: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-13 14:48:18.528893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-13 14:48:18.529943: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-13 14:48:18.533446: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-13 14:48:18.533656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.534175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.534576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-13 14:48:18.540161: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-08-13 14:48:18.540393: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2642f40 executing computations on platform Host. Devices:\n",
            "2019-08-13 14:48:18.540424: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-08-13 14:48:18.671797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.672350: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2643480 executing computations on platform CUDA. Devices:\n",
            "2019-08-13 14:48:18.672382: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-08-13 14:48:18.672694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.673076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-13 14:48:18.673159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:48:18.673189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-13 14:48:18.673215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-13 14:48:18.673241: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-13 14:48:18.673265: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-13 14:48:18.673291: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-13 14:48:18.673318: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-13 14:48:18.673413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.674095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.674700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-13 14:48:18.674787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:48:18.675848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-13 14:48:18.675876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-13 14:48:18.675888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-13 14:48:18.676211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.676669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:18.677020: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-08-13 14:48:18.677063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0813 14:48:18.677953 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0813 14:48:18.679235 140199918995328 saver.py:1280] Restoring parameters from training/model.ckpt-1367\n",
            "2019-08-13 14:48:21.538388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:21.538892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-13 14:48:21.538988: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:48:21.539019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-13 14:48:21.539045: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-13 14:48:21.539074: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-13 14:48:21.539098: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-13 14:48:21.539124: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-13 14:48:21.539151: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-13 14:48:21.539260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:21.539730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:21.540070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-13 14:48:21.540113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-13 14:48:21.540127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-13 14:48:21.540138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-13 14:48:21.540425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:21.540849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:21.541204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0813 14:48:21.542380 140199918995328 saver.py:1280] Restoring parameters from training/model.ckpt-1367\n",
            "W0813 14:48:22.286732 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0813 14:48:22.287001 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "I0813 14:48:22.686296 140199918995328 graph_util_impl.py:311] Froze 404 variables.\n",
            "I0813 14:48:22.766752 140199918995328 graph_util_impl.py:364] Converted 404 variables to const ops.\n",
            "2019-08-13 14:48:22.911233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:22.911765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-13 14:48:22.911872: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-08-13 14:48:22.911899: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-08-13 14:48:22.911925: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-08-13 14:48:22.911948: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-08-13 14:48:22.911976: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-08-13 14:48:22.912017: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-08-13 14:48:22.912061: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-08-13 14:48:22.912184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:22.912659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:22.913160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-13 14:48:22.913233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-13 14:48:22.913253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-13 14:48:22.913268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-13 14:48:22.913813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:22.914545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-13 14:48:22.915163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0813 14:48:23.555090 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0813 14:48:23.555539 140199918995328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0813 14:48:23.556069 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0813 14:48:23.556209 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0813 14:48:23.556388 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0813 14:48:23.556500 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:307: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0813 14:48:23.556778 140199918995328 builder_impl.py:636] No assets to save.\n",
            "I0813 14:48:23.556856 140199918995328 builder_impl.py:456] No assets to write.\n",
            "I0813 14:48:23.843246 140199918995328 builder_impl.py:421] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "W0813 14:48:23.874285 140199918995328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0813 14:48:23.874506 140199918995328 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -r /content/models/research/object_detection/inference_graph /gdrive/My\\ Drive/colabfiles/lektion31"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasenbilder190726_01/bild101.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8-djxitLZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasen.mov /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = 'bild101.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs2cfgcr1AZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Video Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/16/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on a video.\n",
        "# It draws boxes and scores around the objects of interest in each frame\n",
        "# of the video.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import imutils\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "VIDEO_NAME = 'rasen.mov'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "print(CWD_PATH)\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to video\n",
        "PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Open video file\n",
        "video = cv2.VideoCapture(PATH_TO_VIDEO)\n",
        "print(\"ok\")\n",
        "i = 0\n",
        "while(video.isOpened()):\n",
        "\n",
        "    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]\n",
        "    # i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "    ret, frame = video.read()\n",
        "    if ret==True:\n",
        "        \n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={image_tensor: frame_expanded})\n",
        "\n",
        "        # Draw the results of the detection (aka 'visulaize the results')\n",
        "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            frame,\n",
        "            np.squeeze(boxes),\n",
        "            np.squeeze(classes).astype(np.int32),\n",
        "            np.squeeze(scores),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.70)\n",
        "\n",
        "        # All the results have been drawn on the frame, so it's time to display it.\n",
        "        \n",
        "        frame = imutils.resize(frame, 400)\n",
        "        cv2_imshow(frame)\n",
        "        #print(boxes)\n",
        "        time.sleep(1)\n",
        "        clear_output()\n",
        "        \n",
        "\n",
        "        # Press 'q' to quit\n",
        "        #print(i)\n",
        "        #i = i + 1\n",
        "        #if cv2.waitKey(1) == ord('q'):\n",
        "        #    break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Clean up\n",
        "print(\"end1\")\n",
        "#video.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(\"end2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}