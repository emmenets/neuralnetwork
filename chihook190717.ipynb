{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/chihook190717.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cfe79b77-ac53-4a8b-9c43-0b2f9e8227c7"
      },
      "source": [
        "!rm -R /models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/models': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_lt0De55L-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "7177a66b-e641-4186-99d7-0a03f683146d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "80f1e1cc-1ef4-4afe-b5d0-98667a6a2eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-17 09:00:43--  http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.141.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149119618 (142M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’\n",
            "\n",
            "\r          faster_rc   0%[                    ]       0  --.-KB/s               \r         faster_rcn  38%[======>             ]  54.27M   271MB/s               \r        faster_rcnn  87%[================>   ] 123.84M   310MB/s               \rfaster_rcnn_incepti 100%[===================>] 142.21M   276MB/s    in 0.5s    \n",
            "\n",
            "2019-07-17 09:00:44 (276 MB/s) - ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’ saved [149119618/149119618]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf faster_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "447b5fb1-addd-46e3-ad44-f932378b42da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "!git clone https://github.com/seraj94ai/Train-Object-Detection-Classifier.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Train-Object-Detection-Classifier'...\n",
            "remote: Enumerating objects: 482, done.\u001b[K\n",
            "remote: Total 482 (delta 0), reused 0 (delta 0), pack-reused 482\u001b[K\n",
            "Receiving objects: 100% (482/482), 30.90 MiB | 46.13 MiB/s, done.\n",
            "Resolving deltas: 100% (234/234), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnJcNmcI8ALN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGihW7NO8Cvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjBVHu28HdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DGe7nAW9BSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/errs /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/images /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/in /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/out /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/renameFiles.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/xml_to_csv.py /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnSvHWz-3X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!set PYTHONPATH=/content/models,/content/models/research,/content/models/research/slim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEMhWLvdLXKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bafcb00d-ac5b-4e47-fa3a-e1cd9eae3abd"
      },
      "source": [
        "!echo %PYTHONPATH%"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "%PYTHONPATH%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G61sEdQ-Lkaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!set PATH=%PATH%,PYTHONPATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LutDU_-ME6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "605ad50a-5b25-4b4c-ee25-6e641535a3b9"
      },
      "source": [
        "!echo %PATH%"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "%PATH%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "047d77fa-ec81-48e9-ec0c-2ffdfc82f9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "49f2d67a-fe8b-4241-ea14-2834491843f5"
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "outputId": "142bf665-c692-4b35-95f7-ef039cbe71d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!python3 xml_to_csv.py"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VfI6dZGHeFp",
        "colab_type": "code",
        "outputId": "e774c415-4a0b-4a0c-c2e0-7e73ff5b8114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'chinook':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "7a6422a3-fa9c-4d75-b703-3b7c924a42e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 09:12:36.912874 139781921036160 deprecation_wrapper.py:119] From generate_tfrecord.py:101: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0717 09:12:36.913547 139781921036160 deprecation_wrapper.py:119] From generate_tfrecord.py:87: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0717 09:12:36.987495 139781921036160 deprecation_wrapper.py:119] From generate_tfrecord.py:46: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "7737eeb4-2002-4c7d-dd68-1dfc4c18e0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 09:12:44.988749 140396462536576 deprecation_wrapper.py:119] From generate_tfrecord.py:101: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0717 09:12:44.989456 140396462536576 deprecation_wrapper.py:119] From generate_tfrecord.py:87: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0717 09:12:45.030114 140396462536576 deprecation_wrapper.py:119] From generate_tfrecord.py:46: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "0c0904b0-aebd-4f7f-e1eb-3d3b75bda05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'chinook'\n",
        "}"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyuEP-WUUF3y",
        "colab_type": "code",
        "outputId": "6bed3033-d788-43c5-c2e1-3363acf99ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile training/faster_rcnn_inception_v2_pets.config\n",
        "\n",
        "# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 1\n",
        "    image_resizer {\n",
        "      keep_aspect_ratio_resizer {\n",
        "        min_dimension: 600\n",
        "        max_dimension: 1024\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'faster_rcnn_inception_v2'\n",
        "      first_stage_features_stride: 16\n",
        "    }\n",
        "    first_stage_anchor_generator {\n",
        "      grid_anchor_generator {\n",
        "        scales: [0.25, 0.5, 1.0, 2.0]\n",
        "        aspect_ratios: [0.5, 1.0, 2.0]\n",
        "        height_stride: 16\n",
        "        width_stride: 16\n",
        "      }\n",
        "    }\n",
        "    first_stage_box_predictor_conv_hyperparams {\n",
        "      op: CONV\n",
        "      regularizer {\n",
        "        l2_regularizer {\n",
        "          weight: 0.0\n",
        "        }\n",
        "      }\n",
        "      initializer {\n",
        "        truncated_normal_initializer {\n",
        "          stddev: 0.01\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    first_stage_nms_score_threshold: 0.0\n",
        "    first_stage_nms_iou_threshold: 0.7\n",
        "    first_stage_max_proposals: 300\n",
        "    first_stage_localization_loss_weight: 2.0\n",
        "    first_stage_objectness_loss_weight: 1.0\n",
        "    initial_crop_size: 14\n",
        "    maxpool_kernel_size: 2\n",
        "    maxpool_stride: 2\n",
        "    second_stage_box_predictor {\n",
        "      mask_rcnn_box_predictor {\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 1.0\n",
        "        fc_hyperparams {\n",
        "          op: FC\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.0\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            variance_scaling_initializer {\n",
        "              factor: 1.0\n",
        "              uniform: true\n",
        "              mode: FAN_AVG\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    second_stage_post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 0.0\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 300\n",
        "      }\n",
        "      score_converter: SOFTMAX\n",
        "    }\n",
        "    second_stage_localization_loss_weight: 2.0\n",
        "    second_stage_classification_loss_weight: 1.0\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 1\n",
        "  optimizer {\n",
        "    momentum_optimizer: {\n",
        "      learning_rate: {\n",
        "        manual_step_learning_rate {\n",
        "          initial_learning_rate: 0.0002\n",
        "          schedule {\n",
        "            step: 900000\n",
        "            learning_rate: .00002\n",
        "          }\n",
        "          schedule {\n",
        "            step: 1200000\n",
        "            learning_rate: .000002\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  gradient_clipping_by_norm: 10.0\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt\"\n",
        "  from_detection_checkpoint: true\n",
        "  load_all_detection_checkpoint_vars: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  num_examples: 70\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxtt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/faster_rcnn_inception_v2_pets.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "c2ec23b5-c2ce-421a-df0c-aedb4bcb18f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /content/models/research/object_detection/train.py /content/models/research/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "4219567b-0fc6-476e-c94c-be8807947046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 09:14:33.993987 139922188969856 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 52, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/model_builder.py\", line 35, in <module>\n",
            "    from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py\", line 29, in <module>\n",
            "    from nets import inception_resnet_v2\n",
            "ModuleNotFoundError: No module named 'nets'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyJ767koFV4t",
        "colab_type": "code",
        "outputId": "3aef86f6-bde0-44aa-f7f1-ad672296a380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /content/models/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\n",
            "\"\"\"Inception Resnet v2 Faster R-CNN implementation.\n",
            "\n",
            "See \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on\n",
            "Learning\" by Szegedy et al. (https://arxiv.org/abs/1602.07261)\n",
            "as well as\n",
            "\"Speed/accuracy trade-offs for modern convolutional object detectors\" by\n",
            "Huang et al. (https://arxiv.org/abs/1611.10012)\n",
            "\"\"\"\n",
            "\n",
            "import tensorflow as tf\n",
            "\n",
            "from object_detection.meta_architectures import faster_rcnn_meta_arch\n",
            "from object_detection.utils import variables_helper\n",
            "from nets import inception_resnet_v2\n",
            "\n",
            "slim = tf.contrib.slim\n",
            "\n",
            "\n",
            "class FasterRCNNInceptionResnetV2FeatureExtractor(\n",
            "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n",
            "  \"\"\"Faster R-CNN with Inception Resnet v2 feature extractor implementation.\"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               is_training,\n",
            "               first_stage_features_stride,\n",
            "               batch_norm_trainable=False,\n",
            "               reuse_weights=None,\n",
            "               weight_decay=0.0):\n",
            "    \"\"\"Constructor.\n",
            "\n",
            "    Args:\n",
            "      is_training: See base class.\n",
            "      first_stage_features_stride: See base class.\n",
            "      batch_norm_trainable: See base class.\n",
            "      reuse_weights: See base class.\n",
            "      weight_decay: See base class.\n",
            "\n",
            "    Raises:\n",
            "      ValueError: If `first_stage_features_stride` is not 8 or 16.\n",
            "    \"\"\"\n",
            "    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n",
            "      raise ValueError('`first_stage_features_stride` must be 8 or 16.')\n",
            "    super(FasterRCNNInceptionResnetV2FeatureExtractor, self).__init__(\n",
            "        is_training, first_stage_features_stride, batch_norm_trainable,\n",
            "        reuse_weights, weight_decay)\n",
            "\n",
            "  def preprocess(self, resized_inputs):\n",
            "    \"\"\"Faster R-CNN with Inception Resnet v2 preprocessing.\n",
            "\n",
            "    Maps pixel values to the range [-1, 1].\n",
            "\n",
            "    Args:\n",
            "      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n",
            "        representing a batch of images with values between 0 and 255.0.\n",
            "\n",
            "    Returns:\n",
            "      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n",
            "        tensor representing a batch of images.\n",
            "\n",
            "    \"\"\"\n",
            "    return (2.0 / 255.0) * resized_inputs - 1.0\n",
            "\n",
            "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
            "    \"\"\"Extracts first stage RPN features.\n",
            "\n",
            "    Extracts features using the first half of the Inception Resnet v2 network.\n",
            "    We construct the network in `align_feature_maps=True` mode, which means\n",
            "    that all VALID paddings in the network are changed to SAME padding so that\n",
            "    the feature maps are aligned.\n",
            "\n",
            "    Args:\n",
            "      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n",
            "        representing a batch of images.\n",
            "      scope: A scope name.\n",
            "\n",
            "    Returns:\n",
            "      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n",
            "    Raises:\n",
            "      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n",
            "        (height or width) is less than 33.\n",
            "      ValueError: If the created network is missing the required activation.\n",
            "    \"\"\"\n",
            "    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n",
            "      raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
            "                       'tensor of shape %s' % preprocessed_inputs.get_shape())\n",
            "\n",
            "    with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope(\n",
            "        weight_decay=self._weight_decay)):\n",
            "      # Forces is_training to False to disable batch norm update.\n",
            "      with slim.arg_scope([slim.batch_norm],\n",
            "                          is_training=self._train_batch_norm):\n",
            "        with tf.variable_scope('InceptionResnetV2',\n",
            "                               reuse=self._reuse_weights) as scope:\n",
            "          return inception_resnet_v2.inception_resnet_v2_base(\n",
            "              preprocessed_inputs, final_endpoint='PreAuxLogits',\n",
            "              scope=scope, output_stride=self._first_stage_features_stride,\n",
            "              align_feature_maps=True)\n",
            "\n",
            "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
            "    \"\"\"Extracts second stage box classifier features.\n",
            "\n",
            "    This function reconstructs the \"second half\" of the Inception ResNet v2\n",
            "    network after the part defined in `_extract_proposal_features`.\n",
            "\n",
            "    Args:\n",
            "      proposal_feature_maps: A 4-D float tensor with shape\n",
            "        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n",
            "        representing the feature map cropped to each proposal.\n",
            "      scope: A scope name.\n",
            "\n",
            "    Returns:\n",
            "      proposal_classifier_features: A 4-D float tensor with shape\n",
            "        [batch_size * self.max_num_proposals, height, width, depth]\n",
            "        representing box classifier features for each proposal.\n",
            "    \"\"\"\n",
            "    with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights):\n",
            "      with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope(\n",
            "          weight_decay=self._weight_decay)):\n",
            "        # Forces is_training to False to disable batch norm update.\n",
            "        with slim.arg_scope([slim.batch_norm],\n",
            "                            is_training=self._train_batch_norm):\n",
            "          with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
            "                              stride=1, padding='SAME'):\n",
            "            with tf.variable_scope('Mixed_7a'):\n",
            "              with tf.variable_scope('Branch_0'):\n",
            "                tower_conv = slim.conv2d(proposal_feature_maps,\n",
            "                                         256, 1, scope='Conv2d_0a_1x1')\n",
            "                tower_conv_1 = slim.conv2d(\n",
            "                    tower_conv, 384, 3, stride=2,\n",
            "                    padding='VALID', scope='Conv2d_1a_3x3')\n",
            "              with tf.variable_scope('Branch_1'):\n",
            "                tower_conv1 = slim.conv2d(\n",
            "                    proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1')\n",
            "                tower_conv1_1 = slim.conv2d(\n",
            "                    tower_conv1, 288, 3, stride=2,\n",
            "                    padding='VALID', scope='Conv2d_1a_3x3')\n",
            "              with tf.variable_scope('Branch_2'):\n",
            "                tower_conv2 = slim.conv2d(\n",
            "                    proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1')\n",
            "                tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n",
            "                                            scope='Conv2d_0b_3x3')\n",
            "                tower_conv2_2 = slim.conv2d(\n",
            "                    tower_conv2_1, 320, 3, stride=2,\n",
            "                    padding='VALID', scope='Conv2d_1a_3x3')\n",
            "              with tf.variable_scope('Branch_3'):\n",
            "                tower_pool = slim.max_pool2d(\n",
            "                    proposal_feature_maps, 3, stride=2, padding='VALID',\n",
            "                    scope='MaxPool_1a_3x3')\n",
            "              net = tf.concat(\n",
            "                  [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n",
            "            net = slim.repeat(net, 9, inception_resnet_v2.block8, scale=0.20)\n",
            "            net = inception_resnet_v2.block8(net, activation_fn=None)\n",
            "            proposal_classifier_features = slim.conv2d(\n",
            "                net, 1536, 1, scope='Conv2d_7b_1x1')\n",
            "        return proposal_classifier_features\n",
            "\n",
            "  def restore_from_classification_checkpoint_fn(\n",
            "      self,\n",
            "      first_stage_feature_extractor_scope,\n",
            "      second_stage_feature_extractor_scope):\n",
            "    \"\"\"Returns a map of variables to load from a foreign checkpoint.\n",
            "\n",
            "    Note that this overrides the default implementation in\n",
            "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for\n",
            "    InceptionResnetV2 checkpoints.\n",
            "\n",
            "    TODO(jonathanhuang,rathodv): revisit whether it's possible to force the\n",
            "    `Repeat` namescope as created in `_extract_box_classifier_features` to\n",
            "    start counting at 2 (e.g. `Repeat_2`) so that the default restore_fn can\n",
            "    be used.\n",
            "\n",
            "    Args:\n",
            "      first_stage_feature_extractor_scope: A scope name for the first stage\n",
            "        feature extractor.\n",
            "      second_stage_feature_extractor_scope: A scope name for the second stage\n",
            "        feature extractor.\n",
            "\n",
            "    Returns:\n",
            "      A dict mapping variable names (to load from a checkpoint) to variables in\n",
            "      the model graph.\n",
            "    \"\"\"\n",
            "\n",
            "    variables_to_restore = {}\n",
            "    for variable in variables_helper.get_global_variables_safely():\n",
            "      if variable.op.name.startswith(\n",
            "          first_stage_feature_extractor_scope):\n",
            "        var_name = variable.op.name.replace(\n",
            "            first_stage_feature_extractor_scope + '/', '')\n",
            "        variables_to_restore[var_name] = variable\n",
            "      if variable.op.name.startswith(\n",
            "          second_stage_feature_extractor_scope):\n",
            "        var_name = variable.op.name.replace(\n",
            "            second_stage_feature_extractor_scope\n",
            "            + '/InceptionResnetV2/Repeat', 'InceptionResnetV2/Repeat_2')\n",
            "        var_name = var_name.replace(\n",
            "            second_stage_feature_extractor_scope + '/', '')\n",
            "        variables_to_restore[var_name] = variable\n",
            "    return variables_to_restore\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}