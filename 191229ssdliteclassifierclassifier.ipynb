{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/191229ssdliteclassifierclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "cbd2d3f9-e942-40c1-e490-d4799d11b786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "1b6e86ac-d4dd-4605-c258-f568b7049097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 33473, done.\u001b[K\n",
            "remote: Total 33473 (delta 0), reused 0 (delta 0), pack-reused 33473\u001b[K\n",
            "Receiving objects: 100% (33473/33473), 511.86 MiB | 14.13 MiB/s, done.\n",
            "Resolving deltas: 100% (21405/21405), done.\n",
            "Checking out files: 100% (3194/3194), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "7918b3dd-3f55-49ea-bf12-1a3c4fa899a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "f83fed8b-52eb-4586-bca8-7965b3d97d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
        "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-29 10:24:37--  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 64.233.189.128, 2404:6800:4008:c07::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|64.233.189.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144806142 (138M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_qu 100%[===================>] 138.10M  54.6MB/s    in 2.5s    \n",
            "\n",
            "2019-12-29 10:24:40 (54.6 MB/s) - ‘ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz’ saved [144806142/144806142]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -xf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
        "!tar -xf ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "dd62546f-392b-4f93-e270-e6a802c635fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!git clone https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10'...\n",
            "remote: Enumerating objects: 1129, done.\u001b[K\n",
            "remote: Total 1129 (delta 0), reused 0 (delta 0), pack-reused 1129\u001b[K\n",
            "Receiving objects: 100% (1129/1129), 57.63 MiB | 10.92 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/doc /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/inference_graph /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/training /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/translate /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_webcam.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/resizer.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test.mov /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test1.JPG /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/README.md /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "fcf1aa68-cb45-4f9d-adb4-24028cafd92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "outputId": "2cde8641-b55b-4e03-8714-a0f6db3da906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images\n",
        "!mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6n8GqZS-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98GmqrIhe3z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm /content/models/research/object_detection/images/test_labels.csv\n",
        "#!rm /content/models/research/object_detection/images/train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "outputId": "ba87296d-e365-4428-d048-91291208d707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!python3 xml_to_csv.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv99g_MdfhZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/models/research/object_detection/generate_tfrecord.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab_type": "code",
        "outputId": "f10d4ded-1f7e-49a7-8372-8931514bb16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "845cfb9f-f8fd-4e1a-8b50-98af03dcf43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1229 10:30:58.521225 140705021605760 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1229 10:30:58.596099 140705021605760 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "880af572-5b1f-4550-e714-9caabad2056f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1229 10:31:06.054595 140117047293824 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1229 10:31:06.115637 140117047293824 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "a4e80bfe-a5ed-4efb-c24b-dbb3a7f0ecbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiYCKlkF2eOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat /content/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config\n",
        "!cat /content/models/research/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ayapmyjQt4W",
        "colab_type": "code",
        "outputId": "94116c70-77cb-49dc-824c-91c00c974e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/ssd_mobilenet_v2_quantized_300x300_coco.config\n",
        "\n",
        "# Quantized trained SSD with Mobilenet v2 on MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 6\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 126\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}\n",
        "\n",
        "graph_rewriter {\n",
        "  quantization {\n",
        "    delay: 48000\n",
        "    weight_bits: 8\n",
        "    activation_bits: 8\n",
        "  }\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/ssd_mobilenet_v2_quantized_300x300_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "dbe46d0c-e0db-4727-8487-0b683150de59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "73484337-0d06-4832-c72c-736e6d65da74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_quantized_300x300_coco.config"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W1229 10:46:08.221961 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "WARNING:tensorflow:From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1229 10:46:08.222440 140034317584256 module_wrapper.py:139] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1229 10:46:08.222807 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W1229 10:46:08.226690 140034317584256 module_wrapper.py:139] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W1229 10:46:08.239843 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W1229 10:46:08.244770 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W1229 10:46:08.245053 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W1229 10:46:08.259516 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W1229 10:46:08.261146 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1229 10:46:08.261329 140034317584256 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W1229 10:46:08.269884 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W1229 10:46:08.270130 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1229 10:46:08.305195 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W1229 10:46:08.975691 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1229 10:46:08.983332 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W1229 10:46:08.991079 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1229 10:46:09.058551 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1229 10:46:09.071191 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W1229 10:46:09.827018 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1229 10:46:09.833118 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1229 10:46:09.834553 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W1229 10:46:09.840919 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W1229 10:46:09.845148 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W1229 10:46:09.848909 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1229 10:46:09.849363 140034317584256 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W1229 10:46:09.849556 140034317584256 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1229 10:46:10.106532 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1229 10:46:10.193324 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1229 10:46:13.619176 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1229 10:46:13.633358 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.633527 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.671449 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.718120 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.766076 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.807602 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 10:46:13.844653 140034317584256 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W1229 10:46:14.031573 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W1229 10:46:15.205710 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W1229 10:46:15.207481 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W1229 10:46:15.209055 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W1229 10:46:15.450729 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/graph_rewriter_builder.py:36: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1229 10:46:15.451352 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/graph_rewriter_builder.py:36: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
            "I1229 10:46:24.918913 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
            "I1229 10:46:24.919481 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
            "I1229 10:46:24.920338 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
            "I1229 10:46:24.920727 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
            "I1229 10:46:24.921384 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
            "I1229 10:46:24.921782 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
            "I1229 10:46:24.922429 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
            "I1229 10:46:24.922810 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
            "I1229 10:46:24.923486 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
            "I1229 10:46:24.923952 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
            "I1229 10:46:24.924731 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
            "I1229 10:46:24.925155 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
            "I1229 10:46:24.925941 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
            "I1229 10:46:24.926369 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
            "I1229 10:46:24.927021 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
            "I1229 10:46:24.927428 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
            "I1229 10:46:24.928236 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
            "I1229 10:46:24.928724 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
            "I1229 10:46:24.929403 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
            "I1229 10:46:24.929823 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
            "I1229 10:46:24.930503 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
            "I1229 10:46:24.930915 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
            "I1229 10:46:24.931584 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
            "I1229 10:46:24.932009 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
            "I1229 10:46:24.932657 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
            "I1229 10:46:24.933081 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
            "I1229 10:46:24.933697 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
            "I1229 10:46:24.934100 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
            "I1229 10:46:24.934721 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
            "I1229 10:46:24.935110 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
            "I1229 10:46:24.935877 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
            "I1229 10:46:24.936308 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
            "I1229 10:46:24.936911 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
            "I1229 10:46:24.937303 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
            "I1229 10:46:24.938053 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
            "I1229 10:46:24.938445 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
            "I1229 10:46:24.938920 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
            "I1229 10:46:24.939299 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
            "I1229 10:46:24.939660 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
            "I1229 10:46:24.940036 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
            "I1229 10:46:24.940406 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
            "I1229 10:46:24.940775 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
            "I1229 10:46:24.941164 140034317584256 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W1229 10:46:24.959293 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W1229 10:46:24.959625 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W1229 10:46:24.970457 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "W1229 10:46:34.899736 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W1229 10:46:34.901992 140034317584256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W1229 10:46:37.120438 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W1229 10:46:41.753288 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W1229 10:46:42.113106 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W1229 10:46:42.115528 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W1229 10:46:42.119836 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1229 10:46:42.127449 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1229 10:46:42.127817 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W1229 10:46:43.797040 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1229 10:46:43.801568 140034317584256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "W1229 10:46:46.272924 140034317584256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-12-29 10:46:48.865035: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-12-29 10:46:48.867326: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20686140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-29 10:46:48.867368: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-29 10:46:48.877055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-29 10:46:49.067427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:49.068486: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20684e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-29 10:46:49.068518: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-29 10:46:49.069926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:49.070837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-29 10:46:49.090126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 10:46:49.342911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-29 10:46:49.478252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-29 10:46:49.504418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-29 10:46:49.786834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-29 10:46:49.814820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-29 10:46:50.332460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 10:46:50.332717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:50.333717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:50.334583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-29 10:46:50.338144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 10:46:50.339921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-29 10:46:50.339953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-29 10:46:50.339967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-29 10:46:50.341285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:50.342288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 10:46:50.343143: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-29 10:46:50.343188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from /content/models/research/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\n",
            "I1229 10:46:56.127201 140034317584256 saver.py:1284] Restoring parameters from /content/models/research/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I1229 10:46:58.343345 140034317584256 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I1229 10:46:59.258326 140034317584256 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "I1229 10:47:16.862408 140034317584256 learning.py:754] Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 10:47:17.667703 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "I1229 10:47:17.672266 140034317584256 learning.py:768] Starting Queues.\n",
            "2019-12-29 10:47:34.175366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 10:47:38.894018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "I1229 10:47:41.925039 140030580766464 supervisor.py:1050] Recording summary at step 0.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "I1229 10:47:51.668004 140030555588352 supervisor.py:1099] global_step/sec: 0\n",
            "INFO:tensorflow:global step 1: loss = 14.2172 (39.528 sec/step)\n",
            "I1229 10:47:58.925139 140034317584256 learning.py:507] global step 1: loss = 14.2172 (39.528 sec/step)\n",
            "INFO:tensorflow:global step 2: loss = 14.7748 (5.083 sec/step)\n",
            "I1229 10:48:05.344259 140034317584256 learning.py:507] global step 2: loss = 14.7748 (5.083 sec/step)\n",
            "INFO:tensorflow:global step 3: loss = 12.1901 (1.265 sec/step)\n",
            "I1229 10:48:06.611389 140034317584256 learning.py:507] global step 3: loss = 12.1901 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 4: loss = 10.3604 (1.242 sec/step)\n",
            "I1229 10:48:07.855282 140034317584256 learning.py:507] global step 4: loss = 10.3604 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 5: loss = 9.4528 (1.230 sec/step)\n",
            "I1229 10:48:09.087128 140034317584256 learning.py:507] global step 5: loss = 9.4528 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 6: loss = 10.0912 (1.278 sec/step)\n",
            "I1229 10:48:10.367089 140034317584256 learning.py:507] global step 6: loss = 10.0912 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 7: loss = 8.9656 (1.283 sec/step)\n",
            "I1229 10:48:11.651542 140034317584256 learning.py:507] global step 7: loss = 8.9656 (1.283 sec/step)\n",
            "INFO:tensorflow:global step 8: loss = 9.0512 (1.229 sec/step)\n",
            "I1229 10:48:12.882581 140034317584256 learning.py:507] global step 8: loss = 9.0512 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 9: loss = 8.7858 (1.230 sec/step)\n",
            "I1229 10:48:14.114952 140034317584256 learning.py:507] global step 9: loss = 8.7858 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 10: loss = 8.3940 (1.208 sec/step)\n",
            "I1229 10:48:15.325237 140034317584256 learning.py:507] global step 10: loss = 8.3940 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 11: loss = 8.1943 (1.285 sec/step)\n",
            "I1229 10:48:16.611810 140034317584256 learning.py:507] global step 11: loss = 8.1943 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 12: loss = 7.9989 (1.224 sec/step)\n",
            "I1229 10:48:17.837596 140034317584256 learning.py:507] global step 12: loss = 7.9989 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 13: loss = 7.3876 (1.255 sec/step)\n",
            "I1229 10:48:19.094532 140034317584256 learning.py:507] global step 13: loss = 7.3876 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 14: loss = 7.8468 (1.340 sec/step)\n",
            "I1229 10:48:20.436480 140034317584256 learning.py:507] global step 14: loss = 7.8468 (1.340 sec/step)\n",
            "INFO:tensorflow:global step 15: loss = 7.5019 (1.286 sec/step)\n",
            "I1229 10:48:21.724388 140034317584256 learning.py:507] global step 15: loss = 7.5019 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 16: loss = 7.3767 (1.210 sec/step)\n",
            "I1229 10:48:22.935564 140034317584256 learning.py:507] global step 16: loss = 7.3767 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 17: loss = 7.8872 (1.294 sec/step)\n",
            "I1229 10:48:24.231592 140034317584256 learning.py:507] global step 17: loss = 7.8872 (1.294 sec/step)\n",
            "INFO:tensorflow:global step 18: loss = 6.8321 (1.284 sec/step)\n",
            "I1229 10:48:25.517296 140034317584256 learning.py:507] global step 18: loss = 6.8321 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 19: loss = 7.2574 (1.236 sec/step)\n",
            "I1229 10:48:26.754712 140034317584256 learning.py:507] global step 19: loss = 7.2574 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 20: loss = 6.5571 (1.240 sec/step)\n",
            "I1229 10:48:27.996563 140034317584256 learning.py:507] global step 20: loss = 6.5571 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 21: loss = 6.6538 (1.227 sec/step)\n",
            "I1229 10:48:29.225194 140034317584256 learning.py:507] global step 21: loss = 6.6538 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 22: loss = 7.4632 (1.236 sec/step)\n",
            "I1229 10:48:30.462488 140034317584256 learning.py:507] global step 22: loss = 7.4632 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 23: loss = 5.9408 (1.281 sec/step)\n",
            "I1229 10:48:31.745355 140034317584256 learning.py:507] global step 23: loss = 5.9408 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 24: loss = 7.4690 (1.276 sec/step)\n",
            "I1229 10:48:33.024001 140034317584256 learning.py:507] global step 24: loss = 7.4690 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 25: loss = 6.1294 (1.252 sec/step)\n",
            "I1229 10:48:34.277944 140034317584256 learning.py:507] global step 25: loss = 6.1294 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 26: loss = 6.8079 (1.248 sec/step)\n",
            "I1229 10:48:35.528143 140034317584256 learning.py:507] global step 26: loss = 6.8079 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 27: loss = 6.6028 (1.220 sec/step)\n",
            "I1229 10:48:36.750603 140034317584256 learning.py:507] global step 27: loss = 6.6028 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 28: loss = 5.8278 (1.199 sec/step)\n",
            "I1229 10:48:37.951711 140034317584256 learning.py:507] global step 28: loss = 5.8278 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 29: loss = 7.1799 (1.215 sec/step)\n",
            "I1229 10:48:39.168366 140034317584256 learning.py:507] global step 29: loss = 7.1799 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 30: loss = 6.7714 (1.228 sec/step)\n",
            "I1229 10:48:40.398167 140034317584256 learning.py:507] global step 30: loss = 6.7714 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 31: loss = 6.2240 (1.306 sec/step)\n",
            "I1229 10:48:41.706132 140034317584256 learning.py:507] global step 31: loss = 6.2240 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 32: loss = 5.9563 (1.202 sec/step)\n",
            "I1229 10:48:42.910198 140034317584256 learning.py:507] global step 32: loss = 5.9563 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 33: loss = 6.1854 (1.224 sec/step)\n",
            "I1229 10:48:44.135499 140034317584256 learning.py:507] global step 33: loss = 6.1854 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 34: loss = 6.3325 (1.239 sec/step)\n",
            "I1229 10:48:45.376138 140034317584256 learning.py:507] global step 34: loss = 6.3325 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 35: loss = 6.1726 (1.313 sec/step)\n",
            "I1229 10:48:46.690721 140034317584256 learning.py:507] global step 35: loss = 6.1726 (1.313 sec/step)\n",
            "INFO:tensorflow:global step 36: loss = 4.7665 (1.255 sec/step)\n",
            "I1229 10:48:47.947475 140034317584256 learning.py:507] global step 36: loss = 4.7665 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 37: loss = 6.5862 (1.239 sec/step)\n",
            "I1229 10:48:49.188297 140034317584256 learning.py:507] global step 37: loss = 6.5862 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 38: loss = 6.0630 (1.209 sec/step)\n",
            "I1229 10:48:50.399018 140034317584256 learning.py:507] global step 38: loss = 6.0630 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 39: loss = 5.9447 (1.237 sec/step)\n",
            "I1229 10:48:51.641160 140034317584256 learning.py:507] global step 39: loss = 5.9447 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 40: loss = 6.4844 (1.211 sec/step)\n",
            "I1229 10:48:52.854787 140034317584256 learning.py:507] global step 40: loss = 6.4844 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 41: loss = 6.6551 (1.246 sec/step)\n",
            "I1229 10:48:54.102427 140034317584256 learning.py:507] global step 41: loss = 6.6551 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 42: loss = 5.3150 (1.221 sec/step)\n",
            "I1229 10:48:55.324936 140034317584256 learning.py:507] global step 42: loss = 5.3150 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 43: loss = 5.6004 (1.281 sec/step)\n",
            "I1229 10:48:56.607616 140034317584256 learning.py:507] global step 43: loss = 5.6004 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 44: loss = 6.8321 (1.206 sec/step)\n",
            "I1229 10:48:57.815478 140034317584256 learning.py:507] global step 44: loss = 6.8321 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 45: loss = 4.9000 (1.237 sec/step)\n",
            "I1229 10:48:59.054052 140034317584256 learning.py:507] global step 45: loss = 4.9000 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 46: loss = 5.9355 (1.198 sec/step)\n",
            "I1229 10:49:00.253500 140034317584256 learning.py:507] global step 46: loss = 5.9355 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 47: loss = 5.3496 (1.238 sec/step)\n",
            "I1229 10:49:01.493085 140034317584256 learning.py:507] global step 47: loss = 5.3496 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 48: loss = 5.0804 (1.273 sec/step)\n",
            "I1229 10:49:02.767331 140034317584256 learning.py:507] global step 48: loss = 5.0804 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 49: loss = 5.1711 (1.192 sec/step)\n",
            "I1229 10:49:03.960925 140034317584256 learning.py:507] global step 49: loss = 5.1711 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 50: loss = 6.8665 (1.215 sec/step)\n",
            "I1229 10:49:05.177360 140034317584256 learning.py:507] global step 50: loss = 6.8665 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 51: loss = 5.3771 (1.251 sec/step)\n",
            "I1229 10:49:06.429834 140034317584256 learning.py:507] global step 51: loss = 5.3771 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 52: loss = 5.8606 (1.256 sec/step)\n",
            "I1229 10:49:07.686970 140034317584256 learning.py:507] global step 52: loss = 5.8606 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 53: loss = 5.0543 (1.248 sec/step)\n",
            "I1229 10:49:08.936899 140034317584256 learning.py:507] global step 53: loss = 5.0543 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 54: loss = 4.6219 (1.301 sec/step)\n",
            "I1229 10:49:10.239392 140034317584256 learning.py:507] global step 54: loss = 4.6219 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 55: loss = 5.0421 (1.281 sec/step)\n",
            "I1229 10:49:11.522418 140034317584256 learning.py:507] global step 55: loss = 5.0421 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 56: loss = 5.2355 (1.302 sec/step)\n",
            "I1229 10:49:12.826431 140034317584256 learning.py:507] global step 56: loss = 5.2355 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 57: loss = 5.7704 (1.177 sec/step)\n",
            "I1229 10:49:14.005528 140034317584256 learning.py:507] global step 57: loss = 5.7704 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 58: loss = 5.3619 (1.230 sec/step)\n",
            "I1229 10:49:15.237195 140034317584256 learning.py:507] global step 58: loss = 5.3619 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 59: loss = 5.3890 (1.338 sec/step)\n",
            "I1229 10:49:16.577373 140034317584256 learning.py:507] global step 59: loss = 5.3890 (1.338 sec/step)\n",
            "INFO:tensorflow:global step 60: loss = 5.0127 (1.354 sec/step)\n",
            "I1229 10:49:17.933928 140034317584256 learning.py:507] global step 60: loss = 5.0127 (1.354 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 60.\n",
            "I1229 10:49:19.848013 140030580766464 supervisor.py:1050] Recording summary at step 60.\n",
            "INFO:tensorflow:global step 61: loss = 5.8340 (2.102 sec/step)\n",
            "I1229 10:49:20.038197 140034317584256 learning.py:507] global step 61: loss = 5.8340 (2.102 sec/step)\n",
            "INFO:tensorflow:global step 62: loss = 6.1362 (1.293 sec/step)\n",
            "I1229 10:49:21.332709 140034317584256 learning.py:507] global step 62: loss = 6.1362 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 63: loss = 5.3656 (1.236 sec/step)\n",
            "I1229 10:49:22.570504 140034317584256 learning.py:507] global step 63: loss = 5.3656 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 64: loss = 5.6439 (1.266 sec/step)\n",
            "I1229 10:49:23.837881 140034317584256 learning.py:507] global step 64: loss = 5.6439 (1.266 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.693552\n",
            "I1229 10:49:23.946593 140030555588352 supervisor.py:1099] global_step/sec: 0.693552\n",
            "INFO:tensorflow:global step 65: loss = 3.9574 (1.253 sec/step)\n",
            "I1229 10:49:25.092238 140034317584256 learning.py:507] global step 65: loss = 3.9574 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 66: loss = 5.5545 (1.198 sec/step)\n",
            "I1229 10:49:26.291864 140034317584256 learning.py:507] global step 66: loss = 5.5545 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 67: loss = 4.5781 (1.276 sec/step)\n",
            "I1229 10:49:27.569279 140034317584256 learning.py:507] global step 67: loss = 4.5781 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 68: loss = 3.8622 (1.227 sec/step)\n",
            "I1229 10:49:28.798118 140034317584256 learning.py:507] global step 68: loss = 3.8622 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 69: loss = 4.0536 (1.180 sec/step)\n",
            "I1229 10:49:29.979930 140034317584256 learning.py:507] global step 69: loss = 4.0536 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 70: loss = 5.6352 (1.194 sec/step)\n",
            "I1229 10:49:31.176191 140034317584256 learning.py:507] global step 70: loss = 5.6352 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 71: loss = 4.9154 (1.210 sec/step)\n",
            "I1229 10:49:32.387974 140034317584256 learning.py:507] global step 71: loss = 4.9154 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 72: loss = 3.9436 (1.261 sec/step)\n",
            "I1229 10:49:33.650842 140034317584256 learning.py:507] global step 72: loss = 3.9436 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 73: loss = 6.0309 (1.184 sec/step)\n",
            "I1229 10:49:34.836969 140034317584256 learning.py:507] global step 73: loss = 6.0309 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 74: loss = 4.7349 (1.220 sec/step)\n",
            "I1229 10:49:36.058701 140034317584256 learning.py:507] global step 74: loss = 4.7349 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 75: loss = 7.4316 (1.261 sec/step)\n",
            "I1229 10:49:37.321161 140034317584256 learning.py:507] global step 75: loss = 7.4316 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 76: loss = 5.5272 (1.243 sec/step)\n",
            "I1229 10:49:38.565612 140034317584256 learning.py:507] global step 76: loss = 5.5272 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 77: loss = 5.5540 (1.247 sec/step)\n",
            "I1229 10:49:39.814492 140034317584256 learning.py:507] global step 77: loss = 5.5540 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 78: loss = 4.9287 (1.245 sec/step)\n",
            "I1229 10:49:41.061818 140034317584256 learning.py:507] global step 78: loss = 4.9287 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 79: loss = 5.1881 (1.236 sec/step)\n",
            "I1229 10:49:42.299981 140034317584256 learning.py:507] global step 79: loss = 5.1881 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 80: loss = 4.4276 (1.213 sec/step)\n",
            "I1229 10:49:43.516674 140034317584256 learning.py:507] global step 80: loss = 4.4276 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 81: loss = 3.9808 (1.259 sec/step)\n",
            "I1229 10:49:44.777778 140034317584256 learning.py:507] global step 81: loss = 3.9808 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 82: loss = 5.2759 (1.187 sec/step)\n",
            "I1229 10:49:45.966042 140034317584256 learning.py:507] global step 82: loss = 5.2759 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 83: loss = 4.7607 (1.247 sec/step)\n",
            "I1229 10:49:47.215038 140034317584256 learning.py:507] global step 83: loss = 4.7607 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 84: loss = 4.3483 (1.251 sec/step)\n",
            "I1229 10:49:48.467256 140034317584256 learning.py:507] global step 84: loss = 4.3483 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 85: loss = 4.0861 (1.196 sec/step)\n",
            "I1229 10:49:49.665109 140034317584256 learning.py:507] global step 85: loss = 4.0861 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 86: loss = 5.7707 (1.204 sec/step)\n",
            "I1229 10:49:50.871419 140034317584256 learning.py:507] global step 86: loss = 5.7707 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 87: loss = 4.2116 (1.252 sec/step)\n",
            "I1229 10:49:52.125320 140034317584256 learning.py:507] global step 87: loss = 4.2116 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 88: loss = 4.2036 (1.192 sec/step)\n",
            "I1229 10:49:53.319405 140034317584256 learning.py:507] global step 88: loss = 4.2036 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 89: loss = 4.5316 (1.274 sec/step)\n",
            "I1229 10:49:54.595098 140034317584256 learning.py:507] global step 89: loss = 4.5316 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 90: loss = 3.9956 (1.214 sec/step)\n",
            "I1229 10:49:55.810746 140034317584256 learning.py:507] global step 90: loss = 3.9956 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 91: loss = 4.1170 (1.201 sec/step)\n",
            "I1229 10:49:57.013911 140034317584256 learning.py:507] global step 91: loss = 4.1170 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 92: loss = 5.7892 (1.236 sec/step)\n",
            "I1229 10:49:58.252589 140034317584256 learning.py:507] global step 92: loss = 5.7892 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 93: loss = 3.0199 (1.213 sec/step)\n",
            "I1229 10:49:59.467075 140034317584256 learning.py:507] global step 93: loss = 3.0199 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 94: loss = 4.2586 (1.211 sec/step)\n",
            "I1229 10:50:00.679247 140034317584256 learning.py:507] global step 94: loss = 4.2586 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 95: loss = 4.5747 (1.192 sec/step)\n",
            "I1229 10:50:01.873144 140034317584256 learning.py:507] global step 95: loss = 4.5747 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 96: loss = 5.3872 (1.247 sec/step)\n",
            "I1229 10:50:03.121439 140034317584256 learning.py:507] global step 96: loss = 5.3872 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 97: loss = 3.8720 (1.281 sec/step)\n",
            "I1229 10:50:04.405570 140034317584256 learning.py:507] global step 97: loss = 3.8720 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 98: loss = 4.3612 (1.296 sec/step)\n",
            "I1229 10:50:05.703426 140034317584256 learning.py:507] global step 98: loss = 4.3612 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 99: loss = 4.8330 (1.215 sec/step)\n",
            "I1229 10:50:06.920139 140034317584256 learning.py:507] global step 99: loss = 4.8330 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 100: loss = 4.9253 (1.217 sec/step)\n",
            "I1229 10:50:08.138626 140034317584256 learning.py:507] global step 100: loss = 4.9253 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 101: loss = 5.1347 (1.271 sec/step)\n",
            "I1229 10:50:09.411552 140034317584256 learning.py:507] global step 101: loss = 5.1347 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 102: loss = 5.4172 (1.200 sec/step)\n",
            "I1229 10:50:10.613155 140034317584256 learning.py:507] global step 102: loss = 5.4172 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 103: loss = 4.1980 (1.206 sec/step)\n",
            "I1229 10:50:11.821078 140034317584256 learning.py:507] global step 103: loss = 4.1980 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 104: loss = 4.7250 (1.211 sec/step)\n",
            "I1229 10:50:13.033447 140034317584256 learning.py:507] global step 104: loss = 4.7250 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 105: loss = 6.0114 (1.206 sec/step)\n",
            "I1229 10:50:14.241471 140034317584256 learning.py:507] global step 105: loss = 6.0114 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 106: loss = 3.7823 (1.262 sec/step)\n",
            "I1229 10:50:15.505138 140034317584256 learning.py:507] global step 106: loss = 3.7823 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 107: loss = 4.1604 (1.253 sec/step)\n",
            "I1229 10:50:16.761121 140034317584256 learning.py:507] global step 107: loss = 4.1604 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 108: loss = 5.1270 (1.282 sec/step)\n",
            "I1229 10:50:18.045036 140034317584256 learning.py:507] global step 108: loss = 5.1270 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 109: loss = 4.1595 (1.215 sec/step)\n",
            "I1229 10:50:19.261670 140034317584256 learning.py:507] global step 109: loss = 4.1595 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 110: loss = 6.0029 (1.257 sec/step)\n",
            "I1229 10:50:20.520325 140034317584256 learning.py:507] global step 110: loss = 6.0029 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 111: loss = 3.6104 (1.221 sec/step)\n",
            "I1229 10:50:21.742712 140034317584256 learning.py:507] global step 111: loss = 3.6104 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 112: loss = 4.1725 (1.290 sec/step)\n",
            "I1229 10:50:23.034588 140034317584256 learning.py:507] global step 112: loss = 4.1725 (1.290 sec/step)\n",
            "INFO:tensorflow:global step 113: loss = 5.8647 (1.194 sec/step)\n",
            "I1229 10:50:24.230828 140034317584256 learning.py:507] global step 113: loss = 5.8647 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 114: loss = 4.1684 (1.226 sec/step)\n",
            "I1229 10:50:25.458299 140034317584256 learning.py:507] global step 114: loss = 4.1684 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 115: loss = 4.2834 (1.257 sec/step)\n",
            "I1229 10:50:26.716362 140034317584256 learning.py:507] global step 115: loss = 4.2834 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 116: loss = 4.2230 (1.237 sec/step)\n",
            "I1229 10:50:27.955417 140034317584256 learning.py:507] global step 116: loss = 4.2230 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 117: loss = 5.7056 (1.178 sec/step)\n",
            "I1229 10:50:29.134924 140034317584256 learning.py:507] global step 117: loss = 5.7056 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 118: loss = 3.0315 (1.253 sec/step)\n",
            "I1229 10:50:30.390316 140034317584256 learning.py:507] global step 118: loss = 3.0315 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 119: loss = 4.4524 (1.232 sec/step)\n",
            "I1229 10:50:31.623573 140034317584256 learning.py:507] global step 119: loss = 4.4524 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 120: loss = 4.1967 (1.210 sec/step)\n",
            "I1229 10:50:32.835901 140034317584256 learning.py:507] global step 120: loss = 4.1967 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 121: loss = 5.3664 (1.225 sec/step)\n",
            "I1229 10:50:34.062678 140034317584256 learning.py:507] global step 121: loss = 5.3664 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 122: loss = 4.9011 (1.225 sec/step)\n",
            "I1229 10:50:35.289334 140034317584256 learning.py:507] global step 122: loss = 4.9011 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 123: loss = 4.0024 (1.273 sec/step)\n",
            "I1229 10:50:36.564882 140034317584256 learning.py:507] global step 123: loss = 4.0024 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 124: loss = 3.4158 (1.194 sec/step)\n",
            "I1229 10:50:37.760582 140034317584256 learning.py:507] global step 124: loss = 3.4158 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 125: loss = 4.4959 (1.253 sec/step)\n",
            "I1229 10:50:39.015244 140034317584256 learning.py:507] global step 125: loss = 4.4959 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 126: loss = 4.5015 (1.206 sec/step)\n",
            "I1229 10:50:40.223010 140034317584256 learning.py:507] global step 126: loss = 4.5015 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 127: loss = 4.4360 (1.207 sec/step)\n",
            "I1229 10:50:41.431947 140034317584256 learning.py:507] global step 127: loss = 4.4360 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 128: loss = 6.2931 (1.275 sec/step)\n",
            "I1229 10:50:42.709023 140034317584256 learning.py:507] global step 128: loss = 6.2931 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 129: loss = 4.5019 (1.234 sec/step)\n",
            "I1229 10:50:43.945277 140034317584256 learning.py:507] global step 129: loss = 4.5019 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 130: loss = 4.5492 (1.213 sec/step)\n",
            "I1229 10:50:45.160078 140034317584256 learning.py:507] global step 130: loss = 4.5492 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 131: loss = 4.8060 (1.230 sec/step)\n",
            "I1229 10:50:46.392155 140034317584256 learning.py:507] global step 131: loss = 4.8060 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 132: loss = 3.5149 (1.296 sec/step)\n",
            "I1229 10:50:47.692925 140034317584256 learning.py:507] global step 132: loss = 3.5149 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 133: loss = 4.4480 (1.272 sec/step)\n",
            "I1229 10:50:48.968199 140034317584256 learning.py:507] global step 133: loss = 4.4480 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 134: loss = 4.5934 (1.235 sec/step)\n",
            "I1229 10:50:50.204552 140034317584256 learning.py:507] global step 134: loss = 4.5934 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 135: loss = 4.6880 (1.255 sec/step)\n",
            "I1229 10:50:51.461806 140034317584256 learning.py:507] global step 135: loss = 4.6880 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 136: loss = 5.2054 (1.204 sec/step)\n",
            "I1229 10:50:52.668305 140034317584256 learning.py:507] global step 136: loss = 5.2054 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 137: loss = 3.1905 (1.214 sec/step)\n",
            "I1229 10:50:53.883660 140034317584256 learning.py:507] global step 137: loss = 3.1905 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 138: loss = 4.1164 (1.254 sec/step)\n",
            "I1229 10:50:55.139317 140034317584256 learning.py:507] global step 138: loss = 4.1164 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 139: loss = 5.6725 (1.270 sec/step)\n",
            "I1229 10:50:56.410506 140034317584256 learning.py:507] global step 139: loss = 5.6725 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 140: loss = 5.0186 (1.257 sec/step)\n",
            "I1229 10:50:57.669628 140034317584256 learning.py:507] global step 140: loss = 5.0186 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 141: loss = 4.8913 (1.220 sec/step)\n",
            "I1229 10:50:58.891093 140034317584256 learning.py:507] global step 141: loss = 4.8913 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 142: loss = 3.8119 (1.200 sec/step)\n",
            "I1229 10:51:00.093067 140034317584256 learning.py:507] global step 142: loss = 3.8119 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 143: loss = 5.2233 (1.300 sec/step)\n",
            "I1229 10:51:01.394263 140034317584256 learning.py:507] global step 143: loss = 5.2233 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 144: loss = 3.8189 (1.278 sec/step)\n",
            "I1229 10:51:02.674162 140034317584256 learning.py:507] global step 144: loss = 3.8189 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 145: loss = 4.8980 (1.242 sec/step)\n",
            "I1229 10:51:03.917924 140034317584256 learning.py:507] global step 145: loss = 4.8980 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 146: loss = 4.1066 (1.217 sec/step)\n",
            "I1229 10:51:05.136398 140034317584256 learning.py:507] global step 146: loss = 4.1066 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 147: loss = 4.1527 (1.258 sec/step)\n",
            "I1229 10:51:06.395982 140034317584256 learning.py:507] global step 147: loss = 4.1527 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 148: loss = 3.5662 (1.200 sec/step)\n",
            "I1229 10:51:07.598020 140034317584256 learning.py:507] global step 148: loss = 3.5662 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 149: loss = 3.7283 (1.255 sec/step)\n",
            "I1229 10:51:08.854870 140034317584256 learning.py:507] global step 149: loss = 3.7283 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 150: loss = 6.2934 (1.205 sec/step)\n",
            "I1229 10:51:10.061701 140034317584256 learning.py:507] global step 150: loss = 6.2934 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 151: loss = 4.2376 (1.240 sec/step)\n",
            "I1229 10:51:11.303832 140034317584256 learning.py:507] global step 151: loss = 4.2376 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 152: loss = 5.4406 (1.179 sec/step)\n",
            "I1229 10:51:12.485060 140034317584256 learning.py:507] global step 152: loss = 5.4406 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 153: loss = 2.8777 (1.197 sec/step)\n",
            "I1229 10:51:13.683924 140034317584256 learning.py:507] global step 153: loss = 2.8777 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 154: loss = 5.0144 (1.222 sec/step)\n",
            "I1229 10:51:14.908032 140034317584256 learning.py:507] global step 154: loss = 5.0144 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 155: loss = 3.5176 (1.288 sec/step)\n",
            "I1229 10:51:16.197187 140034317584256 learning.py:507] global step 155: loss = 3.5176 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 156: loss = 4.0328 (1.253 sec/step)\n",
            "I1229 10:51:17.451676 140034317584256 learning.py:507] global step 156: loss = 4.0328 (1.253 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 156.\n",
            "I1229 10:51:19.536197 140030580766464 supervisor.py:1050] Recording summary at step 156.\n",
            "INFO:tensorflow:global step 157: loss = 4.5667 (2.184 sec/step)\n",
            "I1229 10:51:19.637655 140034317584256 learning.py:507] global step 157: loss = 4.5667 (2.184 sec/step)\n",
            "INFO:tensorflow:global step 158: loss = 5.1469 (1.260 sec/step)\n",
            "I1229 10:51:20.900404 140034317584256 learning.py:507] global step 158: loss = 5.1469 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 159: loss = 4.2564 (1.297 sec/step)\n",
            "I1229 10:51:22.199621 140034317584256 learning.py:507] global step 159: loss = 4.2564 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 160: loss = 4.5597 (1.305 sec/step)\n",
            "I1229 10:51:23.506193 140034317584256 learning.py:507] global step 160: loss = 4.5597 (1.305 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.800175\n",
            "I1229 10:51:23.920393 140030555588352 supervisor.py:1099] global_step/sec: 0.800175\n",
            "INFO:tensorflow:global step 161: loss = 3.3918 (1.229 sec/step)\n",
            "I1229 10:51:24.736817 140034317584256 learning.py:507] global step 161: loss = 3.3918 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 162: loss = 3.8962 (1.192 sec/step)\n",
            "I1229 10:51:25.930303 140034317584256 learning.py:507] global step 162: loss = 3.8962 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 163: loss = 3.5826 (1.215 sec/step)\n",
            "I1229 10:51:27.146749 140034317584256 learning.py:507] global step 163: loss = 3.5826 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 164: loss = 2.8236 (1.256 sec/step)\n",
            "I1229 10:51:28.403963 140034317584256 learning.py:507] global step 164: loss = 2.8236 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 165: loss = 4.9847 (1.217 sec/step)\n",
            "I1229 10:51:29.622677 140034317584256 learning.py:507] global step 165: loss = 4.9847 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 166: loss = 4.9185 (1.245 sec/step)\n",
            "I1229 10:51:30.868983 140034317584256 learning.py:507] global step 166: loss = 4.9185 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 167: loss = 4.8294 (1.206 sec/step)\n",
            "I1229 10:51:32.076402 140034317584256 learning.py:507] global step 167: loss = 4.8294 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 168: loss = 3.3248 (1.234 sec/step)\n",
            "I1229 10:51:33.312155 140034317584256 learning.py:507] global step 168: loss = 3.3248 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 169: loss = 4.5380 (1.210 sec/step)\n",
            "I1229 10:51:34.523918 140034317584256 learning.py:507] global step 169: loss = 4.5380 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 170: loss = 3.3896 (1.216 sec/step)\n",
            "I1229 10:51:35.742157 140034317584256 learning.py:507] global step 170: loss = 3.3896 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 171: loss = 3.2219 (1.209 sec/step)\n",
            "I1229 10:51:36.953268 140034317584256 learning.py:507] global step 171: loss = 3.2219 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 172: loss = 4.1292 (1.195 sec/step)\n",
            "I1229 10:51:38.150130 140034317584256 learning.py:507] global step 172: loss = 4.1292 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 173: loss = 4.6983 (1.266 sec/step)\n",
            "I1229 10:51:39.418002 140034317584256 learning.py:507] global step 173: loss = 4.6983 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 174: loss = 3.8616 (1.189 sec/step)\n",
            "I1229 10:51:40.608951 140034317584256 learning.py:507] global step 174: loss = 3.8616 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 175: loss = 4.1172 (1.191 sec/step)\n",
            "I1229 10:51:41.801319 140034317584256 learning.py:507] global step 175: loss = 4.1172 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 176: loss = 2.9196 (1.229 sec/step)\n",
            "I1229 10:51:43.031677 140034317584256 learning.py:507] global step 176: loss = 2.9196 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 177: loss = 3.3102 (1.182 sec/step)\n",
            "I1229 10:51:44.215089 140034317584256 learning.py:507] global step 177: loss = 3.3102 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 178: loss = 3.8294 (1.180 sec/step)\n",
            "I1229 10:51:45.396761 140034317584256 learning.py:507] global step 178: loss = 3.8294 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 179: loss = 4.4532 (1.207 sec/step)\n",
            "I1229 10:51:46.606041 140034317584256 learning.py:507] global step 179: loss = 4.4532 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 180: loss = 3.4741 (1.261 sec/step)\n",
            "I1229 10:51:47.869377 140034317584256 learning.py:507] global step 180: loss = 3.4741 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 181: loss = 4.5105 (1.224 sec/step)\n",
            "I1229 10:51:49.095582 140034317584256 learning.py:507] global step 181: loss = 4.5105 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 182: loss = 3.2778 (1.222 sec/step)\n",
            "I1229 10:51:50.319307 140034317584256 learning.py:507] global step 182: loss = 3.2778 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 183: loss = 4.2789 (1.225 sec/step)\n",
            "I1229 10:51:51.545929 140034317584256 learning.py:507] global step 183: loss = 4.2789 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 184: loss = 5.0025 (1.212 sec/step)\n",
            "I1229 10:51:52.759389 140034317584256 learning.py:507] global step 184: loss = 5.0025 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 185: loss = 3.4215 (1.172 sec/step)\n",
            "I1229 10:51:53.933307 140034317584256 learning.py:507] global step 185: loss = 3.4215 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 186: loss = 3.9311 (1.226 sec/step)\n",
            "I1229 10:51:55.160674 140034317584256 learning.py:507] global step 186: loss = 3.9311 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 187: loss = 3.8352 (1.207 sec/step)\n",
            "I1229 10:51:56.369061 140034317584256 learning.py:507] global step 187: loss = 3.8352 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 188: loss = 4.2039 (1.210 sec/step)\n",
            "I1229 10:51:57.580548 140034317584256 learning.py:507] global step 188: loss = 4.2039 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 189: loss = 3.4863 (1.202 sec/step)\n",
            "I1229 10:51:58.783903 140034317584256 learning.py:507] global step 189: loss = 3.4863 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 190: loss = 4.1728 (1.210 sec/step)\n",
            "I1229 10:51:59.995357 140034317584256 learning.py:507] global step 190: loss = 4.1728 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 191: loss = 4.5527 (1.236 sec/step)\n",
            "I1229 10:52:01.233115 140034317584256 learning.py:507] global step 191: loss = 4.5527 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 192: loss = 3.4187 (1.194 sec/step)\n",
            "I1229 10:52:02.428551 140034317584256 learning.py:507] global step 192: loss = 3.4187 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 193: loss = 4.2005 (1.195 sec/step)\n",
            "I1229 10:52:03.625642 140034317584256 learning.py:507] global step 193: loss = 4.2005 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 194: loss = 4.1286 (1.219 sec/step)\n",
            "I1229 10:52:04.846275 140034317584256 learning.py:507] global step 194: loss = 4.1286 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 195: loss = 4.3903 (1.195 sec/step)\n",
            "I1229 10:52:06.043488 140034317584256 learning.py:507] global step 195: loss = 4.3903 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 196: loss = 4.2285 (1.201 sec/step)\n",
            "I1229 10:52:07.246580 140034317584256 learning.py:507] global step 196: loss = 4.2285 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 197: loss = 3.5712 (1.256 sec/step)\n",
            "I1229 10:52:08.504598 140034317584256 learning.py:507] global step 197: loss = 3.5712 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 198: loss = 4.7323 (1.207 sec/step)\n",
            "I1229 10:52:09.713687 140034317584256 learning.py:507] global step 198: loss = 4.7323 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 199: loss = 5.4323 (1.232 sec/step)\n",
            "I1229 10:52:10.946991 140034317584256 learning.py:507] global step 199: loss = 5.4323 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 3.3871 (1.191 sec/step)\n",
            "I1229 10:52:12.139567 140034317584256 learning.py:507] global step 200: loss = 3.3871 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 201: loss = 3.5474 (1.246 sec/step)\n",
            "I1229 10:52:13.387499 140034317584256 learning.py:507] global step 201: loss = 3.5474 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 202: loss = 2.9313 (1.197 sec/step)\n",
            "I1229 10:52:14.586154 140034317584256 learning.py:507] global step 202: loss = 2.9313 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 203: loss = 3.1401 (1.261 sec/step)\n",
            "I1229 10:52:15.849363 140034317584256 learning.py:507] global step 203: loss = 3.1401 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 204: loss = 3.8702 (1.219 sec/step)\n",
            "I1229 10:52:17.069543 140034317584256 learning.py:507] global step 204: loss = 3.8702 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 205: loss = 4.3640 (1.263 sec/step)\n",
            "I1229 10:52:18.333965 140034317584256 learning.py:507] global step 205: loss = 4.3640 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 206: loss = 4.3164 (1.203 sec/step)\n",
            "I1229 10:52:19.538069 140034317584256 learning.py:507] global step 206: loss = 4.3164 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 207: loss = 3.6834 (1.207 sec/step)\n",
            "I1229 10:52:20.746069 140034317584256 learning.py:507] global step 207: loss = 3.6834 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 208: loss = 3.8060 (1.306 sec/step)\n",
            "I1229 10:52:22.053803 140034317584256 learning.py:507] global step 208: loss = 3.8060 (1.306 sec/step)\n",
            "INFO:tensorflow:global step 209: loss = 5.2444 (1.250 sec/step)\n",
            "I1229 10:52:23.305794 140034317584256 learning.py:507] global step 209: loss = 5.2444 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 210: loss = 6.2377 (1.213 sec/step)\n",
            "I1229 10:52:24.520526 140034317584256 learning.py:507] global step 210: loss = 6.2377 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 211: loss = 2.9749 (1.183 sec/step)\n",
            "I1229 10:52:25.705442 140034317584256 learning.py:507] global step 211: loss = 2.9749 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 212: loss = 4.2400 (1.190 sec/step)\n",
            "I1229 10:52:26.897520 140034317584256 learning.py:507] global step 212: loss = 4.2400 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 213: loss = 4.7519 (1.216 sec/step)\n",
            "I1229 10:52:28.114767 140034317584256 learning.py:507] global step 213: loss = 4.7519 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 214: loss = 3.1587 (1.196 sec/step)\n",
            "I1229 10:52:29.312385 140034317584256 learning.py:507] global step 214: loss = 3.1587 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 215: loss = 3.0167 (1.205 sec/step)\n",
            "I1229 10:52:30.519449 140034317584256 learning.py:507] global step 215: loss = 3.0167 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 216: loss = 6.0917 (1.215 sec/step)\n",
            "I1229 10:52:31.736067 140034317584256 learning.py:507] global step 216: loss = 6.0917 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 217: loss = 3.5763 (1.286 sec/step)\n",
            "I1229 10:52:33.023873 140034317584256 learning.py:507] global step 217: loss = 3.5763 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 218: loss = 2.5111 (1.206 sec/step)\n",
            "I1229 10:52:34.232041 140034317584256 learning.py:507] global step 218: loss = 2.5111 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 219: loss = 3.2950 (1.190 sec/step)\n",
            "I1229 10:52:35.423705 140034317584256 learning.py:507] global step 219: loss = 3.2950 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 220: loss = 3.2347 (1.228 sec/step)\n",
            "I1229 10:52:36.653441 140034317584256 learning.py:507] global step 220: loss = 3.2347 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 221: loss = 3.3568 (1.176 sec/step)\n",
            "I1229 10:52:37.831085 140034317584256 learning.py:507] global step 221: loss = 3.3568 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 222: loss = 3.0962 (1.251 sec/step)\n",
            "I1229 10:52:39.083392 140034317584256 learning.py:507] global step 222: loss = 3.0962 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 223: loss = 3.0390 (1.253 sec/step)\n",
            "I1229 10:52:40.337987 140034317584256 learning.py:507] global step 223: loss = 3.0390 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 224: loss = 3.6130 (1.207 sec/step)\n",
            "I1229 10:52:41.546715 140034317584256 learning.py:507] global step 224: loss = 3.6130 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 225: loss = 4.1829 (1.208 sec/step)\n",
            "I1229 10:52:42.757417 140034317584256 learning.py:507] global step 225: loss = 4.1829 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 226: loss = 2.9928 (1.222 sec/step)\n",
            "I1229 10:52:43.981295 140034317584256 learning.py:507] global step 226: loss = 2.9928 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 227: loss = 4.4992 (1.218 sec/step)\n",
            "I1229 10:52:45.200877 140034317584256 learning.py:507] global step 227: loss = 4.4992 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 228: loss = 3.1629 (1.219 sec/step)\n",
            "I1229 10:52:46.421928 140034317584256 learning.py:507] global step 228: loss = 3.1629 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 229: loss = 3.6815 (1.199 sec/step)\n",
            "I1229 10:52:47.623095 140034317584256 learning.py:507] global step 229: loss = 3.6815 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 230: loss = 4.3386 (1.221 sec/step)\n",
            "I1229 10:52:48.846026 140034317584256 learning.py:507] global step 230: loss = 4.3386 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 231: loss = 4.7857 (1.163 sec/step)\n",
            "I1229 10:52:50.010173 140034317584256 learning.py:507] global step 231: loss = 4.7857 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 232: loss = 4.2128 (1.193 sec/step)\n",
            "I1229 10:52:51.205528 140034317584256 learning.py:507] global step 232: loss = 4.2128 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 233: loss = 3.5177 (1.233 sec/step)\n",
            "I1229 10:52:52.440614 140034317584256 learning.py:507] global step 233: loss = 3.5177 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 234: loss = 3.6495 (1.218 sec/step)\n",
            "I1229 10:52:53.659971 140034317584256 learning.py:507] global step 234: loss = 3.6495 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 235: loss = 3.7474 (1.196 sec/step)\n",
            "I1229 10:52:54.857680 140034317584256 learning.py:507] global step 235: loss = 3.7474 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 236: loss = 4.0596 (1.200 sec/step)\n",
            "I1229 10:52:56.059686 140034317584256 learning.py:507] global step 236: loss = 4.0596 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 237: loss = 4.5802 (1.203 sec/step)\n",
            "I1229 10:52:57.264733 140034317584256 learning.py:507] global step 237: loss = 4.5802 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 238: loss = 2.7732 (1.232 sec/step)\n",
            "I1229 10:52:58.498340 140034317584256 learning.py:507] global step 238: loss = 2.7732 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 239: loss = 4.9880 (1.203 sec/step)\n",
            "I1229 10:52:59.703333 140034317584256 learning.py:507] global step 239: loss = 4.9880 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 240: loss = 3.1899 (1.208 sec/step)\n",
            "I1229 10:53:00.913303 140034317584256 learning.py:507] global step 240: loss = 3.1899 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 241: loss = 3.6742 (1.185 sec/step)\n",
            "I1229 10:53:02.099657 140034317584256 learning.py:507] global step 241: loss = 3.6742 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 242: loss = 4.5623 (1.218 sec/step)\n",
            "I1229 10:53:03.319441 140034317584256 learning.py:507] global step 242: loss = 4.5623 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 243: loss = 4.0521 (1.272 sec/step)\n",
            "I1229 10:53:04.592886 140034317584256 learning.py:507] global step 243: loss = 4.0521 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 244: loss = 3.4603 (1.225 sec/step)\n",
            "I1229 10:53:05.820199 140034317584256 learning.py:507] global step 244: loss = 3.4603 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 245: loss = 4.1943 (1.221 sec/step)\n",
            "I1229 10:53:07.043771 140034317584256 learning.py:507] global step 245: loss = 4.1943 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 246: loss = 4.4501 (1.189 sec/step)\n",
            "I1229 10:53:08.234523 140034317584256 learning.py:507] global step 246: loss = 4.4501 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 247: loss = 5.8582 (1.207 sec/step)\n",
            "I1229 10:53:09.443909 140034317584256 learning.py:507] global step 247: loss = 5.8582 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 248: loss = 3.7119 (1.252 sec/step)\n",
            "I1229 10:53:10.697888 140034317584256 learning.py:507] global step 248: loss = 3.7119 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 249: loss = 2.8118 (1.229 sec/step)\n",
            "I1229 10:53:11.928785 140034317584256 learning.py:507] global step 249: loss = 2.8118 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 250: loss = 3.3579 (1.197 sec/step)\n",
            "I1229 10:53:13.127740 140034317584256 learning.py:507] global step 250: loss = 3.3579 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 251: loss = 3.9778 (1.229 sec/step)\n",
            "I1229 10:53:14.358811 140034317584256 learning.py:507] global step 251: loss = 3.9778 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 252: loss = 4.1831 (1.203 sec/step)\n",
            "I1229 10:53:15.563405 140034317584256 learning.py:507] global step 252: loss = 4.1831 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 253: loss = 4.0468 (1.226 sec/step)\n",
            "I1229 10:53:16.790981 140034317584256 learning.py:507] global step 253: loss = 4.0468 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 254: loss = 4.2900 (1.828 sec/step)\n",
            "I1229 10:53:18.622376 140034317584256 learning.py:507] global step 254: loss = 4.2900 (1.828 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 254.\n",
            "I1229 10:53:19.791122 140030580766464 supervisor.py:1050] Recording summary at step 254.\n",
            "INFO:tensorflow:global step 255: loss = 4.7778 (1.462 sec/step)\n",
            "I1229 10:53:20.088020 140034317584256 learning.py:507] global step 255: loss = 4.7778 (1.462 sec/step)\n",
            "INFO:tensorflow:global step 256: loss = 3.2553 (1.187 sec/step)\n",
            "I1229 10:53:21.276830 140034317584256 learning.py:507] global step 256: loss = 3.2553 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 257: loss = 3.4878 (1.225 sec/step)\n",
            "I1229 10:53:22.504107 140034317584256 learning.py:507] global step 257: loss = 3.4878 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 258: loss = 2.6486 (1.229 sec/step)\n",
            "I1229 10:53:23.735127 140034317584256 learning.py:507] global step 258: loss = 2.6486 (1.229 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.815704\n",
            "I1229 10:53:24.062056 140030555588352 supervisor.py:1099] global_step/sec: 0.815704\n",
            "INFO:tensorflow:global step 259: loss = 3.3567 (1.269 sec/step)\n",
            "I1229 10:53:25.005583 140034317584256 learning.py:507] global step 259: loss = 3.3567 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 260: loss = 3.0046 (1.206 sec/step)\n",
            "I1229 10:53:26.213200 140034317584256 learning.py:507] global step 260: loss = 3.0046 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 261: loss = 3.4097 (1.218 sec/step)\n",
            "I1229 10:53:27.433377 140034317584256 learning.py:507] global step 261: loss = 3.4097 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 262: loss = 3.5352 (1.246 sec/step)\n",
            "I1229 10:53:28.681104 140034317584256 learning.py:507] global step 262: loss = 3.5352 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 263: loss = 5.2141 (1.223 sec/step)\n",
            "I1229 10:53:29.905697 140034317584256 learning.py:507] global step 263: loss = 5.2141 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 264: loss = 3.6346 (1.189 sec/step)\n",
            "I1229 10:53:31.096118 140034317584256 learning.py:507] global step 264: loss = 3.6346 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 265: loss = 3.6210 (1.173 sec/step)\n",
            "I1229 10:53:32.271005 140034317584256 learning.py:507] global step 265: loss = 3.6210 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 266: loss = 3.8172 (1.192 sec/step)\n",
            "I1229 10:53:33.465016 140034317584256 learning.py:507] global step 266: loss = 3.8172 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 267: loss = 3.2183 (1.206 sec/step)\n",
            "I1229 10:53:34.673281 140034317584256 learning.py:507] global step 267: loss = 3.2183 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 268: loss = 4.0130 (1.211 sec/step)\n",
            "I1229 10:53:35.885562 140034317584256 learning.py:507] global step 268: loss = 4.0130 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 269: loss = 2.7203 (1.192 sec/step)\n",
            "I1229 10:53:37.079633 140034317584256 learning.py:507] global step 269: loss = 2.7203 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 270: loss = 2.7146 (1.248 sec/step)\n",
            "I1229 10:53:38.329605 140034317584256 learning.py:507] global step 270: loss = 2.7146 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 271: loss = 3.9627 (1.228 sec/step)\n",
            "I1229 10:53:39.558942 140034317584256 learning.py:507] global step 271: loss = 3.9627 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 272: loss = 4.2550 (1.183 sec/step)\n",
            "I1229 10:53:40.744248 140034317584256 learning.py:507] global step 272: loss = 4.2550 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 273: loss = 3.3592 (1.181 sec/step)\n",
            "I1229 10:53:41.926788 140034317584256 learning.py:507] global step 273: loss = 3.3592 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 274: loss = 3.3696 (1.158 sec/step)\n",
            "I1229 10:53:43.086656 140034317584256 learning.py:507] global step 274: loss = 3.3696 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 275: loss = 3.6338 (1.188 sec/step)\n",
            "I1229 10:53:44.276809 140034317584256 learning.py:507] global step 275: loss = 3.6338 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 276: loss = 3.6061 (1.180 sec/step)\n",
            "I1229 10:53:45.458397 140034317584256 learning.py:507] global step 276: loss = 3.6061 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 277: loss = 2.3929 (1.191 sec/step)\n",
            "I1229 10:53:46.651128 140034317584256 learning.py:507] global step 277: loss = 2.3929 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 278: loss = 4.2188 (1.213 sec/step)\n",
            "I1229 10:53:47.865476 140034317584256 learning.py:507] global step 278: loss = 4.2188 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 279: loss = 3.0787 (1.181 sec/step)\n",
            "I1229 10:53:49.048563 140034317584256 learning.py:507] global step 279: loss = 3.0787 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 280: loss = 3.6714 (1.195 sec/step)\n",
            "I1229 10:53:50.244916 140034317584256 learning.py:507] global step 280: loss = 3.6714 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 281: loss = 3.0339 (1.172 sec/step)\n",
            "I1229 10:53:51.419093 140034317584256 learning.py:507] global step 281: loss = 3.0339 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 282: loss = 3.9865 (1.183 sec/step)\n",
            "I1229 10:53:52.603682 140034317584256 learning.py:507] global step 282: loss = 3.9865 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 283: loss = 3.3418 (1.192 sec/step)\n",
            "I1229 10:53:53.797097 140034317584256 learning.py:507] global step 283: loss = 3.3418 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 284: loss = 3.2261 (1.250 sec/step)\n",
            "I1229 10:53:55.049002 140034317584256 learning.py:507] global step 284: loss = 3.2261 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 285: loss = 2.8047 (1.291 sec/step)\n",
            "I1229 10:53:56.341868 140034317584256 learning.py:507] global step 285: loss = 2.8047 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 286: loss = 5.2064 (1.210 sec/step)\n",
            "I1229 10:53:57.553374 140034317584256 learning.py:507] global step 286: loss = 5.2064 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 287: loss = 3.5960 (1.164 sec/step)\n",
            "I1229 10:53:58.719024 140034317584256 learning.py:507] global step 287: loss = 3.5960 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 288: loss = 3.6211 (1.173 sec/step)\n",
            "I1229 10:53:59.894047 140034317584256 learning.py:507] global step 288: loss = 3.6211 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 289: loss = 5.1149 (1.158 sec/step)\n",
            "I1229 10:54:01.053926 140034317584256 learning.py:507] global step 289: loss = 5.1149 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 290: loss = 5.0317 (1.218 sec/step)\n",
            "I1229 10:54:02.273597 140034317584256 learning.py:507] global step 290: loss = 5.0317 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 291: loss = 3.0374 (1.182 sec/step)\n",
            "I1229 10:54:03.457937 140034317584256 learning.py:507] global step 291: loss = 3.0374 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 292: loss = 3.0161 (1.241 sec/step)\n",
            "I1229 10:54:04.701088 140034317584256 learning.py:507] global step 292: loss = 3.0161 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 293: loss = 2.8885 (1.181 sec/step)\n",
            "I1229 10:54:05.883745 140034317584256 learning.py:507] global step 293: loss = 2.8885 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 294: loss = 3.8426 (1.191 sec/step)\n",
            "I1229 10:54:07.076642 140034317584256 learning.py:507] global step 294: loss = 3.8426 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 295: loss = 3.9863 (1.172 sec/step)\n",
            "I1229 10:54:08.250625 140034317584256 learning.py:507] global step 295: loss = 3.9863 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 296: loss = 3.7757 (1.230 sec/step)\n",
            "I1229 10:54:09.482276 140034317584256 learning.py:507] global step 296: loss = 3.7757 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 297: loss = 4.6070 (1.217 sec/step)\n",
            "I1229 10:54:10.700586 140034317584256 learning.py:507] global step 297: loss = 4.6070 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 298: loss = 3.6863 (1.228 sec/step)\n",
            "I1229 10:54:11.929978 140034317584256 learning.py:507] global step 298: loss = 3.6863 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 299: loss = 3.3389 (1.201 sec/step)\n",
            "I1229 10:54:13.133338 140034317584256 learning.py:507] global step 299: loss = 3.3389 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 3.8624 (1.194 sec/step)\n",
            "I1229 10:54:14.329679 140034317584256 learning.py:507] global step 300: loss = 3.8624 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 301: loss = 2.7396 (1.204 sec/step)\n",
            "I1229 10:54:15.535877 140034317584256 learning.py:507] global step 301: loss = 2.7396 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 302: loss = 3.4131 (1.214 sec/step)\n",
            "I1229 10:54:16.752386 140034317584256 learning.py:507] global step 302: loss = 3.4131 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 303: loss = 3.0261 (1.250 sec/step)\n",
            "I1229 10:54:18.004476 140034317584256 learning.py:507] global step 303: loss = 3.0261 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 304: loss = 3.5247 (1.155 sec/step)\n",
            "I1229 10:54:19.161579 140034317584256 learning.py:507] global step 304: loss = 3.5247 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 305: loss = 4.6543 (1.245 sec/step)\n",
            "I1229 10:54:20.408125 140034317584256 learning.py:507] global step 305: loss = 4.6543 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 306: loss = 3.6457 (1.239 sec/step)\n",
            "I1229 10:54:21.648514 140034317584256 learning.py:507] global step 306: loss = 3.6457 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 307: loss = 3.2026 (1.248 sec/step)\n",
            "I1229 10:54:22.898176 140034317584256 learning.py:507] global step 307: loss = 3.2026 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 308: loss = 4.3257 (1.263 sec/step)\n",
            "I1229 10:54:24.162586 140034317584256 learning.py:507] global step 308: loss = 4.3257 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 309: loss = 4.0714 (1.203 sec/step)\n",
            "I1229 10:54:25.367202 140034317584256 learning.py:507] global step 309: loss = 4.0714 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 310: loss = 2.7397 (1.207 sec/step)\n",
            "I1229 10:54:26.575530 140034317584256 learning.py:507] global step 310: loss = 2.7397 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 311: loss = 2.8617 (1.286 sec/step)\n",
            "I1229 10:54:27.863281 140034317584256 learning.py:507] global step 311: loss = 2.8617 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 312: loss = 5.5241 (1.225 sec/step)\n",
            "I1229 10:54:29.089998 140034317584256 learning.py:507] global step 312: loss = 5.5241 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 313: loss = 4.4794 (1.183 sec/step)\n",
            "I1229 10:54:30.274101 140034317584256 learning.py:507] global step 313: loss = 4.4794 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 314: loss = 3.5628 (1.208 sec/step)\n",
            "I1229 10:54:31.483729 140034317584256 learning.py:507] global step 314: loss = 3.5628 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 315: loss = 2.8265 (1.219 sec/step)\n",
            "I1229 10:54:32.704676 140034317584256 learning.py:507] global step 315: loss = 2.8265 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 316: loss = 3.9867 (1.198 sec/step)\n",
            "I1229 10:54:33.904265 140034317584256 learning.py:507] global step 316: loss = 3.9867 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 317: loss = 3.6556 (1.183 sec/step)\n",
            "I1229 10:54:35.089126 140034317584256 learning.py:507] global step 317: loss = 3.6556 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 318: loss = 3.5821 (1.186 sec/step)\n",
            "I1229 10:54:36.277145 140034317584256 learning.py:507] global step 318: loss = 3.5821 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 319: loss = 2.8405 (1.201 sec/step)\n",
            "I1229 10:54:37.479582 140034317584256 learning.py:507] global step 319: loss = 2.8405 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 320: loss = 4.4137 (1.177 sec/step)\n",
            "I1229 10:54:38.657547 140034317584256 learning.py:507] global step 320: loss = 4.4137 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 321: loss = 4.3720 (1.179 sec/step)\n",
            "I1229 10:54:39.838328 140034317584256 learning.py:507] global step 321: loss = 4.3720 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 322: loss = 3.4503 (1.223 sec/step)\n",
            "I1229 10:54:41.063258 140034317584256 learning.py:507] global step 322: loss = 3.4503 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 323: loss = 6.8173 (1.181 sec/step)\n",
            "I1229 10:54:42.245755 140034317584256 learning.py:507] global step 323: loss = 6.8173 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 324: loss = 2.9380 (1.177 sec/step)\n",
            "I1229 10:54:43.424650 140034317584256 learning.py:507] global step 324: loss = 2.9380 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 325: loss = 4.0767 (1.168 sec/step)\n",
            "I1229 10:54:44.595339 140034317584256 learning.py:507] global step 325: loss = 4.0767 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 326: loss = 3.2920 (1.170 sec/step)\n",
            "I1229 10:54:45.767342 140034317584256 learning.py:507] global step 326: loss = 3.2920 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 327: loss = 2.7232 (1.187 sec/step)\n",
            "I1229 10:54:46.956141 140034317584256 learning.py:507] global step 327: loss = 2.7232 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 328: loss = 2.9148 (1.286 sec/step)\n",
            "I1229 10:54:48.243550 140034317584256 learning.py:507] global step 328: loss = 2.9148 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 329: loss = 3.5588 (1.184 sec/step)\n",
            "I1229 10:54:49.429492 140034317584256 learning.py:507] global step 329: loss = 3.5588 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 330: loss = 3.5191 (1.130 sec/step)\n",
            "I1229 10:54:50.560726 140034317584256 learning.py:507] global step 330: loss = 3.5191 (1.130 sec/step)\n",
            "INFO:tensorflow:global step 331: loss = 4.0415 (1.209 sec/step)\n",
            "I1229 10:54:51.771097 140034317584256 learning.py:507] global step 331: loss = 4.0415 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 332: loss = 4.1636 (1.203 sec/step)\n",
            "I1229 10:54:52.976450 140034317584256 learning.py:507] global step 332: loss = 4.1636 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 333: loss = 3.4556 (1.236 sec/step)\n",
            "I1229 10:54:54.214443 140034317584256 learning.py:507] global step 333: loss = 3.4556 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 334: loss = 3.6033 (1.201 sec/step)\n",
            "I1229 10:54:55.417263 140034317584256 learning.py:507] global step 334: loss = 3.6033 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 335: loss = 2.9405 (1.253 sec/step)\n",
            "I1229 10:54:56.671803 140034317584256 learning.py:507] global step 335: loss = 2.9405 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 336: loss = 3.7867 (1.174 sec/step)\n",
            "I1229 10:54:57.847557 140034317584256 learning.py:507] global step 336: loss = 3.7867 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 337: loss = 2.5493 (1.178 sec/step)\n",
            "I1229 10:54:59.027120 140034317584256 learning.py:507] global step 337: loss = 2.5493 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 338: loss = 4.2610 (1.201 sec/step)\n",
            "I1229 10:55:00.230571 140034317584256 learning.py:507] global step 338: loss = 4.2610 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 339: loss = 3.8663 (1.185 sec/step)\n",
            "I1229 10:55:01.417657 140034317584256 learning.py:507] global step 339: loss = 3.8663 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 340: loss = 4.0134 (1.190 sec/step)\n",
            "I1229 10:55:02.609194 140034317584256 learning.py:507] global step 340: loss = 4.0134 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 341: loss = 3.1038 (1.203 sec/step)\n",
            "I1229 10:55:03.813667 140034317584256 learning.py:507] global step 341: loss = 3.1038 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 342: loss = 3.7647 (1.175 sec/step)\n",
            "I1229 10:55:04.990548 140034317584256 learning.py:507] global step 342: loss = 3.7647 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 343: loss = 3.2544 (1.217 sec/step)\n",
            "I1229 10:55:06.209085 140034317584256 learning.py:507] global step 343: loss = 3.2544 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 344: loss = 2.7771 (1.212 sec/step)\n",
            "I1229 10:55:07.422692 140034317584256 learning.py:507] global step 344: loss = 2.7771 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 345: loss = 4.3003 (1.199 sec/step)\n",
            "I1229 10:55:08.623585 140034317584256 learning.py:507] global step 345: loss = 4.3003 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 346: loss = 3.5229 (1.287 sec/step)\n",
            "I1229 10:55:09.912138 140034317584256 learning.py:507] global step 346: loss = 3.5229 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 347: loss = 3.8984 (1.222 sec/step)\n",
            "I1229 10:55:11.136044 140034317584256 learning.py:507] global step 347: loss = 3.8984 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 348: loss = 4.2325 (1.229 sec/step)\n",
            "I1229 10:55:12.366926 140034317584256 learning.py:507] global step 348: loss = 4.2325 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 349: loss = 3.5081 (1.208 sec/step)\n",
            "I1229 10:55:13.576813 140034317584256 learning.py:507] global step 349: loss = 3.5081 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 350: loss = 3.4823 (1.155 sec/step)\n",
            "I1229 10:55:14.733456 140034317584256 learning.py:507] global step 350: loss = 3.4823 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 351: loss = 4.2679 (1.162 sec/step)\n",
            "I1229 10:55:15.897724 140034317584256 learning.py:507] global step 351: loss = 4.2679 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 352: loss = 3.0741 (1.176 sec/step)\n",
            "I1229 10:55:17.075080 140034317584256 learning.py:507] global step 352: loss = 3.0741 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 353: loss = 3.0569 (2.034 sec/step)\n",
            "I1229 10:55:19.110841 140034317584256 learning.py:507] global step 353: loss = 3.0569 (2.034 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 353.\n",
            "I1229 10:55:19.737049 140030580766464 supervisor.py:1050] Recording summary at step 353.\n",
            "INFO:tensorflow:global step 354: loss = 3.2193 (1.293 sec/step)\n",
            "I1229 10:55:20.405647 140034317584256 learning.py:507] global step 354: loss = 3.2193 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 355: loss = 4.2656 (1.225 sec/step)\n",
            "I1229 10:55:21.632343 140034317584256 learning.py:507] global step 355: loss = 4.2656 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 356: loss = 3.2226 (1.226 sec/step)\n",
            "I1229 10:55:22.859363 140034317584256 learning.py:507] global step 356: loss = 3.2226 (1.226 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.817633\n",
            "I1229 10:55:23.920135 140030555588352 supervisor.py:1099] global_step/sec: 0.817633\n",
            "INFO:tensorflow:global step 357: loss = 2.5872 (1.238 sec/step)\n",
            "I1229 10:55:24.098512 140034317584256 learning.py:507] global step 357: loss = 2.5872 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 358: loss = 3.3390 (1.202 sec/step)\n",
            "I1229 10:55:25.302584 140034317584256 learning.py:507] global step 358: loss = 3.3390 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 359: loss = 3.9054 (1.138 sec/step)\n",
            "I1229 10:55:26.442336 140034317584256 learning.py:507] global step 359: loss = 3.9054 (1.138 sec/step)\n",
            "INFO:tensorflow:global step 360: loss = 3.3860 (1.201 sec/step)\n",
            "I1229 10:55:27.644816 140034317584256 learning.py:507] global step 360: loss = 3.3860 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 361: loss = 4.5443 (1.208 sec/step)\n",
            "I1229 10:55:28.854369 140034317584256 learning.py:507] global step 361: loss = 4.5443 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 362: loss = 3.6006 (1.210 sec/step)\n",
            "I1229 10:55:30.066451 140034317584256 learning.py:507] global step 362: loss = 3.6006 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 363: loss = 3.9262 (1.220 sec/step)\n",
            "I1229 10:55:31.288114 140034317584256 learning.py:507] global step 363: loss = 3.9262 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 364: loss = 2.8046 (1.217 sec/step)\n",
            "I1229 10:55:32.506675 140034317584256 learning.py:507] global step 364: loss = 2.8046 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 365: loss = 6.2873 (1.205 sec/step)\n",
            "I1229 10:55:33.712955 140034317584256 learning.py:507] global step 365: loss = 6.2873 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 366: loss = 3.7154 (1.213 sec/step)\n",
            "I1229 10:55:34.927857 140034317584256 learning.py:507] global step 366: loss = 3.7154 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 367: loss = 3.8670 (1.234 sec/step)\n",
            "I1229 10:55:36.163772 140034317584256 learning.py:507] global step 367: loss = 3.8670 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 368: loss = 4.1482 (1.197 sec/step)\n",
            "I1229 10:55:37.362771 140034317584256 learning.py:507] global step 368: loss = 4.1482 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 369: loss = 4.1923 (1.195 sec/step)\n",
            "I1229 10:55:38.558875 140034317584256 learning.py:507] global step 369: loss = 4.1923 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 370: loss = 2.5352 (1.213 sec/step)\n",
            "I1229 10:55:39.773896 140034317584256 learning.py:507] global step 370: loss = 2.5352 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 371: loss = 3.4423 (1.235 sec/step)\n",
            "I1229 10:55:41.011115 140034317584256 learning.py:507] global step 371: loss = 3.4423 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 372: loss = 3.9799 (1.187 sec/step)\n",
            "I1229 10:55:42.199721 140034317584256 learning.py:507] global step 372: loss = 3.9799 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 373: loss = 3.0849 (1.209 sec/step)\n",
            "I1229 10:55:43.410437 140034317584256 learning.py:507] global step 373: loss = 3.0849 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 374: loss = 2.8508 (1.165 sec/step)\n",
            "I1229 10:55:44.577087 140034317584256 learning.py:507] global step 374: loss = 2.8508 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 375: loss = 3.9949 (1.187 sec/step)\n",
            "I1229 10:55:45.765484 140034317584256 learning.py:507] global step 375: loss = 3.9949 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 376: loss = 3.0738 (1.236 sec/step)\n",
            "I1229 10:55:47.003423 140034317584256 learning.py:507] global step 376: loss = 3.0738 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 377: loss = 4.0303 (1.242 sec/step)\n",
            "I1229 10:55:48.246872 140034317584256 learning.py:507] global step 377: loss = 4.0303 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 378: loss = 3.3722 (1.178 sec/step)\n",
            "I1229 10:55:49.426254 140034317584256 learning.py:507] global step 378: loss = 3.3722 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 379: loss = 3.3882 (1.154 sec/step)\n",
            "I1229 10:55:50.582088 140034317584256 learning.py:507] global step 379: loss = 3.3882 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 380: loss = 3.8445 (1.204 sec/step)\n",
            "I1229 10:55:51.787556 140034317584256 learning.py:507] global step 380: loss = 3.8445 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 381: loss = 3.6455 (1.206 sec/step)\n",
            "I1229 10:55:52.995139 140034317584256 learning.py:507] global step 381: loss = 3.6455 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 382: loss = 3.6471 (1.211 sec/step)\n",
            "I1229 10:55:54.208271 140034317584256 learning.py:507] global step 382: loss = 3.6471 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 383: loss = 4.1152 (1.193 sec/step)\n",
            "I1229 10:55:55.403285 140034317584256 learning.py:507] global step 383: loss = 4.1152 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 384: loss = 3.0308 (1.246 sec/step)\n",
            "I1229 10:55:56.651635 140034317584256 learning.py:507] global step 384: loss = 3.0308 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 385: loss = 3.6210 (1.235 sec/step)\n",
            "I1229 10:55:57.889274 140034317584256 learning.py:507] global step 385: loss = 3.6210 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 386: loss = 3.0020 (1.215 sec/step)\n",
            "I1229 10:55:59.105911 140034317584256 learning.py:507] global step 386: loss = 3.0020 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 387: loss = 5.8780 (1.187 sec/step)\n",
            "I1229 10:56:00.294393 140034317584256 learning.py:507] global step 387: loss = 5.8780 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 388: loss = 3.4708 (1.262 sec/step)\n",
            "I1229 10:56:01.558533 140034317584256 learning.py:507] global step 388: loss = 3.4708 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 389: loss = 5.1792 (1.242 sec/step)\n",
            "I1229 10:56:02.802124 140034317584256 learning.py:507] global step 389: loss = 5.1792 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 390: loss = 1.9986 (1.227 sec/step)\n",
            "I1229 10:56:04.030852 140034317584256 learning.py:507] global step 390: loss = 1.9986 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 391: loss = 2.9726 (1.206 sec/step)\n",
            "I1229 10:56:05.238354 140034317584256 learning.py:507] global step 391: loss = 2.9726 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 392: loss = 3.0361 (1.195 sec/step)\n",
            "I1229 10:56:06.435440 140034317584256 learning.py:507] global step 392: loss = 3.0361 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 393: loss = 3.4587 (1.221 sec/step)\n",
            "I1229 10:56:07.657909 140034317584256 learning.py:507] global step 393: loss = 3.4587 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 394: loss = 3.1019 (1.180 sec/step)\n",
            "I1229 10:56:08.840226 140034317584256 learning.py:507] global step 394: loss = 3.1019 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 395: loss = 3.7408 (1.222 sec/step)\n",
            "I1229 10:56:10.063344 140034317584256 learning.py:507] global step 395: loss = 3.7408 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 396: loss = 3.5090 (1.172 sec/step)\n",
            "I1229 10:56:11.236902 140034317584256 learning.py:507] global step 396: loss = 3.5090 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 397: loss = 3.1735 (1.228 sec/step)\n",
            "I1229 10:56:12.466096 140034317584256 learning.py:507] global step 397: loss = 3.1735 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 398: loss = 4.6609 (1.210 sec/step)\n",
            "I1229 10:56:13.677817 140034317584256 learning.py:507] global step 398: loss = 4.6609 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 399: loss = 2.4212 (1.172 sec/step)\n",
            "I1229 10:56:14.851406 140034317584256 learning.py:507] global step 399: loss = 2.4212 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 3.1968 (1.215 sec/step)\n",
            "I1229 10:56:16.067723 140034317584256 learning.py:507] global step 400: loss = 3.1968 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 401: loss = 3.2219 (1.148 sec/step)\n",
            "I1229 10:56:17.218234 140034317584256 learning.py:507] global step 401: loss = 3.2219 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 402: loss = 3.4489 (1.226 sec/step)\n",
            "I1229 10:56:18.446394 140034317584256 learning.py:507] global step 402: loss = 3.4489 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 403: loss = 3.8300 (1.194 sec/step)\n",
            "I1229 10:56:19.641541 140034317584256 learning.py:507] global step 403: loss = 3.8300 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 404: loss = 3.0999 (1.177 sec/step)\n",
            "I1229 10:56:20.820401 140034317584256 learning.py:507] global step 404: loss = 3.0999 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 405: loss = 3.9286 (1.204 sec/step)\n",
            "I1229 10:56:22.026121 140034317584256 learning.py:507] global step 405: loss = 3.9286 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 406: loss = 3.6952 (1.259 sec/step)\n",
            "I1229 10:56:23.286464 140034317584256 learning.py:507] global step 406: loss = 3.6952 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 407: loss = 3.8504 (1.211 sec/step)\n",
            "I1229 10:56:24.499437 140034317584256 learning.py:507] global step 407: loss = 3.8504 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 408: loss = 2.9569 (1.232 sec/step)\n",
            "I1229 10:56:25.732843 140034317584256 learning.py:507] global step 408: loss = 2.9569 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 409: loss = 3.1999 (1.200 sec/step)\n",
            "I1229 10:56:26.934725 140034317584256 learning.py:507] global step 409: loss = 3.1999 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 410: loss = 1.9027 (1.180 sec/step)\n",
            "I1229 10:56:28.116360 140034317584256 learning.py:507] global step 410: loss = 1.9027 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 411: loss = 2.9956 (1.161 sec/step)\n",
            "I1229 10:56:29.278521 140034317584256 learning.py:507] global step 411: loss = 2.9956 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 412: loss = 3.5699 (1.158 sec/step)\n",
            "I1229 10:56:30.437807 140034317584256 learning.py:507] global step 412: loss = 3.5699 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 413: loss = 3.5621 (1.195 sec/step)\n",
            "I1229 10:56:31.634447 140034317584256 learning.py:507] global step 413: loss = 3.5621 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 414: loss = 3.1406 (1.192 sec/step)\n",
            "I1229 10:56:32.828609 140034317584256 learning.py:507] global step 414: loss = 3.1406 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 415: loss = 2.9237 (1.221 sec/step)\n",
            "I1229 10:56:34.051663 140034317584256 learning.py:507] global step 415: loss = 2.9237 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 416: loss = 2.4902 (1.181 sec/step)\n",
            "I1229 10:56:35.234125 140034317584256 learning.py:507] global step 416: loss = 2.4902 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 417: loss = 3.3173 (1.178 sec/step)\n",
            "I1229 10:56:36.414154 140034317584256 learning.py:507] global step 417: loss = 3.3173 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 418: loss = 2.9286 (1.162 sec/step)\n",
            "I1229 10:56:37.578462 140034317584256 learning.py:507] global step 418: loss = 2.9286 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 419: loss = 3.9104 (1.170 sec/step)\n",
            "I1229 10:56:38.749860 140034317584256 learning.py:507] global step 419: loss = 3.9104 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 420: loss = 3.8558 (1.158 sec/step)\n",
            "I1229 10:56:39.909280 140034317584256 learning.py:507] global step 420: loss = 3.8558 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 421: loss = 2.8532 (1.184 sec/step)\n",
            "I1229 10:56:41.094516 140034317584256 learning.py:507] global step 421: loss = 2.8532 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 422: loss = 4.8043 (1.192 sec/step)\n",
            "I1229 10:56:42.288076 140034317584256 learning.py:507] global step 422: loss = 4.8043 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 423: loss = 3.9802 (1.203 sec/step)\n",
            "I1229 10:56:43.492281 140034317584256 learning.py:507] global step 423: loss = 3.9802 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 424: loss = 4.5080 (1.172 sec/step)\n",
            "I1229 10:56:44.665779 140034317584256 learning.py:507] global step 424: loss = 4.5080 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 425: loss = 2.5693 (1.142 sec/step)\n",
            "I1229 10:56:45.809584 140034317584256 learning.py:507] global step 425: loss = 2.5693 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 426: loss = 3.0324 (1.119 sec/step)\n",
            "I1229 10:56:46.930269 140034317584256 learning.py:507] global step 426: loss = 3.0324 (1.119 sec/step)\n",
            "INFO:tensorflow:global step 427: loss = 3.0976 (1.186 sec/step)\n",
            "I1229 10:56:48.118009 140034317584256 learning.py:507] global step 427: loss = 3.0976 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 428: loss = 2.7261 (1.184 sec/step)\n",
            "I1229 10:56:49.303843 140034317584256 learning.py:507] global step 428: loss = 2.7261 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 429: loss = 3.3134 (1.167 sec/step)\n",
            "I1229 10:56:50.472475 140034317584256 learning.py:507] global step 429: loss = 3.3134 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 430: loss = 2.5781 (1.238 sec/step)\n",
            "I1229 10:56:51.712144 140034317584256 learning.py:507] global step 430: loss = 2.5781 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 431: loss = 3.9496 (1.214 sec/step)\n",
            "I1229 10:56:52.928071 140034317584256 learning.py:507] global step 431: loss = 3.9496 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 432: loss = 3.2866 (1.158 sec/step)\n",
            "I1229 10:56:54.087353 140034317584256 learning.py:507] global step 432: loss = 3.2866 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 433: loss = 3.6879 (1.205 sec/step)\n",
            "I1229 10:56:55.293972 140034317584256 learning.py:507] global step 433: loss = 3.6879 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 434: loss = 2.6318 (1.157 sec/step)\n",
            "I1229 10:56:56.452686 140034317584256 learning.py:507] global step 434: loss = 2.6318 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 435: loss = 3.7674 (1.204 sec/step)\n",
            "I1229 10:56:57.658088 140034317584256 learning.py:507] global step 435: loss = 3.7674 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 436: loss = 2.2881 (1.183 sec/step)\n",
            "I1229 10:56:58.842630 140034317584256 learning.py:507] global step 436: loss = 2.2881 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 437: loss = 3.1692 (1.162 sec/step)\n",
            "I1229 10:57:00.006244 140034317584256 learning.py:507] global step 437: loss = 3.1692 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 438: loss = 3.2461 (1.135 sec/step)\n",
            "I1229 10:57:01.142783 140034317584256 learning.py:507] global step 438: loss = 3.2461 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 439: loss = 4.1982 (1.207 sec/step)\n",
            "I1229 10:57:02.351367 140034317584256 learning.py:507] global step 439: loss = 4.1982 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 440: loss = 3.0820 (1.171 sec/step)\n",
            "I1229 10:57:03.524274 140034317584256 learning.py:507] global step 440: loss = 3.0820 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 441: loss = 3.0643 (1.194 sec/step)\n",
            "I1229 10:57:04.719539 140034317584256 learning.py:507] global step 441: loss = 3.0643 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 442: loss = 4.3202 (1.188 sec/step)\n",
            "I1229 10:57:05.909459 140034317584256 learning.py:507] global step 442: loss = 4.3202 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 443: loss = 3.6082 (1.182 sec/step)\n",
            "I1229 10:57:07.094024 140034317584256 learning.py:507] global step 443: loss = 3.6082 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 444: loss = 3.5128 (1.193 sec/step)\n",
            "I1229 10:57:08.288951 140034317584256 learning.py:507] global step 444: loss = 3.5128 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 445: loss = 2.8902 (1.214 sec/step)\n",
            "I1229 10:57:09.504659 140034317584256 learning.py:507] global step 445: loss = 2.8902 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 446: loss = 2.7563 (1.203 sec/step)\n",
            "I1229 10:57:10.709362 140034317584256 learning.py:507] global step 446: loss = 2.7563 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 447: loss = 4.4296 (1.233 sec/step)\n",
            "I1229 10:57:11.943907 140034317584256 learning.py:507] global step 447: loss = 4.4296 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 448: loss = 3.4766 (1.227 sec/step)\n",
            "I1229 10:57:13.172974 140034317584256 learning.py:507] global step 448: loss = 3.4766 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 449: loss = 3.5122 (1.189 sec/step)\n",
            "I1229 10:57:14.363443 140034317584256 learning.py:507] global step 449: loss = 3.5122 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 450: loss = 2.6834 (1.188 sec/step)\n",
            "I1229 10:57:15.552909 140034317584256 learning.py:507] global step 450: loss = 2.6834 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 451: loss = 3.5301 (1.177 sec/step)\n",
            "I1229 10:57:16.732059 140034317584256 learning.py:507] global step 451: loss = 3.5301 (1.177 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 10:57:17.668092 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 452: loss = 3.0136 (2.404 sec/step)\n",
            "I1229 10:57:19.142869 140034317584256 learning.py:507] global step 452: loss = 3.0136 (2.404 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 452.\n",
            "I1229 10:57:20.336291 140030580766464 supervisor.py:1050] Recording summary at step 452.\n",
            "INFO:tensorflow:global step 453: loss = 4.3916 (1.836 sec/step)\n",
            "I1229 10:57:20.991478 140034317584256 learning.py:507] global step 453: loss = 4.3916 (1.836 sec/step)\n",
            "INFO:tensorflow:global step 454: loss = 3.8664 (1.687 sec/step)\n",
            "I1229 10:57:22.691040 140034317584256 learning.py:507] global step 454: loss = 3.8664 (1.687 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.82097\n",
            "I1229 10:57:24.509090 140030555588352 supervisor.py:1099] global_step/sec: 0.82097\n",
            "INFO:tensorflow:global step 455: loss = 3.6005 (1.805 sec/step)\n",
            "I1229 10:57:24.515685 140034317584256 learning.py:507] global step 455: loss = 3.6005 (1.805 sec/step)\n",
            "INFO:tensorflow:global step 456: loss = 3.7641 (1.597 sec/step)\n",
            "I1229 10:57:26.114509 140034317584256 learning.py:507] global step 456: loss = 3.7641 (1.597 sec/step)\n",
            "INFO:tensorflow:global step 457: loss = 3.6598 (1.281 sec/step)\n",
            "I1229 10:57:27.397099 140034317584256 learning.py:507] global step 457: loss = 3.6598 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 458: loss = 2.8345 (1.166 sec/step)\n",
            "I1229 10:57:28.565279 140034317584256 learning.py:507] global step 458: loss = 2.8345 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 459: loss = 2.3826 (1.220 sec/step)\n",
            "I1229 10:57:29.787816 140034317584256 learning.py:507] global step 459: loss = 2.3826 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 460: loss = 4.0748 (1.197 sec/step)\n",
            "I1229 10:57:30.986796 140034317584256 learning.py:507] global step 460: loss = 4.0748 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 461: loss = 1.9382 (1.176 sec/step)\n",
            "I1229 10:57:32.164487 140034317584256 learning.py:507] global step 461: loss = 1.9382 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 462: loss = 2.8773 (1.184 sec/step)\n",
            "I1229 10:57:33.350429 140034317584256 learning.py:507] global step 462: loss = 2.8773 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 463: loss = 3.4996 (1.156 sec/step)\n",
            "I1229 10:57:34.508185 140034317584256 learning.py:507] global step 463: loss = 3.4996 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 464: loss = 4.5041 (1.165 sec/step)\n",
            "I1229 10:57:35.675581 140034317584256 learning.py:507] global step 464: loss = 4.5041 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 465: loss = 3.6523 (1.183 sec/step)\n",
            "I1229 10:57:36.860033 140034317584256 learning.py:507] global step 465: loss = 3.6523 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 466: loss = 4.5395 (1.186 sec/step)\n",
            "I1229 10:57:38.047810 140034317584256 learning.py:507] global step 466: loss = 4.5395 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 467: loss = 3.5204 (1.190 sec/step)\n",
            "I1229 10:57:39.239864 140034317584256 learning.py:507] global step 467: loss = 3.5204 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 468: loss = 3.7939 (1.206 sec/step)\n",
            "I1229 10:57:40.448012 140034317584256 learning.py:507] global step 468: loss = 3.7939 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 469: loss = 2.4411 (1.152 sec/step)\n",
            "I1229 10:57:41.601634 140034317584256 learning.py:507] global step 469: loss = 2.4411 (1.152 sec/step)\n",
            "INFO:tensorflow:global step 470: loss = 2.9858 (1.192 sec/step)\n",
            "I1229 10:57:42.795596 140034317584256 learning.py:507] global step 470: loss = 2.9858 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 471: loss = 4.3911 (1.183 sec/step)\n",
            "I1229 10:57:43.980359 140034317584256 learning.py:507] global step 471: loss = 4.3911 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 472: loss = 2.8732 (1.189 sec/step)\n",
            "I1229 10:57:45.171268 140034317584256 learning.py:507] global step 472: loss = 2.8732 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 473: loss = 3.4042 (1.208 sec/step)\n",
            "I1229 10:57:46.381054 140034317584256 learning.py:507] global step 473: loss = 3.4042 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 474: loss = 3.1881 (1.239 sec/step)\n",
            "I1229 10:57:47.621522 140034317584256 learning.py:507] global step 474: loss = 3.1881 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 475: loss = 2.6411 (1.198 sec/step)\n",
            "I1229 10:57:48.821158 140034317584256 learning.py:507] global step 475: loss = 2.6411 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 476: loss = 3.2769 (1.158 sec/step)\n",
            "I1229 10:57:49.980709 140034317584256 learning.py:507] global step 476: loss = 3.2769 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 477: loss = 3.2795 (1.166 sec/step)\n",
            "I1229 10:57:51.148337 140034317584256 learning.py:507] global step 477: loss = 3.2795 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 478: loss = 2.0500 (1.216 sec/step)\n",
            "I1229 10:57:52.365843 140034317584256 learning.py:507] global step 478: loss = 2.0500 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 479: loss = 6.2371 (1.209 sec/step)\n",
            "I1229 10:57:53.576422 140034317584256 learning.py:507] global step 479: loss = 6.2371 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 480: loss = 2.7672 (1.193 sec/step)\n",
            "I1229 10:57:54.771461 140034317584256 learning.py:507] global step 480: loss = 2.7672 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 481: loss = 2.1083 (1.186 sec/step)\n",
            "I1229 10:57:55.959114 140034317584256 learning.py:507] global step 481: loss = 2.1083 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 482: loss = 4.5679 (1.164 sec/step)\n",
            "I1229 10:57:57.124612 140034317584256 learning.py:507] global step 482: loss = 4.5679 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 483: loss = 5.0237 (1.213 sec/step)\n",
            "I1229 10:57:58.339562 140034317584256 learning.py:507] global step 483: loss = 5.0237 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 484: loss = 2.7298 (1.163 sec/step)\n",
            "I1229 10:57:59.504106 140034317584256 learning.py:507] global step 484: loss = 2.7298 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 485: loss = 1.7204 (1.127 sec/step)\n",
            "I1229 10:58:00.633229 140034317584256 learning.py:507] global step 485: loss = 1.7204 (1.127 sec/step)\n",
            "INFO:tensorflow:global step 486: loss = 4.4250 (1.142 sec/step)\n",
            "I1229 10:58:01.777197 140034317584256 learning.py:507] global step 486: loss = 4.4250 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 487: loss = 3.2452 (1.152 sec/step)\n",
            "I1229 10:58:02.931478 140034317584256 learning.py:507] global step 487: loss = 3.2452 (1.152 sec/step)\n",
            "INFO:tensorflow:global step 488: loss = 2.8115 (1.131 sec/step)\n",
            "I1229 10:58:04.064421 140034317584256 learning.py:507] global step 488: loss = 2.8115 (1.131 sec/step)\n",
            "INFO:tensorflow:global step 489: loss = 2.7350 (1.164 sec/step)\n",
            "I1229 10:58:05.230351 140034317584256 learning.py:507] global step 489: loss = 2.7350 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 490: loss = 3.1406 (1.190 sec/step)\n",
            "I1229 10:58:06.422074 140034317584256 learning.py:507] global step 490: loss = 3.1406 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 491: loss = 4.1949 (1.158 sec/step)\n",
            "I1229 10:58:07.581748 140034317584256 learning.py:507] global step 491: loss = 4.1949 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 492: loss = 3.6195 (1.185 sec/step)\n",
            "I1229 10:58:08.768313 140034317584256 learning.py:507] global step 492: loss = 3.6195 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 493: loss = 3.7164 (1.197 sec/step)\n",
            "I1229 10:58:09.967077 140034317584256 learning.py:507] global step 493: loss = 3.7164 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 494: loss = 3.8223 (1.194 sec/step)\n",
            "I1229 10:58:11.163048 140034317584256 learning.py:507] global step 494: loss = 3.8223 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 495: loss = 2.7507 (1.206 sec/step)\n",
            "I1229 10:58:12.370529 140034317584256 learning.py:507] global step 495: loss = 2.7507 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 496: loss = 2.2089 (1.195 sec/step)\n",
            "I1229 10:58:13.567399 140034317584256 learning.py:507] global step 496: loss = 2.2089 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 497: loss = 4.1216 (1.164 sec/step)\n",
            "I1229 10:58:14.733731 140034317584256 learning.py:507] global step 497: loss = 4.1216 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 498: loss = 1.9012 (1.176 sec/step)\n",
            "I1229 10:58:15.911680 140034317584256 learning.py:507] global step 498: loss = 1.9012 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 499: loss = 3.9229 (1.144 sec/step)\n",
            "I1229 10:58:17.057515 140034317584256 learning.py:507] global step 499: loss = 3.9229 (1.144 sec/step)\n",
            "INFO:tensorflow:global step 500: loss = 3.9702 (1.232 sec/step)\n",
            "I1229 10:58:18.290611 140034317584256 learning.py:507] global step 500: loss = 3.9702 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 501: loss = 4.0186 (1.176 sec/step)\n",
            "I1229 10:58:19.467891 140034317584256 learning.py:507] global step 501: loss = 4.0186 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 502: loss = 2.9260 (1.192 sec/step)\n",
            "I1229 10:58:20.661119 140034317584256 learning.py:507] global step 502: loss = 2.9260 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 503: loss = 4.1293 (1.198 sec/step)\n",
            "I1229 10:58:21.860962 140034317584256 learning.py:507] global step 503: loss = 4.1293 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 504: loss = 2.1022 (1.202 sec/step)\n",
            "I1229 10:58:23.065006 140034317584256 learning.py:507] global step 504: loss = 2.1022 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 505: loss = 3.0805 (1.201 sec/step)\n",
            "I1229 10:58:24.267971 140034317584256 learning.py:507] global step 505: loss = 3.0805 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 506: loss = 4.2881 (1.187 sec/step)\n",
            "I1229 10:58:25.456674 140034317584256 learning.py:507] global step 506: loss = 4.2881 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 507: loss = 2.8411 (1.211 sec/step)\n",
            "I1229 10:58:26.668822 140034317584256 learning.py:507] global step 507: loss = 2.8411 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 508: loss = 2.8091 (1.202 sec/step)\n",
            "I1229 10:58:27.873026 140034317584256 learning.py:507] global step 508: loss = 2.8091 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 509: loss = 3.0280 (1.234 sec/step)\n",
            "I1229 10:58:29.108289 140034317584256 learning.py:507] global step 509: loss = 3.0280 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 510: loss = 6.1672 (1.132 sec/step)\n",
            "I1229 10:58:30.242262 140034317584256 learning.py:507] global step 510: loss = 6.1672 (1.132 sec/step)\n",
            "INFO:tensorflow:global step 511: loss = 2.4918 (1.185 sec/step)\n",
            "I1229 10:58:31.429198 140034317584256 learning.py:507] global step 511: loss = 2.4918 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 512: loss = 2.7737 (1.181 sec/step)\n",
            "I1229 10:58:32.611886 140034317584256 learning.py:507] global step 512: loss = 2.7737 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 513: loss = 3.4411 (1.195 sec/step)\n",
            "I1229 10:58:33.808822 140034317584256 learning.py:507] global step 513: loss = 3.4411 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 514: loss = 2.8416 (1.169 sec/step)\n",
            "I1229 10:58:34.979993 140034317584256 learning.py:507] global step 514: loss = 2.8416 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 515: loss = 2.9858 (1.156 sec/step)\n",
            "I1229 10:58:36.138728 140034317584256 learning.py:507] global step 515: loss = 2.9858 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 516: loss = 3.6065 (1.169 sec/step)\n",
            "I1229 10:58:37.309939 140034317584256 learning.py:507] global step 516: loss = 3.6065 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 517: loss = 2.9537 (1.169 sec/step)\n",
            "I1229 10:58:38.481173 140034317584256 learning.py:507] global step 517: loss = 2.9537 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 518: loss = 4.5012 (1.165 sec/step)\n",
            "I1229 10:58:39.647466 140034317584256 learning.py:507] global step 518: loss = 4.5012 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 519: loss = 2.2556 (1.184 sec/step)\n",
            "I1229 10:58:40.833464 140034317584256 learning.py:507] global step 519: loss = 2.2556 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 520: loss = 3.3468 (1.204 sec/step)\n",
            "I1229 10:58:42.039413 140034317584256 learning.py:507] global step 520: loss = 3.3468 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 521: loss = 3.5743 (1.207 sec/step)\n",
            "I1229 10:58:43.247630 140034317584256 learning.py:507] global step 521: loss = 3.5743 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 522: loss = 3.2821 (1.201 sec/step)\n",
            "I1229 10:58:44.450316 140034317584256 learning.py:507] global step 522: loss = 3.2821 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 523: loss = 3.0549 (1.195 sec/step)\n",
            "I1229 10:58:45.647267 140034317584256 learning.py:507] global step 523: loss = 3.0549 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 524: loss = 2.5598 (1.167 sec/step)\n",
            "I1229 10:58:46.815763 140034317584256 learning.py:507] global step 524: loss = 2.5598 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 525: loss = 4.4215 (1.255 sec/step)\n",
            "I1229 10:58:48.072964 140034317584256 learning.py:507] global step 525: loss = 4.4215 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 526: loss = 2.8100 (1.156 sec/step)\n",
            "I1229 10:58:49.230697 140034317584256 learning.py:507] global step 526: loss = 2.8100 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 527: loss = 4.4173 (1.182 sec/step)\n",
            "I1229 10:58:50.414345 140034317584256 learning.py:507] global step 527: loss = 4.4173 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 528: loss = 3.5625 (1.199 sec/step)\n",
            "I1229 10:58:51.615307 140034317584256 learning.py:507] global step 528: loss = 3.5625 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 529: loss = 3.0552 (1.189 sec/step)\n",
            "I1229 10:58:52.806184 140034317584256 learning.py:507] global step 529: loss = 3.0552 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 530: loss = 4.0507 (1.194 sec/step)\n",
            "I1229 10:58:54.001980 140034317584256 learning.py:507] global step 530: loss = 4.0507 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 531: loss = 2.9131 (1.222 sec/step)\n",
            "I1229 10:58:55.225146 140034317584256 learning.py:507] global step 531: loss = 2.9131 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 532: loss = 3.5198 (1.170 sec/step)\n",
            "I1229 10:58:56.396452 140034317584256 learning.py:507] global step 532: loss = 3.5198 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 533: loss = 2.5405 (1.174 sec/step)\n",
            "I1229 10:58:57.572484 140034317584256 learning.py:507] global step 533: loss = 2.5405 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 534: loss = 3.1862 (1.248 sec/step)\n",
            "I1229 10:58:58.822245 140034317584256 learning.py:507] global step 534: loss = 3.1862 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 535: loss = 3.7849 (1.174 sec/step)\n",
            "I1229 10:58:59.998806 140034317584256 learning.py:507] global step 535: loss = 3.7849 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 536: loss = 2.6096 (1.215 sec/step)\n",
            "I1229 10:59:01.216178 140034317584256 learning.py:507] global step 536: loss = 2.6096 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 537: loss = 2.7740 (1.180 sec/step)\n",
            "I1229 10:59:02.398291 140034317584256 learning.py:507] global step 537: loss = 2.7740 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 538: loss = 2.2057 (1.181 sec/step)\n",
            "I1229 10:59:03.580997 140034317584256 learning.py:507] global step 538: loss = 2.2057 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 539: loss = 2.8435 (1.171 sec/step)\n",
            "I1229 10:59:04.753420 140034317584256 learning.py:507] global step 539: loss = 2.8435 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 540: loss = 3.8079 (1.219 sec/step)\n",
            "I1229 10:59:05.973720 140034317584256 learning.py:507] global step 540: loss = 3.8079 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 541: loss = 3.7249 (1.202 sec/step)\n",
            "I1229 10:59:07.177200 140034317584256 learning.py:507] global step 541: loss = 3.7249 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 542: loss = 2.7811 (1.190 sec/step)\n",
            "I1229 10:59:08.368584 140034317584256 learning.py:507] global step 542: loss = 2.7811 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 543: loss = 2.5598 (1.196 sec/step)\n",
            "I1229 10:59:09.566238 140034317584256 learning.py:507] global step 543: loss = 2.5598 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 544: loss = 2.7451 (1.243 sec/step)\n",
            "I1229 10:59:10.811132 140034317584256 learning.py:507] global step 544: loss = 2.7451 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 545: loss = 2.5234 (1.206 sec/step)\n",
            "I1229 10:59:12.018921 140034317584256 learning.py:507] global step 545: loss = 2.5234 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 546: loss = 2.9403 (1.154 sec/step)\n",
            "I1229 10:59:13.174341 140034317584256 learning.py:507] global step 546: loss = 2.9403 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 547: loss = 4.4988 (1.208 sec/step)\n",
            "I1229 10:59:14.384107 140034317584256 learning.py:507] global step 547: loss = 4.4988 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 548: loss = 4.0309 (1.147 sec/step)\n",
            "I1229 10:59:15.532965 140034317584256 learning.py:507] global step 548: loss = 4.0309 (1.147 sec/step)\n",
            "INFO:tensorflow:global step 549: loss = 3.1850 (1.168 sec/step)\n",
            "I1229 10:59:16.702908 140034317584256 learning.py:507] global step 549: loss = 3.1850 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 550: loss = 3.8990 (1.786 sec/step)\n",
            "I1229 10:59:18.490432 140034317584256 learning.py:507] global step 550: loss = 3.8990 (1.786 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 550.\n",
            "I1229 10:59:19.855328 140030580766464 supervisor.py:1050] Recording summary at step 550.\n",
            "INFO:tensorflow:global step 551: loss = 3.1914 (1.654 sec/step)\n",
            "I1229 10:59:20.146385 140034317584256 learning.py:507] global step 551: loss = 3.1914 (1.654 sec/step)\n",
            "INFO:tensorflow:global step 552: loss = 3.4400 (1.255 sec/step)\n",
            "I1229 10:59:21.403681 140034317584256 learning.py:507] global step 552: loss = 3.4400 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 553: loss = 3.2579 (1.225 sec/step)\n",
            "I1229 10:59:22.630371 140034317584256 learning.py:507] global step 553: loss = 3.2579 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 554: loss = 3.1994 (1.240 sec/step)\n",
            "I1229 10:59:23.871422 140034317584256 learning.py:507] global step 554: loss = 3.1994 (1.240 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.828516\n",
            "I1229 10:59:24.000006 140030555588352 supervisor.py:1099] global_step/sec: 0.828516\n",
            "INFO:tensorflow:global step 555: loss = 3.1471 (1.238 sec/step)\n",
            "I1229 10:59:25.111124 140034317584256 learning.py:507] global step 555: loss = 3.1471 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 556: loss = 2.3401 (1.231 sec/step)\n",
            "I1229 10:59:26.344118 140034317584256 learning.py:507] global step 556: loss = 2.3401 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 557: loss = 3.0771 (1.253 sec/step)\n",
            "I1229 10:59:27.598822 140034317584256 learning.py:507] global step 557: loss = 3.0771 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 558: loss = 4.0597 (1.261 sec/step)\n",
            "I1229 10:59:28.862181 140034317584256 learning.py:507] global step 558: loss = 4.0597 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 559: loss = 2.7756 (1.202 sec/step)\n",
            "I1229 10:59:30.065240 140034317584256 learning.py:507] global step 559: loss = 2.7756 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 560: loss = 2.6389 (1.228 sec/step)\n",
            "I1229 10:59:31.294903 140034317584256 learning.py:507] global step 560: loss = 2.6389 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 561: loss = 3.1010 (1.185 sec/step)\n",
            "I1229 10:59:32.481081 140034317584256 learning.py:507] global step 561: loss = 3.1010 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 562: loss = 4.1566 (1.176 sec/step)\n",
            "I1229 10:59:33.659138 140034317584256 learning.py:507] global step 562: loss = 4.1566 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 563: loss = 1.9025 (1.177 sec/step)\n",
            "I1229 10:59:34.837889 140034317584256 learning.py:507] global step 563: loss = 1.9025 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 564: loss = 3.1449 (1.195 sec/step)\n",
            "I1229 10:59:36.035022 140034317584256 learning.py:507] global step 564: loss = 3.1449 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 565: loss = 2.4001 (1.172 sec/step)\n",
            "I1229 10:59:37.208368 140034317584256 learning.py:507] global step 565: loss = 2.4001 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 566: loss = 2.7520 (1.165 sec/step)\n",
            "I1229 10:59:38.375263 140034317584256 learning.py:507] global step 566: loss = 2.7520 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 567: loss = 2.8762 (1.196 sec/step)\n",
            "I1229 10:59:39.573596 140034317584256 learning.py:507] global step 567: loss = 2.8762 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 568: loss = 2.9250 (1.172 sec/step)\n",
            "I1229 10:59:40.747827 140034317584256 learning.py:507] global step 568: loss = 2.9250 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 569: loss = 3.3811 (1.133 sec/step)\n",
            "I1229 10:59:41.882334 140034317584256 learning.py:507] global step 569: loss = 3.3811 (1.133 sec/step)\n",
            "INFO:tensorflow:global step 570: loss = 3.2039 (1.190 sec/step)\n",
            "I1229 10:59:43.074494 140034317584256 learning.py:507] global step 570: loss = 3.2039 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 571: loss = 3.3092 (1.179 sec/step)\n",
            "I1229 10:59:44.255306 140034317584256 learning.py:507] global step 571: loss = 3.3092 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 572: loss = 2.7856 (1.196 sec/step)\n",
            "I1229 10:59:45.453029 140034317584256 learning.py:507] global step 572: loss = 2.7856 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 573: loss = 2.0744 (1.243 sec/step)\n",
            "I1229 10:59:46.697303 140034317584256 learning.py:507] global step 573: loss = 2.0744 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 574: loss = 4.4362 (1.245 sec/step)\n",
            "I1229 10:59:47.944274 140034317584256 learning.py:507] global step 574: loss = 4.4362 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 575: loss = 3.9444 (1.220 sec/step)\n",
            "I1229 10:59:49.166583 140034317584256 learning.py:507] global step 575: loss = 3.9444 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 576: loss = 3.1632 (1.204 sec/step)\n",
            "I1229 10:59:50.372416 140034317584256 learning.py:507] global step 576: loss = 3.1632 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 577: loss = 3.7879 (1.231 sec/step)\n",
            "I1229 10:59:51.604943 140034317584256 learning.py:507] global step 577: loss = 3.7879 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 578: loss = 2.8476 (1.217 sec/step)\n",
            "I1229 10:59:52.823566 140034317584256 learning.py:507] global step 578: loss = 2.8476 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 579: loss = 2.6533 (1.190 sec/step)\n",
            "I1229 10:59:54.015518 140034317584256 learning.py:507] global step 579: loss = 2.6533 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 580: loss = 3.8639 (1.187 sec/step)\n",
            "I1229 10:59:55.204146 140034317584256 learning.py:507] global step 580: loss = 3.8639 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 581: loss = 4.4147 (1.186 sec/step)\n",
            "I1229 10:59:56.392009 140034317584256 learning.py:507] global step 581: loss = 4.4147 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 582: loss = 3.4843 (1.175 sec/step)\n",
            "I1229 10:59:57.569073 140034317584256 learning.py:507] global step 582: loss = 3.4843 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 583: loss = 4.2580 (1.262 sec/step)\n",
            "I1229 10:59:58.832640 140034317584256 learning.py:507] global step 583: loss = 4.2580 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 584: loss = 3.9114 (1.194 sec/step)\n",
            "I1229 11:00:00.028990 140034317584256 learning.py:507] global step 584: loss = 3.9114 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 585: loss = 3.7444 (1.215 sec/step)\n",
            "I1229 11:00:01.245805 140034317584256 learning.py:507] global step 585: loss = 3.7444 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 586: loss = 2.8765 (1.240 sec/step)\n",
            "I1229 11:00:02.487884 140034317584256 learning.py:507] global step 586: loss = 2.8765 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 587: loss = 4.6709 (1.233 sec/step)\n",
            "I1229 11:00:03.722200 140034317584256 learning.py:507] global step 587: loss = 4.6709 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 588: loss = 3.4641 (1.204 sec/step)\n",
            "I1229 11:00:04.928095 140034317584256 learning.py:507] global step 588: loss = 3.4641 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 589: loss = 3.6743 (1.217 sec/step)\n",
            "I1229 11:00:06.146548 140034317584256 learning.py:507] global step 589: loss = 3.6743 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 590: loss = 3.8766 (1.255 sec/step)\n",
            "I1229 11:00:07.403589 140034317584256 learning.py:507] global step 590: loss = 3.8766 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 591: loss = 3.5432 (1.279 sec/step)\n",
            "I1229 11:00:08.684086 140034317584256 learning.py:507] global step 591: loss = 3.5432 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 592: loss = 2.2664 (1.280 sec/step)\n",
            "I1229 11:00:09.965482 140034317584256 learning.py:507] global step 592: loss = 2.2664 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 593: loss = 2.3381 (1.197 sec/step)\n",
            "I1229 11:00:11.163623 140034317584256 learning.py:507] global step 593: loss = 2.3381 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 594: loss = 3.1210 (1.251 sec/step)\n",
            "I1229 11:00:12.416162 140034317584256 learning.py:507] global step 594: loss = 3.1210 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 595: loss = 4.4689 (1.274 sec/step)\n",
            "I1229 11:00:13.691680 140034317584256 learning.py:507] global step 595: loss = 4.4689 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 596: loss = 3.4189 (1.270 sec/step)\n",
            "I1229 11:00:14.963616 140034317584256 learning.py:507] global step 596: loss = 3.4189 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 597: loss = 3.3334 (1.249 sec/step)\n",
            "I1229 11:00:16.213884 140034317584256 learning.py:507] global step 597: loss = 3.3334 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 598: loss = 2.9273 (1.246 sec/step)\n",
            "I1229 11:00:17.461673 140034317584256 learning.py:507] global step 598: loss = 2.9273 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 599: loss = 5.1111 (1.227 sec/step)\n",
            "I1229 11:00:18.690471 140034317584256 learning.py:507] global step 599: loss = 5.1111 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 2.7676 (1.176 sec/step)\n",
            "I1229 11:00:19.867581 140034317584256 learning.py:507] global step 600: loss = 2.7676 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 601: loss = 3.3269 (1.217 sec/step)\n",
            "I1229 11:00:21.086451 140034317584256 learning.py:507] global step 601: loss = 3.3269 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 602: loss = 3.5376 (1.258 sec/step)\n",
            "I1229 11:00:22.346577 140034317584256 learning.py:507] global step 602: loss = 3.5376 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 603: loss = 3.9436 (1.260 sec/step)\n",
            "I1229 11:00:23.608530 140034317584256 learning.py:507] global step 603: loss = 3.9436 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 604: loss = 2.8340 (1.269 sec/step)\n",
            "I1229 11:00:24.879076 140034317584256 learning.py:507] global step 604: loss = 2.8340 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 605: loss = 2.6202 (1.226 sec/step)\n",
            "I1229 11:00:26.106359 140034317584256 learning.py:507] global step 605: loss = 2.6202 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 606: loss = 3.5555 (1.292 sec/step)\n",
            "I1229 11:00:27.399772 140034317584256 learning.py:507] global step 606: loss = 3.5555 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 607: loss = 2.4768 (1.253 sec/step)\n",
            "I1229 11:00:28.654312 140034317584256 learning.py:507] global step 607: loss = 2.4768 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 608: loss = 3.3959 (1.220 sec/step)\n",
            "I1229 11:00:29.876197 140034317584256 learning.py:507] global step 608: loss = 3.3959 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 609: loss = 3.2224 (1.233 sec/step)\n",
            "I1229 11:00:31.111121 140034317584256 learning.py:507] global step 609: loss = 3.2224 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 610: loss = 1.5116 (1.212 sec/step)\n",
            "I1229 11:00:32.324567 140034317584256 learning.py:507] global step 610: loss = 1.5116 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 611: loss = 4.0492 (1.259 sec/step)\n",
            "I1229 11:00:33.585045 140034317584256 learning.py:507] global step 611: loss = 4.0492 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 612: loss = 3.9782 (1.182 sec/step)\n",
            "I1229 11:00:34.769012 140034317584256 learning.py:507] global step 612: loss = 3.9782 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 613: loss = 2.5384 (1.205 sec/step)\n",
            "I1229 11:00:35.975867 140034317584256 learning.py:507] global step 613: loss = 2.5384 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 614: loss = 3.4095 (1.217 sec/step)\n",
            "I1229 11:00:37.195308 140034317584256 learning.py:507] global step 614: loss = 3.4095 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 615: loss = 3.8370 (1.231 sec/step)\n",
            "I1229 11:00:38.428724 140034317584256 learning.py:507] global step 615: loss = 3.8370 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 616: loss = 2.8807 (1.203 sec/step)\n",
            "I1229 11:00:39.633336 140034317584256 learning.py:507] global step 616: loss = 2.8807 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 617: loss = 3.6345 (1.254 sec/step)\n",
            "I1229 11:00:40.888516 140034317584256 learning.py:507] global step 617: loss = 3.6345 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 618: loss = 6.1179 (1.187 sec/step)\n",
            "I1229 11:00:42.076998 140034317584256 learning.py:507] global step 618: loss = 6.1179 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 619: loss = 2.8948 (1.243 sec/step)\n",
            "I1229 11:00:43.321484 140034317584256 learning.py:507] global step 619: loss = 2.8948 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 620: loss = 2.8331 (1.189 sec/step)\n",
            "I1229 11:00:44.512512 140034317584256 learning.py:507] global step 620: loss = 2.8331 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 621: loss = 2.8054 (1.225 sec/step)\n",
            "I1229 11:00:45.739686 140034317584256 learning.py:507] global step 621: loss = 2.8054 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 622: loss = 4.5169 (1.246 sec/step)\n",
            "I1229 11:00:46.988748 140034317584256 learning.py:507] global step 622: loss = 4.5169 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 623: loss = 5.0584 (1.237 sec/step)\n",
            "I1229 11:00:48.228151 140034317584256 learning.py:507] global step 623: loss = 5.0584 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 624: loss = 2.9326 (1.214 sec/step)\n",
            "I1229 11:00:49.444047 140034317584256 learning.py:507] global step 624: loss = 2.9326 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 625: loss = 3.8724 (1.302 sec/step)\n",
            "I1229 11:00:50.747390 140034317584256 learning.py:507] global step 625: loss = 3.8724 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 626: loss = 3.9364 (1.270 sec/step)\n",
            "I1229 11:00:52.018834 140034317584256 learning.py:507] global step 626: loss = 3.9364 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 627: loss = 3.7039 (1.245 sec/step)\n",
            "I1229 11:00:53.265324 140034317584256 learning.py:507] global step 627: loss = 3.7039 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 628: loss = 3.5552 (1.225 sec/step)\n",
            "I1229 11:00:54.492424 140034317584256 learning.py:507] global step 628: loss = 3.5552 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 629: loss = 3.0233 (1.295 sec/step)\n",
            "I1229 11:00:55.788749 140034317584256 learning.py:507] global step 629: loss = 3.0233 (1.295 sec/step)\n",
            "INFO:tensorflow:global step 630: loss = 2.4589 (1.248 sec/step)\n",
            "I1229 11:00:57.038347 140034317584256 learning.py:507] global step 630: loss = 2.4589 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 631: loss = 4.6440 (1.276 sec/step)\n",
            "I1229 11:00:58.316375 140034317584256 learning.py:507] global step 631: loss = 4.6440 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 632: loss = 3.5215 (1.253 sec/step)\n",
            "I1229 11:00:59.571129 140034317584256 learning.py:507] global step 632: loss = 3.5215 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 633: loss = 2.6283 (1.230 sec/step)\n",
            "I1229 11:01:00.802957 140034317584256 learning.py:507] global step 633: loss = 2.6283 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 634: loss = 3.3484 (1.265 sec/step)\n",
            "I1229 11:01:02.069265 140034317584256 learning.py:507] global step 634: loss = 3.3484 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 635: loss = 2.1391 (1.242 sec/step)\n",
            "I1229 11:01:03.312892 140034317584256 learning.py:507] global step 635: loss = 2.1391 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 636: loss = 3.7129 (1.244 sec/step)\n",
            "I1229 11:01:04.559095 140034317584256 learning.py:507] global step 636: loss = 3.7129 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 637: loss = 2.2404 (1.263 sec/step)\n",
            "I1229 11:01:05.823855 140034317584256 learning.py:507] global step 637: loss = 2.2404 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 638: loss = 3.4846 (1.258 sec/step)\n",
            "I1229 11:01:07.083849 140034317584256 learning.py:507] global step 638: loss = 3.4846 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 639: loss = 3.3691 (1.255 sec/step)\n",
            "I1229 11:01:08.340473 140034317584256 learning.py:507] global step 639: loss = 3.3691 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 640: loss = 2.4297 (1.214 sec/step)\n",
            "I1229 11:01:09.556073 140034317584256 learning.py:507] global step 640: loss = 2.4297 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 641: loss = 3.2067 (1.244 sec/step)\n",
            "I1229 11:01:10.801262 140034317584256 learning.py:507] global step 641: loss = 3.2067 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 642: loss = 2.7928 (1.247 sec/step)\n",
            "I1229 11:01:12.049840 140034317584256 learning.py:507] global step 642: loss = 2.7928 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 643: loss = 3.0636 (1.200 sec/step)\n",
            "I1229 11:01:13.251546 140034317584256 learning.py:507] global step 643: loss = 3.0636 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 644: loss = 2.9548 (1.206 sec/step)\n",
            "I1229 11:01:14.459379 140034317584256 learning.py:507] global step 644: loss = 2.9548 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 645: loss = 3.9004 (1.214 sec/step)\n",
            "I1229 11:01:15.675370 140034317584256 learning.py:507] global step 645: loss = 3.9004 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 646: loss = 3.0585 (1.212 sec/step)\n",
            "I1229 11:01:16.888988 140034317584256 learning.py:507] global step 646: loss = 3.0585 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 647: loss = 2.9538 (2.007 sec/step)\n",
            "I1229 11:01:18.898684 140034317584256 learning.py:507] global step 647: loss = 2.9538 (2.007 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 647.\n",
            "I1229 11:01:19.727052 140030580766464 supervisor.py:1050] Recording summary at step 647.\n",
            "INFO:tensorflow:global step 648: loss = 3.3835 (1.413 sec/step)\n",
            "I1229 11:01:20.313973 140034317584256 learning.py:507] global step 648: loss = 3.3835 (1.413 sec/step)\n",
            "INFO:tensorflow:global step 649: loss = 3.4784 (1.239 sec/step)\n",
            "I1229 11:01:21.554180 140034317584256 learning.py:507] global step 649: loss = 3.4784 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 650: loss = 3.1986 (1.243 sec/step)\n",
            "I1229 11:01:22.799474 140034317584256 learning.py:507] global step 650: loss = 3.1986 (1.243 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.800532\n",
            "I1229 11:01:23.920182 140030555588352 supervisor.py:1099] global_step/sec: 0.800532\n",
            "INFO:tensorflow:global step 651: loss = 2.5892 (1.147 sec/step)\n",
            "I1229 11:01:23.949033 140034317584256 learning.py:507] global step 651: loss = 2.5892 (1.147 sec/step)\n",
            "INFO:tensorflow:global step 652: loss = 2.3720 (1.213 sec/step)\n",
            "I1229 11:01:25.164184 140034317584256 learning.py:507] global step 652: loss = 2.3720 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 653: loss = 3.6989 (1.218 sec/step)\n",
            "I1229 11:01:26.385011 140034317584256 learning.py:507] global step 653: loss = 3.6989 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 654: loss = 2.5550 (1.249 sec/step)\n",
            "I1229 11:01:27.636244 140034317584256 learning.py:507] global step 654: loss = 2.5550 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 655: loss = 2.3536 (1.174 sec/step)\n",
            "I1229 11:01:28.812264 140034317584256 learning.py:507] global step 655: loss = 2.3536 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 656: loss = 2.9033 (1.245 sec/step)\n",
            "I1229 11:01:30.059171 140034317584256 learning.py:507] global step 656: loss = 2.9033 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 657: loss = 4.5767 (1.191 sec/step)\n",
            "I1229 11:01:31.251470 140034317584256 learning.py:507] global step 657: loss = 4.5767 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 658: loss = 2.9522 (1.215 sec/step)\n",
            "I1229 11:01:32.468383 140034317584256 learning.py:507] global step 658: loss = 2.9522 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 659: loss = 4.7180 (1.179 sec/step)\n",
            "I1229 11:01:33.648343 140034317584256 learning.py:507] global step 659: loss = 4.7180 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 660: loss = 2.9112 (1.180 sec/step)\n",
            "I1229 11:01:34.830375 140034317584256 learning.py:507] global step 660: loss = 2.9112 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 661: loss = 2.9950 (1.183 sec/step)\n",
            "I1229 11:01:36.015107 140034317584256 learning.py:507] global step 661: loss = 2.9950 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 662: loss = 2.5647 (1.238 sec/step)\n",
            "I1229 11:01:37.255147 140034317584256 learning.py:507] global step 662: loss = 2.5647 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 663: loss = 3.2220 (1.173 sec/step)\n",
            "I1229 11:01:38.429506 140034317584256 learning.py:507] global step 663: loss = 3.2220 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 664: loss = 3.2992 (1.181 sec/step)\n",
            "I1229 11:01:39.612440 140034317584256 learning.py:507] global step 664: loss = 3.2992 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 665: loss = 2.7104 (1.178 sec/step)\n",
            "I1229 11:01:40.792185 140034317584256 learning.py:507] global step 665: loss = 2.7104 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 666: loss = 5.4463 (1.200 sec/step)\n",
            "I1229 11:01:41.993816 140034317584256 learning.py:507] global step 666: loss = 5.4463 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 667: loss = 3.4199 (1.196 sec/step)\n",
            "I1229 11:01:43.191687 140034317584256 learning.py:507] global step 667: loss = 3.4199 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 668: loss = 2.9554 (1.193 sec/step)\n",
            "I1229 11:01:44.386157 140034317584256 learning.py:507] global step 668: loss = 2.9554 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 669: loss = 4.4992 (1.206 sec/step)\n",
            "I1229 11:01:45.594259 140034317584256 learning.py:507] global step 669: loss = 4.4992 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 670: loss = 3.1548 (1.217 sec/step)\n",
            "I1229 11:01:46.812632 140034317584256 learning.py:507] global step 670: loss = 3.1548 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 671: loss = 2.3422 (1.274 sec/step)\n",
            "I1229 11:01:48.088026 140034317584256 learning.py:507] global step 671: loss = 2.3422 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 672: loss = 2.5268 (1.191 sec/step)\n",
            "I1229 11:01:49.280837 140034317584256 learning.py:507] global step 672: loss = 2.5268 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 673: loss = 3.6922 (1.201 sec/step)\n",
            "I1229 11:01:50.484049 140034317584256 learning.py:507] global step 673: loss = 3.6922 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 674: loss = 2.6408 (1.213 sec/step)\n",
            "I1229 11:01:51.698753 140034317584256 learning.py:507] global step 674: loss = 2.6408 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 675: loss = 3.7772 (1.192 sec/step)\n",
            "I1229 11:01:52.893120 140034317584256 learning.py:507] global step 675: loss = 3.7772 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 676: loss = 3.4668 (1.173 sec/step)\n",
            "I1229 11:01:54.067600 140034317584256 learning.py:507] global step 676: loss = 3.4668 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 677: loss = 3.3344 (1.199 sec/step)\n",
            "I1229 11:01:55.267799 140034317584256 learning.py:507] global step 677: loss = 3.3344 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 678: loss = 3.3123 (1.225 sec/step)\n",
            "I1229 11:01:56.494616 140034317584256 learning.py:507] global step 678: loss = 3.3123 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 679: loss = 2.6183 (1.218 sec/step)\n",
            "I1229 11:01:57.714341 140034317584256 learning.py:507] global step 679: loss = 2.6183 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 680: loss = 2.6250 (1.197 sec/step)\n",
            "I1229 11:01:58.913187 140034317584256 learning.py:507] global step 680: loss = 2.6250 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 681: loss = 3.0080 (1.186 sec/step)\n",
            "I1229 11:02:00.103359 140034317584256 learning.py:507] global step 681: loss = 3.0080 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 682: loss = 2.6971 (1.124 sec/step)\n",
            "I1229 11:02:01.229505 140034317584256 learning.py:507] global step 682: loss = 2.6971 (1.124 sec/step)\n",
            "INFO:tensorflow:global step 683: loss = 3.4863 (1.141 sec/step)\n",
            "I1229 11:02:02.372051 140034317584256 learning.py:507] global step 683: loss = 3.4863 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 684: loss = 3.4823 (1.195 sec/step)\n",
            "I1229 11:02:03.568078 140034317584256 learning.py:507] global step 684: loss = 3.4823 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 685: loss = 2.4210 (1.207 sec/step)\n",
            "I1229 11:02:04.776645 140034317584256 learning.py:507] global step 685: loss = 2.4210 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 686: loss = 3.2786 (1.195 sec/step)\n",
            "I1229 11:02:05.973331 140034317584256 learning.py:507] global step 686: loss = 3.2786 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 687: loss = 2.5132 (1.186 sec/step)\n",
            "I1229 11:02:07.161150 140034317584256 learning.py:507] global step 687: loss = 2.5132 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 688: loss = 4.8303 (1.176 sec/step)\n",
            "I1229 11:02:08.338875 140034317584256 learning.py:507] global step 688: loss = 4.8303 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 689: loss = 3.2942 (1.142 sec/step)\n",
            "I1229 11:02:09.482036 140034317584256 learning.py:507] global step 689: loss = 3.2942 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 690: loss = 2.6221 (1.199 sec/step)\n",
            "I1229 11:02:10.682178 140034317584256 learning.py:507] global step 690: loss = 2.6221 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 691: loss = 2.3371 (1.182 sec/step)\n",
            "I1229 11:02:11.866426 140034317584256 learning.py:507] global step 691: loss = 2.3371 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 692: loss = 4.3728 (1.219 sec/step)\n",
            "I1229 11:02:13.087436 140034317584256 learning.py:507] global step 692: loss = 4.3728 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 693: loss = 2.9740 (1.212 sec/step)\n",
            "I1229 11:02:14.301603 140034317584256 learning.py:507] global step 693: loss = 2.9740 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 694: loss = 3.2686 (1.214 sec/step)\n",
            "I1229 11:02:15.517194 140034317584256 learning.py:507] global step 694: loss = 3.2686 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 695: loss = 3.8083 (1.211 sec/step)\n",
            "I1229 11:02:16.732431 140034317584256 learning.py:507] global step 695: loss = 3.8083 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 696: loss = 4.1254 (1.223 sec/step)\n",
            "I1229 11:02:17.957441 140034317584256 learning.py:507] global step 696: loss = 4.1254 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 697: loss = 2.8037 (1.217 sec/step)\n",
            "I1229 11:02:19.175745 140034317584256 learning.py:507] global step 697: loss = 2.8037 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 698: loss = 1.9306 (1.191 sec/step)\n",
            "I1229 11:02:20.368560 140034317584256 learning.py:507] global step 698: loss = 1.9306 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 699: loss = 3.4169 (1.223 sec/step)\n",
            "I1229 11:02:21.593600 140034317584256 learning.py:507] global step 699: loss = 3.4169 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 2.9941 (1.281 sec/step)\n",
            "I1229 11:02:22.876851 140034317584256 learning.py:507] global step 700: loss = 2.9941 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 701: loss = 3.3269 (1.236 sec/step)\n",
            "I1229 11:02:24.114540 140034317584256 learning.py:507] global step 701: loss = 3.3269 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 702: loss = 3.0622 (1.228 sec/step)\n",
            "I1229 11:02:25.343960 140034317584256 learning.py:507] global step 702: loss = 3.0622 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 703: loss = 3.9204 (1.243 sec/step)\n",
            "I1229 11:02:26.588483 140034317584256 learning.py:507] global step 703: loss = 3.9204 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 704: loss = 3.0077 (1.251 sec/step)\n",
            "I1229 11:02:27.841896 140034317584256 learning.py:507] global step 704: loss = 3.0077 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 705: loss = 2.8542 (1.247 sec/step)\n",
            "I1229 11:02:29.090653 140034317584256 learning.py:507] global step 705: loss = 2.8542 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 706: loss = 2.4380 (1.278 sec/step)\n",
            "I1229 11:02:30.369925 140034317584256 learning.py:507] global step 706: loss = 2.4380 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 707: loss = 2.8021 (1.235 sec/step)\n",
            "I1229 11:02:31.606661 140034317584256 learning.py:507] global step 707: loss = 2.8021 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 708: loss = 3.3716 (1.185 sec/step)\n",
            "I1229 11:02:32.793305 140034317584256 learning.py:507] global step 708: loss = 3.3716 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 709: loss = 2.6504 (1.283 sec/step)\n",
            "I1229 11:02:34.077497 140034317584256 learning.py:507] global step 709: loss = 2.6504 (1.283 sec/step)\n",
            "INFO:tensorflow:global step 710: loss = 2.7482 (1.185 sec/step)\n",
            "I1229 11:02:35.263788 140034317584256 learning.py:507] global step 710: loss = 2.7482 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 711: loss = 2.3582 (1.207 sec/step)\n",
            "I1229 11:02:36.472348 140034317584256 learning.py:507] global step 711: loss = 2.3582 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 712: loss = 3.5394 (1.225 sec/step)\n",
            "I1229 11:02:37.699075 140034317584256 learning.py:507] global step 712: loss = 3.5394 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 713: loss = 2.9358 (1.213 sec/step)\n",
            "I1229 11:02:38.913679 140034317584256 learning.py:507] global step 713: loss = 2.9358 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 714: loss = 2.9846 (1.247 sec/step)\n",
            "I1229 11:02:40.162390 140034317584256 learning.py:507] global step 714: loss = 2.9846 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 715: loss = 3.1934 (1.241 sec/step)\n",
            "I1229 11:02:41.405014 140034317584256 learning.py:507] global step 715: loss = 3.1934 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 716: loss = 3.0329 (1.195 sec/step)\n",
            "I1229 11:02:42.601982 140034317584256 learning.py:507] global step 716: loss = 3.0329 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 717: loss = 2.7132 (1.227 sec/step)\n",
            "I1229 11:02:43.830316 140034317584256 learning.py:507] global step 717: loss = 2.7132 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 718: loss = 2.5097 (1.235 sec/step)\n",
            "I1229 11:02:45.067174 140034317584256 learning.py:507] global step 718: loss = 2.5097 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 719: loss = 3.3116 (1.229 sec/step)\n",
            "I1229 11:02:46.297773 140034317584256 learning.py:507] global step 719: loss = 3.3116 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 720: loss = 2.0040 (1.235 sec/step)\n",
            "I1229 11:02:47.534072 140034317584256 learning.py:507] global step 720: loss = 2.0040 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 721: loss = 2.6097 (1.264 sec/step)\n",
            "I1229 11:02:48.799842 140034317584256 learning.py:507] global step 721: loss = 2.6097 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 722: loss = 3.5681 (1.187 sec/step)\n",
            "I1229 11:02:49.989298 140034317584256 learning.py:507] global step 722: loss = 3.5681 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 723: loss = 3.1470 (1.260 sec/step)\n",
            "I1229 11:02:51.251399 140034317584256 learning.py:507] global step 723: loss = 3.1470 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 724: loss = 3.4217 (1.232 sec/step)\n",
            "I1229 11:02:52.485253 140034317584256 learning.py:507] global step 724: loss = 3.4217 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 725: loss = 2.7259 (1.199 sec/step)\n",
            "I1229 11:02:53.686022 140034317584256 learning.py:507] global step 725: loss = 2.7259 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 726: loss = 3.4521 (1.238 sec/step)\n",
            "I1229 11:02:54.925625 140034317584256 learning.py:507] global step 726: loss = 3.4521 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 727: loss = 3.0587 (1.197 sec/step)\n",
            "I1229 11:02:56.124527 140034317584256 learning.py:507] global step 727: loss = 3.0587 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 728: loss = 2.8278 (1.202 sec/step)\n",
            "I1229 11:02:57.327677 140034317584256 learning.py:507] global step 728: loss = 2.8278 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 729: loss = 2.8001 (1.204 sec/step)\n",
            "I1229 11:02:58.534027 140034317584256 learning.py:507] global step 729: loss = 2.8001 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 730: loss = 3.7333 (1.219 sec/step)\n",
            "I1229 11:02:59.755315 140034317584256 learning.py:507] global step 730: loss = 3.7333 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 731: loss = 2.9231 (1.237 sec/step)\n",
            "I1229 11:03:00.994371 140034317584256 learning.py:507] global step 731: loss = 2.9231 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 732: loss = 2.3608 (1.216 sec/step)\n",
            "I1229 11:03:02.211616 140034317584256 learning.py:507] global step 732: loss = 2.3608 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 733: loss = 2.6944 (1.213 sec/step)\n",
            "I1229 11:03:03.426306 140034317584256 learning.py:507] global step 733: loss = 2.6944 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 734: loss = 2.2918 (1.215 sec/step)\n",
            "I1229 11:03:04.642752 140034317584256 learning.py:507] global step 734: loss = 2.2918 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 735: loss = 1.6690 (1.196 sec/step)\n",
            "I1229 11:03:05.840446 140034317584256 learning.py:507] global step 735: loss = 1.6690 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 736: loss = 2.9210 (1.185 sec/step)\n",
            "I1229 11:03:07.026815 140034317584256 learning.py:507] global step 736: loss = 2.9210 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 737: loss = 3.6324 (1.225 sec/step)\n",
            "I1229 11:03:08.253223 140034317584256 learning.py:507] global step 737: loss = 3.6324 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 738: loss = 4.5166 (1.216 sec/step)\n",
            "I1229 11:03:09.471399 140034317584256 learning.py:507] global step 738: loss = 4.5166 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 739: loss = 2.5656 (1.174 sec/step)\n",
            "I1229 11:03:10.647375 140034317584256 learning.py:507] global step 739: loss = 2.5656 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 740: loss = 2.6137 (1.272 sec/step)\n",
            "I1229 11:03:11.920781 140034317584256 learning.py:507] global step 740: loss = 2.6137 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 741: loss = 2.9902 (1.204 sec/step)\n",
            "I1229 11:03:13.126202 140034317584256 learning.py:507] global step 741: loss = 2.9902 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 742: loss = 3.8275 (1.200 sec/step)\n",
            "I1229 11:03:14.327462 140034317584256 learning.py:507] global step 742: loss = 3.8275 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 743: loss = 3.2881 (1.237 sec/step)\n",
            "I1229 11:03:15.566362 140034317584256 learning.py:507] global step 743: loss = 3.2881 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 744: loss = 2.3111 (1.238 sec/step)\n",
            "I1229 11:03:16.806393 140034317584256 learning.py:507] global step 744: loss = 2.3111 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 745: loss = 3.1089 (2.020 sec/step)\n",
            "I1229 11:03:18.829582 140034317584256 learning.py:507] global step 745: loss = 3.1089 (2.020 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 745.\n",
            "I1229 11:03:19.767727 140030580766464 supervisor.py:1050] Recording summary at step 745.\n",
            "INFO:tensorflow:global step 746: loss = 2.6241 (1.382 sec/step)\n",
            "I1229 11:03:20.213282 140034317584256 learning.py:507] global step 746: loss = 2.6241 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 747: loss = 4.0766 (1.222 sec/step)\n",
            "I1229 11:03:21.437398 140034317584256 learning.py:507] global step 747: loss = 4.0766 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 748: loss = 2.6109 (1.210 sec/step)\n",
            "I1229 11:03:22.648730 140034317584256 learning.py:507] global step 748: loss = 2.6109 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 749: loss = 2.1137 (1.223 sec/step)\n",
            "I1229 11:03:23.873664 140034317584256 learning.py:507] global step 749: loss = 2.1137 (1.223 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.824548\n",
            "I1229 11:03:23.986127 140030555588352 supervisor.py:1099] global_step/sec: 0.824548\n",
            "INFO:tensorflow:global step 750: loss = 2.4598 (1.238 sec/step)\n",
            "I1229 11:03:25.113221 140034317584256 learning.py:507] global step 750: loss = 2.4598 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 751: loss = 3.3139 (1.254 sec/step)\n",
            "I1229 11:03:26.368723 140034317584256 learning.py:507] global step 751: loss = 3.3139 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 752: loss = 3.7174 (1.241 sec/step)\n",
            "I1229 11:03:27.611691 140034317584256 learning.py:507] global step 752: loss = 3.7174 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 753: loss = 2.4355 (1.265 sec/step)\n",
            "I1229 11:03:28.878896 140034317584256 learning.py:507] global step 753: loss = 2.4355 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 754: loss = 2.4299 (1.211 sec/step)\n",
            "I1229 11:03:30.091800 140034317584256 learning.py:507] global step 754: loss = 2.4299 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 755: loss = 2.7791 (1.198 sec/step)\n",
            "I1229 11:03:31.291726 140034317584256 learning.py:507] global step 755: loss = 2.7791 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 756: loss = 2.8937 (1.237 sec/step)\n",
            "I1229 11:03:32.530843 140034317584256 learning.py:507] global step 756: loss = 2.8937 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 757: loss = 3.8776 (1.187 sec/step)\n",
            "I1229 11:03:33.720287 140034317584256 learning.py:507] global step 757: loss = 3.8776 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 758: loss = 3.1620 (1.215 sec/step)\n",
            "I1229 11:03:34.937393 140034317584256 learning.py:507] global step 758: loss = 3.1620 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 759: loss = 2.8600 (1.200 sec/step)\n",
            "I1229 11:03:36.138874 140034317584256 learning.py:507] global step 759: loss = 2.8600 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 760: loss = 1.8869 (1.227 sec/step)\n",
            "I1229 11:03:37.367168 140034317584256 learning.py:507] global step 760: loss = 1.8869 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 761: loss = 3.8103 (1.250 sec/step)\n",
            "I1229 11:03:38.619278 140034317584256 learning.py:507] global step 761: loss = 3.8103 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 762: loss = 4.1016 (1.223 sec/step)\n",
            "I1229 11:03:39.844044 140034317584256 learning.py:507] global step 762: loss = 4.1016 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 763: loss = 2.0343 (1.207 sec/step)\n",
            "I1229 11:03:41.052310 140034317584256 learning.py:507] global step 763: loss = 2.0343 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 764: loss = 3.0198 (1.200 sec/step)\n",
            "I1229 11:03:42.254421 140034317584256 learning.py:507] global step 764: loss = 3.0198 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 765: loss = 3.2662 (1.265 sec/step)\n",
            "I1229 11:03:43.521001 140034317584256 learning.py:507] global step 765: loss = 3.2662 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 766: loss = 3.6444 (1.198 sec/step)\n",
            "I1229 11:03:44.720242 140034317584256 learning.py:507] global step 766: loss = 3.6444 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 767: loss = 2.4947 (1.213 sec/step)\n",
            "I1229 11:03:45.934960 140034317584256 learning.py:507] global step 767: loss = 2.4947 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 768: loss = 5.0452 (1.200 sec/step)\n",
            "I1229 11:03:47.137124 140034317584256 learning.py:507] global step 768: loss = 5.0452 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 769: loss = 4.1011 (1.253 sec/step)\n",
            "I1229 11:03:48.392913 140034317584256 learning.py:507] global step 769: loss = 4.1011 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 770: loss = 2.4565 (1.182 sec/step)\n",
            "I1229 11:03:49.576918 140034317584256 learning.py:507] global step 770: loss = 2.4565 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 771: loss = 2.5335 (1.209 sec/step)\n",
            "I1229 11:03:50.787777 140034317584256 learning.py:507] global step 771: loss = 2.5335 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 772: loss = 3.1073 (1.231 sec/step)\n",
            "I1229 11:03:52.020159 140034317584256 learning.py:507] global step 772: loss = 3.1073 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 773: loss = 2.1823 (1.186 sec/step)\n",
            "I1229 11:03:53.207695 140034317584256 learning.py:507] global step 773: loss = 2.1823 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 774: loss = 2.8878 (1.219 sec/step)\n",
            "I1229 11:03:54.428757 140034317584256 learning.py:507] global step 774: loss = 2.8878 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 775: loss = 2.6534 (1.187 sec/step)\n",
            "I1229 11:03:55.617450 140034317584256 learning.py:507] global step 775: loss = 2.6534 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 776: loss = 2.4303 (1.235 sec/step)\n",
            "I1229 11:03:56.854472 140034317584256 learning.py:507] global step 776: loss = 2.4303 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 777: loss = 3.0871 (1.199 sec/step)\n",
            "I1229 11:03:58.055314 140034317584256 learning.py:507] global step 777: loss = 3.0871 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 778: loss = 3.8289 (1.230 sec/step)\n",
            "I1229 11:03:59.286857 140034317584256 learning.py:507] global step 778: loss = 3.8289 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 779: loss = 2.7742 (1.191 sec/step)\n",
            "I1229 11:04:00.479328 140034317584256 learning.py:507] global step 779: loss = 2.7742 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 780: loss = 4.5490 (1.141 sec/step)\n",
            "I1229 11:04:01.621832 140034317584256 learning.py:507] global step 780: loss = 4.5490 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 781: loss = 3.1862 (1.204 sec/step)\n",
            "I1229 11:04:02.827386 140034317584256 learning.py:507] global step 781: loss = 3.1862 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 782: loss = 3.4303 (1.181 sec/step)\n",
            "I1229 11:04:04.010479 140034317584256 learning.py:507] global step 782: loss = 3.4303 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 783: loss = 2.2133 (1.210 sec/step)\n",
            "I1229 11:04:05.222388 140034317584256 learning.py:507] global step 783: loss = 2.2133 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 784: loss = 3.6044 (1.182 sec/step)\n",
            "I1229 11:04:06.405994 140034317584256 learning.py:507] global step 784: loss = 3.6044 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 785: loss = 4.4794 (1.183 sec/step)\n",
            "I1229 11:04:07.590622 140034317584256 learning.py:507] global step 785: loss = 4.4794 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 786: loss = 4.1373 (1.195 sec/step)\n",
            "I1229 11:04:08.787144 140034317584256 learning.py:507] global step 786: loss = 4.1373 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 787: loss = 2.5675 (1.212 sec/step)\n",
            "I1229 11:04:10.001057 140034317584256 learning.py:507] global step 787: loss = 2.5675 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 788: loss = 2.7574 (1.219 sec/step)\n",
            "I1229 11:04:11.222115 140034317584256 learning.py:507] global step 788: loss = 2.7574 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 789: loss = 3.0970 (1.178 sec/step)\n",
            "I1229 11:04:12.402031 140034317584256 learning.py:507] global step 789: loss = 3.0970 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 790: loss = 4.4773 (1.167 sec/step)\n",
            "I1229 11:04:13.570820 140034317584256 learning.py:507] global step 790: loss = 4.4773 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 791: loss = 2.8346 (1.227 sec/step)\n",
            "I1229 11:04:14.799071 140034317584256 learning.py:507] global step 791: loss = 2.8346 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 792: loss = 2.3931 (1.238 sec/step)\n",
            "I1229 11:04:16.038499 140034317584256 learning.py:507] global step 792: loss = 2.3931 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 793: loss = 3.3587 (1.143 sec/step)\n",
            "I1229 11:04:17.182796 140034317584256 learning.py:507] global step 793: loss = 3.3587 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 794: loss = 3.1968 (1.204 sec/step)\n",
            "I1229 11:04:18.388587 140034317584256 learning.py:507] global step 794: loss = 3.1968 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 795: loss = 2.5556 (1.192 sec/step)\n",
            "I1229 11:04:19.582734 140034317584256 learning.py:507] global step 795: loss = 2.5556 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 796: loss = 3.1353 (1.148 sec/step)\n",
            "I1229 11:04:20.732484 140034317584256 learning.py:507] global step 796: loss = 3.1353 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 797: loss = 1.6943 (1.241 sec/step)\n",
            "I1229 11:04:21.975507 140034317584256 learning.py:507] global step 797: loss = 1.6943 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 798: loss = 3.0101 (1.235 sec/step)\n",
            "I1229 11:04:23.212836 140034317584256 learning.py:507] global step 798: loss = 3.0101 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 799: loss = 3.1866 (1.268 sec/step)\n",
            "I1229 11:04:24.483097 140034317584256 learning.py:507] global step 799: loss = 3.1866 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 1.4051 (1.268 sec/step)\n",
            "I1229 11:04:25.752679 140034317584256 learning.py:507] global step 800: loss = 1.4051 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 801: loss = 2.8813 (1.233 sec/step)\n",
            "I1229 11:04:26.987823 140034317584256 learning.py:507] global step 801: loss = 2.8813 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 802: loss = 2.9460 (1.319 sec/step)\n",
            "I1229 11:04:28.308998 140034317584256 learning.py:507] global step 802: loss = 2.9460 (1.319 sec/step)\n",
            "INFO:tensorflow:global step 803: loss = 1.7540 (1.274 sec/step)\n",
            "I1229 11:04:29.584790 140034317584256 learning.py:507] global step 803: loss = 1.7540 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 804: loss = 3.8630 (1.256 sec/step)\n",
            "I1229 11:04:30.841925 140034317584256 learning.py:507] global step 804: loss = 3.8630 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 805: loss = 3.9502 (1.229 sec/step)\n",
            "I1229 11:04:32.072174 140034317584256 learning.py:507] global step 805: loss = 3.9502 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 806: loss = 2.1712 (1.218 sec/step)\n",
            "I1229 11:04:33.291558 140034317584256 learning.py:507] global step 806: loss = 2.1712 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 807: loss = 2.3751 (1.165 sec/step)\n",
            "I1229 11:04:34.458098 140034317584256 learning.py:507] global step 807: loss = 2.3751 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 808: loss = 2.3670 (1.209 sec/step)\n",
            "I1229 11:04:35.669238 140034317584256 learning.py:507] global step 808: loss = 2.3670 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 809: loss = 2.4638 (1.203 sec/step)\n",
            "I1229 11:04:36.874311 140034317584256 learning.py:507] global step 809: loss = 2.4638 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 810: loss = 3.0931 (1.186 sec/step)\n",
            "I1229 11:04:38.061550 140034317584256 learning.py:507] global step 810: loss = 3.0931 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 811: loss = 2.4712 (1.164 sec/step)\n",
            "I1229 11:04:39.227323 140034317584256 learning.py:507] global step 811: loss = 2.4712 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 812: loss = 3.2178 (1.177 sec/step)\n",
            "I1229 11:04:40.406441 140034317584256 learning.py:507] global step 812: loss = 3.2178 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 813: loss = 2.5316 (1.189 sec/step)\n",
            "I1229 11:04:41.596803 140034317584256 learning.py:507] global step 813: loss = 2.5316 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 814: loss = 4.3995 (1.159 sec/step)\n",
            "I1229 11:04:42.757687 140034317584256 learning.py:507] global step 814: loss = 4.3995 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 815: loss = 3.4319 (1.191 sec/step)\n",
            "I1229 11:04:43.950159 140034317584256 learning.py:507] global step 815: loss = 3.4319 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 816: loss = 4.0038 (1.205 sec/step)\n",
            "I1229 11:04:45.156949 140034317584256 learning.py:507] global step 816: loss = 4.0038 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 817: loss = 5.1346 (1.173 sec/step)\n",
            "I1229 11:04:46.331572 140034317584256 learning.py:507] global step 817: loss = 5.1346 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 818: loss = 4.3342 (1.148 sec/step)\n",
            "I1229 11:04:47.480900 140034317584256 learning.py:507] global step 818: loss = 4.3342 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 819: loss = 2.6220 (1.186 sec/step)\n",
            "I1229 11:04:48.667883 140034317584256 learning.py:507] global step 819: loss = 2.6220 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 820: loss = 3.0758 (1.142 sec/step)\n",
            "I1229 11:04:49.812048 140034317584256 learning.py:507] global step 820: loss = 3.0758 (1.142 sec/step)\n",
            "INFO:tensorflow:global step 821: loss = 3.8747 (1.174 sec/step)\n",
            "I1229 11:04:50.987998 140034317584256 learning.py:507] global step 821: loss = 3.8747 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 822: loss = 2.5121 (1.194 sec/step)\n",
            "I1229 11:04:52.184176 140034317584256 learning.py:507] global step 822: loss = 2.5121 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 823: loss = 2.9710 (1.202 sec/step)\n",
            "I1229 11:04:53.387636 140034317584256 learning.py:507] global step 823: loss = 2.9710 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 824: loss = 3.7720 (1.187 sec/step)\n",
            "I1229 11:04:54.576736 140034317584256 learning.py:507] global step 824: loss = 3.7720 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 825: loss = 2.7998 (1.206 sec/step)\n",
            "I1229 11:04:55.784689 140034317584256 learning.py:507] global step 825: loss = 2.7998 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 826: loss = 2.9117 (1.247 sec/step)\n",
            "I1229 11:04:57.033752 140034317584256 learning.py:507] global step 826: loss = 2.9117 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 827: loss = 3.4132 (1.180 sec/step)\n",
            "I1229 11:04:58.215314 140034317584256 learning.py:507] global step 827: loss = 3.4132 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 828: loss = 2.9576 (1.220 sec/step)\n",
            "I1229 11:04:59.437257 140034317584256 learning.py:507] global step 828: loss = 2.9576 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 829: loss = 3.4468 (1.209 sec/step)\n",
            "I1229 11:05:00.647596 140034317584256 learning.py:507] global step 829: loss = 3.4468 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 830: loss = 2.4014 (1.183 sec/step)\n",
            "I1229 11:05:01.832188 140034317584256 learning.py:507] global step 830: loss = 2.4014 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 831: loss = 4.0437 (1.186 sec/step)\n",
            "I1229 11:05:03.019830 140034317584256 learning.py:507] global step 831: loss = 4.0437 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 832: loss = 2.0517 (1.211 sec/step)\n",
            "I1229 11:05:04.231948 140034317584256 learning.py:507] global step 832: loss = 2.0517 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 833: loss = 2.7305 (1.198 sec/step)\n",
            "I1229 11:05:05.431943 140034317584256 learning.py:507] global step 833: loss = 2.7305 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 834: loss = 2.8349 (1.212 sec/step)\n",
            "I1229 11:05:06.645957 140034317584256 learning.py:507] global step 834: loss = 2.8349 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 835: loss = 2.1696 (1.207 sec/step)\n",
            "I1229 11:05:07.854367 140034317584256 learning.py:507] global step 835: loss = 2.1696 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 836: loss = 3.4126 (1.228 sec/step)\n",
            "I1229 11:05:09.083915 140034317584256 learning.py:507] global step 836: loss = 3.4126 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 837: loss = 3.1555 (1.200 sec/step)\n",
            "I1229 11:05:10.285504 140034317584256 learning.py:507] global step 837: loss = 3.1555 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 838: loss = 2.4874 (1.169 sec/step)\n",
            "I1229 11:05:11.456428 140034317584256 learning.py:507] global step 838: loss = 2.4874 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 839: loss = 4.5249 (1.132 sec/step)\n",
            "I1229 11:05:12.590190 140034317584256 learning.py:507] global step 839: loss = 4.5249 (1.132 sec/step)\n",
            "INFO:tensorflow:global step 840: loss = 3.1231 (1.157 sec/step)\n",
            "I1229 11:05:13.748500 140034317584256 learning.py:507] global step 840: loss = 3.1231 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 841: loss = 2.6311 (1.207 sec/step)\n",
            "I1229 11:05:14.957691 140034317584256 learning.py:507] global step 841: loss = 2.6311 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 842: loss = 3.2383 (1.236 sec/step)\n",
            "I1229 11:05:16.195333 140034317584256 learning.py:507] global step 842: loss = 3.2383 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 843: loss = 3.1581 (1.229 sec/step)\n",
            "I1229 11:05:17.426122 140034317584256 learning.py:507] global step 843: loss = 3.1581 (1.229 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 843.\n",
            "I1229 11:05:19.393431 140030580766464 supervisor.py:1050] Recording summary at step 843.\n",
            "INFO:tensorflow:global step 844: loss = 1.5448 (2.150 sec/step)\n",
            "I1229 11:05:19.578304 140034317584256 learning.py:507] global step 844: loss = 1.5448 (2.150 sec/step)\n",
            "INFO:tensorflow:global step 845: loss = 3.0494 (1.182 sec/step)\n",
            "I1229 11:05:20.761793 140034317584256 learning.py:507] global step 845: loss = 3.0494 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 846: loss = 3.2609 (1.221 sec/step)\n",
            "I1229 11:05:21.984318 140034317584256 learning.py:507] global step 846: loss = 3.2609 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 847: loss = 2.8130 (1.187 sec/step)\n",
            "I1229 11:05:23.172646 140034317584256 learning.py:507] global step 847: loss = 2.8130 (1.187 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.817115\n",
            "I1229 11:05:23.920187 140030555588352 supervisor.py:1099] global_step/sec: 0.817115\n",
            "INFO:tensorflow:global step 848: loss = 3.1494 (1.198 sec/step)\n",
            "I1229 11:05:24.371987 140034317584256 learning.py:507] global step 848: loss = 3.1494 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 849: loss = 2.4405 (1.212 sec/step)\n",
            "I1229 11:05:25.585097 140034317584256 learning.py:507] global step 849: loss = 2.4405 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 850: loss = 4.6676 (1.193 sec/step)\n",
            "I1229 11:05:26.779558 140034317584256 learning.py:507] global step 850: loss = 4.6676 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 851: loss = 3.1723 (1.261 sec/step)\n",
            "I1229 11:05:28.042960 140034317584256 learning.py:507] global step 851: loss = 3.1723 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 852: loss = 2.4694 (1.233 sec/step)\n",
            "I1229 11:05:29.277976 140034317584256 learning.py:507] global step 852: loss = 2.4694 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 853: loss = 3.3294 (1.226 sec/step)\n",
            "I1229 11:05:30.505805 140034317584256 learning.py:507] global step 853: loss = 3.3294 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 854: loss = 2.3892 (1.192 sec/step)\n",
            "I1229 11:05:31.699125 140034317584256 learning.py:507] global step 854: loss = 2.3892 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 855: loss = 2.3694 (1.194 sec/step)\n",
            "I1229 11:05:32.895396 140034317584256 learning.py:507] global step 855: loss = 2.3694 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 856: loss = 3.2259 (1.200 sec/step)\n",
            "I1229 11:05:34.097016 140034317584256 learning.py:507] global step 856: loss = 3.2259 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 857: loss = 4.2426 (1.208 sec/step)\n",
            "I1229 11:05:35.306293 140034317584256 learning.py:507] global step 857: loss = 4.2426 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 858: loss = 2.5404 (1.181 sec/step)\n",
            "I1229 11:05:36.489387 140034317584256 learning.py:507] global step 858: loss = 2.5404 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 859: loss = 4.2599 (1.177 sec/step)\n",
            "I1229 11:05:37.668271 140034317584256 learning.py:507] global step 859: loss = 4.2599 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 860: loss = 2.4102 (1.180 sec/step)\n",
            "I1229 11:05:38.849889 140034317584256 learning.py:507] global step 860: loss = 2.4102 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 861: loss = 3.2587 (1.190 sec/step)\n",
            "I1229 11:05:40.041320 140034317584256 learning.py:507] global step 861: loss = 3.2587 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 862: loss = 3.1714 (1.165 sec/step)\n",
            "I1229 11:05:41.208436 140034317584256 learning.py:507] global step 862: loss = 3.1714 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 863: loss = 2.3634 (1.159 sec/step)\n",
            "I1229 11:05:42.369153 140034317584256 learning.py:507] global step 863: loss = 2.3634 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 864: loss = 2.7657 (1.190 sec/step)\n",
            "I1229 11:05:43.560602 140034317584256 learning.py:507] global step 864: loss = 2.7657 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 865: loss = 2.7032 (1.194 sec/step)\n",
            "I1229 11:05:44.756718 140034317584256 learning.py:507] global step 865: loss = 2.7032 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 866: loss = 2.3387 (1.240 sec/step)\n",
            "I1229 11:05:45.998362 140034317584256 learning.py:507] global step 866: loss = 2.3387 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 867: loss = 3.5587 (1.173 sec/step)\n",
            "I1229 11:05:47.172631 140034317584256 learning.py:507] global step 867: loss = 3.5587 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 868: loss = 2.9704 (1.208 sec/step)\n",
            "I1229 11:05:48.382570 140034317584256 learning.py:507] global step 868: loss = 2.9704 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 869: loss = 2.5289 (1.237 sec/step)\n",
            "I1229 11:05:49.621095 140034317584256 learning.py:507] global step 869: loss = 2.5289 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 870: loss = 3.7175 (1.157 sec/step)\n",
            "I1229 11:05:50.780102 140034317584256 learning.py:507] global step 870: loss = 3.7175 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 871: loss = 1.9292 (1.199 sec/step)\n",
            "I1229 11:05:51.980935 140034317584256 learning.py:507] global step 871: loss = 1.9292 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 872: loss = 3.2476 (1.203 sec/step)\n",
            "I1229 11:05:53.186338 140034317584256 learning.py:507] global step 872: loss = 3.2476 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 873: loss = 2.0556 (1.178 sec/step)\n",
            "I1229 11:05:54.366046 140034317584256 learning.py:507] global step 873: loss = 2.0556 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 874: loss = 2.7140 (1.191 sec/step)\n",
            "I1229 11:05:55.558624 140034317584256 learning.py:507] global step 874: loss = 2.7140 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 875: loss = 2.9303 (1.190 sec/step)\n",
            "I1229 11:05:56.749935 140034317584256 learning.py:507] global step 875: loss = 2.9303 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 876: loss = 2.5255 (1.159 sec/step)\n",
            "I1229 11:05:57.910178 140034317584256 learning.py:507] global step 876: loss = 2.5255 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 877: loss = 2.7861 (1.155 sec/step)\n",
            "I1229 11:05:59.066724 140034317584256 learning.py:507] global step 877: loss = 2.7861 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 878: loss = 2.9756 (1.174 sec/step)\n",
            "I1229 11:06:00.242594 140034317584256 learning.py:507] global step 878: loss = 2.9756 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 879: loss = 3.5873 (1.186 sec/step)\n",
            "I1229 11:06:01.430473 140034317584256 learning.py:507] global step 879: loss = 3.5873 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 880: loss = 3.0807 (1.174 sec/step)\n",
            "I1229 11:06:02.606414 140034317584256 learning.py:507] global step 880: loss = 3.0807 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 881: loss = 2.7857 (1.147 sec/step)\n",
            "I1229 11:06:03.754980 140034317584256 learning.py:507] global step 881: loss = 2.7857 (1.147 sec/step)\n",
            "INFO:tensorflow:global step 882: loss = 2.0125 (1.195 sec/step)\n",
            "I1229 11:06:04.951989 140034317584256 learning.py:507] global step 882: loss = 2.0125 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 883: loss = 2.1007 (1.156 sec/step)\n",
            "I1229 11:06:06.110444 140034317584256 learning.py:507] global step 883: loss = 2.1007 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 884: loss = 3.0933 (1.173 sec/step)\n",
            "I1229 11:06:07.285190 140034317584256 learning.py:507] global step 884: loss = 3.0933 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 885: loss = 3.6852 (1.194 sec/step)\n",
            "I1229 11:06:08.481338 140034317584256 learning.py:507] global step 885: loss = 3.6852 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 886: loss = 3.7449 (1.162 sec/step)\n",
            "I1229 11:06:09.645126 140034317584256 learning.py:507] global step 886: loss = 3.7449 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 887: loss = 2.5604 (1.216 sec/step)\n",
            "I1229 11:06:10.862267 140034317584256 learning.py:507] global step 887: loss = 2.5604 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 888: loss = 3.8476 (1.203 sec/step)\n",
            "I1229 11:06:12.067346 140034317584256 learning.py:507] global step 888: loss = 3.8476 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 889: loss = 3.0448 (1.173 sec/step)\n",
            "I1229 11:06:13.241851 140034317584256 learning.py:507] global step 889: loss = 3.0448 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 890: loss = 4.5975 (1.243 sec/step)\n",
            "I1229 11:06:14.486553 140034317584256 learning.py:507] global step 890: loss = 4.5975 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 891: loss = 3.0098 (1.172 sec/step)\n",
            "I1229 11:06:15.659967 140034317584256 learning.py:507] global step 891: loss = 3.0098 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 892: loss = 1.8913 (1.179 sec/step)\n",
            "I1229 11:06:16.840615 140034317584256 learning.py:507] global step 892: loss = 1.8913 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 893: loss = 1.9730 (1.224 sec/step)\n",
            "I1229 11:06:18.065748 140034317584256 learning.py:507] global step 893: loss = 1.9730 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 894: loss = 2.2305 (1.209 sec/step)\n",
            "I1229 11:06:19.276950 140034317584256 learning.py:507] global step 894: loss = 2.2305 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 895: loss = 1.8542 (1.257 sec/step)\n",
            "I1229 11:06:20.535827 140034317584256 learning.py:507] global step 895: loss = 1.8542 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 896: loss = 2.4465 (1.243 sec/step)\n",
            "I1229 11:06:21.780686 140034317584256 learning.py:507] global step 896: loss = 2.4465 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 897: loss = 3.8629 (1.170 sec/step)\n",
            "I1229 11:06:22.952092 140034317584256 learning.py:507] global step 897: loss = 3.8629 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 898: loss = 3.1340 (1.199 sec/step)\n",
            "I1229 11:06:24.153119 140034317584256 learning.py:507] global step 898: loss = 3.1340 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 899: loss = 2.8526 (1.164 sec/step)\n",
            "I1229 11:06:25.319044 140034317584256 learning.py:507] global step 899: loss = 2.8526 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 2.5554 (1.150 sec/step)\n",
            "I1229 11:06:26.470814 140034317584256 learning.py:507] global step 900: loss = 2.5554 (1.150 sec/step)\n",
            "INFO:tensorflow:global step 901: loss = 2.8352 (1.184 sec/step)\n",
            "I1229 11:06:27.656198 140034317584256 learning.py:507] global step 901: loss = 2.8352 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 902: loss = 3.0659 (1.166 sec/step)\n",
            "I1229 11:06:28.823503 140034317584256 learning.py:507] global step 902: loss = 3.0659 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 903: loss = 4.8091 (1.197 sec/step)\n",
            "I1229 11:06:30.022000 140034317584256 learning.py:507] global step 903: loss = 4.8091 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 904: loss = 4.2095 (1.207 sec/step)\n",
            "I1229 11:06:31.231157 140034317584256 learning.py:507] global step 904: loss = 4.2095 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 905: loss = 2.7853 (1.184 sec/step)\n",
            "I1229 11:06:32.417165 140034317584256 learning.py:507] global step 905: loss = 2.7853 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 906: loss = 2.7051 (1.158 sec/step)\n",
            "I1229 11:06:33.576609 140034317584256 learning.py:507] global step 906: loss = 2.7051 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 907: loss = 2.7865 (1.160 sec/step)\n",
            "I1229 11:06:34.738188 140034317584256 learning.py:507] global step 907: loss = 2.7865 (1.160 sec/step)\n",
            "INFO:tensorflow:global step 908: loss = 2.6617 (1.189 sec/step)\n",
            "I1229 11:06:35.928993 140034317584256 learning.py:507] global step 908: loss = 2.6617 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 909: loss = 3.7131 (1.157 sec/step)\n",
            "I1229 11:06:37.087945 140034317584256 learning.py:507] global step 909: loss = 3.7131 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 910: loss = 2.9932 (1.156 sec/step)\n",
            "I1229 11:06:38.245975 140034317584256 learning.py:507] global step 910: loss = 2.9932 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 911: loss = 2.8108 (1.141 sec/step)\n",
            "I1229 11:06:39.388760 140034317584256 learning.py:507] global step 911: loss = 2.8108 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 912: loss = 3.3154 (1.174 sec/step)\n",
            "I1229 11:06:40.563912 140034317584256 learning.py:507] global step 912: loss = 3.3154 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 913: loss = 4.2433 (1.200 sec/step)\n",
            "I1229 11:06:41.765886 140034317584256 learning.py:507] global step 913: loss = 4.2433 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 914: loss = 3.4188 (1.169 sec/step)\n",
            "I1229 11:06:42.936110 140034317584256 learning.py:507] global step 914: loss = 3.4188 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 915: loss = 3.3916 (1.190 sec/step)\n",
            "I1229 11:06:44.127694 140034317584256 learning.py:507] global step 915: loss = 3.3916 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 916: loss = 1.8236 (1.184 sec/step)\n",
            "I1229 11:06:45.313971 140034317584256 learning.py:507] global step 916: loss = 1.8236 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 917: loss = 2.9565 (1.175 sec/step)\n",
            "I1229 11:06:46.490967 140034317584256 learning.py:507] global step 917: loss = 2.9565 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 918: loss = 2.5266 (1.158 sec/step)\n",
            "I1229 11:06:47.650723 140034317584256 learning.py:507] global step 918: loss = 2.5266 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 919: loss = 2.4878 (1.201 sec/step)\n",
            "I1229 11:06:48.853988 140034317584256 learning.py:507] global step 919: loss = 2.4878 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 920: loss = 2.3962 (1.176 sec/step)\n",
            "I1229 11:06:50.031759 140034317584256 learning.py:507] global step 920: loss = 2.3962 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 921: loss = 1.8523 (1.182 sec/step)\n",
            "I1229 11:06:51.215376 140034317584256 learning.py:507] global step 921: loss = 1.8523 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 922: loss = 2.5421 (1.210 sec/step)\n",
            "I1229 11:06:52.427131 140034317584256 learning.py:507] global step 922: loss = 2.5421 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 923: loss = 2.2369 (1.169 sec/step)\n",
            "I1229 11:06:53.597840 140034317584256 learning.py:507] global step 923: loss = 2.2369 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 924: loss = 4.2153 (1.178 sec/step)\n",
            "I1229 11:06:54.777243 140034317584256 learning.py:507] global step 924: loss = 4.2153 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 925: loss = 3.5840 (1.192 sec/step)\n",
            "I1229 11:06:55.970883 140034317584256 learning.py:507] global step 925: loss = 3.5840 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 926: loss = 2.4384 (1.161 sec/step)\n",
            "I1229 11:06:57.133751 140034317584256 learning.py:507] global step 926: loss = 2.4384 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 927: loss = 2.6263 (1.176 sec/step)\n",
            "I1229 11:06:58.311472 140034317584256 learning.py:507] global step 927: loss = 2.6263 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 928: loss = 2.5682 (1.161 sec/step)\n",
            "I1229 11:06:59.474317 140034317584256 learning.py:507] global step 928: loss = 2.5682 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 929: loss = 1.4696 (1.240 sec/step)\n",
            "I1229 11:07:00.716120 140034317584256 learning.py:507] global step 929: loss = 1.4696 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 930: loss = 2.9868 (1.210 sec/step)\n",
            "I1229 11:07:01.927715 140034317584256 learning.py:507] global step 930: loss = 2.9868 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 931: loss = 2.8427 (1.342 sec/step)\n",
            "I1229 11:07:03.271066 140034317584256 learning.py:507] global step 931: loss = 2.8427 (1.342 sec/step)\n",
            "INFO:tensorflow:global step 932: loss = 2.4440 (1.168 sec/step)\n",
            "I1229 11:07:04.440944 140034317584256 learning.py:507] global step 932: loss = 2.4440 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 933: loss = 1.9473 (1.211 sec/step)\n",
            "I1229 11:07:05.653870 140034317584256 learning.py:507] global step 933: loss = 1.9473 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 934: loss = 2.5741 (1.180 sec/step)\n",
            "I1229 11:07:06.835913 140034317584256 learning.py:507] global step 934: loss = 2.5741 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 935: loss = 2.4197 (1.181 sec/step)\n",
            "I1229 11:07:08.018992 140034317584256 learning.py:507] global step 935: loss = 2.4197 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 936: loss = 3.2140 (1.232 sec/step)\n",
            "I1229 11:07:09.252351 140034317584256 learning.py:507] global step 936: loss = 3.2140 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 937: loss = 2.6879 (1.213 sec/step)\n",
            "I1229 11:07:10.467179 140034317584256 learning.py:507] global step 937: loss = 2.6879 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 938: loss = 3.2490 (1.184 sec/step)\n",
            "I1229 11:07:11.652725 140034317584256 learning.py:507] global step 938: loss = 3.2490 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 939: loss = 5.3069 (1.182 sec/step)\n",
            "I1229 11:07:12.835927 140034317584256 learning.py:507] global step 939: loss = 5.3069 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 940: loss = 2.3797 (1.206 sec/step)\n",
            "I1229 11:07:14.043262 140034317584256 learning.py:507] global step 940: loss = 2.3797 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 941: loss = 3.6968 (1.171 sec/step)\n",
            "I1229 11:07:15.215785 140034317584256 learning.py:507] global step 941: loss = 3.6968 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 942: loss = 2.7863 (1.184 sec/step)\n",
            "I1229 11:07:16.400768 140034317584256 learning.py:507] global step 942: loss = 2.7863 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 943: loss = 2.4076 (1.211 sec/step)\n",
            "I1229 11:07:17.613242 140034317584256 learning.py:507] global step 943: loss = 2.4076 (1.211 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 11:07:17.668582 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 943.\n",
            "I1229 11:07:20.168404 140030580766464 supervisor.py:1050] Recording summary at step 943.\n",
            "INFO:tensorflow:global step 944: loss = 2.9787 (2.699 sec/step)\n",
            "I1229 11:07:20.319447 140034317584256 learning.py:507] global step 944: loss = 2.9787 (2.699 sec/step)\n",
            "INFO:tensorflow:global step 945: loss = 2.4725 (1.982 sec/step)\n",
            "I1229 11:07:22.319109 140034317584256 learning.py:507] global step 945: loss = 2.4725 (1.982 sec/step)\n",
            "INFO:tensorflow:global step 946: loss = 2.1479 (1.629 sec/step)\n",
            "I1229 11:07:24.305869 140034317584256 learning.py:507] global step 946: loss = 2.1479 (1.629 sec/step)\n",
            "INFO:tensorflow:global step 947: loss = 1.4618 (1.143 sec/step)\n",
            "I1229 11:07:25.623652 140034317584256 learning.py:507] global step 947: loss = 1.4618 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 948: loss = 2.6174 (1.191 sec/step)\n",
            "I1229 11:07:26.816330 140034317584256 learning.py:507] global step 948: loss = 2.6174 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 949: loss = 3.4477 (1.202 sec/step)\n",
            "I1229 11:07:28.020374 140034317584256 learning.py:507] global step 949: loss = 3.4477 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 950: loss = 2.0322 (1.254 sec/step)\n",
            "I1229 11:07:29.275818 140034317584256 learning.py:507] global step 950: loss = 2.0322 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 951: loss = 2.0440 (1.375 sec/step)\n",
            "I1229 11:07:30.652072 140034317584256 learning.py:507] global step 951: loss = 2.0440 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 952: loss = 3.4777 (1.225 sec/step)\n",
            "I1229 11:07:31.878980 140034317584256 learning.py:507] global step 952: loss = 3.4777 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 953: loss = 4.1797 (1.173 sec/step)\n",
            "I1229 11:07:33.054089 140034317584256 learning.py:507] global step 953: loss = 4.1797 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 954: loss = 2.0590 (1.185 sec/step)\n",
            "I1229 11:07:34.240370 140034317584256 learning.py:507] global step 954: loss = 2.0590 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 955: loss = 2.3263 (1.250 sec/step)\n",
            "I1229 11:07:35.492132 140034317584256 learning.py:507] global step 955: loss = 2.3263 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 956: loss = 1.8043 (1.171 sec/step)\n",
            "I1229 11:07:36.664692 140034317584256 learning.py:507] global step 956: loss = 1.8043 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 957: loss = 4.1016 (1.133 sec/step)\n",
            "I1229 11:07:37.799084 140034317584256 learning.py:507] global step 957: loss = 4.1016 (1.133 sec/step)\n",
            "INFO:tensorflow:global step 958: loss = 3.6842 (1.233 sec/step)\n",
            "I1229 11:07:39.033402 140034317584256 learning.py:507] global step 958: loss = 3.6842 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 959: loss = 4.3265 (1.181 sec/step)\n",
            "I1229 11:07:40.215598 140034317584256 learning.py:507] global step 959: loss = 4.3265 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 960: loss = 2.8188 (1.217 sec/step)\n",
            "I1229 11:07:41.434528 140034317584256 learning.py:507] global step 960: loss = 2.8188 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 961: loss = 3.0225 (1.137 sec/step)\n",
            "I1229 11:07:42.573275 140034317584256 learning.py:507] global step 961: loss = 3.0225 (1.137 sec/step)\n",
            "INFO:tensorflow:global step 962: loss = 2.1570 (1.159 sec/step)\n",
            "I1229 11:07:43.733831 140034317584256 learning.py:507] global step 962: loss = 2.1570 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 963: loss = 2.6598 (1.201 sec/step)\n",
            "I1229 11:07:44.936974 140034317584256 learning.py:507] global step 963: loss = 2.6598 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 964: loss = 2.8419 (1.170 sec/step)\n",
            "I1229 11:07:46.108664 140034317584256 learning.py:507] global step 964: loss = 2.8419 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 965: loss = 2.9602 (1.189 sec/step)\n",
            "I1229 11:07:47.298789 140034317584256 learning.py:507] global step 965: loss = 2.9602 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 966: loss = 3.6565 (1.212 sec/step)\n",
            "I1229 11:07:48.512244 140034317584256 learning.py:507] global step 966: loss = 3.6565 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 967: loss = 3.0494 (1.153 sec/step)\n",
            "I1229 11:07:49.667546 140034317584256 learning.py:507] global step 967: loss = 3.0494 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 968: loss = 2.4959 (1.159 sec/step)\n",
            "I1229 11:07:50.828191 140034317584256 learning.py:507] global step 968: loss = 2.4959 (1.159 sec/step)\n",
            "INFO:tensorflow:global step 969: loss = 4.2074 (1.161 sec/step)\n",
            "I1229 11:07:51.990936 140034317584256 learning.py:507] global step 969: loss = 4.2074 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 970: loss = 3.1452 (1.195 sec/step)\n",
            "I1229 11:07:53.187581 140034317584256 learning.py:507] global step 970: loss = 3.1452 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 971: loss = 2.6554 (1.183 sec/step)\n",
            "I1229 11:07:54.371881 140034317584256 learning.py:507] global step 971: loss = 2.6554 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 972: loss = 2.0320 (1.199 sec/step)\n",
            "I1229 11:07:55.572489 140034317584256 learning.py:507] global step 972: loss = 2.0320 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 973: loss = 3.4534 (1.189 sec/step)\n",
            "I1229 11:07:56.763196 140034317584256 learning.py:507] global step 973: loss = 3.4534 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 974: loss = 1.9350 (1.162 sec/step)\n",
            "I1229 11:07:57.926390 140034317584256 learning.py:507] global step 974: loss = 1.9350 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 975: loss = 1.9593 (1.175 sec/step)\n",
            "I1229 11:07:59.102959 140034317584256 learning.py:507] global step 975: loss = 1.9593 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 976: loss = 3.4188 (1.164 sec/step)\n",
            "I1229 11:08:00.268743 140034317584256 learning.py:507] global step 976: loss = 3.4188 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 977: loss = 6.2610 (1.188 sec/step)\n",
            "I1229 11:08:01.457813 140034317584256 learning.py:507] global step 977: loss = 6.2610 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 978: loss = 2.5115 (1.189 sec/step)\n",
            "I1229 11:08:02.648406 140034317584256 learning.py:507] global step 978: loss = 2.5115 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 979: loss = 2.5417 (1.135 sec/step)\n",
            "I1229 11:08:03.785461 140034317584256 learning.py:507] global step 979: loss = 2.5417 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 980: loss = 2.9819 (1.180 sec/step)\n",
            "I1229 11:08:04.967113 140034317584256 learning.py:507] global step 980: loss = 2.9819 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 981: loss = 5.9098 (1.221 sec/step)\n",
            "I1229 11:08:06.189125 140034317584256 learning.py:507] global step 981: loss = 5.9098 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 982: loss = 3.0941 (1.177 sec/step)\n",
            "I1229 11:08:07.367506 140034317584256 learning.py:507] global step 982: loss = 3.0941 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 983: loss = 2.2399 (1.190 sec/step)\n",
            "I1229 11:08:08.559145 140034317584256 learning.py:507] global step 983: loss = 2.2399 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 984: loss = 3.1764 (1.223 sec/step)\n",
            "I1229 11:08:09.784186 140034317584256 learning.py:507] global step 984: loss = 3.1764 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 985: loss = 2.0585 (1.192 sec/step)\n",
            "I1229 11:08:10.977855 140034317584256 learning.py:507] global step 985: loss = 2.0585 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 986: loss = 3.3871 (1.198 sec/step)\n",
            "I1229 11:08:12.177494 140034317584256 learning.py:507] global step 986: loss = 3.3871 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 987: loss = 1.8489 (1.167 sec/step)\n",
            "I1229 11:08:13.346673 140034317584256 learning.py:507] global step 987: loss = 1.8489 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 988: loss = 2.1868 (1.178 sec/step)\n",
            "I1229 11:08:14.526560 140034317584256 learning.py:507] global step 988: loss = 2.1868 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 989: loss = 4.1698 (1.173 sec/step)\n",
            "I1229 11:08:15.701347 140034317584256 learning.py:507] global step 989: loss = 4.1698 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 990: loss = 3.0750 (1.160 sec/step)\n",
            "I1229 11:08:16.862755 140034317584256 learning.py:507] global step 990: loss = 3.0750 (1.160 sec/step)\n",
            "INFO:tensorflow:global step 991: loss = 2.7127 (1.242 sec/step)\n",
            "I1229 11:08:18.106558 140034317584256 learning.py:507] global step 991: loss = 2.7127 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 992: loss = 3.0516 (1.216 sec/step)\n",
            "I1229 11:08:19.324258 140034317584256 learning.py:507] global step 992: loss = 3.0516 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 993: loss = 3.1491 (1.181 sec/step)\n",
            "I1229 11:08:20.506728 140034317584256 learning.py:507] global step 993: loss = 3.1491 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 994: loss = 2.9482 (1.218 sec/step)\n",
            "I1229 11:08:21.727022 140034317584256 learning.py:507] global step 994: loss = 2.9482 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 995: loss = 2.0860 (1.191 sec/step)\n",
            "I1229 11:08:22.920191 140034317584256 learning.py:507] global step 995: loss = 2.0860 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 996: loss = 2.1744 (1.171 sec/step)\n",
            "I1229 11:08:24.092415 140034317584256 learning.py:507] global step 996: loss = 2.1744 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 997: loss = 2.4739 (1.153 sec/step)\n",
            "I1229 11:08:25.247188 140034317584256 learning.py:507] global step 997: loss = 2.4739 (1.153 sec/step)\n",
            "INFO:tensorflow:global step 998: loss = 2.9697 (1.179 sec/step)\n",
            "I1229 11:08:26.427986 140034317584256 learning.py:507] global step 998: loss = 2.9697 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 999: loss = 2.7826 (1.226 sec/step)\n",
            "I1229 11:08:27.655516 140034317584256 learning.py:507] global step 999: loss = 2.7826 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 2.3706 (1.237 sec/step)\n",
            "I1229 11:08:28.894046 140034317584256 learning.py:507] global step 1000: loss = 2.3706 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1001: loss = 3.1349 (1.206 sec/step)\n",
            "I1229 11:08:30.102005 140034317584256 learning.py:507] global step 1001: loss = 3.1349 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1002: loss = 3.4142 (1.197 sec/step)\n",
            "I1229 11:08:31.300155 140034317584256 learning.py:507] global step 1002: loss = 3.4142 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1003: loss = 3.5879 (1.212 sec/step)\n",
            "I1229 11:08:32.514103 140034317584256 learning.py:507] global step 1003: loss = 3.5879 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1004: loss = 3.7006 (1.184 sec/step)\n",
            "I1229 11:08:33.700016 140034317584256 learning.py:507] global step 1004: loss = 3.7006 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1005: loss = 2.9520 (1.182 sec/step)\n",
            "I1229 11:08:34.883794 140034317584256 learning.py:507] global step 1005: loss = 2.9520 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1006: loss = 2.9490 (1.164 sec/step)\n",
            "I1229 11:08:36.049805 140034317584256 learning.py:507] global step 1006: loss = 2.9490 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 1007: loss = 2.8186 (1.175 sec/step)\n",
            "I1229 11:08:37.226138 140034317584256 learning.py:507] global step 1007: loss = 2.8186 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1008: loss = 2.4441 (1.203 sec/step)\n",
            "I1229 11:08:38.430962 140034317584256 learning.py:507] global step 1008: loss = 2.4441 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1009: loss = 1.9277 (1.181 sec/step)\n",
            "I1229 11:08:39.613488 140034317584256 learning.py:507] global step 1009: loss = 1.9277 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1010: loss = 2.3769 (1.231 sec/step)\n",
            "I1229 11:08:40.846137 140034317584256 learning.py:507] global step 1010: loss = 2.3769 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1011: loss = 2.2519 (1.208 sec/step)\n",
            "I1229 11:08:42.055670 140034317584256 learning.py:507] global step 1011: loss = 2.2519 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1012: loss = 2.0031 (1.216 sec/step)\n",
            "I1229 11:08:43.273239 140034317584256 learning.py:507] global step 1012: loss = 2.0031 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1013: loss = 2.3047 (1.218 sec/step)\n",
            "I1229 11:08:44.493276 140034317584256 learning.py:507] global step 1013: loss = 2.3047 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1014: loss = 2.2989 (1.199 sec/step)\n",
            "I1229 11:08:45.694086 140034317584256 learning.py:507] global step 1014: loss = 2.2989 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1015: loss = 3.9213 (1.227 sec/step)\n",
            "I1229 11:08:46.923199 140034317584256 learning.py:507] global step 1015: loss = 3.9213 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1016: loss = 3.2903 (1.301 sec/step)\n",
            "I1229 11:08:48.226374 140034317584256 learning.py:507] global step 1016: loss = 3.2903 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 1017: loss = 3.3291 (1.172 sec/step)\n",
            "I1229 11:08:49.400450 140034317584256 learning.py:507] global step 1017: loss = 3.3291 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 1018: loss = 3.5659 (1.155 sec/step)\n",
            "I1229 11:08:50.557801 140034317584256 learning.py:507] global step 1018: loss = 3.5659 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 1019: loss = 2.8543 (1.175 sec/step)\n",
            "I1229 11:08:51.735101 140034317584256 learning.py:507] global step 1019: loss = 2.8543 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1020: loss = 3.7454 (1.185 sec/step)\n",
            "I1229 11:08:52.921330 140034317584256 learning.py:507] global step 1020: loss = 3.7454 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1021: loss = 1.5069 (1.197 sec/step)\n",
            "I1229 11:08:54.119789 140034317584256 learning.py:507] global step 1021: loss = 1.5069 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1022: loss = 3.7315 (1.226 sec/step)\n",
            "I1229 11:08:55.347529 140034317584256 learning.py:507] global step 1022: loss = 3.7315 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1023: loss = 3.0367 (1.183 sec/step)\n",
            "I1229 11:08:56.532062 140034317584256 learning.py:507] global step 1023: loss = 3.0367 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1024: loss = 2.3113 (1.178 sec/step)\n",
            "I1229 11:08:57.711427 140034317584256 learning.py:507] global step 1024: loss = 2.3113 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1025: loss = 2.7015 (1.184 sec/step)\n",
            "I1229 11:08:58.897796 140034317584256 learning.py:507] global step 1025: loss = 2.7015 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1026: loss = 2.2121 (1.230 sec/step)\n",
            "I1229 11:09:00.130097 140034317584256 learning.py:507] global step 1026: loss = 2.2121 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1027: loss = 2.3741 (1.226 sec/step)\n",
            "I1229 11:09:01.358170 140034317584256 learning.py:507] global step 1027: loss = 2.3741 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1028: loss = 2.5527 (1.242 sec/step)\n",
            "I1229 11:09:02.602012 140034317584256 learning.py:507] global step 1028: loss = 2.5527 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1029: loss = 3.2301 (1.185 sec/step)\n",
            "I1229 11:09:03.789165 140034317584256 learning.py:507] global step 1029: loss = 3.2301 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1030: loss = 3.1215 (1.184 sec/step)\n",
            "I1229 11:09:04.974513 140034317584256 learning.py:507] global step 1030: loss = 3.1215 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1031: loss = 3.4544 (1.214 sec/step)\n",
            "I1229 11:09:06.189891 140034317584256 learning.py:507] global step 1031: loss = 3.4544 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1032: loss = 2.5056 (1.236 sec/step)\n",
            "I1229 11:09:07.427274 140034317584256 learning.py:507] global step 1032: loss = 2.5056 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1033: loss = 2.8233 (1.235 sec/step)\n",
            "I1229 11:09:08.664328 140034317584256 learning.py:507] global step 1033: loss = 2.8233 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1034: loss = 3.0900 (1.202 sec/step)\n",
            "I1229 11:09:09.869569 140034317584256 learning.py:507] global step 1034: loss = 3.0900 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1035: loss = 3.4383 (1.202 sec/step)\n",
            "I1229 11:09:11.073411 140034317584256 learning.py:507] global step 1035: loss = 3.4383 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1036: loss = 2.4810 (1.217 sec/step)\n",
            "I1229 11:09:12.292329 140034317584256 learning.py:507] global step 1036: loss = 2.4810 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1037: loss = 4.0000 (1.197 sec/step)\n",
            "I1229 11:09:13.491523 140034317584256 learning.py:507] global step 1037: loss = 4.0000 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1038: loss = 2.7564 (1.189 sec/step)\n",
            "I1229 11:09:14.682368 140034317584256 learning.py:507] global step 1038: loss = 2.7564 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1039: loss = 3.3906 (1.245 sec/step)\n",
            "I1229 11:09:15.928854 140034317584256 learning.py:507] global step 1039: loss = 3.3906 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1040: loss = 3.4867 (1.181 sec/step)\n",
            "I1229 11:09:17.111805 140034317584256 learning.py:507] global step 1040: loss = 3.4867 (1.181 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1040.\n",
            "I1229 11:09:19.193796 140030580766464 supervisor.py:1050] Recording summary at step 1040.\n",
            "INFO:tensorflow:global step 1041: loss = 2.6102 (2.187 sec/step)\n",
            "I1229 11:09:19.299949 140034317584256 learning.py:507] global step 1041: loss = 2.6102 (2.187 sec/step)\n",
            "INFO:tensorflow:global step 1042: loss = 3.0939 (1.209 sec/step)\n",
            "I1229 11:09:20.510246 140034317584256 learning.py:507] global step 1042: loss = 3.0939 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1043: loss = 2.2208 (1.238 sec/step)\n",
            "I1229 11:09:21.749778 140034317584256 learning.py:507] global step 1043: loss = 2.2208 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1044: loss = 1.9661 (1.252 sec/step)\n",
            "I1229 11:09:23.003176 140034317584256 learning.py:507] global step 1044: loss = 1.9661 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1045: loss = 1.8515 (1.220 sec/step)\n",
            "I1229 11:09:24.224840 140034317584256 learning.py:507] global step 1045: loss = 1.8515 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1046: loss = 3.2327 (1.219 sec/step)\n",
            "I1229 11:09:25.445885 140034317584256 learning.py:507] global step 1046: loss = 3.2327 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1047: loss = 2.1711 (1.189 sec/step)\n",
            "I1229 11:09:26.636306 140034317584256 learning.py:507] global step 1047: loss = 2.1711 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1048: loss = 2.0247 (1.242 sec/step)\n",
            "I1229 11:09:27.879694 140034317584256 learning.py:507] global step 1048: loss = 2.0247 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1049: loss = 2.9931 (1.197 sec/step)\n",
            "I1229 11:09:29.078388 140034317584256 learning.py:507] global step 1049: loss = 2.9931 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1050: loss = 1.6468 (1.275 sec/step)\n",
            "I1229 11:09:30.355363 140034317584256 learning.py:507] global step 1050: loss = 1.6468 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 1051: loss = 2.6072 (1.251 sec/step)\n",
            "I1229 11:09:31.607664 140034317584256 learning.py:507] global step 1051: loss = 2.6072 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1052: loss = 2.5463 (1.253 sec/step)\n",
            "I1229 11:09:32.861742 140034317584256 learning.py:507] global step 1052: loss = 2.5463 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1053: loss = 3.3206 (1.314 sec/step)\n",
            "I1229 11:09:34.177334 140034317584256 learning.py:507] global step 1053: loss = 3.3206 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1054: loss = 3.6697 (1.243 sec/step)\n",
            "I1229 11:09:35.422038 140034317584256 learning.py:507] global step 1054: loss = 3.6697 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1055: loss = 1.7140 (1.198 sec/step)\n",
            "I1229 11:09:36.621234 140034317584256 learning.py:507] global step 1055: loss = 1.7140 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1056: loss = 3.3175 (1.191 sec/step)\n",
            "I1229 11:09:37.814151 140034317584256 learning.py:507] global step 1056: loss = 3.3175 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1057: loss = 2.4633 (1.202 sec/step)\n",
            "I1229 11:09:39.017879 140034317584256 learning.py:507] global step 1057: loss = 2.4633 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1058: loss = 3.2871 (1.223 sec/step)\n",
            "I1229 11:09:40.242623 140034317584256 learning.py:507] global step 1058: loss = 3.2871 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1059: loss = 2.3071 (1.166 sec/step)\n",
            "I1229 11:09:41.410604 140034317584256 learning.py:507] global step 1059: loss = 2.3071 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 1060: loss = 2.2357 (1.185 sec/step)\n",
            "I1229 11:09:42.597613 140034317584256 learning.py:507] global step 1060: loss = 2.2357 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1061: loss = 3.1247 (1.195 sec/step)\n",
            "I1229 11:09:43.794853 140034317584256 learning.py:507] global step 1061: loss = 3.1247 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1062: loss = 2.9975 (1.210 sec/step)\n",
            "I1229 11:09:45.007039 140034317584256 learning.py:507] global step 1062: loss = 2.9975 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1063: loss = 2.1475 (1.211 sec/step)\n",
            "I1229 11:09:46.219425 140034317584256 learning.py:507] global step 1063: loss = 2.1475 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1064: loss = 2.7623 (1.172 sec/step)\n",
            "I1229 11:09:47.393408 140034317584256 learning.py:507] global step 1064: loss = 2.7623 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 1065: loss = 1.9413 (1.216 sec/step)\n",
            "I1229 11:09:48.610670 140034317584256 learning.py:507] global step 1065: loss = 1.9413 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1066: loss = 2.3224 (1.158 sec/step)\n",
            "I1229 11:09:49.769925 140034317584256 learning.py:507] global step 1066: loss = 2.3224 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 1067: loss = 3.8181 (1.200 sec/step)\n",
            "I1229 11:09:50.971738 140034317584256 learning.py:507] global step 1067: loss = 3.8181 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1068: loss = 1.9950 (1.189 sec/step)\n",
            "I1229 11:09:52.162902 140034317584256 learning.py:507] global step 1068: loss = 1.9950 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1069: loss = 2.0608 (1.173 sec/step)\n",
            "I1229 11:09:53.337574 140034317584256 learning.py:507] global step 1069: loss = 2.0608 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1070: loss = 2.6235 (1.198 sec/step)\n",
            "I1229 11:09:54.537705 140034317584256 learning.py:507] global step 1070: loss = 2.6235 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1071: loss = 3.4952 (1.210 sec/step)\n",
            "I1229 11:09:55.749275 140034317584256 learning.py:507] global step 1071: loss = 3.4952 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1072: loss = 2.7418 (1.176 sec/step)\n",
            "I1229 11:09:56.926547 140034317584256 learning.py:507] global step 1072: loss = 2.7418 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1073: loss = 2.3551 (1.183 sec/step)\n",
            "I1229 11:09:58.110704 140034317584256 learning.py:507] global step 1073: loss = 2.3551 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1074: loss = 1.3461 (1.238 sec/step)\n",
            "I1229 11:09:59.350193 140034317584256 learning.py:507] global step 1074: loss = 1.3461 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1075: loss = 3.0047 (1.222 sec/step)\n",
            "I1229 11:10:00.573416 140034317584256 learning.py:507] global step 1075: loss = 3.0047 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1076: loss = 3.5552 (1.210 sec/step)\n",
            "I1229 11:10:01.784991 140034317584256 learning.py:507] global step 1076: loss = 3.5552 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1077: loss = 3.1559 (1.203 sec/step)\n",
            "I1229 11:10:02.989779 140034317584256 learning.py:507] global step 1077: loss = 3.1559 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1078: loss = 2.5051 (1.213 sec/step)\n",
            "I1229 11:10:04.204400 140034317584256 learning.py:507] global step 1078: loss = 2.5051 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1079: loss = 2.4882 (1.212 sec/step)\n",
            "I1229 11:10:05.418686 140034317584256 learning.py:507] global step 1079: loss = 2.4882 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1080: loss = 2.1885 (1.186 sec/step)\n",
            "I1229 11:10:06.606028 140034317584256 learning.py:507] global step 1080: loss = 2.1885 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1081: loss = 2.9697 (1.180 sec/step)\n",
            "I1229 11:10:07.788257 140034317584256 learning.py:507] global step 1081: loss = 2.9697 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1082: loss = 2.6221 (1.261 sec/step)\n",
            "I1229 11:10:09.051276 140034317584256 learning.py:507] global step 1082: loss = 2.6221 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 1083: loss = 2.3914 (1.240 sec/step)\n",
            "I1229 11:10:10.293519 140034317584256 learning.py:507] global step 1083: loss = 2.3914 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 1084: loss = 2.3935 (1.227 sec/step)\n",
            "I1229 11:10:11.522646 140034317584256 learning.py:507] global step 1084: loss = 2.3935 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1085: loss = 2.8214 (1.186 sec/step)\n",
            "I1229 11:10:12.710234 140034317584256 learning.py:507] global step 1085: loss = 2.8214 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1086: loss = 3.1462 (1.188 sec/step)\n",
            "I1229 11:10:13.900319 140034317584256 learning.py:507] global step 1086: loss = 3.1462 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1087: loss = 2.5698 (1.182 sec/step)\n",
            "I1229 11:10:15.084369 140034317584256 learning.py:507] global step 1087: loss = 2.5698 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1088: loss = 2.5426 (1.288 sec/step)\n",
            "I1229 11:10:16.373845 140034317584256 learning.py:507] global step 1088: loss = 2.5426 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 1089: loss = 1.8067 (1.232 sec/step)\n",
            "I1229 11:10:17.608133 140034317584256 learning.py:507] global step 1089: loss = 1.8067 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1090: loss = 2.2458 (1.228 sec/step)\n",
            "I1229 11:10:18.837720 140034317584256 learning.py:507] global step 1090: loss = 2.2458 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1091: loss = 1.8254 (1.204 sec/step)\n",
            "I1229 11:10:20.042942 140034317584256 learning.py:507] global step 1091: loss = 1.8254 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1092: loss = 3.6962 (1.180 sec/step)\n",
            "I1229 11:10:21.224877 140034317584256 learning.py:507] global step 1092: loss = 3.6962 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1093: loss = 4.3478 (1.205 sec/step)\n",
            "I1229 11:10:22.431576 140034317584256 learning.py:507] global step 1093: loss = 4.3478 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1094: loss = 1.9078 (1.187 sec/step)\n",
            "I1229 11:10:23.620342 140034317584256 learning.py:507] global step 1094: loss = 1.9078 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1095: loss = 2.0860 (1.191 sec/step)\n",
            "I1229 11:10:24.812983 140034317584256 learning.py:507] global step 1095: loss = 2.0860 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1096: loss = 3.1321 (1.158 sec/step)\n",
            "I1229 11:10:25.972448 140034317584256 learning.py:507] global step 1096: loss = 3.1321 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 1097: loss = 3.1301 (1.235 sec/step)\n",
            "I1229 11:10:27.209293 140034317584256 learning.py:507] global step 1097: loss = 3.1301 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1098: loss = 4.4204 (1.195 sec/step)\n",
            "I1229 11:10:28.405860 140034317584256 learning.py:507] global step 1098: loss = 4.4204 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1099: loss = 1.9594 (1.269 sec/step)\n",
            "I1229 11:10:29.676343 140034317584256 learning.py:507] global step 1099: loss = 1.9594 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 1100: loss = 4.5001 (1.205 sec/step)\n",
            "I1229 11:10:30.883296 140034317584256 learning.py:507] global step 1100: loss = 4.5001 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1101: loss = 3.1696 (1.298 sec/step)\n",
            "I1229 11:10:32.182894 140034317584256 learning.py:507] global step 1101: loss = 3.1696 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 1102: loss = 3.5731 (1.220 sec/step)\n",
            "I1229 11:10:33.405235 140034317584256 learning.py:507] global step 1102: loss = 3.5731 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1103: loss = 2.1321 (1.233 sec/step)\n",
            "I1229 11:10:34.639737 140034317584256 learning.py:507] global step 1103: loss = 2.1321 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1104: loss = 3.4635 (1.215 sec/step)\n",
            "I1229 11:10:35.856328 140034317584256 learning.py:507] global step 1104: loss = 3.4635 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1105: loss = 3.0709 (1.198 sec/step)\n",
            "I1229 11:10:37.056396 140034317584256 learning.py:507] global step 1105: loss = 3.0709 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1106: loss = 3.1875 (1.198 sec/step)\n",
            "I1229 11:10:38.255963 140034317584256 learning.py:507] global step 1106: loss = 3.1875 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1107: loss = 2.7454 (1.193 sec/step)\n",
            "I1229 11:10:39.451045 140034317584256 learning.py:507] global step 1107: loss = 2.7454 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1108: loss = 2.4711 (1.192 sec/step)\n",
            "I1229 11:10:40.644317 140034317584256 learning.py:507] global step 1108: loss = 2.4711 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1109: loss = 4.2672 (1.204 sec/step)\n",
            "I1229 11:10:41.850024 140034317584256 learning.py:507] global step 1109: loss = 4.2672 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1110: loss = 3.0561 (1.202 sec/step)\n",
            "I1229 11:10:43.054135 140034317584256 learning.py:507] global step 1110: loss = 3.0561 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1111: loss = 3.0955 (1.235 sec/step)\n",
            "I1229 11:10:44.290855 140034317584256 learning.py:507] global step 1111: loss = 3.0955 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1112: loss = 2.6787 (1.178 sec/step)\n",
            "I1229 11:10:45.470560 140034317584256 learning.py:507] global step 1112: loss = 2.6787 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1113: loss = 3.8565 (1.168 sec/step)\n",
            "I1229 11:10:46.640552 140034317584256 learning.py:507] global step 1113: loss = 3.8565 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 1114: loss = 2.0205 (1.204 sec/step)\n",
            "I1229 11:10:47.846522 140034317584256 learning.py:507] global step 1114: loss = 2.0205 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1115: loss = 2.9339 (1.216 sec/step)\n",
            "I1229 11:10:49.063730 140034317584256 learning.py:507] global step 1115: loss = 2.9339 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1116: loss = 3.0216 (1.260 sec/step)\n",
            "I1229 11:10:50.324956 140034317584256 learning.py:507] global step 1116: loss = 3.0216 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1117: loss = 3.0707 (1.221 sec/step)\n",
            "I1229 11:10:51.547847 140034317584256 learning.py:507] global step 1117: loss = 3.0707 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1118: loss = 3.2852 (1.211 sec/step)\n",
            "I1229 11:10:52.761167 140034317584256 learning.py:507] global step 1118: loss = 3.2852 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1119: loss = 3.3161 (1.206 sec/step)\n",
            "I1229 11:10:53.968853 140034317584256 learning.py:507] global step 1119: loss = 3.3161 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1120: loss = 3.1686 (1.175 sec/step)\n",
            "I1229 11:10:55.145712 140034317584256 learning.py:507] global step 1120: loss = 3.1686 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1121: loss = 3.0775 (1.223 sec/step)\n",
            "I1229 11:10:56.371377 140034317584256 learning.py:507] global step 1121: loss = 3.0775 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1122: loss = 3.7749 (1.223 sec/step)\n",
            "I1229 11:10:57.596464 140034317584256 learning.py:507] global step 1122: loss = 3.7749 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1123: loss = 2.7466 (1.219 sec/step)\n",
            "I1229 11:10:58.817272 140034317584256 learning.py:507] global step 1123: loss = 2.7466 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1124: loss = 2.5253 (1.206 sec/step)\n",
            "I1229 11:11:00.024376 140034317584256 learning.py:507] global step 1124: loss = 2.5253 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1125: loss = 3.2628 (1.178 sec/step)\n",
            "I1229 11:11:01.204297 140034317584256 learning.py:507] global step 1125: loss = 3.2628 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1126: loss = 2.3875 (1.216 sec/step)\n",
            "I1229 11:11:02.422003 140034317584256 learning.py:507] global step 1126: loss = 2.3875 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1127: loss = 2.4534 (1.195 sec/step)\n",
            "I1229 11:11:03.618107 140034317584256 learning.py:507] global step 1127: loss = 2.4534 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1128: loss = 3.0112 (1.203 sec/step)\n",
            "I1229 11:11:04.822417 140034317584256 learning.py:507] global step 1128: loss = 3.0112 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1129: loss = 3.4212 (1.191 sec/step)\n",
            "I1229 11:11:06.014824 140034317584256 learning.py:507] global step 1129: loss = 3.4212 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1130: loss = 2.8182 (1.217 sec/step)\n",
            "I1229 11:11:07.233139 140034317584256 learning.py:507] global step 1130: loss = 2.8182 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1131: loss = 2.4520 (1.234 sec/step)\n",
            "I1229 11:11:08.468744 140034317584256 learning.py:507] global step 1131: loss = 2.4520 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1132: loss = 2.3962 (1.194 sec/step)\n",
            "I1229 11:11:09.664362 140034317584256 learning.py:507] global step 1132: loss = 2.3962 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1133: loss = 2.4907 (1.225 sec/step)\n",
            "I1229 11:11:10.891165 140034317584256 learning.py:507] global step 1133: loss = 2.4907 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1134: loss = 2.2402 (1.218 sec/step)\n",
            "I1229 11:11:12.110620 140034317584256 learning.py:507] global step 1134: loss = 2.2402 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1135: loss = 3.0515 (1.186 sec/step)\n",
            "I1229 11:11:13.298519 140034317584256 learning.py:507] global step 1135: loss = 3.0515 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1136: loss = 3.1831 (1.193 sec/step)\n",
            "I1229 11:11:14.493081 140034317584256 learning.py:507] global step 1136: loss = 3.1831 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1137: loss = 2.5652 (1.260 sec/step)\n",
            "I1229 11:11:15.755415 140034317584256 learning.py:507] global step 1137: loss = 2.5652 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1138: loss = 3.0794 (1.225 sec/step)\n",
            "I1229 11:11:16.982475 140034317584256 learning.py:507] global step 1138: loss = 3.0794 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1139: loss = 3.5113 (2.093 sec/step)\n",
            "I1229 11:11:19.078354 140034317584256 learning.py:507] global step 1139: loss = 3.5113 (2.093 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1139.\n",
            "I1229 11:11:19.766970 140030580766464 supervisor.py:1050] Recording summary at step 1139.\n",
            "INFO:tensorflow:global step 1140: loss = 2.0592 (1.314 sec/step)\n",
            "I1229 11:11:20.394189 140034317584256 learning.py:507] global step 1140: loss = 2.0592 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 1141: loss = 3.3211 (1.230 sec/step)\n",
            "I1229 11:11:21.625479 140034317584256 learning.py:507] global step 1141: loss = 3.3211 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1142: loss = 2.3548 (1.226 sec/step)\n",
            "I1229 11:11:22.852990 140034317584256 learning.py:507] global step 1142: loss = 2.3548 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1143: loss = 3.5440 (1.211 sec/step)\n",
            "I1229 11:11:24.066095 140034317584256 learning.py:507] global step 1143: loss = 3.5440 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1144: loss = 3.5563 (1.245 sec/step)\n",
            "I1229 11:11:25.313093 140034317584256 learning.py:507] global step 1144: loss = 3.5563 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1145: loss = 3.1667 (1.303 sec/step)\n",
            "I1229 11:11:26.618177 140034317584256 learning.py:507] global step 1145: loss = 3.1667 (1.303 sec/step)\n",
            "INFO:tensorflow:global step 1146: loss = 2.4191 (1.267 sec/step)\n",
            "I1229 11:11:27.887356 140034317584256 learning.py:507] global step 1146: loss = 2.4191 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 1147: loss = 2.0538 (1.239 sec/step)\n",
            "I1229 11:11:29.128443 140034317584256 learning.py:507] global step 1147: loss = 2.0538 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1148: loss = 2.2477 (1.250 sec/step)\n",
            "I1229 11:11:30.380389 140034317584256 learning.py:507] global step 1148: loss = 2.2477 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 1149: loss = 2.8914 (1.286 sec/step)\n",
            "I1229 11:11:31.667882 140034317584256 learning.py:507] global step 1149: loss = 2.8914 (1.286 sec/step)\n",
            "INFO:tensorflow:global step 1150: loss = 2.1313 (1.256 sec/step)\n",
            "I1229 11:11:32.925794 140034317584256 learning.py:507] global step 1150: loss = 2.1313 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1151: loss = 2.5803 (1.280 sec/step)\n",
            "I1229 11:11:34.207441 140034317584256 learning.py:507] global step 1151: loss = 2.5803 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 1152: loss = 3.7003 (1.241 sec/step)\n",
            "I1229 11:11:35.450325 140034317584256 learning.py:507] global step 1152: loss = 3.7003 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1153: loss = 2.9654 (1.207 sec/step)\n",
            "I1229 11:11:36.658599 140034317584256 learning.py:507] global step 1153: loss = 2.9654 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1154: loss = 2.8306 (1.216 sec/step)\n",
            "I1229 11:11:37.876314 140034317584256 learning.py:507] global step 1154: loss = 2.8306 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1155: loss = 3.0973 (1.192 sec/step)\n",
            "I1229 11:11:39.070491 140034317584256 learning.py:507] global step 1155: loss = 3.0973 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1156: loss = 3.6837 (1.183 sec/step)\n",
            "I1229 11:11:40.255279 140034317584256 learning.py:507] global step 1156: loss = 3.6837 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1157: loss = 1.5276 (1.242 sec/step)\n",
            "I1229 11:11:41.498588 140034317584256 learning.py:507] global step 1157: loss = 1.5276 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1158: loss = 3.9742 (1.201 sec/step)\n",
            "I1229 11:11:42.700906 140034317584256 learning.py:507] global step 1158: loss = 3.9742 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1159: loss = 3.0223 (1.268 sec/step)\n",
            "I1229 11:11:43.971034 140034317584256 learning.py:507] global step 1159: loss = 3.0223 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 1160: loss = 2.6994 (1.217 sec/step)\n",
            "I1229 11:11:45.189274 140034317584256 learning.py:507] global step 1160: loss = 2.6994 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1161: loss = 3.1678 (1.225 sec/step)\n",
            "I1229 11:11:46.416455 140034317584256 learning.py:507] global step 1161: loss = 3.1678 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1162: loss = 1.9146 (1.199 sec/step)\n",
            "I1229 11:11:47.617229 140034317584256 learning.py:507] global step 1162: loss = 1.9146 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1163: loss = 2.3924 (1.193 sec/step)\n",
            "I1229 11:11:48.811696 140034317584256 learning.py:507] global step 1163: loss = 2.3924 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1164: loss = 2.5880 (1.197 sec/step)\n",
            "I1229 11:11:50.010130 140034317584256 learning.py:507] global step 1164: loss = 2.5880 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1165: loss = 2.9752 (1.225 sec/step)\n",
            "I1229 11:11:51.236763 140034317584256 learning.py:507] global step 1165: loss = 2.9752 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1166: loss = 2.5088 (1.222 sec/step)\n",
            "I1229 11:11:52.460034 140034317584256 learning.py:507] global step 1166: loss = 2.5088 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1167: loss = 3.3104 (1.200 sec/step)\n",
            "I1229 11:11:53.662144 140034317584256 learning.py:507] global step 1167: loss = 3.3104 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1168: loss = 3.4736 (1.180 sec/step)\n",
            "I1229 11:11:54.843966 140034317584256 learning.py:507] global step 1168: loss = 3.4736 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1169: loss = 4.6133 (1.197 sec/step)\n",
            "I1229 11:11:56.042331 140034317584256 learning.py:507] global step 1169: loss = 4.6133 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1170: loss = 2.9834 (1.236 sec/step)\n",
            "I1229 11:11:57.280506 140034317584256 learning.py:507] global step 1170: loss = 2.9834 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1171: loss = 2.9011 (1.193 sec/step)\n",
            "I1229 11:11:58.476618 140034317584256 learning.py:507] global step 1171: loss = 2.9011 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1172: loss = 2.4836 (1.173 sec/step)\n",
            "I1229 11:11:59.651196 140034317584256 learning.py:507] global step 1172: loss = 2.4836 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1173: loss = 2.3219 (1.190 sec/step)\n",
            "I1229 11:12:00.843178 140034317584256 learning.py:507] global step 1173: loss = 2.3219 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1174: loss = 2.5778 (1.173 sec/step)\n",
            "I1229 11:12:02.017747 140034317584256 learning.py:507] global step 1174: loss = 2.5778 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1175: loss = 2.8039 (1.189 sec/step)\n",
            "I1229 11:12:03.209043 140034317584256 learning.py:507] global step 1175: loss = 2.8039 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1176: loss = 2.6617 (1.240 sec/step)\n",
            "I1229 11:12:04.450653 140034317584256 learning.py:507] global step 1176: loss = 2.6617 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 1177: loss = 2.0988 (1.195 sec/step)\n",
            "I1229 11:12:05.647043 140034317584256 learning.py:507] global step 1177: loss = 2.0988 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1178: loss = 2.0326 (1.196 sec/step)\n",
            "I1229 11:12:06.844470 140034317584256 learning.py:507] global step 1178: loss = 2.0326 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1179: loss = 3.0662 (1.252 sec/step)\n",
            "I1229 11:12:08.098359 140034317584256 learning.py:507] global step 1179: loss = 3.0662 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1180: loss = 2.0611 (1.237 sec/step)\n",
            "I1229 11:12:09.337825 140034317584256 learning.py:507] global step 1180: loss = 2.0611 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1181: loss = 3.4601 (1.200 sec/step)\n",
            "I1229 11:12:10.539817 140034317584256 learning.py:507] global step 1181: loss = 3.4601 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1182: loss = 2.3894 (1.188 sec/step)\n",
            "I1229 11:12:11.728939 140034317584256 learning.py:507] global step 1182: loss = 2.3894 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1183: loss = 3.0948 (1.181 sec/step)\n",
            "I1229 11:12:12.911819 140034317584256 learning.py:507] global step 1183: loss = 3.0948 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1184: loss = 3.0696 (1.223 sec/step)\n",
            "I1229 11:12:14.136768 140034317584256 learning.py:507] global step 1184: loss = 3.0696 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1185: loss = 3.0343 (1.170 sec/step)\n",
            "I1229 11:12:15.309011 140034317584256 learning.py:507] global step 1185: loss = 3.0343 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1186: loss = 3.7949 (1.193 sec/step)\n",
            "I1229 11:12:16.504051 140034317584256 learning.py:507] global step 1186: loss = 3.7949 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1187: loss = 2.6654 (1.206 sec/step)\n",
            "I1229 11:12:17.712035 140034317584256 learning.py:507] global step 1187: loss = 2.6654 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1188: loss = 3.1138 (1.215 sec/step)\n",
            "I1229 11:12:18.928524 140034317584256 learning.py:507] global step 1188: loss = 3.1138 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1189: loss = 2.4842 (1.236 sec/step)\n",
            "I1229 11:12:20.165690 140034317584256 learning.py:507] global step 1189: loss = 2.4842 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1190: loss = 2.7992 (1.198 sec/step)\n",
            "I1229 11:12:21.365898 140034317584256 learning.py:507] global step 1190: loss = 2.7992 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1191: loss = 2.1893 (1.248 sec/step)\n",
            "I1229 11:12:22.615765 140034317584256 learning.py:507] global step 1191: loss = 2.1893 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 1192: loss = 2.1909 (1.196 sec/step)\n",
            "I1229 11:12:23.813036 140034317584256 learning.py:507] global step 1192: loss = 2.1909 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1193: loss = 3.2351 (1.185 sec/step)\n",
            "I1229 11:12:24.999575 140034317584256 learning.py:507] global step 1193: loss = 3.2351 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1194: loss = 4.1873 (1.146 sec/step)\n",
            "I1229 11:12:26.146857 140034317584256 learning.py:507] global step 1194: loss = 4.1873 (1.146 sec/step)\n",
            "INFO:tensorflow:global step 1195: loss = 2.0573 (1.211 sec/step)\n",
            "I1229 11:12:27.359589 140034317584256 learning.py:507] global step 1195: loss = 2.0573 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1196: loss = 2.4897 (1.245 sec/step)\n",
            "I1229 11:12:28.606630 140034317584256 learning.py:507] global step 1196: loss = 2.4897 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1197: loss = 1.7143 (1.194 sec/step)\n",
            "I1229 11:12:29.802546 140034317584256 learning.py:507] global step 1197: loss = 1.7143 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1198: loss = 2.5125 (1.253 sec/step)\n",
            "I1229 11:12:31.056980 140034317584256 learning.py:507] global step 1198: loss = 2.5125 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1199: loss = 3.6071 (1.292 sec/step)\n",
            "I1229 11:12:32.350364 140034317584256 learning.py:507] global step 1199: loss = 3.6071 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 1200: loss = 2.8750 (1.227 sec/step)\n",
            "I1229 11:12:33.579104 140034317584256 learning.py:507] global step 1200: loss = 2.8750 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1201: loss = 2.8916 (1.222 sec/step)\n",
            "I1229 11:12:34.802680 140034317584256 learning.py:507] global step 1201: loss = 2.8916 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1202: loss = 1.6409 (1.242 sec/step)\n",
            "I1229 11:12:36.046347 140034317584256 learning.py:507] global step 1202: loss = 1.6409 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1203: loss = 2.9046 (1.202 sec/step)\n",
            "I1229 11:12:37.250231 140034317584256 learning.py:507] global step 1203: loss = 2.9046 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1204: loss = 2.3036 (1.271 sec/step)\n",
            "I1229 11:12:38.523235 140034317584256 learning.py:507] global step 1204: loss = 2.3036 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 1205: loss = 2.9099 (1.213 sec/step)\n",
            "I1229 11:12:39.738419 140034317584256 learning.py:507] global step 1205: loss = 2.9099 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1206: loss = 1.4471 (1.219 sec/step)\n",
            "I1229 11:12:40.959315 140034317584256 learning.py:507] global step 1206: loss = 1.4471 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1207: loss = 3.5943 (1.245 sec/step)\n",
            "I1229 11:12:42.206191 140034317584256 learning.py:507] global step 1207: loss = 3.5943 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1208: loss = 4.1437 (1.217 sec/step)\n",
            "I1229 11:12:43.425062 140034317584256 learning.py:507] global step 1208: loss = 4.1437 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1209: loss = 2.9741 (1.382 sec/step)\n",
            "I1229 11:12:44.808587 140034317584256 learning.py:507] global step 1209: loss = 2.9741 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 1210: loss = 3.1359 (1.242 sec/step)\n",
            "I1229 11:12:46.051908 140034317584256 learning.py:507] global step 1210: loss = 3.1359 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1211: loss = 2.1037 (1.274 sec/step)\n",
            "I1229 11:12:47.327739 140034317584256 learning.py:507] global step 1211: loss = 2.1037 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 1212: loss = 3.4234 (1.219 sec/step)\n",
            "I1229 11:12:48.548279 140034317584256 learning.py:507] global step 1212: loss = 3.4234 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1213: loss = 2.5440 (1.321 sec/step)\n",
            "I1229 11:12:49.870706 140034317584256 learning.py:507] global step 1213: loss = 2.5440 (1.321 sec/step)\n",
            "INFO:tensorflow:global step 1214: loss = 2.2998 (1.238 sec/step)\n",
            "I1229 11:12:51.110321 140034317584256 learning.py:507] global step 1214: loss = 2.2998 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1215: loss = 4.0843 (1.238 sec/step)\n",
            "I1229 11:12:52.349675 140034317584256 learning.py:507] global step 1215: loss = 4.0843 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1216: loss = 3.3321 (1.246 sec/step)\n",
            "I1229 11:12:53.597097 140034317584256 learning.py:507] global step 1216: loss = 3.3321 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1217: loss = 4.0003 (1.218 sec/step)\n",
            "I1229 11:12:54.816990 140034317584256 learning.py:507] global step 1217: loss = 4.0003 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1218: loss = 3.5522 (1.230 sec/step)\n",
            "I1229 11:12:56.048557 140034317584256 learning.py:507] global step 1218: loss = 3.5522 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1219: loss = 3.4057 (1.256 sec/step)\n",
            "I1229 11:12:57.306193 140034317584256 learning.py:507] global step 1219: loss = 3.4057 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1220: loss = 2.8785 (1.249 sec/step)\n",
            "I1229 11:12:58.557071 140034317584256 learning.py:507] global step 1220: loss = 2.8785 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1221: loss = 2.5359 (1.288 sec/step)\n",
            "I1229 11:12:59.846921 140034317584256 learning.py:507] global step 1221: loss = 2.5359 (1.288 sec/step)\n",
            "INFO:tensorflow:global step 1222: loss = 2.7588 (1.208 sec/step)\n",
            "I1229 11:13:01.057060 140034317584256 learning.py:507] global step 1222: loss = 2.7588 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1223: loss = 2.4757 (1.235 sec/step)\n",
            "I1229 11:13:02.294100 140034317584256 learning.py:507] global step 1223: loss = 2.4757 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1224: loss = 1.8060 (1.228 sec/step)\n",
            "I1229 11:13:03.524533 140034317584256 learning.py:507] global step 1224: loss = 1.8060 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1225: loss = 3.2668 (1.263 sec/step)\n",
            "I1229 11:13:04.789401 140034317584256 learning.py:507] global step 1225: loss = 3.2668 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 1226: loss = 3.4908 (1.224 sec/step)\n",
            "I1229 11:13:06.014664 140034317584256 learning.py:507] global step 1226: loss = 3.4908 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1227: loss = 3.3175 (1.196 sec/step)\n",
            "I1229 11:13:07.211686 140034317584256 learning.py:507] global step 1227: loss = 3.3175 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1228: loss = 2.9857 (1.184 sec/step)\n",
            "I1229 11:13:08.396705 140034317584256 learning.py:507] global step 1228: loss = 2.9857 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1229: loss = 2.1570 (1.217 sec/step)\n",
            "I1229 11:13:09.615020 140034317584256 learning.py:507] global step 1229: loss = 2.1570 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1230: loss = 2.2171 (1.232 sec/step)\n",
            "I1229 11:13:10.848895 140034317584256 learning.py:507] global step 1230: loss = 2.2171 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1231: loss = 2.9672 (1.232 sec/step)\n",
            "I1229 11:13:12.082293 140034317584256 learning.py:507] global step 1231: loss = 2.9672 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1232: loss = 3.0975 (1.175 sec/step)\n",
            "I1229 11:13:13.259092 140034317584256 learning.py:507] global step 1232: loss = 3.0975 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1233: loss = 2.5316 (1.221 sec/step)\n",
            "I1229 11:13:14.481901 140034317584256 learning.py:507] global step 1233: loss = 2.5316 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1234: loss = 1.9008 (1.238 sec/step)\n",
            "I1229 11:13:15.721832 140034317584256 learning.py:507] global step 1234: loss = 1.9008 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1235: loss = 1.7082 (1.218 sec/step)\n",
            "I1229 11:13:16.941679 140034317584256 learning.py:507] global step 1235: loss = 1.7082 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1236: loss = 2.4605 (1.994 sec/step)\n",
            "I1229 11:13:18.937946 140034317584256 learning.py:507] global step 1236: loss = 2.4605 (1.994 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1236.\n",
            "I1229 11:13:19.816461 140030580766464 supervisor.py:1050] Recording summary at step 1236.\n",
            "INFO:tensorflow:global step 1237: loss = 2.6563 (1.399 sec/step)\n",
            "I1229 11:13:20.339514 140034317584256 learning.py:507] global step 1237: loss = 2.6563 (1.399 sec/step)\n",
            "INFO:tensorflow:global step 1238: loss = 2.8082 (1.272 sec/step)\n",
            "I1229 11:13:21.613052 140034317584256 learning.py:507] global step 1238: loss = 2.8082 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 1239: loss = 2.8885 (1.200 sec/step)\n",
            "I1229 11:13:22.815016 140034317584256 learning.py:507] global step 1239: loss = 2.8885 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1240: loss = 1.8909 (1.281 sec/step)\n",
            "I1229 11:13:24.098288 140034317584256 learning.py:507] global step 1240: loss = 1.8909 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 1241: loss = 1.7528 (1.231 sec/step)\n",
            "I1229 11:13:25.331157 140034317584256 learning.py:507] global step 1241: loss = 1.7528 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1242: loss = 3.3634 (1.195 sec/step)\n",
            "I1229 11:13:26.527704 140034317584256 learning.py:507] global step 1242: loss = 3.3634 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1243: loss = 2.7756 (1.244 sec/step)\n",
            "I1229 11:13:27.773694 140034317584256 learning.py:507] global step 1243: loss = 2.7756 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1244: loss = 2.5556 (1.204 sec/step)\n",
            "I1229 11:13:28.980157 140034317584256 learning.py:507] global step 1244: loss = 2.5556 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1245: loss = 5.0784 (1.188 sec/step)\n",
            "I1229 11:13:30.169388 140034317584256 learning.py:507] global step 1245: loss = 5.0784 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1246: loss = 2.6077 (1.208 sec/step)\n",
            "I1229 11:13:31.378719 140034317584256 learning.py:507] global step 1246: loss = 2.6077 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1247: loss = 3.8339 (1.233 sec/step)\n",
            "I1229 11:13:32.613571 140034317584256 learning.py:507] global step 1247: loss = 3.8339 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1248: loss = 2.3344 (1.216 sec/step)\n",
            "I1229 11:13:33.831388 140034317584256 learning.py:507] global step 1248: loss = 2.3344 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1249: loss = 2.7526 (1.226 sec/step)\n",
            "I1229 11:13:35.059452 140034317584256 learning.py:507] global step 1249: loss = 2.7526 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1250: loss = 2.4236 (1.208 sec/step)\n",
            "I1229 11:13:36.268522 140034317584256 learning.py:507] global step 1250: loss = 2.4236 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1251: loss = 3.9860 (1.225 sec/step)\n",
            "I1229 11:13:37.495024 140034317584256 learning.py:507] global step 1251: loss = 3.9860 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1252: loss = 3.8857 (1.211 sec/step)\n",
            "I1229 11:13:38.707340 140034317584256 learning.py:507] global step 1252: loss = 3.8857 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1253: loss = 3.7527 (1.183 sec/step)\n",
            "I1229 11:13:39.891624 140034317584256 learning.py:507] global step 1253: loss = 3.7527 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1254: loss = 3.4247 (1.193 sec/step)\n",
            "I1229 11:13:41.086323 140034317584256 learning.py:507] global step 1254: loss = 3.4247 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1255: loss = 2.4943 (1.272 sec/step)\n",
            "I1229 11:13:42.360151 140034317584256 learning.py:507] global step 1255: loss = 2.4943 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 1256: loss = 2.3285 (1.189 sec/step)\n",
            "I1229 11:13:43.551180 140034317584256 learning.py:507] global step 1256: loss = 2.3285 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1257: loss = 1.8752 (1.180 sec/step)\n",
            "I1229 11:13:44.733064 140034317584256 learning.py:507] global step 1257: loss = 1.8752 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1258: loss = 2.2043 (1.227 sec/step)\n",
            "I1229 11:13:45.962033 140034317584256 learning.py:507] global step 1258: loss = 2.2043 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1259: loss = 2.7664 (1.220 sec/step)\n",
            "I1229 11:13:47.183954 140034317584256 learning.py:507] global step 1259: loss = 2.7664 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1260: loss = 3.5772 (1.222 sec/step)\n",
            "I1229 11:13:48.407856 140034317584256 learning.py:507] global step 1260: loss = 3.5772 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1261: loss = 3.3907 (1.201 sec/step)\n",
            "I1229 11:13:49.610714 140034317584256 learning.py:507] global step 1261: loss = 3.3907 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1262: loss = 3.9903 (1.210 sec/step)\n",
            "I1229 11:13:50.822261 140034317584256 learning.py:507] global step 1262: loss = 3.9903 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1263: loss = 3.8575 (1.283 sec/step)\n",
            "I1229 11:13:52.107587 140034317584256 learning.py:507] global step 1263: loss = 3.8575 (1.283 sec/step)\n",
            "INFO:tensorflow:global step 1264: loss = 2.6780 (1.231 sec/step)\n",
            "I1229 11:13:53.340302 140034317584256 learning.py:507] global step 1264: loss = 2.6780 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1265: loss = 3.1555 (1.187 sec/step)\n",
            "I1229 11:13:54.529505 140034317584256 learning.py:507] global step 1265: loss = 3.1555 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1266: loss = 2.3928 (1.206 sec/step)\n",
            "I1229 11:13:55.736982 140034317584256 learning.py:507] global step 1266: loss = 2.3928 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1267: loss = 2.6455 (1.244 sec/step)\n",
            "I1229 11:13:56.982693 140034317584256 learning.py:507] global step 1267: loss = 2.6455 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1268: loss = 2.1424 (1.233 sec/step)\n",
            "I1229 11:13:58.216934 140034317584256 learning.py:507] global step 1268: loss = 2.1424 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1269: loss = 3.2538 (1.208 sec/step)\n",
            "I1229 11:13:59.426770 140034317584256 learning.py:507] global step 1269: loss = 3.2538 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1270: loss = 2.9509 (1.180 sec/step)\n",
            "I1229 11:14:00.608719 140034317584256 learning.py:507] global step 1270: loss = 2.9509 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1271: loss = 3.0328 (1.198 sec/step)\n",
            "I1229 11:14:01.808708 140034317584256 learning.py:507] global step 1271: loss = 3.0328 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1272: loss = 3.2742 (1.188 sec/step)\n",
            "I1229 11:14:02.998471 140034317584256 learning.py:507] global step 1272: loss = 3.2742 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1273: loss = 3.3250 (1.242 sec/step)\n",
            "I1229 11:14:04.241919 140034317584256 learning.py:507] global step 1273: loss = 3.3250 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1274: loss = 3.3035 (1.242 sec/step)\n",
            "I1229 11:14:05.485951 140034317584256 learning.py:507] global step 1274: loss = 3.3035 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1275: loss = 3.2587 (1.188 sec/step)\n",
            "I1229 11:14:06.675708 140034317584256 learning.py:507] global step 1275: loss = 3.2587 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1276: loss = 2.8528 (1.213 sec/step)\n",
            "I1229 11:14:07.890488 140034317584256 learning.py:507] global step 1276: loss = 2.8528 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1277: loss = 2.7697 (1.207 sec/step)\n",
            "I1229 11:14:09.098710 140034317584256 learning.py:507] global step 1277: loss = 2.7697 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1278: loss = 2.7107 (1.216 sec/step)\n",
            "I1229 11:14:10.316332 140034317584256 learning.py:507] global step 1278: loss = 2.7107 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1279: loss = 2.8493 (1.190 sec/step)\n",
            "I1229 11:14:11.507759 140034317584256 learning.py:507] global step 1279: loss = 2.8493 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1280: loss = 2.5599 (1.207 sec/step)\n",
            "I1229 11:14:12.716376 140034317584256 learning.py:507] global step 1280: loss = 2.5599 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1281: loss = 2.2092 (1.228 sec/step)\n",
            "I1229 11:14:13.945593 140034317584256 learning.py:507] global step 1281: loss = 2.2092 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1282: loss = 2.2418 (1.227 sec/step)\n",
            "I1229 11:14:15.174718 140034317584256 learning.py:507] global step 1282: loss = 2.2418 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1283: loss = 2.8892 (1.292 sec/step)\n",
            "I1229 11:14:16.468934 140034317584256 learning.py:507] global step 1283: loss = 2.8892 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 1284: loss = 2.7666 (1.256 sec/step)\n",
            "I1229 11:14:17.727536 140034317584256 learning.py:507] global step 1284: loss = 2.7666 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1285: loss = 2.6279 (1.230 sec/step)\n",
            "I1229 11:14:18.959607 140034317584256 learning.py:507] global step 1285: loss = 2.6279 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1286: loss = 2.9095 (1.210 sec/step)\n",
            "I1229 11:14:20.170842 140034317584256 learning.py:507] global step 1286: loss = 2.9095 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1287: loss = 2.0945 (1.202 sec/step)\n",
            "I1229 11:14:21.374786 140034317584256 learning.py:507] global step 1287: loss = 2.0945 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1288: loss = 2.9300 (1.224 sec/step)\n",
            "I1229 11:14:22.600489 140034317584256 learning.py:507] global step 1288: loss = 2.9300 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1289: loss = 2.5880 (1.247 sec/step)\n",
            "I1229 11:14:23.849350 140034317584256 learning.py:507] global step 1289: loss = 2.5880 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1290: loss = 1.7636 (1.211 sec/step)\n",
            "I1229 11:14:25.061451 140034317584256 learning.py:507] global step 1290: loss = 1.7636 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1291: loss = 3.0722 (1.208 sec/step)\n",
            "I1229 11:14:26.270935 140034317584256 learning.py:507] global step 1291: loss = 3.0722 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1292: loss = 2.2687 (1.249 sec/step)\n",
            "I1229 11:14:27.522264 140034317584256 learning.py:507] global step 1292: loss = 2.2687 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1293: loss = 2.2825 (1.261 sec/step)\n",
            "I1229 11:14:28.784759 140034317584256 learning.py:507] global step 1293: loss = 2.2825 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 1294: loss = 2.2908 (1.234 sec/step)\n",
            "I1229 11:14:30.020753 140034317584256 learning.py:507] global step 1294: loss = 2.2908 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1295: loss = 1.2026 (1.193 sec/step)\n",
            "I1229 11:14:31.215952 140034317584256 learning.py:507] global step 1295: loss = 1.2026 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1296: loss = 1.9435 (1.241 sec/step)\n",
            "I1229 11:14:32.458448 140034317584256 learning.py:507] global step 1296: loss = 1.9435 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1297: loss = 2.4901 (1.256 sec/step)\n",
            "I1229 11:14:33.716525 140034317584256 learning.py:507] global step 1297: loss = 2.4901 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1298: loss = 2.9117 (1.259 sec/step)\n",
            "I1229 11:14:34.977102 140034317584256 learning.py:507] global step 1298: loss = 2.9117 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 1299: loss = 2.6964 (1.246 sec/step)\n",
            "I1229 11:14:36.225338 140034317584256 learning.py:507] global step 1299: loss = 2.6964 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1300: loss = 2.5374 (1.195 sec/step)\n",
            "I1229 11:14:37.422199 140034317584256 learning.py:507] global step 1300: loss = 2.5374 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1301: loss = 2.1070 (1.233 sec/step)\n",
            "I1229 11:14:38.656719 140034317584256 learning.py:507] global step 1301: loss = 2.1070 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1302: loss = 2.7715 (1.243 sec/step)\n",
            "I1229 11:14:39.901788 140034317584256 learning.py:507] global step 1302: loss = 2.7715 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1303: loss = 3.0984 (1.198 sec/step)\n",
            "I1229 11:14:41.101315 140034317584256 learning.py:507] global step 1303: loss = 3.0984 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1304: loss = 2.8337 (1.181 sec/step)\n",
            "I1229 11:14:42.283883 140034317584256 learning.py:507] global step 1304: loss = 2.8337 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1305: loss = 2.7231 (1.245 sec/step)\n",
            "I1229 11:14:43.530423 140034317584256 learning.py:507] global step 1305: loss = 2.7231 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1306: loss = 2.2377 (1.233 sec/step)\n",
            "I1229 11:14:44.764855 140034317584256 learning.py:507] global step 1306: loss = 2.2377 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1307: loss = 2.3278 (1.174 sec/step)\n",
            "I1229 11:14:45.940489 140034317584256 learning.py:507] global step 1307: loss = 2.3278 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1308: loss = 2.5524 (1.241 sec/step)\n",
            "I1229 11:14:47.184379 140034317584256 learning.py:507] global step 1308: loss = 2.5524 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1309: loss = 3.2243 (1.226 sec/step)\n",
            "I1229 11:14:48.413009 140034317584256 learning.py:507] global step 1309: loss = 3.2243 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1310: loss = 3.7433 (1.176 sec/step)\n",
            "I1229 11:14:49.590936 140034317584256 learning.py:507] global step 1310: loss = 3.7433 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1311: loss = 2.4421 (1.193 sec/step)\n",
            "I1229 11:14:50.785929 140034317584256 learning.py:507] global step 1311: loss = 2.4421 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1312: loss = 3.7916 (1.232 sec/step)\n",
            "I1229 11:14:52.020076 140034317584256 learning.py:507] global step 1312: loss = 3.7916 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1313: loss = 4.3715 (1.205 sec/step)\n",
            "I1229 11:14:53.226310 140034317584256 learning.py:507] global step 1313: loss = 4.3715 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1314: loss = 2.1931 (1.207 sec/step)\n",
            "I1229 11:14:54.435274 140034317584256 learning.py:507] global step 1314: loss = 2.1931 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1315: loss = 2.1818 (1.243 sec/step)\n",
            "I1229 11:14:55.680529 140034317584256 learning.py:507] global step 1315: loss = 2.1818 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1316: loss = 2.6412 (1.208 sec/step)\n",
            "I1229 11:14:56.889738 140034317584256 learning.py:507] global step 1316: loss = 2.6412 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1317: loss = 2.6478 (1.210 sec/step)\n",
            "I1229 11:14:58.101997 140034317584256 learning.py:507] global step 1317: loss = 2.6478 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1318: loss = 2.0485 (1.165 sec/step)\n",
            "I1229 11:14:59.268959 140034317584256 learning.py:507] global step 1318: loss = 2.0485 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 1319: loss = 2.0673 (1.188 sec/step)\n",
            "I1229 11:15:00.459272 140034317584256 learning.py:507] global step 1319: loss = 2.0673 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1320: loss = 3.9875 (1.209 sec/step)\n",
            "I1229 11:15:01.670035 140034317584256 learning.py:507] global step 1320: loss = 3.9875 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1321: loss = 3.1178 (1.191 sec/step)\n",
            "I1229 11:15:02.862870 140034317584256 learning.py:507] global step 1321: loss = 3.1178 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1322: loss = 3.3388 (1.184 sec/step)\n",
            "I1229 11:15:04.048531 140034317584256 learning.py:507] global step 1322: loss = 3.3388 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1323: loss = 2.4045 (1.225 sec/step)\n",
            "I1229 11:15:05.275598 140034317584256 learning.py:507] global step 1323: loss = 2.4045 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1324: loss = 3.1865 (1.219 sec/step)\n",
            "I1229 11:15:06.496494 140034317584256 learning.py:507] global step 1324: loss = 3.1865 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1325: loss = 4.1133 (1.210 sec/step)\n",
            "I1229 11:15:07.707944 140034317584256 learning.py:507] global step 1325: loss = 4.1133 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1326: loss = 2.5242 (1.248 sec/step)\n",
            "I1229 11:15:08.957521 140034317584256 learning.py:507] global step 1326: loss = 2.5242 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 1327: loss = 2.8485 (1.244 sec/step)\n",
            "I1229 11:15:10.202770 140034317584256 learning.py:507] global step 1327: loss = 2.8485 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1328: loss = 2.1569 (1.216 sec/step)\n",
            "I1229 11:15:11.420309 140034317584256 learning.py:507] global step 1328: loss = 2.1569 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1329: loss = 2.0668 (1.201 sec/step)\n",
            "I1229 11:15:12.623382 140034317584256 learning.py:507] global step 1329: loss = 2.0668 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1330: loss = 2.5497 (1.216 sec/step)\n",
            "I1229 11:15:13.841603 140034317584256 learning.py:507] global step 1330: loss = 2.5497 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1331: loss = 2.4482 (1.219 sec/step)\n",
            "I1229 11:15:15.062094 140034317584256 learning.py:507] global step 1331: loss = 2.4482 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1332: loss = 1.5271 (1.290 sec/step)\n",
            "I1229 11:15:16.354019 140034317584256 learning.py:507] global step 1332: loss = 1.5271 (1.290 sec/step)\n",
            "INFO:tensorflow:global step 1333: loss = 2.7102 (1.200 sec/step)\n",
            "I1229 11:15:17.555586 140034317584256 learning.py:507] global step 1333: loss = 2.7102 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1334: loss = 3.4241 (2.103 sec/step)\n",
            "I1229 11:15:19.663949 140034317584256 learning.py:507] global step 1334: loss = 3.4241 (2.103 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1334.\n",
            "I1229 11:15:19.827463 140030580766464 supervisor.py:1050] Recording summary at step 1334.\n",
            "INFO:tensorflow:global step 1335: loss = 2.7587 (1.222 sec/step)\n",
            "I1229 11:15:20.896683 140034317584256 learning.py:507] global step 1335: loss = 2.7587 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1336: loss = 3.1447 (1.238 sec/step)\n",
            "I1229 11:15:22.137201 140034317584256 learning.py:507] global step 1336: loss = 3.1447 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1337: loss = 2.1859 (1.197 sec/step)\n",
            "I1229 11:15:23.336162 140034317584256 learning.py:507] global step 1337: loss = 2.1859 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1338: loss = 2.7111 (1.194 sec/step)\n",
            "I1229 11:15:24.531566 140034317584256 learning.py:507] global step 1338: loss = 2.7111 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1339: loss = 3.3115 (1.170 sec/step)\n",
            "I1229 11:15:25.703053 140034317584256 learning.py:507] global step 1339: loss = 3.3115 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1340: loss = 2.4507 (1.216 sec/step)\n",
            "I1229 11:15:26.921089 140034317584256 learning.py:507] global step 1340: loss = 2.4507 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1341: loss = 2.9822 (1.188 sec/step)\n",
            "I1229 11:15:28.110537 140034317584256 learning.py:507] global step 1341: loss = 2.9822 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1342: loss = 1.9217 (1.157 sec/step)\n",
            "I1229 11:15:29.269123 140034317584256 learning.py:507] global step 1342: loss = 1.9217 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 1343: loss = 3.4056 (1.189 sec/step)\n",
            "I1229 11:15:30.460175 140034317584256 learning.py:507] global step 1343: loss = 3.4056 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1344: loss = 2.7653 (1.278 sec/step)\n",
            "I1229 11:15:31.740273 140034317584256 learning.py:507] global step 1344: loss = 2.7653 (1.278 sec/step)\n",
            "INFO:tensorflow:global step 1345: loss = 4.4208 (1.272 sec/step)\n",
            "I1229 11:15:33.013837 140034317584256 learning.py:507] global step 1345: loss = 4.4208 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 1346: loss = 2.3677 (1.265 sec/step)\n",
            "I1229 11:15:34.282192 140034317584256 learning.py:507] global step 1346: loss = 2.3677 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 1347: loss = 2.5322 (1.242 sec/step)\n",
            "I1229 11:15:35.525623 140034317584256 learning.py:507] global step 1347: loss = 2.5322 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1348: loss = 2.3375 (1.238 sec/step)\n",
            "I1229 11:15:36.766031 140034317584256 learning.py:507] global step 1348: loss = 2.3375 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1349: loss = 3.1039 (1.222 sec/step)\n",
            "I1229 11:15:37.989886 140034317584256 learning.py:507] global step 1349: loss = 3.1039 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1350: loss = 2.1673 (1.231 sec/step)\n",
            "I1229 11:15:39.222703 140034317584256 learning.py:507] global step 1350: loss = 2.1673 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1351: loss = 3.1328 (1.216 sec/step)\n",
            "I1229 11:15:40.440244 140034317584256 learning.py:507] global step 1351: loss = 3.1328 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1352: loss = 3.0983 (1.191 sec/step)\n",
            "I1229 11:15:41.633238 140034317584256 learning.py:507] global step 1352: loss = 3.0983 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1353: loss = 2.8218 (1.181 sec/step)\n",
            "I1229 11:15:42.816604 140034317584256 learning.py:507] global step 1353: loss = 2.8218 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1354: loss = 3.2651 (1.215 sec/step)\n",
            "I1229 11:15:44.033321 140034317584256 learning.py:507] global step 1354: loss = 3.2651 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1355: loss = 3.1613 (1.247 sec/step)\n",
            "I1229 11:15:45.281730 140034317584256 learning.py:507] global step 1355: loss = 3.1613 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1356: loss = 2.1331 (1.208 sec/step)\n",
            "I1229 11:15:46.491804 140034317584256 learning.py:507] global step 1356: loss = 2.1331 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1357: loss = 2.5334 (1.257 sec/step)\n",
            "I1229 11:15:47.750810 140034317584256 learning.py:507] global step 1357: loss = 2.5334 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 1358: loss = 3.6691 (1.226 sec/step)\n",
            "I1229 11:15:48.979131 140034317584256 learning.py:507] global step 1358: loss = 3.6691 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1359: loss = 4.1805 (1.226 sec/step)\n",
            "I1229 11:15:50.207277 140034317584256 learning.py:507] global step 1359: loss = 4.1805 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1360: loss = 2.6805 (1.214 sec/step)\n",
            "I1229 11:15:51.422994 140034317584256 learning.py:507] global step 1360: loss = 2.6805 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1361: loss = 2.2837 (1.238 sec/step)\n",
            "I1229 11:15:52.662858 140034317584256 learning.py:507] global step 1361: loss = 2.2837 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1362: loss = 4.0355 (1.243 sec/step)\n",
            "I1229 11:15:53.908253 140034317584256 learning.py:507] global step 1362: loss = 4.0355 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1363: loss = 3.2246 (1.223 sec/step)\n",
            "I1229 11:15:55.133022 140034317584256 learning.py:507] global step 1363: loss = 3.2246 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1364: loss = 3.1722 (1.230 sec/step)\n",
            "I1229 11:15:56.365132 140034317584256 learning.py:507] global step 1364: loss = 3.1722 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1365: loss = 2.9800 (1.285 sec/step)\n",
            "I1229 11:15:57.651929 140034317584256 learning.py:507] global step 1365: loss = 2.9800 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 1366: loss = 2.2116 (1.223 sec/step)\n",
            "I1229 11:15:58.876377 140034317584256 learning.py:507] global step 1366: loss = 2.2116 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1367: loss = 2.6730 (1.241 sec/step)\n",
            "I1229 11:16:00.119322 140034317584256 learning.py:507] global step 1367: loss = 2.6730 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1368: loss = 2.2479 (1.232 sec/step)\n",
            "I1229 11:16:01.352936 140034317584256 learning.py:507] global step 1368: loss = 2.2479 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1369: loss = 3.5574 (1.247 sec/step)\n",
            "I1229 11:16:02.601640 140034317584256 learning.py:507] global step 1369: loss = 3.5574 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1370: loss = 4.1906 (1.235 sec/step)\n",
            "I1229 11:16:03.837836 140034317584256 learning.py:507] global step 1370: loss = 4.1906 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1371: loss = 2.9455 (1.239 sec/step)\n",
            "I1229 11:16:05.078992 140034317584256 learning.py:507] global step 1371: loss = 2.9455 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1372: loss = 2.3186 (1.187 sec/step)\n",
            "I1229 11:16:06.268095 140034317584256 learning.py:507] global step 1372: loss = 2.3186 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1373: loss = 2.4465 (1.203 sec/step)\n",
            "I1229 11:16:07.472930 140034317584256 learning.py:507] global step 1373: loss = 2.4465 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1374: loss = 2.5648 (1.229 sec/step)\n",
            "I1229 11:16:08.703164 140034317584256 learning.py:507] global step 1374: loss = 2.5648 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 1375: loss = 2.9050 (1.203 sec/step)\n",
            "I1229 11:16:09.908149 140034317584256 learning.py:507] global step 1375: loss = 2.9050 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1376: loss = 1.8020 (1.206 sec/step)\n",
            "I1229 11:16:11.116008 140034317584256 learning.py:507] global step 1376: loss = 1.8020 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1377: loss = 2.4134 (1.222 sec/step)\n",
            "I1229 11:16:12.339550 140034317584256 learning.py:507] global step 1377: loss = 2.4134 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1378: loss = 1.5908 (1.203 sec/step)\n",
            "I1229 11:16:13.544045 140034317584256 learning.py:507] global step 1378: loss = 1.5908 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1379: loss = 2.3698 (1.184 sec/step)\n",
            "I1229 11:16:14.730126 140034317584256 learning.py:507] global step 1379: loss = 2.3698 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1380: loss = 3.4381 (1.264 sec/step)\n",
            "I1229 11:16:15.995585 140034317584256 learning.py:507] global step 1380: loss = 3.4381 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1381: loss = 3.1410 (1.185 sec/step)\n",
            "I1229 11:16:17.182549 140034317584256 learning.py:507] global step 1381: loss = 3.1410 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1382: loss = 1.9841 (1.243 sec/step)\n",
            "I1229 11:16:18.427531 140034317584256 learning.py:507] global step 1382: loss = 1.9841 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1383: loss = 2.7393 (1.209 sec/step)\n",
            "I1229 11:16:19.638127 140034317584256 learning.py:507] global step 1383: loss = 2.7393 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1384: loss = 1.9900 (1.212 sec/step)\n",
            "I1229 11:16:20.852226 140034317584256 learning.py:507] global step 1384: loss = 1.9900 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1385: loss = 2.1792 (1.247 sec/step)\n",
            "I1229 11:16:22.100717 140034317584256 learning.py:507] global step 1385: loss = 2.1792 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1386: loss = 1.8696 (1.240 sec/step)\n",
            "I1229 11:16:23.342845 140034317584256 learning.py:507] global step 1386: loss = 1.8696 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 1387: loss = 3.1643 (1.241 sec/step)\n",
            "I1229 11:16:24.585413 140034317584256 learning.py:507] global step 1387: loss = 3.1643 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1388: loss = 2.6793 (1.235 sec/step)\n",
            "I1229 11:16:25.822223 140034317584256 learning.py:507] global step 1388: loss = 2.6793 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1389: loss = 2.3495 (1.242 sec/step)\n",
            "I1229 11:16:27.066630 140034317584256 learning.py:507] global step 1389: loss = 2.3495 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1390: loss = 3.7200 (1.271 sec/step)\n",
            "I1229 11:16:28.339248 140034317584256 learning.py:507] global step 1390: loss = 3.7200 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 1391: loss = 2.5718 (1.235 sec/step)\n",
            "I1229 11:16:29.576026 140034317584256 learning.py:507] global step 1391: loss = 2.5718 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1392: loss = 1.7068 (1.199 sec/step)\n",
            "I1229 11:16:30.776673 140034317584256 learning.py:507] global step 1392: loss = 1.7068 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1393: loss = 2.3930 (1.214 sec/step)\n",
            "I1229 11:16:31.992726 140034317584256 learning.py:507] global step 1393: loss = 2.3930 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1394: loss = 3.0532 (1.267 sec/step)\n",
            "I1229 11:16:33.261377 140034317584256 learning.py:507] global step 1394: loss = 3.0532 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 1395: loss = 2.9212 (1.265 sec/step)\n",
            "I1229 11:16:34.528008 140034317584256 learning.py:507] global step 1395: loss = 2.9212 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 1396: loss = 3.0214 (1.295 sec/step)\n",
            "I1229 11:16:35.825073 140034317584256 learning.py:507] global step 1396: loss = 3.0214 (1.295 sec/step)\n",
            "INFO:tensorflow:global step 1397: loss = 3.1430 (1.256 sec/step)\n",
            "I1229 11:16:37.082877 140034317584256 learning.py:507] global step 1397: loss = 3.1430 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1398: loss = 2.4913 (1.272 sec/step)\n",
            "I1229 11:16:38.356561 140034317584256 learning.py:507] global step 1398: loss = 2.4913 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 1399: loss = 2.3125 (1.262 sec/step)\n",
            "I1229 11:16:39.620353 140034317584256 learning.py:507] global step 1399: loss = 2.3125 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 1400: loss = 2.6879 (1.229 sec/step)\n",
            "I1229 11:16:40.851357 140034317584256 learning.py:507] global step 1400: loss = 2.6879 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 1401: loss = 3.9186 (1.232 sec/step)\n",
            "I1229 11:16:42.085200 140034317584256 learning.py:507] global step 1401: loss = 3.9186 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1402: loss = 2.1272 (1.218 sec/step)\n",
            "I1229 11:16:43.305137 140034317584256 learning.py:507] global step 1402: loss = 2.1272 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1403: loss = 1.8123 (1.232 sec/step)\n",
            "I1229 11:16:44.538985 140034317584256 learning.py:507] global step 1403: loss = 1.8123 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1404: loss = 1.9774 (1.249 sec/step)\n",
            "I1229 11:16:45.789821 140034317584256 learning.py:507] global step 1404: loss = 1.9774 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1405: loss = 2.7562 (1.252 sec/step)\n",
            "I1229 11:16:47.043759 140034317584256 learning.py:507] global step 1405: loss = 2.7562 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1406: loss = 2.2210 (1.234 sec/step)\n",
            "I1229 11:16:48.279328 140034317584256 learning.py:507] global step 1406: loss = 2.2210 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1407: loss = 2.7523 (1.243 sec/step)\n",
            "I1229 11:16:49.524258 140034317584256 learning.py:507] global step 1407: loss = 2.7523 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1408: loss = 2.9931 (1.272 sec/step)\n",
            "I1229 11:16:50.798479 140034317584256 learning.py:507] global step 1408: loss = 2.9931 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 1409: loss = 3.3142 (1.260 sec/step)\n",
            "I1229 11:16:52.060349 140034317584256 learning.py:507] global step 1409: loss = 3.3142 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1410: loss = 2.9248 (1.216 sec/step)\n",
            "I1229 11:16:53.278027 140034317584256 learning.py:507] global step 1410: loss = 2.9248 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1411: loss = 2.0572 (1.213 sec/step)\n",
            "I1229 11:16:54.492554 140034317584256 learning.py:507] global step 1411: loss = 2.0572 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1412: loss = 2.0272 (1.255 sec/step)\n",
            "I1229 11:16:55.749523 140034317584256 learning.py:507] global step 1412: loss = 2.0272 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1413: loss = 1.8387 (1.216 sec/step)\n",
            "I1229 11:16:56.967098 140034317584256 learning.py:507] global step 1413: loss = 1.8387 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1414: loss = 2.3049 (1.242 sec/step)\n",
            "I1229 11:16:58.211053 140034317584256 learning.py:507] global step 1414: loss = 2.3049 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1415: loss = 2.3348 (1.252 sec/step)\n",
            "I1229 11:16:59.464886 140034317584256 learning.py:507] global step 1415: loss = 2.3348 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1416: loss = 2.3423 (1.188 sec/step)\n",
            "I1229 11:17:00.654273 140034317584256 learning.py:507] global step 1416: loss = 2.3423 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1417: loss = 2.0629 (1.217 sec/step)\n",
            "I1229 11:17:01.873132 140034317584256 learning.py:507] global step 1417: loss = 2.0629 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1418: loss = 1.9762 (1.206 sec/step)\n",
            "I1229 11:17:03.081096 140034317584256 learning.py:507] global step 1418: loss = 1.9762 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1419: loss = 2.2773 (1.208 sec/step)\n",
            "I1229 11:17:04.291000 140034317584256 learning.py:507] global step 1419: loss = 2.2773 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1420: loss = 2.6108 (1.264 sec/step)\n",
            "I1229 11:17:05.557259 140034317584256 learning.py:507] global step 1420: loss = 2.6108 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1421: loss = 2.2182 (1.195 sec/step)\n",
            "I1229 11:17:06.754414 140034317584256 learning.py:507] global step 1421: loss = 2.2182 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1422: loss = 1.6627 (1.221 sec/step)\n",
            "I1229 11:17:07.976881 140034317584256 learning.py:507] global step 1422: loss = 1.6627 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1423: loss = 3.1276 (1.243 sec/step)\n",
            "I1229 11:17:09.221426 140034317584256 learning.py:507] global step 1423: loss = 3.1276 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1424: loss = 3.3277 (1.232 sec/step)\n",
            "I1229 11:17:10.455311 140034317584256 learning.py:507] global step 1424: loss = 3.3277 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1425: loss = 2.7877 (1.234 sec/step)\n",
            "I1229 11:17:11.690593 140034317584256 learning.py:507] global step 1425: loss = 2.7877 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1426: loss = 1.9343 (1.207 sec/step)\n",
            "I1229 11:17:12.898839 140034317584256 learning.py:507] global step 1426: loss = 1.9343 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1427: loss = 2.4183 (1.207 sec/step)\n",
            "I1229 11:17:14.107452 140034317584256 learning.py:507] global step 1427: loss = 2.4183 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1428: loss = 3.0039 (1.193 sec/step)\n",
            "I1229 11:17:15.302116 140034317584256 learning.py:507] global step 1428: loss = 3.0039 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1429: loss = 1.5252 (1.268 sec/step)\n",
            "I1229 11:17:16.572237 140034317584256 learning.py:507] global step 1429: loss = 1.5252 (1.268 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 11:17:17.668361 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 1430: loss = 3.3659 (1.586 sec/step)\n",
            "I1229 11:17:18.354082 140034317584256 learning.py:507] global step 1430: loss = 3.3659 (1.586 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1430.\n",
            "I1229 11:17:20.429260 140030580766464 supervisor.py:1050] Recording summary at step 1430.\n",
            "INFO:tensorflow:global step 1431: loss = 2.3989 (2.191 sec/step)\n",
            "I1229 11:17:20.770068 140034317584256 learning.py:507] global step 1431: loss = 2.3989 (2.191 sec/step)\n",
            "INFO:tensorflow:global step 1432: loss = 2.4848 (1.925 sec/step)\n",
            "I1229 11:17:22.707115 140034317584256 learning.py:507] global step 1432: loss = 2.4848 (1.925 sec/step)\n",
            "INFO:tensorflow:global step 1433: loss = 2.7085 (1.737 sec/step)\n",
            "I1229 11:17:24.457428 140034317584256 learning.py:507] global step 1433: loss = 2.7085 (1.737 sec/step)\n",
            "INFO:tensorflow:global step 1434: loss = 3.7782 (1.553 sec/step)\n",
            "I1229 11:17:26.011932 140034317584256 learning.py:507] global step 1434: loss = 3.7782 (1.553 sec/step)\n",
            "INFO:tensorflow:global step 1435: loss = 2.8952 (1.214 sec/step)\n",
            "I1229 11:17:27.228330 140034317584256 learning.py:507] global step 1435: loss = 2.8952 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1436: loss = 2.9975 (1.232 sec/step)\n",
            "I1229 11:17:28.462244 140034317584256 learning.py:507] global step 1436: loss = 2.9975 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1437: loss = 2.0267 (1.202 sec/step)\n",
            "I1229 11:17:29.666063 140034317584256 learning.py:507] global step 1437: loss = 2.0267 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1438: loss = 2.6933 (1.244 sec/step)\n",
            "I1229 11:17:30.911509 140034317584256 learning.py:507] global step 1438: loss = 2.6933 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1439: loss = 2.0851 (1.249 sec/step)\n",
            "I1229 11:17:32.162356 140034317584256 learning.py:507] global step 1439: loss = 2.0851 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1440: loss = 3.9701 (1.262 sec/step)\n",
            "I1229 11:17:33.426412 140034317584256 learning.py:507] global step 1440: loss = 3.9701 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 1441: loss = 2.5025 (1.251 sec/step)\n",
            "I1229 11:17:34.678814 140034317584256 learning.py:507] global step 1441: loss = 2.5025 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1442: loss = 1.9871 (1.252 sec/step)\n",
            "I1229 11:17:35.932577 140034317584256 learning.py:507] global step 1442: loss = 1.9871 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1443: loss = 3.2734 (1.216 sec/step)\n",
            "I1229 11:17:37.150030 140034317584256 learning.py:507] global step 1443: loss = 3.2734 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1444: loss = 1.9486 (1.191 sec/step)\n",
            "I1229 11:17:38.342881 140034317584256 learning.py:507] global step 1444: loss = 1.9486 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1445: loss = 2.7090 (1.201 sec/step)\n",
            "I1229 11:17:39.546071 140034317584256 learning.py:507] global step 1445: loss = 2.7090 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1446: loss = 1.4224 (1.238 sec/step)\n",
            "I1229 11:17:40.786036 140034317584256 learning.py:507] global step 1446: loss = 1.4224 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1447: loss = 3.3514 (1.203 sec/step)\n",
            "I1229 11:17:41.990894 140034317584256 learning.py:507] global step 1447: loss = 3.3514 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1448: loss = 2.6839 (1.330 sec/step)\n",
            "I1229 11:17:43.323375 140034317584256 learning.py:507] global step 1448: loss = 2.6839 (1.330 sec/step)\n",
            "INFO:tensorflow:global step 1449: loss = 2.5135 (1.215 sec/step)\n",
            "I1229 11:17:44.540339 140034317584256 learning.py:507] global step 1449: loss = 2.5135 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1450: loss = 3.5970 (1.192 sec/step)\n",
            "I1229 11:17:45.733964 140034317584256 learning.py:507] global step 1450: loss = 3.5970 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1451: loss = 3.1379 (1.178 sec/step)\n",
            "I1229 11:17:46.913319 140034317584256 learning.py:507] global step 1451: loss = 3.1379 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1452: loss = 1.8106 (1.310 sec/step)\n",
            "I1229 11:17:48.225027 140034317584256 learning.py:507] global step 1452: loss = 1.8106 (1.310 sec/step)\n",
            "INFO:tensorflow:global step 1453: loss = 3.5542 (1.256 sec/step)\n",
            "I1229 11:17:49.482642 140034317584256 learning.py:507] global step 1453: loss = 3.5542 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1454: loss = 4.5299 (1.173 sec/step)\n",
            "I1229 11:17:50.656984 140034317584256 learning.py:507] global step 1454: loss = 4.5299 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1455: loss = 3.2094 (1.225 sec/step)\n",
            "I1229 11:17:51.884074 140034317584256 learning.py:507] global step 1455: loss = 3.2094 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1456: loss = 4.2322 (1.198 sec/step)\n",
            "I1229 11:17:53.084128 140034317584256 learning.py:507] global step 1456: loss = 4.2322 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1457: loss = 3.7392 (1.225 sec/step)\n",
            "I1229 11:17:54.311117 140034317584256 learning.py:507] global step 1457: loss = 3.7392 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1458: loss = 2.8329 (1.200 sec/step)\n",
            "I1229 11:17:55.512661 140034317584256 learning.py:507] global step 1458: loss = 2.8329 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1459: loss = 2.4316 (1.210 sec/step)\n",
            "I1229 11:17:56.724318 140034317584256 learning.py:507] global step 1459: loss = 2.4316 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1460: loss = 2.4106 (1.264 sec/step)\n",
            "I1229 11:17:57.990422 140034317584256 learning.py:507] global step 1460: loss = 2.4106 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1461: loss = 2.0084 (1.188 sec/step)\n",
            "I1229 11:17:59.179739 140034317584256 learning.py:507] global step 1461: loss = 2.0084 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1462: loss = 2.6651 (1.206 sec/step)\n",
            "I1229 11:18:00.388476 140034317584256 learning.py:507] global step 1462: loss = 2.6651 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1463: loss = 2.9265 (1.187 sec/step)\n",
            "I1229 11:18:01.577065 140034317584256 learning.py:507] global step 1463: loss = 2.9265 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1464: loss = 3.3431 (1.198 sec/step)\n",
            "I1229 11:18:02.777432 140034317584256 learning.py:507] global step 1464: loss = 3.3431 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1465: loss = 2.6711 (1.201 sec/step)\n",
            "I1229 11:18:03.980065 140034317584256 learning.py:507] global step 1465: loss = 2.6711 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1466: loss = 3.1011 (1.204 sec/step)\n",
            "I1229 11:18:05.185499 140034317584256 learning.py:507] global step 1466: loss = 3.1011 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1467: loss = 2.3579 (1.197 sec/step)\n",
            "I1229 11:18:06.383744 140034317584256 learning.py:507] global step 1467: loss = 2.3579 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1468: loss = 2.1005 (1.220 sec/step)\n",
            "I1229 11:18:07.605293 140034317584256 learning.py:507] global step 1468: loss = 2.1005 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1469: loss = 2.3875 (1.209 sec/step)\n",
            "I1229 11:18:08.815999 140034317584256 learning.py:507] global step 1469: loss = 2.3875 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1470: loss = 3.0276 (1.197 sec/step)\n",
            "I1229 11:18:10.014567 140034317584256 learning.py:507] global step 1470: loss = 3.0276 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1471: loss = 2.6088 (1.215 sec/step)\n",
            "I1229 11:18:11.231042 140034317584256 learning.py:507] global step 1471: loss = 2.6088 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1472: loss = 3.0500 (1.236 sec/step)\n",
            "I1229 11:18:12.468600 140034317584256 learning.py:507] global step 1472: loss = 3.0500 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1473: loss = 2.5919 (1.223 sec/step)\n",
            "I1229 11:18:13.692769 140034317584256 learning.py:507] global step 1473: loss = 2.5919 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1474: loss = 2.2874 (1.210 sec/step)\n",
            "I1229 11:18:14.904187 140034317584256 learning.py:507] global step 1474: loss = 2.2874 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1475: loss = 2.9328 (1.242 sec/step)\n",
            "I1229 11:18:16.147773 140034317584256 learning.py:507] global step 1475: loss = 2.9328 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1476: loss = 3.9450 (1.210 sec/step)\n",
            "I1229 11:18:17.359257 140034317584256 learning.py:507] global step 1476: loss = 3.9450 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1477: loss = 2.1696 (1.236 sec/step)\n",
            "I1229 11:18:18.596827 140034317584256 learning.py:507] global step 1477: loss = 2.1696 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1478: loss = 1.9377 (1.220 sec/step)\n",
            "I1229 11:18:19.818313 140034317584256 learning.py:507] global step 1478: loss = 1.9377 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1479: loss = 2.7032 (1.207 sec/step)\n",
            "I1229 11:18:21.026616 140034317584256 learning.py:507] global step 1479: loss = 2.7032 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1480: loss = 3.6904 (1.173 sec/step)\n",
            "I1229 11:18:22.200981 140034317584256 learning.py:507] global step 1480: loss = 3.6904 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1481: loss = 2.7622 (1.242 sec/step)\n",
            "I1229 11:18:23.444643 140034317584256 learning.py:507] global step 1481: loss = 2.7622 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1482: loss = 3.0876 (1.212 sec/step)\n",
            "I1229 11:18:24.658729 140034317584256 learning.py:507] global step 1482: loss = 3.0876 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1483: loss = 3.3359 (1.223 sec/step)\n",
            "I1229 11:18:25.883162 140034317584256 learning.py:507] global step 1483: loss = 3.3359 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1484: loss = 3.4723 (1.235 sec/step)\n",
            "I1229 11:18:27.119816 140034317584256 learning.py:507] global step 1484: loss = 3.4723 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1485: loss = 3.5565 (1.209 sec/step)\n",
            "I1229 11:18:28.330471 140034317584256 learning.py:507] global step 1485: loss = 3.5565 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1486: loss = 3.0108 (1.181 sec/step)\n",
            "I1229 11:18:29.512755 140034317584256 learning.py:507] global step 1486: loss = 3.0108 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1487: loss = 2.4345 (1.177 sec/step)\n",
            "I1229 11:18:30.691999 140034317584256 learning.py:507] global step 1487: loss = 2.4345 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1488: loss = 3.1462 (1.180 sec/step)\n",
            "I1229 11:18:31.873901 140034317584256 learning.py:507] global step 1488: loss = 3.1462 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1489: loss = 2.9346 (1.215 sec/step)\n",
            "I1229 11:18:33.090932 140034317584256 learning.py:507] global step 1489: loss = 2.9346 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1490: loss = 4.2067 (1.299 sec/step)\n",
            "I1229 11:18:34.391721 140034317584256 learning.py:507] global step 1490: loss = 4.2067 (1.299 sec/step)\n",
            "INFO:tensorflow:global step 1491: loss = 2.5042 (1.315 sec/step)\n",
            "I1229 11:18:35.709437 140034317584256 learning.py:507] global step 1491: loss = 2.5042 (1.315 sec/step)\n",
            "INFO:tensorflow:global step 1492: loss = 2.2862 (1.243 sec/step)\n",
            "I1229 11:18:36.954688 140034317584256 learning.py:507] global step 1492: loss = 2.2862 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1493: loss = 2.0723 (1.211 sec/step)\n",
            "I1229 11:18:38.167992 140034317584256 learning.py:507] global step 1493: loss = 2.0723 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1494: loss = 3.1230 (1.195 sec/step)\n",
            "I1229 11:18:39.364471 140034317584256 learning.py:507] global step 1494: loss = 3.1230 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1495: loss = 3.1167 (1.194 sec/step)\n",
            "I1229 11:18:40.560495 140034317584256 learning.py:507] global step 1495: loss = 3.1167 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1496: loss = 2.3852 (1.275 sec/step)\n",
            "I1229 11:18:41.836695 140034317584256 learning.py:507] global step 1496: loss = 2.3852 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 1497: loss = 3.1519 (1.257 sec/step)\n",
            "I1229 11:18:43.095381 140034317584256 learning.py:507] global step 1497: loss = 3.1519 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 1498: loss = 1.8536 (1.239 sec/step)\n",
            "I1229 11:18:44.336314 140034317584256 learning.py:507] global step 1498: loss = 1.8536 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1499: loss = 3.1059 (1.299 sec/step)\n",
            "I1229 11:18:45.637468 140034317584256 learning.py:507] global step 1499: loss = 3.1059 (1.299 sec/step)\n",
            "INFO:tensorflow:global step 1500: loss = 2.2532 (1.237 sec/step)\n",
            "I1229 11:18:46.876658 140034317584256 learning.py:507] global step 1500: loss = 2.2532 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1501: loss = 2.2575 (1.275 sec/step)\n",
            "I1229 11:18:48.153558 140034317584256 learning.py:507] global step 1501: loss = 2.2575 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 1502: loss = 2.4822 (1.233 sec/step)\n",
            "I1229 11:18:49.388550 140034317584256 learning.py:507] global step 1502: loss = 2.4822 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1503: loss = 2.6001 (1.194 sec/step)\n",
            "I1229 11:18:50.584134 140034317584256 learning.py:507] global step 1503: loss = 2.6001 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1504: loss = 2.3590 (1.206 sec/step)\n",
            "I1229 11:18:51.791552 140034317584256 learning.py:507] global step 1504: loss = 2.3590 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1505: loss = 2.6133 (1.238 sec/step)\n",
            "I1229 11:18:53.031392 140034317584256 learning.py:507] global step 1505: loss = 2.6133 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1506: loss = 3.3833 (1.217 sec/step)\n",
            "I1229 11:18:54.250565 140034317584256 learning.py:507] global step 1506: loss = 3.3833 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1507: loss = 3.2673 (1.214 sec/step)\n",
            "I1229 11:18:55.466385 140034317584256 learning.py:507] global step 1507: loss = 3.2673 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1508: loss = 3.5695 (1.284 sec/step)\n",
            "I1229 11:18:56.751676 140034317584256 learning.py:507] global step 1508: loss = 3.5695 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 1509: loss = 2.7662 (1.218 sec/step)\n",
            "I1229 11:18:57.972003 140034317584256 learning.py:507] global step 1509: loss = 2.7662 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1510: loss = 2.9504 (1.179 sec/step)\n",
            "I1229 11:18:59.152966 140034317584256 learning.py:507] global step 1510: loss = 2.9504 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 1511: loss = 2.4887 (1.215 sec/step)\n",
            "I1229 11:19:00.370036 140034317584256 learning.py:507] global step 1511: loss = 2.4887 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1512: loss = 3.0547 (1.184 sec/step)\n",
            "I1229 11:19:01.556033 140034317584256 learning.py:507] global step 1512: loss = 3.0547 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1513: loss = 2.4391 (1.244 sec/step)\n",
            "I1229 11:19:02.803320 140034317584256 learning.py:507] global step 1513: loss = 2.4391 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1514: loss = 2.8242 (1.220 sec/step)\n",
            "I1229 11:19:04.025198 140034317584256 learning.py:507] global step 1514: loss = 2.8242 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1515: loss = 2.7890 (1.251 sec/step)\n",
            "I1229 11:19:05.278234 140034317584256 learning.py:507] global step 1515: loss = 2.7890 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1516: loss = 4.4196 (1.230 sec/step)\n",
            "I1229 11:19:06.510509 140034317584256 learning.py:507] global step 1516: loss = 4.4196 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1517: loss = 2.5030 (1.217 sec/step)\n",
            "I1229 11:19:07.729640 140034317584256 learning.py:507] global step 1517: loss = 2.5030 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1518: loss = 2.4205 (1.174 sec/step)\n",
            "I1229 11:19:08.905155 140034317584256 learning.py:507] global step 1518: loss = 2.4205 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1519: loss = 2.5701 (1.232 sec/step)\n",
            "I1229 11:19:10.138455 140034317584256 learning.py:507] global step 1519: loss = 2.5701 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1520: loss = 3.1649 (1.200 sec/step)\n",
            "I1229 11:19:11.340433 140034317584256 learning.py:507] global step 1520: loss = 3.1649 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1521: loss = 5.3993 (1.195 sec/step)\n",
            "I1229 11:19:12.536698 140034317584256 learning.py:507] global step 1521: loss = 5.3993 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1522: loss = 2.3418 (1.240 sec/step)\n",
            "I1229 11:19:13.778140 140034317584256 learning.py:507] global step 1522: loss = 2.3418 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 1523: loss = 2.7007 (1.220 sec/step)\n",
            "I1229 11:19:15.000662 140034317584256 learning.py:507] global step 1523: loss = 2.7007 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1524: loss = 3.5297 (1.276 sec/step)\n",
            "I1229 11:19:16.278392 140034317584256 learning.py:507] global step 1524: loss = 3.5297 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 1525: loss = 2.4088 (1.265 sec/step)\n",
            "I1229 11:19:17.545246 140034317584256 learning.py:507] global step 1525: loss = 2.4088 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 1526: loss = 3.0265 (2.119 sec/step)\n",
            "I1229 11:19:19.666569 140034317584256 learning.py:507] global step 1526: loss = 3.0265 (2.119 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1526.\n",
            "I1229 11:19:19.761730 140030580766464 supervisor.py:1050] Recording summary at step 1526.\n",
            "INFO:tensorflow:global step 1527: loss = 3.7186 (1.261 sec/step)\n",
            "I1229 11:19:20.928785 140034317584256 learning.py:507] global step 1527: loss = 3.7186 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 1528: loss = 4.3029 (1.223 sec/step)\n",
            "I1229 11:19:22.153061 140034317584256 learning.py:507] global step 1528: loss = 4.3029 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1529: loss = 3.0735 (1.213 sec/step)\n",
            "I1229 11:19:23.368529 140034317584256 learning.py:507] global step 1529: loss = 3.0735 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1530: loss = 2.4825 (1.210 sec/step)\n",
            "I1229 11:19:24.580610 140034317584256 learning.py:507] global step 1530: loss = 2.4825 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1531: loss = 2.1055 (1.179 sec/step)\n",
            "I1229 11:19:25.760903 140034317584256 learning.py:507] global step 1531: loss = 2.1055 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 1532: loss = 3.6511 (1.248 sec/step)\n",
            "I1229 11:19:27.010681 140034317584256 learning.py:507] global step 1532: loss = 3.6511 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 1533: loss = 2.6323 (1.203 sec/step)\n",
            "I1229 11:19:28.215348 140034317584256 learning.py:507] global step 1533: loss = 2.6323 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1534: loss = 2.7382 (1.202 sec/step)\n",
            "I1229 11:19:29.418652 140034317584256 learning.py:507] global step 1534: loss = 2.7382 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1535: loss = 2.8152 (1.206 sec/step)\n",
            "I1229 11:19:30.626819 140034317584256 learning.py:507] global step 1535: loss = 2.8152 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1536: loss = 3.2528 (1.247 sec/step)\n",
            "I1229 11:19:31.877647 140034317584256 learning.py:507] global step 1536: loss = 3.2528 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1537: loss = 3.3130 (1.181 sec/step)\n",
            "I1229 11:19:33.059881 140034317584256 learning.py:507] global step 1537: loss = 3.3130 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1538: loss = 2.1098 (1.200 sec/step)\n",
            "I1229 11:19:34.261878 140034317584256 learning.py:507] global step 1538: loss = 2.1098 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1539: loss = 2.8424 (1.238 sec/step)\n",
            "I1229 11:19:35.501519 140034317584256 learning.py:507] global step 1539: loss = 2.8424 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1540: loss = 2.0543 (1.224 sec/step)\n",
            "I1229 11:19:36.727832 140034317584256 learning.py:507] global step 1540: loss = 2.0543 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1541: loss = 1.7601 (1.166 sec/step)\n",
            "I1229 11:19:37.895275 140034317584256 learning.py:507] global step 1541: loss = 1.7601 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 1542: loss = 2.3496 (1.209 sec/step)\n",
            "I1229 11:19:39.105592 140034317584256 learning.py:507] global step 1542: loss = 2.3496 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1543: loss = 2.6185 (1.208 sec/step)\n",
            "I1229 11:19:40.315546 140034317584256 learning.py:507] global step 1543: loss = 2.6185 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1544: loss = 2.9587 (1.203 sec/step)\n",
            "I1229 11:19:41.519864 140034317584256 learning.py:507] global step 1544: loss = 2.9587 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1545: loss = 2.3596 (1.233 sec/step)\n",
            "I1229 11:19:42.754817 140034317584256 learning.py:507] global step 1545: loss = 2.3596 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1546: loss = 2.3708 (1.232 sec/step)\n",
            "I1229 11:19:43.988356 140034317584256 learning.py:507] global step 1546: loss = 2.3708 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1547: loss = 3.4735 (1.218 sec/step)\n",
            "I1229 11:19:45.208928 140034317584256 learning.py:507] global step 1547: loss = 3.4735 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1548: loss = 2.0531 (1.225 sec/step)\n",
            "I1229 11:19:46.435734 140034317584256 learning.py:507] global step 1548: loss = 2.0531 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1549: loss = 2.6550 (1.226 sec/step)\n",
            "I1229 11:19:47.663590 140034317584256 learning.py:507] global step 1549: loss = 2.6550 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1550: loss = 2.9808 (1.281 sec/step)\n",
            "I1229 11:19:48.946098 140034317584256 learning.py:507] global step 1550: loss = 2.9808 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 1551: loss = 2.4797 (1.209 sec/step)\n",
            "I1229 11:19:50.156504 140034317584256 learning.py:507] global step 1551: loss = 2.4797 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1552: loss = 2.4230 (1.300 sec/step)\n",
            "I1229 11:19:51.457983 140034317584256 learning.py:507] global step 1552: loss = 2.4230 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 1553: loss = 2.8178 (1.178 sec/step)\n",
            "I1229 11:19:52.637721 140034317584256 learning.py:507] global step 1553: loss = 2.8178 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1554: loss = 2.8973 (1.176 sec/step)\n",
            "I1229 11:19:53.815432 140034317584256 learning.py:507] global step 1554: loss = 2.8973 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1555: loss = 2.0889 (1.157 sec/step)\n",
            "I1229 11:19:54.974186 140034317584256 learning.py:507] global step 1555: loss = 2.0889 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 1556: loss = 2.6061 (1.154 sec/step)\n",
            "I1229 11:19:56.129917 140034317584256 learning.py:507] global step 1556: loss = 2.6061 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 1557: loss = 2.1912 (1.212 sec/step)\n",
            "I1229 11:19:57.344114 140034317584256 learning.py:507] global step 1557: loss = 2.1912 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1558: loss = 1.8033 (1.198 sec/step)\n",
            "I1229 11:19:58.543733 140034317584256 learning.py:507] global step 1558: loss = 1.8033 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1559: loss = 2.4563 (1.211 sec/step)\n",
            "I1229 11:19:59.756547 140034317584256 learning.py:507] global step 1559: loss = 2.4563 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1560: loss = 3.1465 (1.170 sec/step)\n",
            "I1229 11:20:00.928782 140034317584256 learning.py:507] global step 1560: loss = 3.1465 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1561: loss = 2.5141 (1.175 sec/step)\n",
            "I1229 11:20:02.105131 140034317584256 learning.py:507] global step 1561: loss = 2.5141 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1562: loss = 2.8100 (1.213 sec/step)\n",
            "I1229 11:20:03.320154 140034317584256 learning.py:507] global step 1562: loss = 2.8100 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1563: loss = 2.1870 (1.168 sec/step)\n",
            "I1229 11:20:04.489978 140034317584256 learning.py:507] global step 1563: loss = 2.1870 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 1564: loss = 3.3201 (1.200 sec/step)\n",
            "I1229 11:20:05.691771 140034317584256 learning.py:507] global step 1564: loss = 3.3201 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1565: loss = 2.5288 (1.217 sec/step)\n",
            "I1229 11:20:06.910858 140034317584256 learning.py:507] global step 1565: loss = 2.5288 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 1566: loss = 3.9967 (1.194 sec/step)\n",
            "I1229 11:20:08.107249 140034317584256 learning.py:507] global step 1566: loss = 3.9967 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1567: loss = 2.4894 (1.256 sec/step)\n",
            "I1229 11:20:09.364804 140034317584256 learning.py:507] global step 1567: loss = 2.4894 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1568: loss = 2.0143 (1.223 sec/step)\n",
            "I1229 11:20:10.589550 140034317584256 learning.py:507] global step 1568: loss = 2.0143 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1569: loss = 2.9333 (1.160 sec/step)\n",
            "I1229 11:20:11.750919 140034317584256 learning.py:507] global step 1569: loss = 2.9333 (1.160 sec/step)\n",
            "INFO:tensorflow:global step 1570: loss = 1.8348 (1.175 sec/step)\n",
            "I1229 11:20:12.927453 140034317584256 learning.py:507] global step 1570: loss = 1.8348 (1.175 sec/step)\n",
            "INFO:tensorflow:global step 1571: loss = 4.1676 (1.183 sec/step)\n",
            "I1229 11:20:14.111924 140034317584256 learning.py:507] global step 1571: loss = 4.1676 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1572: loss = 2.3300 (1.149 sec/step)\n",
            "I1229 11:20:15.262363 140034317584256 learning.py:507] global step 1572: loss = 2.3300 (1.149 sec/step)\n",
            "INFO:tensorflow:global step 1573: loss = 2.8426 (1.289 sec/step)\n",
            "I1229 11:20:16.552872 140034317584256 learning.py:507] global step 1573: loss = 2.8426 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 1574: loss = 2.0338 (1.194 sec/step)\n",
            "I1229 11:20:17.748293 140034317584256 learning.py:507] global step 1574: loss = 2.0338 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1575: loss = 1.9085 (1.224 sec/step)\n",
            "I1229 11:20:18.973960 140034317584256 learning.py:507] global step 1575: loss = 1.9085 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1576: loss = 3.2136 (1.148 sec/step)\n",
            "I1229 11:20:20.123647 140034317584256 learning.py:507] global step 1576: loss = 3.2136 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 1577: loss = 2.2097 (1.216 sec/step)\n",
            "I1229 11:20:21.341308 140034317584256 learning.py:507] global step 1577: loss = 2.2097 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1578: loss = 3.0065 (1.204 sec/step)\n",
            "I1229 11:20:22.546643 140034317584256 learning.py:507] global step 1578: loss = 3.0065 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1579: loss = 3.7556 (1.227 sec/step)\n",
            "I1229 11:20:23.775717 140034317584256 learning.py:507] global step 1579: loss = 3.7556 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1580: loss = 3.0369 (1.189 sec/step)\n",
            "I1229 11:20:24.966856 140034317584256 learning.py:507] global step 1580: loss = 3.0369 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1581: loss = 3.3919 (1.182 sec/step)\n",
            "I1229 11:20:26.150982 140034317584256 learning.py:507] global step 1581: loss = 3.3919 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1582: loss = 2.1142 (1.190 sec/step)\n",
            "I1229 11:20:27.343348 140034317584256 learning.py:507] global step 1582: loss = 2.1142 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1583: loss = 1.7293 (1.215 sec/step)\n",
            "I1229 11:20:28.559562 140034317584256 learning.py:507] global step 1583: loss = 1.7293 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1584: loss = 3.9080 (1.177 sec/step)\n",
            "I1229 11:20:29.738369 140034317584256 learning.py:507] global step 1584: loss = 3.9080 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1585: loss = 2.1627 (1.210 sec/step)\n",
            "I1229 11:20:30.949689 140034317584256 learning.py:507] global step 1585: loss = 2.1627 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1586: loss = 1.7128 (1.218 sec/step)\n",
            "I1229 11:20:32.169775 140034317584256 learning.py:507] global step 1586: loss = 1.7128 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1587: loss = 3.6960 (1.173 sec/step)\n",
            "I1229 11:20:33.344697 140034317584256 learning.py:507] global step 1587: loss = 3.6960 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1588: loss = 2.5376 (1.216 sec/step)\n",
            "I1229 11:20:34.563161 140034317584256 learning.py:507] global step 1588: loss = 2.5376 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1589: loss = 1.9386 (1.207 sec/step)\n",
            "I1229 11:20:35.771969 140034317584256 learning.py:507] global step 1589: loss = 1.9386 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1590: loss = 2.1243 (1.218 sec/step)\n",
            "I1229 11:20:36.992022 140034317584256 learning.py:507] global step 1590: loss = 2.1243 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1591: loss = 2.1319 (1.170 sec/step)\n",
            "I1229 11:20:38.164021 140034317584256 learning.py:507] global step 1591: loss = 2.1319 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1592: loss = 3.0620 (1.190 sec/step)\n",
            "I1229 11:20:39.356317 140034317584256 learning.py:507] global step 1592: loss = 3.0620 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1593: loss = 2.1366 (1.154 sec/step)\n",
            "I1229 11:20:40.512046 140034317584256 learning.py:507] global step 1593: loss = 2.1366 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 1594: loss = 2.5399 (1.207 sec/step)\n",
            "I1229 11:20:41.720250 140034317584256 learning.py:507] global step 1594: loss = 2.5399 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1595: loss = 2.4546 (1.165 sec/step)\n",
            "I1229 11:20:42.886420 140034317584256 learning.py:507] global step 1595: loss = 2.4546 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 1596: loss = 2.1750 (1.169 sec/step)\n",
            "I1229 11:20:44.056695 140034317584256 learning.py:507] global step 1596: loss = 2.1750 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 1597: loss = 3.4254 (1.162 sec/step)\n",
            "I1229 11:20:45.220605 140034317584256 learning.py:507] global step 1597: loss = 3.4254 (1.162 sec/step)\n",
            "INFO:tensorflow:global step 1598: loss = 2.5222 (1.152 sec/step)\n",
            "I1229 11:20:46.374751 140034317584256 learning.py:507] global step 1598: loss = 2.5222 (1.152 sec/step)\n",
            "INFO:tensorflow:global step 1599: loss = 3.1542 (1.176 sec/step)\n",
            "I1229 11:20:47.552482 140034317584256 learning.py:507] global step 1599: loss = 3.1542 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1600: loss = 2.6933 (1.227 sec/step)\n",
            "I1229 11:20:48.781695 140034317584256 learning.py:507] global step 1600: loss = 2.6933 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1601: loss = 1.7247 (1.208 sec/step)\n",
            "I1229 11:20:49.992223 140034317584256 learning.py:507] global step 1601: loss = 1.7247 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1602: loss = 2.5321 (1.176 sec/step)\n",
            "I1229 11:20:51.170374 140034317584256 learning.py:507] global step 1602: loss = 2.5321 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1603: loss = 2.8562 (1.212 sec/step)\n",
            "I1229 11:20:52.384407 140034317584256 learning.py:507] global step 1603: loss = 2.8562 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1604: loss = 2.2095 (1.193 sec/step)\n",
            "I1229 11:20:53.579374 140034317584256 learning.py:507] global step 1604: loss = 2.2095 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1605: loss = 2.2193 (1.164 sec/step)\n",
            "I1229 11:20:54.744994 140034317584256 learning.py:507] global step 1605: loss = 2.2193 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 1606: loss = 2.7028 (1.163 sec/step)\n",
            "I1229 11:20:55.910117 140034317584256 learning.py:507] global step 1606: loss = 2.7028 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 1607: loss = 2.2698 (1.180 sec/step)\n",
            "I1229 11:20:57.092263 140034317584256 learning.py:507] global step 1607: loss = 2.2698 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1608: loss = 1.8273 (1.170 sec/step)\n",
            "I1229 11:20:58.263866 140034317584256 learning.py:507] global step 1608: loss = 1.8273 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1609: loss = 1.5746 (1.192 sec/step)\n",
            "I1229 11:20:59.457936 140034317584256 learning.py:507] global step 1609: loss = 1.5746 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1610: loss = 3.1186 (1.163 sec/step)\n",
            "I1229 11:21:00.622169 140034317584256 learning.py:507] global step 1610: loss = 3.1186 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 1611: loss = 2.8562 (1.202 sec/step)\n",
            "I1229 11:21:01.825934 140034317584256 learning.py:507] global step 1611: loss = 2.8562 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1612: loss = 3.7515 (1.189 sec/step)\n",
            "I1229 11:21:03.016288 140034317584256 learning.py:507] global step 1612: loss = 3.7515 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1613: loss = 2.9971 (1.220 sec/step)\n",
            "I1229 11:21:04.238010 140034317584256 learning.py:507] global step 1613: loss = 2.9971 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1614: loss = 3.3374 (1.189 sec/step)\n",
            "I1229 11:21:05.428674 140034317584256 learning.py:507] global step 1614: loss = 3.3374 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1615: loss = 1.8697 (1.208 sec/step)\n",
            "I1229 11:21:06.638119 140034317584256 learning.py:507] global step 1615: loss = 1.8697 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1616: loss = 2.1258 (1.209 sec/step)\n",
            "I1229 11:21:07.849270 140034317584256 learning.py:507] global step 1616: loss = 2.1258 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1617: loss = 2.5768 (1.185 sec/step)\n",
            "I1229 11:21:09.035584 140034317584256 learning.py:507] global step 1617: loss = 2.5768 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1618: loss = 2.3370 (1.226 sec/step)\n",
            "I1229 11:21:10.262854 140034317584256 learning.py:507] global step 1618: loss = 2.3370 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1619: loss = 2.5553 (1.227 sec/step)\n",
            "I1229 11:21:11.491456 140034317584256 learning.py:507] global step 1619: loss = 2.5553 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1620: loss = 3.1593 (1.185 sec/step)\n",
            "I1229 11:21:12.677959 140034317584256 learning.py:507] global step 1620: loss = 3.1593 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1621: loss = 2.9182 (1.269 sec/step)\n",
            "I1229 11:21:13.948411 140034317584256 learning.py:507] global step 1621: loss = 2.9182 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 1622: loss = 3.5207 (1.226 sec/step)\n",
            "I1229 11:21:15.176316 140034317584256 learning.py:507] global step 1622: loss = 3.5207 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1623: loss = 2.3018 (1.239 sec/step)\n",
            "I1229 11:21:16.416918 140034317584256 learning.py:507] global step 1623: loss = 2.3018 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1624: loss = 2.3919 (1.180 sec/step)\n",
            "I1229 11:21:17.598082 140034317584256 learning.py:507] global step 1624: loss = 2.3919 (1.180 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1624.\n",
            "I1229 11:21:19.726875 140030580766464 supervisor.py:1050] Recording summary at step 1624.\n",
            "INFO:tensorflow:global step 1625: loss = 2.8784 (2.354 sec/step)\n",
            "I1229 11:21:19.954254 140034317584256 learning.py:507] global step 1625: loss = 2.8784 (2.354 sec/step)\n",
            "INFO:tensorflow:global step 1626: loss = 2.5567 (1.213 sec/step)\n",
            "I1229 11:21:21.168261 140034317584256 learning.py:507] global step 1626: loss = 2.5567 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1627: loss = 2.7624 (1.223 sec/step)\n",
            "I1229 11:21:22.392574 140034317584256 learning.py:507] global step 1627: loss = 2.7624 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1628: loss = 2.2885 (1.172 sec/step)\n",
            "I1229 11:21:23.566583 140034317584256 learning.py:507] global step 1628: loss = 2.2885 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 1629: loss = 2.0621 (1.233 sec/step)\n",
            "I1229 11:21:24.801331 140034317584256 learning.py:507] global step 1629: loss = 2.0621 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1630: loss = 1.9535 (1.205 sec/step)\n",
            "I1229 11:21:26.008905 140034317584256 learning.py:507] global step 1630: loss = 1.9535 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1631: loss = 3.3510 (1.197 sec/step)\n",
            "I1229 11:21:27.208569 140034317584256 learning.py:507] global step 1631: loss = 3.3510 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1632: loss = 3.1417 (1.186 sec/step)\n",
            "I1229 11:21:28.395840 140034317584256 learning.py:507] global step 1632: loss = 3.1417 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1633: loss = 2.4146 (1.127 sec/step)\n",
            "I1229 11:21:29.524267 140034317584256 learning.py:507] global step 1633: loss = 2.4146 (1.127 sec/step)\n",
            "INFO:tensorflow:global step 1634: loss = 3.6898 (1.173 sec/step)\n",
            "I1229 11:21:30.698734 140034317584256 learning.py:507] global step 1634: loss = 3.6898 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1635: loss = 2.8964 (1.220 sec/step)\n",
            "I1229 11:21:31.920523 140034317584256 learning.py:507] global step 1635: loss = 2.8964 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1636: loss = 2.0330 (1.174 sec/step)\n",
            "I1229 11:21:33.096011 140034317584256 learning.py:507] global step 1636: loss = 2.0330 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1637: loss = 1.8442 (1.213 sec/step)\n",
            "I1229 11:21:34.310747 140034317584256 learning.py:507] global step 1637: loss = 1.8442 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1638: loss = 2.1404 (1.257 sec/step)\n",
            "I1229 11:21:35.568953 140034317584256 learning.py:507] global step 1638: loss = 2.1404 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 1639: loss = 3.8542 (1.188 sec/step)\n",
            "I1229 11:21:36.758327 140034317584256 learning.py:507] global step 1639: loss = 3.8542 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1640: loss = 3.0308 (1.178 sec/step)\n",
            "I1229 11:21:37.938290 140034317584256 learning.py:507] global step 1640: loss = 3.0308 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1641: loss = 2.6559 (1.178 sec/step)\n",
            "I1229 11:21:39.117607 140034317584256 learning.py:507] global step 1641: loss = 2.6559 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1642: loss = 2.0401 (1.170 sec/step)\n",
            "I1229 11:21:40.289712 140034317584256 learning.py:507] global step 1642: loss = 2.0401 (1.170 sec/step)\n",
            "INFO:tensorflow:global step 1643: loss = 1.9799 (1.141 sec/step)\n",
            "I1229 11:21:41.432150 140034317584256 learning.py:507] global step 1643: loss = 1.9799 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 1644: loss = 4.1297 (1.172 sec/step)\n",
            "I1229 11:21:42.606255 140034317584256 learning.py:507] global step 1644: loss = 4.1297 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 1645: loss = 2.1513 (1.200 sec/step)\n",
            "I1229 11:21:43.807866 140034317584256 learning.py:507] global step 1645: loss = 2.1513 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1646: loss = 3.0120 (1.215 sec/step)\n",
            "I1229 11:21:45.024149 140034317584256 learning.py:507] global step 1646: loss = 3.0120 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1647: loss = 4.2385 (1.291 sec/step)\n",
            "I1229 11:21:46.317370 140034317584256 learning.py:507] global step 1647: loss = 4.2385 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 1648: loss = 2.0837 (1.231 sec/step)\n",
            "I1229 11:21:47.550122 140034317584256 learning.py:507] global step 1648: loss = 2.0837 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1649: loss = 3.4133 (1.252 sec/step)\n",
            "I1229 11:21:48.804101 140034317584256 learning.py:507] global step 1649: loss = 3.4133 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1650: loss = 2.1120 (1.227 sec/step)\n",
            "I1229 11:21:50.032751 140034317584256 learning.py:507] global step 1650: loss = 2.1120 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1651: loss = 2.9943 (1.252 sec/step)\n",
            "I1229 11:21:51.287428 140034317584256 learning.py:507] global step 1651: loss = 2.9943 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1652: loss = 2.4781 (1.268 sec/step)\n",
            "I1229 11:21:52.557177 140034317584256 learning.py:507] global step 1652: loss = 2.4781 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 1653: loss = 3.4259 (1.201 sec/step)\n",
            "I1229 11:21:53.759400 140034317584256 learning.py:507] global step 1653: loss = 3.4259 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1654: loss = 1.5504 (1.194 sec/step)\n",
            "I1229 11:21:54.955182 140034317584256 learning.py:507] global step 1654: loss = 1.5504 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1655: loss = 1.7418 (1.279 sec/step)\n",
            "I1229 11:21:56.235875 140034317584256 learning.py:507] global step 1655: loss = 1.7418 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 1656: loss = 3.7293 (1.251 sec/step)\n",
            "I1229 11:21:57.488100 140034317584256 learning.py:507] global step 1656: loss = 3.7293 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1657: loss = 1.7638 (1.194 sec/step)\n",
            "I1229 11:21:58.683510 140034317584256 learning.py:507] global step 1657: loss = 1.7638 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1658: loss = 2.5507 (1.188 sec/step)\n",
            "I1229 11:21:59.873271 140034317584256 learning.py:507] global step 1658: loss = 2.5507 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1659: loss = 2.5787 (1.165 sec/step)\n",
            "I1229 11:22:01.040151 140034317584256 learning.py:507] global step 1659: loss = 2.5787 (1.165 sec/step)\n",
            "INFO:tensorflow:global step 1660: loss = 2.2528 (1.183 sec/step)\n",
            "I1229 11:22:02.224665 140034317584256 learning.py:507] global step 1660: loss = 2.2528 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1661: loss = 3.4077 (1.177 sec/step)\n",
            "I1229 11:22:03.403077 140034317584256 learning.py:507] global step 1661: loss = 3.4077 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1662: loss = 3.8109 (1.164 sec/step)\n",
            "I1229 11:22:04.569291 140034317584256 learning.py:507] global step 1662: loss = 3.8109 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 1663: loss = 1.3177 (1.198 sec/step)\n",
            "I1229 11:22:05.769587 140034317584256 learning.py:507] global step 1663: loss = 1.3177 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1664: loss = 2.1345 (1.239 sec/step)\n",
            "I1229 11:22:07.009959 140034317584256 learning.py:507] global step 1664: loss = 2.1345 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1665: loss = 3.0168 (1.213 sec/step)\n",
            "I1229 11:22:08.224704 140034317584256 learning.py:507] global step 1665: loss = 3.0168 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1666: loss = 2.6628 (1.184 sec/step)\n",
            "I1229 11:22:09.410869 140034317584256 learning.py:507] global step 1666: loss = 2.6628 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 1667: loss = 2.0205 (1.187 sec/step)\n",
            "I1229 11:22:10.599339 140034317584256 learning.py:507] global step 1667: loss = 2.0205 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1668: loss = 2.8162 (1.191 sec/step)\n",
            "I1229 11:22:11.792537 140034317584256 learning.py:507] global step 1668: loss = 2.8162 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1669: loss = 2.0620 (1.211 sec/step)\n",
            "I1229 11:22:13.005827 140034317584256 learning.py:507] global step 1669: loss = 2.0620 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1670: loss = 3.0937 (1.171 sec/step)\n",
            "I1229 11:22:14.178628 140034317584256 learning.py:507] global step 1670: loss = 3.0937 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1671: loss = 2.2683 (1.199 sec/step)\n",
            "I1229 11:22:15.379003 140034317584256 learning.py:507] global step 1671: loss = 2.2683 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1672: loss = 2.5547 (1.270 sec/step)\n",
            "I1229 11:22:16.650624 140034317584256 learning.py:507] global step 1672: loss = 2.5547 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 1673: loss = 3.4639 (1.283 sec/step)\n",
            "I1229 11:22:17.935011 140034317584256 learning.py:507] global step 1673: loss = 3.4639 (1.283 sec/step)\n",
            "INFO:tensorflow:global step 1674: loss = 3.0806 (1.204 sec/step)\n",
            "I1229 11:22:19.140770 140034317584256 learning.py:507] global step 1674: loss = 3.0806 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1675: loss = 3.5001 (1.163 sec/step)\n",
            "I1229 11:22:20.305845 140034317584256 learning.py:507] global step 1675: loss = 3.5001 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 1676: loss = 2.6221 (1.203 sec/step)\n",
            "I1229 11:22:21.510567 140034317584256 learning.py:507] global step 1676: loss = 2.6221 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1677: loss = 2.5647 (1.220 sec/step)\n",
            "I1229 11:22:22.732608 140034317584256 learning.py:507] global step 1677: loss = 2.5647 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1678: loss = 2.4362 (1.154 sec/step)\n",
            "I1229 11:22:23.888439 140034317584256 learning.py:507] global step 1678: loss = 2.4362 (1.154 sec/step)\n",
            "INFO:tensorflow:global step 1679: loss = 2.5395 (1.207 sec/step)\n",
            "I1229 11:22:25.097389 140034317584256 learning.py:507] global step 1679: loss = 2.5395 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1680: loss = 2.1330 (1.191 sec/step)\n",
            "I1229 11:22:26.290522 140034317584256 learning.py:507] global step 1680: loss = 2.1330 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1681: loss = 3.1317 (1.227 sec/step)\n",
            "I1229 11:22:27.519719 140034317584256 learning.py:507] global step 1681: loss = 3.1317 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1682: loss = 2.7297 (1.195 sec/step)\n",
            "I1229 11:22:28.716601 140034317584256 learning.py:507] global step 1682: loss = 2.7297 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1683: loss = 2.7170 (1.185 sec/step)\n",
            "I1229 11:22:29.903156 140034317584256 learning.py:507] global step 1683: loss = 2.7170 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1684: loss = 3.4298 (1.218 sec/step)\n",
            "I1229 11:22:31.122615 140034317584256 learning.py:507] global step 1684: loss = 3.4298 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1685: loss = 3.0418 (1.178 sec/step)\n",
            "I1229 11:22:32.302400 140034317584256 learning.py:507] global step 1685: loss = 3.0418 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1686: loss = 2.6882 (1.241 sec/step)\n",
            "I1229 11:22:33.545372 140034317584256 learning.py:507] global step 1686: loss = 2.6882 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1687: loss = 2.4419 (1.222 sec/step)\n",
            "I1229 11:22:34.769753 140034317584256 learning.py:507] global step 1687: loss = 2.4419 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1688: loss = 2.4199 (1.234 sec/step)\n",
            "I1229 11:22:36.005238 140034317584256 learning.py:507] global step 1688: loss = 2.4199 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1689: loss = 2.0903 (1.212 sec/step)\n",
            "I1229 11:22:37.220465 140034317584256 learning.py:507] global step 1689: loss = 2.0903 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1690: loss = 3.1167 (1.251 sec/step)\n",
            "I1229 11:22:38.472697 140034317584256 learning.py:507] global step 1690: loss = 3.1167 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1691: loss = 2.2383 (1.173 sec/step)\n",
            "I1229 11:22:39.647645 140034317584256 learning.py:507] global step 1691: loss = 2.2383 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1692: loss = 1.6947 (1.135 sec/step)\n",
            "I1229 11:22:40.783893 140034317584256 learning.py:507] global step 1692: loss = 1.6947 (1.135 sec/step)\n",
            "INFO:tensorflow:global step 1693: loss = 2.5188 (1.187 sec/step)\n",
            "I1229 11:22:41.972527 140034317584256 learning.py:507] global step 1693: loss = 2.5188 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1694: loss = 2.2656 (1.181 sec/step)\n",
            "I1229 11:22:43.155503 140034317584256 learning.py:507] global step 1694: loss = 2.2656 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1695: loss = 2.5637 (1.189 sec/step)\n",
            "I1229 11:22:44.346054 140034317584256 learning.py:507] global step 1695: loss = 2.5637 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1696: loss = 2.2770 (1.152 sec/step)\n",
            "I1229 11:22:45.499854 140034317584256 learning.py:507] global step 1696: loss = 2.2770 (1.152 sec/step)\n",
            "INFO:tensorflow:global step 1697: loss = 2.2886 (1.195 sec/step)\n",
            "I1229 11:22:46.696934 140034317584256 learning.py:507] global step 1697: loss = 2.2886 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1698: loss = 2.4621 (1.186 sec/step)\n",
            "I1229 11:22:47.884265 140034317584256 learning.py:507] global step 1698: loss = 2.4621 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1699: loss = 2.0479 (1.232 sec/step)\n",
            "I1229 11:22:49.118282 140034317584256 learning.py:507] global step 1699: loss = 2.0479 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1700: loss = 2.5595 (1.222 sec/step)\n",
            "I1229 11:22:50.341747 140034317584256 learning.py:507] global step 1700: loss = 2.5595 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1701: loss = 2.0853 (1.247 sec/step)\n",
            "I1229 11:22:51.591016 140034317584256 learning.py:507] global step 1701: loss = 2.0853 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1702: loss = 2.2307 (1.277 sec/step)\n",
            "I1229 11:22:52.869337 140034317584256 learning.py:507] global step 1702: loss = 2.2307 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 1703: loss = 2.9437 (1.206 sec/step)\n",
            "I1229 11:22:54.077296 140034317584256 learning.py:507] global step 1703: loss = 2.9437 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1704: loss = 2.9068 (1.186 sec/step)\n",
            "I1229 11:22:55.265349 140034317584256 learning.py:507] global step 1704: loss = 2.9068 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1705: loss = 2.0856 (1.173 sec/step)\n",
            "I1229 11:22:56.440471 140034317584256 learning.py:507] global step 1705: loss = 2.0856 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1706: loss = 2.6652 (1.202 sec/step)\n",
            "I1229 11:22:57.644793 140034317584256 learning.py:507] global step 1706: loss = 2.6652 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1707: loss = 1.3176 (1.191 sec/step)\n",
            "I1229 11:22:58.837327 140034317584256 learning.py:507] global step 1707: loss = 1.3176 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1708: loss = 1.6870 (1.236 sec/step)\n",
            "I1229 11:23:00.075087 140034317584256 learning.py:507] global step 1708: loss = 1.6870 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1709: loss = 1.8747 (1.177 sec/step)\n",
            "I1229 11:23:01.253789 140034317584256 learning.py:507] global step 1709: loss = 1.8747 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1710: loss = 2.9505 (1.213 sec/step)\n",
            "I1229 11:23:02.468439 140034317584256 learning.py:507] global step 1710: loss = 2.9505 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1711: loss = 2.5407 (1.173 sec/step)\n",
            "I1229 11:23:03.643049 140034317584256 learning.py:507] global step 1711: loss = 2.5407 (1.173 sec/step)\n",
            "INFO:tensorflow:global step 1712: loss = 2.0268 (1.221 sec/step)\n",
            "I1229 11:23:04.865674 140034317584256 learning.py:507] global step 1712: loss = 2.0268 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1713: loss = 2.7028 (1.188 sec/step)\n",
            "I1229 11:23:06.056049 140034317584256 learning.py:507] global step 1713: loss = 2.7028 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1714: loss = 2.7471 (1.205 sec/step)\n",
            "I1229 11:23:07.262939 140034317584256 learning.py:507] global step 1714: loss = 2.7471 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1715: loss = 2.3683 (1.239 sec/step)\n",
            "I1229 11:23:08.503298 140034317584256 learning.py:507] global step 1715: loss = 2.3683 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1716: loss = 3.6792 (1.215 sec/step)\n",
            "I1229 11:23:09.720473 140034317584256 learning.py:507] global step 1716: loss = 3.6792 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1717: loss = 2.4522 (1.185 sec/step)\n",
            "I1229 11:23:10.907035 140034317584256 learning.py:507] global step 1717: loss = 2.4522 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1718: loss = 2.6299 (1.206 sec/step)\n",
            "I1229 11:23:12.114702 140034317584256 learning.py:507] global step 1718: loss = 2.6299 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1719: loss = 2.0485 (1.207 sec/step)\n",
            "I1229 11:23:13.323645 140034317584256 learning.py:507] global step 1719: loss = 2.0485 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1720: loss = 2.5506 (1.233 sec/step)\n",
            "I1229 11:23:14.559006 140034317584256 learning.py:507] global step 1720: loss = 2.5506 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1721: loss = 2.0157 (1.246 sec/step)\n",
            "I1229 11:23:15.806308 140034317584256 learning.py:507] global step 1721: loss = 2.0157 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1722: loss = 1.8396 (1.214 sec/step)\n",
            "I1229 11:23:17.022120 140034317584256 learning.py:507] global step 1722: loss = 1.8396 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1723: loss = 2.5008 (2.144 sec/step)\n",
            "I1229 11:23:19.173151 140034317584256 learning.py:507] global step 1723: loss = 2.5008 (2.144 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1723.\n",
            "I1229 11:23:19.613686 140030580766464 supervisor.py:1050] Recording summary at step 1723.\n",
            "INFO:tensorflow:global step 1724: loss = 3.0106 (1.292 sec/step)\n",
            "I1229 11:23:20.468281 140034317584256 learning.py:507] global step 1724: loss = 3.0106 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 1725: loss = 2.2135 (1.228 sec/step)\n",
            "I1229 11:23:21.697804 140034317584256 learning.py:507] global step 1725: loss = 2.2135 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1726: loss = 3.4553 (1.177 sec/step)\n",
            "I1229 11:23:22.876898 140034317584256 learning.py:507] global step 1726: loss = 3.4553 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 1727: loss = 3.3152 (1.221 sec/step)\n",
            "I1229 11:23:24.099169 140034317584256 learning.py:507] global step 1727: loss = 3.3152 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1728: loss = 2.5067 (1.232 sec/step)\n",
            "I1229 11:23:25.333288 140034317584256 learning.py:507] global step 1728: loss = 2.5067 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1729: loss = 2.4701 (1.216 sec/step)\n",
            "I1229 11:23:26.551372 140034317584256 learning.py:507] global step 1729: loss = 2.4701 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1730: loss = 3.5059 (1.237 sec/step)\n",
            "I1229 11:23:27.789489 140034317584256 learning.py:507] global step 1730: loss = 3.5059 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1731: loss = 3.0901 (1.187 sec/step)\n",
            "I1229 11:23:28.978448 140034317584256 learning.py:507] global step 1731: loss = 3.0901 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1732: loss = 3.6165 (1.163 sec/step)\n",
            "I1229 11:23:30.143267 140034317584256 learning.py:507] global step 1732: loss = 3.6165 (1.163 sec/step)\n",
            "INFO:tensorflow:global step 1733: loss = 1.9658 (1.238 sec/step)\n",
            "I1229 11:23:31.382776 140034317584256 learning.py:507] global step 1733: loss = 1.9658 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1734: loss = 2.4924 (1.213 sec/step)\n",
            "I1229 11:23:32.597043 140034317584256 learning.py:507] global step 1734: loss = 2.4924 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1735: loss = 2.3803 (1.245 sec/step)\n",
            "I1229 11:23:33.843113 140034317584256 learning.py:507] global step 1735: loss = 2.3803 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1736: loss = 2.8766 (1.187 sec/step)\n",
            "I1229 11:23:35.032082 140034317584256 learning.py:507] global step 1736: loss = 2.8766 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1737: loss = 2.7665 (1.216 sec/step)\n",
            "I1229 11:23:36.249874 140034317584256 learning.py:507] global step 1737: loss = 2.7665 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1738: loss = 3.8769 (1.263 sec/step)\n",
            "I1229 11:23:37.514612 140034317584256 learning.py:507] global step 1738: loss = 3.8769 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 1739: loss = 2.5645 (1.256 sec/step)\n",
            "I1229 11:23:38.772307 140034317584256 learning.py:507] global step 1739: loss = 2.5645 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1740: loss = 3.1256 (1.212 sec/step)\n",
            "I1229 11:23:39.985760 140034317584256 learning.py:507] global step 1740: loss = 3.1256 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1741: loss = 1.8251 (1.259 sec/step)\n",
            "I1229 11:23:41.246145 140034317584256 learning.py:507] global step 1741: loss = 1.8251 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 1742: loss = 2.0631 (1.232 sec/step)\n",
            "I1229 11:23:42.479414 140034317584256 learning.py:507] global step 1742: loss = 2.0631 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1743: loss = 3.3758 (1.233 sec/step)\n",
            "I1229 11:23:43.714193 140034317584256 learning.py:507] global step 1743: loss = 3.3758 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1744: loss = 2.4382 (1.220 sec/step)\n",
            "I1229 11:23:44.935747 140034317584256 learning.py:507] global step 1744: loss = 2.4382 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1745: loss = 2.6582 (1.179 sec/step)\n",
            "I1229 11:23:46.116412 140034317584256 learning.py:507] global step 1745: loss = 2.6582 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 1746: loss = 2.4746 (1.171 sec/step)\n",
            "I1229 11:23:47.289225 140034317584256 learning.py:507] global step 1746: loss = 2.4746 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1747: loss = 2.5973 (1.196 sec/step)\n",
            "I1229 11:23:48.486826 140034317584256 learning.py:507] global step 1747: loss = 2.5973 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1748: loss = 2.3649 (1.214 sec/step)\n",
            "I1229 11:23:49.702906 140034317584256 learning.py:507] global step 1748: loss = 2.3649 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1749: loss = 2.5960 (1.172 sec/step)\n",
            "I1229 11:23:50.877095 140034317584256 learning.py:507] global step 1749: loss = 2.5960 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 1750: loss = 3.5257 (1.224 sec/step)\n",
            "I1229 11:23:52.103356 140034317584256 learning.py:507] global step 1750: loss = 3.5257 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1751: loss = 1.9152 (1.195 sec/step)\n",
            "I1229 11:23:53.299660 140034317584256 learning.py:507] global step 1751: loss = 1.9152 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1752: loss = 2.9962 (1.182 sec/step)\n",
            "I1229 11:23:54.482860 140034317584256 learning.py:507] global step 1752: loss = 2.9962 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1753: loss = 3.2617 (1.157 sec/step)\n",
            "I1229 11:23:55.641544 140034317584256 learning.py:507] global step 1753: loss = 3.2617 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 1754: loss = 1.9971 (1.221 sec/step)\n",
            "I1229 11:23:56.864717 140034317584256 learning.py:507] global step 1754: loss = 1.9971 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1755: loss = 1.6654 (1.202 sec/step)\n",
            "I1229 11:23:58.068174 140034317584256 learning.py:507] global step 1755: loss = 1.6654 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1756: loss = 2.6320 (1.178 sec/step)\n",
            "I1229 11:23:59.248168 140034317584256 learning.py:507] global step 1756: loss = 2.6320 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 1757: loss = 2.8551 (1.174 sec/step)\n",
            "I1229 11:24:00.423264 140034317584256 learning.py:507] global step 1757: loss = 2.8551 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1758: loss = 2.7368 (1.193 sec/step)\n",
            "I1229 11:24:01.618060 140034317584256 learning.py:507] global step 1758: loss = 2.7368 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1759: loss = 3.4073 (1.194 sec/step)\n",
            "I1229 11:24:02.813925 140034317584256 learning.py:507] global step 1759: loss = 3.4073 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1760: loss = 2.3594 (1.223 sec/step)\n",
            "I1229 11:24:04.038884 140034317584256 learning.py:507] global step 1760: loss = 2.3594 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1761: loss = 2.2251 (1.181 sec/step)\n",
            "I1229 11:24:05.221452 140034317584256 learning.py:507] global step 1761: loss = 2.2251 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 1762: loss = 2.3395 (1.189 sec/step)\n",
            "I1229 11:24:06.412662 140034317584256 learning.py:507] global step 1762: loss = 2.3395 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1763: loss = 2.1466 (1.236 sec/step)\n",
            "I1229 11:24:07.651018 140034317584256 learning.py:507] global step 1763: loss = 2.1466 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1764: loss = 2.1988 (1.207 sec/step)\n",
            "I1229 11:24:08.860308 140034317584256 learning.py:507] global step 1764: loss = 2.1988 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1765: loss = 2.2026 (1.222 sec/step)\n",
            "I1229 11:24:10.084130 140034317584256 learning.py:507] global step 1765: loss = 2.2026 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1766: loss = 2.7002 (1.189 sec/step)\n",
            "I1229 11:24:11.275515 140034317584256 learning.py:507] global step 1766: loss = 2.7002 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1767: loss = 1.9655 (1.193 sec/step)\n",
            "I1229 11:24:12.470466 140034317584256 learning.py:507] global step 1767: loss = 1.9655 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1768: loss = 2.7447 (1.228 sec/step)\n",
            "I1229 11:24:13.700232 140034317584256 learning.py:507] global step 1768: loss = 2.7447 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1769: loss = 2.3416 (1.166 sec/step)\n",
            "I1229 11:24:14.867392 140034317584256 learning.py:507] global step 1769: loss = 2.3416 (1.166 sec/step)\n",
            "INFO:tensorflow:global step 1770: loss = 1.9168 (1.253 sec/step)\n",
            "I1229 11:24:16.122267 140034317584256 learning.py:507] global step 1770: loss = 1.9168 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1771: loss = 1.6004 (1.258 sec/step)\n",
            "I1229 11:24:17.381786 140034317584256 learning.py:507] global step 1771: loss = 1.6004 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 1772: loss = 1.6224 (1.188 sec/step)\n",
            "I1229 11:24:18.570965 140034317584256 learning.py:507] global step 1772: loss = 1.6224 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1773: loss = 2.3140 (1.200 sec/step)\n",
            "I1229 11:24:19.772798 140034317584256 learning.py:507] global step 1773: loss = 2.3140 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1774: loss = 2.2496 (1.171 sec/step)\n",
            "I1229 11:24:20.945590 140034317584256 learning.py:507] global step 1774: loss = 2.2496 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1775: loss = 2.0844 (1.200 sec/step)\n",
            "I1229 11:24:22.147693 140034317584256 learning.py:507] global step 1775: loss = 2.0844 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1776: loss = 1.9071 (1.232 sec/step)\n",
            "I1229 11:24:23.381146 140034317584256 learning.py:507] global step 1776: loss = 1.9071 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1777: loss = 3.1991 (1.274 sec/step)\n",
            "I1229 11:24:24.656978 140034317584256 learning.py:507] global step 1777: loss = 3.1991 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 1778: loss = 1.8506 (1.232 sec/step)\n",
            "I1229 11:24:25.891016 140034317584256 learning.py:507] global step 1778: loss = 1.8506 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1779: loss = 2.6526 (1.226 sec/step)\n",
            "I1229 11:24:27.118360 140034317584256 learning.py:507] global step 1779: loss = 2.6526 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1780: loss = 2.5284 (1.212 sec/step)\n",
            "I1229 11:24:28.332044 140034317584256 learning.py:507] global step 1780: loss = 2.5284 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1781: loss = 2.0433 (1.255 sec/step)\n",
            "I1229 11:24:29.589540 140034317584256 learning.py:507] global step 1781: loss = 2.0433 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1782: loss = 4.4795 (1.265 sec/step)\n",
            "I1229 11:24:30.856265 140034317584256 learning.py:507] global step 1782: loss = 4.4795 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 1783: loss = 2.5301 (1.189 sec/step)\n",
            "I1229 11:24:32.046617 140034317584256 learning.py:507] global step 1783: loss = 2.5301 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 1784: loss = 2.3529 (1.202 sec/step)\n",
            "I1229 11:24:33.250192 140034317584256 learning.py:507] global step 1784: loss = 2.3529 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 1785: loss = 3.8577 (1.222 sec/step)\n",
            "I1229 11:24:34.474547 140034317584256 learning.py:507] global step 1785: loss = 3.8577 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1786: loss = 3.2562 (1.206 sec/step)\n",
            "I1229 11:24:35.682175 140034317584256 learning.py:507] global step 1786: loss = 3.2562 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 1787: loss = 3.6531 (1.194 sec/step)\n",
            "I1229 11:24:36.877349 140034317584256 learning.py:507] global step 1787: loss = 3.6531 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 1788: loss = 1.9721 (1.157 sec/step)\n",
            "I1229 11:24:38.036118 140034317584256 learning.py:507] global step 1788: loss = 1.9721 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 1789: loss = 2.3366 (1.224 sec/step)\n",
            "I1229 11:24:39.262776 140034317584256 learning.py:507] global step 1789: loss = 2.3366 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 1790: loss = 2.8118 (1.190 sec/step)\n",
            "I1229 11:24:40.454601 140034317584256 learning.py:507] global step 1790: loss = 2.8118 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1791: loss = 2.0917 (1.183 sec/step)\n",
            "I1229 11:24:41.639672 140034317584256 learning.py:507] global step 1791: loss = 2.0917 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1792: loss = 2.7441 (1.182 sec/step)\n",
            "I1229 11:24:42.823775 140034317584256 learning.py:507] global step 1792: loss = 2.7441 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 1793: loss = 5.1238 (1.156 sec/step)\n",
            "I1229 11:24:43.981117 140034317584256 learning.py:507] global step 1793: loss = 5.1238 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 1794: loss = 3.0849 (1.232 sec/step)\n",
            "I1229 11:24:45.214498 140034317584256 learning.py:507] global step 1794: loss = 3.0849 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 1795: loss = 2.3799 (1.237 sec/step)\n",
            "I1229 11:24:46.453455 140034317584256 learning.py:507] global step 1795: loss = 2.3799 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1796: loss = 3.4704 (1.187 sec/step)\n",
            "I1229 11:24:47.642832 140034317584256 learning.py:507] global step 1796: loss = 3.4704 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1797: loss = 3.7545 (1.255 sec/step)\n",
            "I1229 11:24:48.899664 140034317584256 learning.py:507] global step 1797: loss = 3.7545 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1798: loss = 2.2148 (1.191 sec/step)\n",
            "I1229 11:24:50.092305 140034317584256 learning.py:507] global step 1798: loss = 2.2148 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 1799: loss = 2.7602 (1.218 sec/step)\n",
            "I1229 11:24:51.311683 140034317584256 learning.py:507] global step 1799: loss = 2.7602 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1800: loss = 2.1326 (1.233 sec/step)\n",
            "I1229 11:24:52.546784 140034317584256 learning.py:507] global step 1800: loss = 2.1326 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1801: loss = 3.8021 (1.185 sec/step)\n",
            "I1229 11:24:53.733567 140034317584256 learning.py:507] global step 1801: loss = 3.8021 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 1802: loss = 2.6619 (1.212 sec/step)\n",
            "I1229 11:24:54.947682 140034317584256 learning.py:507] global step 1802: loss = 2.6619 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1803: loss = 3.0833 (1.221 sec/step)\n",
            "I1229 11:24:56.170583 140034317584256 learning.py:507] global step 1803: loss = 3.0833 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1804: loss = 2.6690 (1.245 sec/step)\n",
            "I1229 11:24:57.417840 140034317584256 learning.py:507] global step 1804: loss = 2.6690 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1805: loss = 3.1017 (1.207 sec/step)\n",
            "I1229 11:24:58.626448 140034317584256 learning.py:507] global step 1805: loss = 3.1017 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1806: loss = 1.8737 (1.208 sec/step)\n",
            "I1229 11:24:59.836781 140034317584256 learning.py:507] global step 1806: loss = 1.8737 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1807: loss = 2.2866 (1.234 sec/step)\n",
            "I1229 11:25:01.072076 140034317584256 learning.py:507] global step 1807: loss = 2.2866 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1808: loss = 2.4809 (1.167 sec/step)\n",
            "I1229 11:25:02.241370 140034317584256 learning.py:507] global step 1808: loss = 2.4809 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 1809: loss = 2.3892 (1.223 sec/step)\n",
            "I1229 11:25:03.465914 140034317584256 learning.py:507] global step 1809: loss = 2.3892 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1810: loss = 2.4343 (1.226 sec/step)\n",
            "I1229 11:25:04.693864 140034317584256 learning.py:507] global step 1810: loss = 2.4343 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1811: loss = 3.2368 (1.188 sec/step)\n",
            "I1229 11:25:05.883507 140034317584256 learning.py:507] global step 1811: loss = 3.2368 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1812: loss = 3.4181 (1.268 sec/step)\n",
            "I1229 11:25:07.153161 140034317584256 learning.py:507] global step 1812: loss = 3.4181 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 1813: loss = 3.2418 (1.157 sec/step)\n",
            "I1229 11:25:08.312309 140034317584256 learning.py:507] global step 1813: loss = 3.2418 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 1814: loss = 3.1476 (1.241 sec/step)\n",
            "I1229 11:25:09.555518 140034317584256 learning.py:507] global step 1814: loss = 3.1476 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1815: loss = 2.4455 (1.198 sec/step)\n",
            "I1229 11:25:10.755347 140034317584256 learning.py:507] global step 1815: loss = 2.4455 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 1816: loss = 2.3816 (1.197 sec/step)\n",
            "I1229 11:25:11.954529 140034317584256 learning.py:507] global step 1816: loss = 2.3816 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 1817: loss = 2.8564 (1.247 sec/step)\n",
            "I1229 11:25:13.203788 140034317584256 learning.py:507] global step 1817: loss = 2.8564 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1818: loss = 2.1197 (1.205 sec/step)\n",
            "I1229 11:25:14.412158 140034317584256 learning.py:507] global step 1818: loss = 2.1197 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1819: loss = 2.4818 (1.236 sec/step)\n",
            "I1229 11:25:15.651313 140034317584256 learning.py:507] global step 1819: loss = 2.4818 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1820: loss = 2.4813 (1.193 sec/step)\n",
            "I1229 11:25:16.846073 140034317584256 learning.py:507] global step 1820: loss = 2.4813 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1821: loss = 1.7189 (1.973 sec/step)\n",
            "I1229 11:25:18.821829 140034317584256 learning.py:507] global step 1821: loss = 1.7189 (1.973 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1821.\n",
            "I1229 11:25:19.819696 140030580766464 supervisor.py:1050] Recording summary at step 1821.\n",
            "INFO:tensorflow:global step 1822: loss = 2.9108 (1.457 sec/step)\n",
            "I1229 11:25:20.280950 140034317584256 learning.py:507] global step 1822: loss = 2.9108 (1.457 sec/step)\n",
            "INFO:tensorflow:global step 1823: loss = 1.7729 (1.192 sec/step)\n",
            "I1229 11:25:21.474334 140034317584256 learning.py:507] global step 1823: loss = 1.7729 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1824: loss = 1.4002 (1.254 sec/step)\n",
            "I1229 11:25:22.729908 140034317584256 learning.py:507] global step 1824: loss = 1.4002 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 1825: loss = 2.7688 (1.249 sec/step)\n",
            "I1229 11:25:23.980481 140034317584256 learning.py:507] global step 1825: loss = 2.7688 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1826: loss = 1.7882 (1.199 sec/step)\n",
            "I1229 11:25:25.181340 140034317584256 learning.py:507] global step 1826: loss = 1.7882 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1827: loss = 2.6108 (1.201 sec/step)\n",
            "I1229 11:25:26.384235 140034317584256 learning.py:507] global step 1827: loss = 2.6108 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1828: loss = 3.3502 (1.195 sec/step)\n",
            "I1229 11:25:27.581130 140034317584256 learning.py:507] global step 1828: loss = 3.3502 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1829: loss = 2.2503 (1.275 sec/step)\n",
            "I1229 11:25:28.857501 140034317584256 learning.py:507] global step 1829: loss = 2.2503 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 1830: loss = 1.8628 (1.226 sec/step)\n",
            "I1229 11:25:30.085754 140034317584256 learning.py:507] global step 1830: loss = 1.8628 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 1831: loss = 1.8217 (1.233 sec/step)\n",
            "I1229 11:25:31.320770 140034317584256 learning.py:507] global step 1831: loss = 1.8217 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1832: loss = 2.7872 (1.211 sec/step)\n",
            "I1229 11:25:32.533836 140034317584256 learning.py:507] global step 1832: loss = 2.7872 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1833: loss = 3.4582 (1.223 sec/step)\n",
            "I1229 11:25:33.758400 140034317584256 learning.py:507] global step 1833: loss = 3.4582 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1834: loss = 3.4181 (1.222 sec/step)\n",
            "I1229 11:25:34.982009 140034317584256 learning.py:507] global step 1834: loss = 3.4181 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1835: loss = 3.7555 (1.220 sec/step)\n",
            "I1229 11:25:36.203240 140034317584256 learning.py:507] global step 1835: loss = 3.7555 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 1836: loss = 2.4888 (1.222 sec/step)\n",
            "I1229 11:25:37.427280 140034317584256 learning.py:507] global step 1836: loss = 2.4888 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1837: loss = 3.2576 (1.290 sec/step)\n",
            "I1229 11:25:38.719063 140034317584256 learning.py:507] global step 1837: loss = 3.2576 (1.290 sec/step)\n",
            "INFO:tensorflow:global step 1838: loss = 1.5443 (1.228 sec/step)\n",
            "I1229 11:25:39.948467 140034317584256 learning.py:507] global step 1838: loss = 1.5443 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1839: loss = 3.1077 (1.199 sec/step)\n",
            "I1229 11:25:41.149088 140034317584256 learning.py:507] global step 1839: loss = 3.1077 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1840: loss = 2.9875 (1.176 sec/step)\n",
            "I1229 11:25:42.326924 140034317584256 learning.py:507] global step 1840: loss = 2.9875 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 1841: loss = 3.4156 (1.204 sec/step)\n",
            "I1229 11:25:43.532935 140034317584256 learning.py:507] global step 1841: loss = 3.4156 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1842: loss = 2.3742 (1.161 sec/step)\n",
            "I1229 11:25:44.695721 140034317584256 learning.py:507] global step 1842: loss = 2.3742 (1.161 sec/step)\n",
            "INFO:tensorflow:global step 1843: loss = 2.8055 (1.245 sec/step)\n",
            "I1229 11:25:45.942630 140034317584256 learning.py:507] global step 1843: loss = 2.8055 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1844: loss = 2.3012 (1.214 sec/step)\n",
            "I1229 11:25:47.158560 140034317584256 learning.py:507] global step 1844: loss = 2.3012 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1845: loss = 3.1670 (1.245 sec/step)\n",
            "I1229 11:25:48.405514 140034317584256 learning.py:507] global step 1845: loss = 3.1670 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1846: loss = 2.4623 (1.201 sec/step)\n",
            "I1229 11:25:49.609545 140034317584256 learning.py:507] global step 1846: loss = 2.4623 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1847: loss = 3.4313 (1.195 sec/step)\n",
            "I1229 11:25:50.806252 140034317584256 learning.py:507] global step 1847: loss = 3.4313 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1848: loss = 2.0669 (1.236 sec/step)\n",
            "I1229 11:25:52.043630 140034317584256 learning.py:507] global step 1848: loss = 2.0669 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 1849: loss = 2.1316 (1.211 sec/step)\n",
            "I1229 11:25:53.256310 140034317584256 learning.py:507] global step 1849: loss = 2.1316 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1850: loss = 2.2728 (1.192 sec/step)\n",
            "I1229 11:25:54.449898 140034317584256 learning.py:507] global step 1850: loss = 2.2728 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1851: loss = 3.8403 (1.205 sec/step)\n",
            "I1229 11:25:55.656157 140034317584256 learning.py:507] global step 1851: loss = 3.8403 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1852: loss = 4.1279 (1.193 sec/step)\n",
            "I1229 11:25:56.850714 140034317584256 learning.py:507] global step 1852: loss = 4.1279 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1853: loss = 2.7616 (1.193 sec/step)\n",
            "I1229 11:25:58.046238 140034317584256 learning.py:507] global step 1853: loss = 2.7616 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1854: loss = 1.7835 (1.171 sec/step)\n",
            "I1229 11:25:59.219030 140034317584256 learning.py:507] global step 1854: loss = 1.7835 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1855: loss = 3.3934 (1.225 sec/step)\n",
            "I1229 11:26:00.445957 140034317584256 learning.py:507] global step 1855: loss = 3.3934 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 1856: loss = 1.8108 (1.214 sec/step)\n",
            "I1229 11:26:01.661156 140034317584256 learning.py:507] global step 1856: loss = 1.8108 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1857: loss = 1.6390 (1.213 sec/step)\n",
            "I1229 11:26:02.876016 140034317584256 learning.py:507] global step 1857: loss = 1.6390 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1858: loss = 1.6860 (1.249 sec/step)\n",
            "I1229 11:26:04.126881 140034317584256 learning.py:507] global step 1858: loss = 1.6860 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1859: loss = 3.0043 (1.167 sec/step)\n",
            "I1229 11:26:05.295649 140034317584256 learning.py:507] global step 1859: loss = 3.0043 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 1860: loss = 3.6171 (1.180 sec/step)\n",
            "I1229 11:26:06.477006 140034317584256 learning.py:507] global step 1860: loss = 3.6171 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1861: loss = 2.7344 (1.180 sec/step)\n",
            "I1229 11:26:07.658057 140034317584256 learning.py:507] global step 1861: loss = 2.7344 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 1862: loss = 3.3759 (1.193 sec/step)\n",
            "I1229 11:26:08.852538 140034317584256 learning.py:507] global step 1862: loss = 3.3759 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 1863: loss = 2.3134 (1.316 sec/step)\n",
            "I1229 11:26:10.170727 140034317584256 learning.py:507] global step 1863: loss = 2.3134 (1.316 sec/step)\n",
            "INFO:tensorflow:global step 1864: loss = 2.6112 (1.289 sec/step)\n",
            "I1229 11:26:11.461107 140034317584256 learning.py:507] global step 1864: loss = 2.6112 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 1865: loss = 2.7295 (1.171 sec/step)\n",
            "I1229 11:26:12.633325 140034317584256 learning.py:507] global step 1865: loss = 2.7295 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 1866: loss = 3.0024 (1.227 sec/step)\n",
            "I1229 11:26:13.862554 140034317584256 learning.py:507] global step 1866: loss = 3.0024 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1867: loss = 3.5877 (1.186 sec/step)\n",
            "I1229 11:26:15.050345 140034317584256 learning.py:507] global step 1867: loss = 3.5877 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 1868: loss = 2.5880 (1.248 sec/step)\n",
            "I1229 11:26:16.300175 140034317584256 learning.py:507] global step 1868: loss = 2.5880 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 1869: loss = 2.5096 (1.169 sec/step)\n",
            "I1229 11:26:17.470672 140034317584256 learning.py:507] global step 1869: loss = 2.5096 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 1870: loss = 2.0325 (1.243 sec/step)\n",
            "I1229 11:26:18.715680 140034317584256 learning.py:507] global step 1870: loss = 2.0325 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1871: loss = 3.2782 (1.212 sec/step)\n",
            "I1229 11:26:19.929819 140034317584256 learning.py:507] global step 1871: loss = 3.2782 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1872: loss = 2.3664 (1.262 sec/step)\n",
            "I1229 11:26:21.193428 140034317584256 learning.py:507] global step 1872: loss = 2.3664 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 1873: loss = 2.5804 (1.262 sec/step)\n",
            "I1229 11:26:22.456583 140034317584256 learning.py:507] global step 1873: loss = 2.5804 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 1874: loss = 2.5898 (1.200 sec/step)\n",
            "I1229 11:26:23.658133 140034317584256 learning.py:507] global step 1874: loss = 2.5898 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1875: loss = 2.8516 (1.228 sec/step)\n",
            "I1229 11:26:24.888679 140034317584256 learning.py:507] global step 1875: loss = 2.8516 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1876: loss = 2.4903 (1.210 sec/step)\n",
            "I1229 11:26:26.100353 140034317584256 learning.py:507] global step 1876: loss = 2.4903 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1877: loss = 2.0497 (1.188 sec/step)\n",
            "I1229 11:26:27.290627 140034317584256 learning.py:507] global step 1877: loss = 2.0497 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1878: loss = 1.8180 (1.207 sec/step)\n",
            "I1229 11:26:28.499497 140034317584256 learning.py:507] global step 1878: loss = 1.8180 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 1879: loss = 1.8807 (1.234 sec/step)\n",
            "I1229 11:26:29.735092 140034317584256 learning.py:507] global step 1879: loss = 1.8807 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1880: loss = 2.9617 (1.254 sec/step)\n",
            "I1229 11:26:30.990808 140034317584256 learning.py:507] global step 1880: loss = 2.9617 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 1881: loss = 1.3451 (1.208 sec/step)\n",
            "I1229 11:26:32.200655 140034317584256 learning.py:507] global step 1881: loss = 1.3451 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1882: loss = 3.6340 (1.222 sec/step)\n",
            "I1229 11:26:33.424752 140034317584256 learning.py:507] global step 1882: loss = 3.6340 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 1883: loss = 3.2502 (1.260 sec/step)\n",
            "I1229 11:26:34.686383 140034317584256 learning.py:507] global step 1883: loss = 3.2502 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1884: loss = 1.7651 (1.214 sec/step)\n",
            "I1229 11:26:35.902326 140034317584256 learning.py:507] global step 1884: loss = 1.7651 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1885: loss = 3.9540 (1.246 sec/step)\n",
            "I1229 11:26:37.150076 140034317584256 learning.py:507] global step 1885: loss = 3.9540 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1886: loss = 3.5110 (1.252 sec/step)\n",
            "I1229 11:26:38.403932 140034317584256 learning.py:507] global step 1886: loss = 3.5110 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 1887: loss = 3.4334 (1.244 sec/step)\n",
            "I1229 11:26:39.649705 140034317584256 learning.py:507] global step 1887: loss = 3.4334 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1888: loss = 2.9446 (1.258 sec/step)\n",
            "I1229 11:26:40.909783 140034317584256 learning.py:507] global step 1888: loss = 2.9446 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 1889: loss = 2.9930 (1.235 sec/step)\n",
            "I1229 11:26:42.146973 140034317584256 learning.py:507] global step 1889: loss = 2.9930 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1890: loss = 2.5346 (1.195 sec/step)\n",
            "I1229 11:26:43.344135 140034317584256 learning.py:507] global step 1890: loss = 2.5346 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 1891: loss = 1.9341 (1.204 sec/step)\n",
            "I1229 11:26:44.550262 140034317584256 learning.py:507] global step 1891: loss = 1.9341 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1892: loss = 3.5207 (1.253 sec/step)\n",
            "I1229 11:26:45.805594 140034317584256 learning.py:507] global step 1892: loss = 3.5207 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1893: loss = 3.2388 (1.190 sec/step)\n",
            "I1229 11:26:46.997544 140034317584256 learning.py:507] global step 1893: loss = 3.2388 (1.190 sec/step)\n",
            "INFO:tensorflow:global step 1894: loss = 2.4013 (1.259 sec/step)\n",
            "I1229 11:26:48.258921 140034317584256 learning.py:507] global step 1894: loss = 2.4013 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 1895: loss = 2.8859 (1.260 sec/step)\n",
            "I1229 11:26:49.521155 140034317584256 learning.py:507] global step 1895: loss = 2.8859 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1896: loss = 2.7043 (1.192 sec/step)\n",
            "I1229 11:26:50.715122 140034317584256 learning.py:507] global step 1896: loss = 2.7043 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1897: loss = 4.0593 (1.255 sec/step)\n",
            "I1229 11:26:51.971890 140034317584256 learning.py:507] global step 1897: loss = 4.0593 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1898: loss = 2.0770 (1.247 sec/step)\n",
            "I1229 11:26:53.220520 140034317584256 learning.py:507] global step 1898: loss = 2.0770 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1899: loss = 1.9665 (1.196 sec/step)\n",
            "I1229 11:26:54.418413 140034317584256 learning.py:507] global step 1899: loss = 1.9665 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 1900: loss = 3.4395 (1.164 sec/step)\n",
            "I1229 11:26:55.584270 140034317584256 learning.py:507] global step 1900: loss = 3.4395 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 1901: loss = 2.3407 (1.301 sec/step)\n",
            "I1229 11:26:56.886766 140034317584256 learning.py:507] global step 1901: loss = 2.3407 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 1902: loss = 3.2446 (1.290 sec/step)\n",
            "I1229 11:26:58.179285 140034317584256 learning.py:507] global step 1902: loss = 3.2446 (1.290 sec/step)\n",
            "INFO:tensorflow:global step 1903: loss = 2.9461 (1.219 sec/step)\n",
            "I1229 11:26:59.400306 140034317584256 learning.py:507] global step 1903: loss = 2.9461 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1904: loss = 2.4909 (1.213 sec/step)\n",
            "I1229 11:27:00.615270 140034317584256 learning.py:507] global step 1904: loss = 2.4909 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1905: loss = 1.6383 (1.233 sec/step)\n",
            "I1229 11:27:01.850320 140034317584256 learning.py:507] global step 1905: loss = 1.6383 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1906: loss = 3.5576 (1.363 sec/step)\n",
            "I1229 11:27:03.215071 140034317584256 learning.py:507] global step 1906: loss = 3.5576 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 1907: loss = 4.8921 (1.261 sec/step)\n",
            "I1229 11:27:04.478035 140034317584256 learning.py:507] global step 1907: loss = 4.8921 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 1908: loss = 3.2699 (1.247 sec/step)\n",
            "I1229 11:27:05.726495 140034317584256 learning.py:507] global step 1908: loss = 3.2699 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1909: loss = 2.6085 (1.253 sec/step)\n",
            "I1229 11:27:06.981538 140034317584256 learning.py:507] global step 1909: loss = 2.6085 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1910: loss = 2.8000 (1.235 sec/step)\n",
            "I1229 11:27:08.218602 140034317584256 learning.py:507] global step 1910: loss = 2.8000 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1911: loss = 2.8366 (1.213 sec/step)\n",
            "I1229 11:27:09.433856 140034317584256 learning.py:507] global step 1911: loss = 2.8366 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 1912: loss = 1.9601 (1.242 sec/step)\n",
            "I1229 11:27:10.677201 140034317584256 learning.py:507] global step 1912: loss = 1.9601 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1913: loss = 1.3620 (1.227 sec/step)\n",
            "I1229 11:27:11.906053 140034317584256 learning.py:507] global step 1913: loss = 1.3620 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 1914: loss = 2.1653 (1.233 sec/step)\n",
            "I1229 11:27:13.141001 140034317584256 learning.py:507] global step 1914: loss = 2.1653 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 1915: loss = 2.9062 (1.230 sec/step)\n",
            "I1229 11:27:14.372695 140034317584256 learning.py:507] global step 1915: loss = 2.9062 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 1916: loss = 3.3253 (1.212 sec/step)\n",
            "I1229 11:27:15.586562 140034317584256 learning.py:507] global step 1916: loss = 3.3253 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1917: loss = 2.5620 (1.173 sec/step)\n",
            "I1229 11:27:16.761784 140034317584256 learning.py:507] global step 1917: loss = 2.5620 (1.173 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 11:27:17.668249 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 1918: loss = 1.8631 (2.445 sec/step)\n",
            "I1229 11:27:19.213788 140034317584256 learning.py:507] global step 1918: loss = 1.8631 (2.445 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1918.\n",
            "I1229 11:27:20.542383 140030580766464 supervisor.py:1050] Recording summary at step 1918.\n",
            "INFO:tensorflow:global step 1919: loss = 3.2029 (1.922 sec/step)\n",
            "I1229 11:27:21.148155 140034317584256 learning.py:507] global step 1919: loss = 3.2029 (1.922 sec/step)\n",
            "INFO:tensorflow:global step 1920: loss = 3.2137 (1.762 sec/step)\n",
            "I1229 11:27:22.922813 140034317584256 learning.py:507] global step 1920: loss = 3.2137 (1.762 sec/step)\n",
            "INFO:tensorflow:global step 1921: loss = 2.6766 (1.592 sec/step)\n",
            "I1229 11:27:24.524972 140034317584256 learning.py:507] global step 1921: loss = 2.6766 (1.592 sec/step)\n",
            "INFO:tensorflow:global step 1922: loss = 2.4460 (1.734 sec/step)\n",
            "I1229 11:27:26.262652 140034317584256 learning.py:507] global step 1922: loss = 2.4460 (1.734 sec/step)\n",
            "INFO:tensorflow:global step 1923: loss = 2.0621 (1.246 sec/step)\n",
            "I1229 11:27:27.511049 140034317584256 learning.py:507] global step 1923: loss = 2.0621 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1924: loss = 3.8372 (1.215 sec/step)\n",
            "I1229 11:27:28.728391 140034317584256 learning.py:507] global step 1924: loss = 3.8372 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 1925: loss = 2.5430 (1.239 sec/step)\n",
            "I1229 11:27:29.969170 140034317584256 learning.py:507] global step 1925: loss = 2.5430 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 1926: loss = 2.5462 (1.281 sec/step)\n",
            "I1229 11:27:31.251992 140034317584256 learning.py:507] global step 1926: loss = 2.5462 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 1927: loss = 2.4546 (1.212 sec/step)\n",
            "I1229 11:27:32.466140 140034317584256 learning.py:507] global step 1927: loss = 2.4546 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1928: loss = 2.4837 (1.187 sec/step)\n",
            "I1229 11:27:33.654571 140034317584256 learning.py:507] global step 1928: loss = 2.4837 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 1929: loss = 3.7625 (1.281 sec/step)\n",
            "I1229 11:27:34.937554 140034317584256 learning.py:507] global step 1929: loss = 3.7625 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 1930: loss = 2.7204 (1.264 sec/step)\n",
            "I1229 11:27:36.203064 140034317584256 learning.py:507] global step 1930: loss = 2.7204 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1931: loss = 4.0797 (1.218 sec/step)\n",
            "I1229 11:27:37.422928 140034317584256 learning.py:507] global step 1931: loss = 4.0797 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1932: loss = 1.6568 (1.218 sec/step)\n",
            "I1229 11:27:38.642973 140034317584256 learning.py:507] global step 1932: loss = 1.6568 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1933: loss = 2.0443 (1.264 sec/step)\n",
            "I1229 11:27:39.908743 140034317584256 learning.py:507] global step 1933: loss = 2.0443 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 1934: loss = 2.5523 (1.219 sec/step)\n",
            "I1229 11:27:41.129375 140034317584256 learning.py:507] global step 1934: loss = 2.5523 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 1935: loss = 1.7233 (1.183 sec/step)\n",
            "I1229 11:27:42.314343 140034317584256 learning.py:507] global step 1935: loss = 1.7233 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 1936: loss = 2.3640 (1.210 sec/step)\n",
            "I1229 11:27:43.526154 140034317584256 learning.py:507] global step 1936: loss = 2.3640 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 1937: loss = 4.2059 (1.237 sec/step)\n",
            "I1229 11:27:44.764446 140034317584256 learning.py:507] global step 1937: loss = 4.2059 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1938: loss = 4.3499 (1.214 sec/step)\n",
            "I1229 11:27:45.980248 140034317584256 learning.py:507] global step 1938: loss = 4.3499 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1939: loss = 2.9870 (1.204 sec/step)\n",
            "I1229 11:27:47.185701 140034317584256 learning.py:507] global step 1939: loss = 2.9870 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1940: loss = 2.4813 (1.249 sec/step)\n",
            "I1229 11:27:48.436705 140034317584256 learning.py:507] global step 1940: loss = 2.4813 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1941: loss = 2.8355 (1.244 sec/step)\n",
            "I1229 11:27:49.682321 140034317584256 learning.py:507] global step 1941: loss = 2.8355 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1942: loss = 2.8942 (1.234 sec/step)\n",
            "I1229 11:27:50.918067 140034317584256 learning.py:507] global step 1942: loss = 2.8942 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1943: loss = 1.5395 (1.218 sec/step)\n",
            "I1229 11:27:52.137681 140034317584256 learning.py:507] global step 1943: loss = 1.5395 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1944: loss = 2.6651 (1.246 sec/step)\n",
            "I1229 11:27:53.385166 140034317584256 learning.py:507] global step 1944: loss = 2.6651 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1945: loss = 1.7608 (1.209 sec/step)\n",
            "I1229 11:27:54.595914 140034317584256 learning.py:507] global step 1945: loss = 1.7608 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 1946: loss = 2.3066 (1.214 sec/step)\n",
            "I1229 11:27:55.811836 140034317584256 learning.py:507] global step 1946: loss = 2.3066 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1947: loss = 2.1224 (1.251 sec/step)\n",
            "I1229 11:27:57.065101 140034317584256 learning.py:507] global step 1947: loss = 2.1224 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 1948: loss = 2.5196 (1.212 sec/step)\n",
            "I1229 11:27:58.278545 140034317584256 learning.py:507] global step 1948: loss = 2.5196 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1949: loss = 1.8233 (1.229 sec/step)\n",
            "I1229 11:27:59.509022 140034317584256 learning.py:507] global step 1949: loss = 1.8233 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 1950: loss = 3.8956 (1.200 sec/step)\n",
            "I1229 11:28:00.710659 140034317584256 learning.py:507] global step 1950: loss = 3.8956 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1951: loss = 3.1135 (1.174 sec/step)\n",
            "I1229 11:28:01.885963 140034317584256 learning.py:507] global step 1951: loss = 3.1135 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 1952: loss = 2.1344 (1.223 sec/step)\n",
            "I1229 11:28:03.111139 140034317584256 learning.py:507] global step 1952: loss = 2.1344 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1953: loss = 4.0012 (1.169 sec/step)\n",
            "I1229 11:28:04.281306 140034317584256 learning.py:507] global step 1953: loss = 4.0012 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 1954: loss = 4.3168 (1.216 sec/step)\n",
            "I1229 11:28:05.498976 140034317584256 learning.py:507] global step 1954: loss = 4.3168 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 1955: loss = 2.6145 (1.285 sec/step)\n",
            "I1229 11:28:06.786046 140034317584256 learning.py:507] global step 1955: loss = 2.6145 (1.285 sec/step)\n",
            "INFO:tensorflow:global step 1956: loss = 2.5297 (1.223 sec/step)\n",
            "I1229 11:28:08.010365 140034317584256 learning.py:507] global step 1956: loss = 2.5297 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1957: loss = 3.7601 (1.214 sec/step)\n",
            "I1229 11:28:09.225382 140034317584256 learning.py:507] global step 1957: loss = 3.7601 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 1958: loss = 1.8280 (1.243 sec/step)\n",
            "I1229 11:28:10.469846 140034317584256 learning.py:507] global step 1958: loss = 1.8280 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 1959: loss = 3.0697 (1.211 sec/step)\n",
            "I1229 11:28:11.682571 140034317584256 learning.py:507] global step 1959: loss = 3.0697 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 1960: loss = 2.3764 (1.201 sec/step)\n",
            "I1229 11:28:12.885931 140034317584256 learning.py:507] global step 1960: loss = 2.3764 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1961: loss = 3.0609 (1.237 sec/step)\n",
            "I1229 11:28:14.124988 140034317584256 learning.py:507] global step 1961: loss = 3.0609 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 1962: loss = 2.4890 (1.244 sec/step)\n",
            "I1229 11:28:15.370565 140034317584256 learning.py:507] global step 1962: loss = 2.4890 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 1963: loss = 2.4150 (1.235 sec/step)\n",
            "I1229 11:28:16.607918 140034317584256 learning.py:507] global step 1963: loss = 2.4150 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1964: loss = 2.9298 (1.234 sec/step)\n",
            "I1229 11:28:17.843807 140034317584256 learning.py:507] global step 1964: loss = 2.9298 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1965: loss = 2.5939 (1.218 sec/step)\n",
            "I1229 11:28:19.063694 140034317584256 learning.py:507] global step 1965: loss = 2.5939 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 1966: loss = 2.0638 (1.200 sec/step)\n",
            "I1229 11:28:20.265625 140034317584256 learning.py:507] global step 1966: loss = 2.0638 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 1967: loss = 2.0945 (1.256 sec/step)\n",
            "I1229 11:28:21.523163 140034317584256 learning.py:507] global step 1967: loss = 2.0945 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1968: loss = 2.5472 (1.249 sec/step)\n",
            "I1229 11:28:22.773931 140034317584256 learning.py:507] global step 1968: loss = 2.5472 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 1969: loss = 3.3883 (1.203 sec/step)\n",
            "I1229 11:28:23.978517 140034317584256 learning.py:507] global step 1969: loss = 3.3883 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1970: loss = 2.3456 (1.201 sec/step)\n",
            "I1229 11:28:25.181707 140034317584256 learning.py:507] global step 1970: loss = 2.3456 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 1971: loss = 2.4350 (1.208 sec/step)\n",
            "I1229 11:28:26.391294 140034317584256 learning.py:507] global step 1971: loss = 2.4350 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1972: loss = 2.8940 (1.326 sec/step)\n",
            "I1229 11:28:27.719347 140034317584256 learning.py:507] global step 1972: loss = 2.8940 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 1973: loss = 3.6492 (1.208 sec/step)\n",
            "I1229 11:28:28.929254 140034317584256 learning.py:507] global step 1973: loss = 3.6492 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 1974: loss = 2.5071 (1.223 sec/step)\n",
            "I1229 11:28:30.154261 140034317584256 learning.py:507] global step 1974: loss = 2.5071 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1975: loss = 2.6773 (1.205 sec/step)\n",
            "I1229 11:28:31.360605 140034317584256 learning.py:507] global step 1975: loss = 2.6773 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 1976: loss = 3.6439 (1.231 sec/step)\n",
            "I1229 11:28:32.593502 140034317584256 learning.py:507] global step 1976: loss = 3.6439 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1977: loss = 1.7592 (1.188 sec/step)\n",
            "I1229 11:28:33.783471 140034317584256 learning.py:507] global step 1977: loss = 1.7592 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 1978: loss = 2.6162 (1.203 sec/step)\n",
            "I1229 11:28:34.988255 140034317584256 learning.py:507] global step 1978: loss = 2.6162 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1979: loss = 2.7781 (1.199 sec/step)\n",
            "I1229 11:28:36.188616 140034317584256 learning.py:507] global step 1979: loss = 2.7781 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 1980: loss = 3.2784 (1.263 sec/step)\n",
            "I1229 11:28:37.453821 140034317584256 learning.py:507] global step 1980: loss = 3.2784 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 1981: loss = 2.1785 (1.238 sec/step)\n",
            "I1229 11:28:38.693858 140034317584256 learning.py:507] global step 1981: loss = 2.1785 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 1982: loss = 1.8903 (1.229 sec/step)\n",
            "I1229 11:28:39.925123 140034317584256 learning.py:507] global step 1982: loss = 1.8903 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 1983: loss = 3.0336 (1.250 sec/step)\n",
            "I1229 11:28:41.177185 140034317584256 learning.py:507] global step 1983: loss = 3.0336 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 1984: loss = 3.3307 (1.279 sec/step)\n",
            "I1229 11:28:42.457720 140034317584256 learning.py:507] global step 1984: loss = 3.3307 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 1985: loss = 2.8372 (1.269 sec/step)\n",
            "I1229 11:28:43.729810 140034317584256 learning.py:507] global step 1985: loss = 2.8372 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 1986: loss = 2.7007 (1.255 sec/step)\n",
            "I1229 11:28:44.987307 140034317584256 learning.py:507] global step 1986: loss = 2.7007 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1987: loss = 2.1804 (1.246 sec/step)\n",
            "I1229 11:28:46.234623 140034317584256 learning.py:507] global step 1987: loss = 2.1804 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 1988: loss = 2.6661 (1.235 sec/step)\n",
            "I1229 11:28:47.471662 140034317584256 learning.py:507] global step 1988: loss = 2.6661 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 1989: loss = 4.3958 (1.343 sec/step)\n",
            "I1229 11:28:48.816493 140034317584256 learning.py:507] global step 1989: loss = 4.3958 (1.343 sec/step)\n",
            "INFO:tensorflow:global step 1990: loss = 3.2093 (1.241 sec/step)\n",
            "I1229 11:28:50.058882 140034317584256 learning.py:507] global step 1990: loss = 3.2093 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 1991: loss = 2.1789 (1.234 sec/step)\n",
            "I1229 11:28:51.294959 140034317584256 learning.py:507] global step 1991: loss = 2.1789 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 1992: loss = 3.0914 (1.221 sec/step)\n",
            "I1229 11:28:52.517173 140034317584256 learning.py:507] global step 1992: loss = 3.0914 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 1993: loss = 1.5085 (1.228 sec/step)\n",
            "I1229 11:28:53.746969 140034317584256 learning.py:507] global step 1993: loss = 1.5085 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 1994: loss = 2.3735 (1.247 sec/step)\n",
            "I1229 11:28:54.995806 140034317584256 learning.py:507] global step 1994: loss = 2.3735 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 1995: loss = 1.7492 (1.242 sec/step)\n",
            "I1229 11:28:56.238987 140034317584256 learning.py:507] global step 1995: loss = 1.7492 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 1996: loss = 2.3617 (1.256 sec/step)\n",
            "I1229 11:28:57.496528 140034317584256 learning.py:507] global step 1996: loss = 2.3617 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 1997: loss = 2.7250 (1.231 sec/step)\n",
            "I1229 11:28:58.729499 140034317584256 learning.py:507] global step 1997: loss = 2.7250 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 1998: loss = 2.7264 (1.204 sec/step)\n",
            "I1229 11:28:59.935747 140034317584256 learning.py:507] global step 1998: loss = 2.7264 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 1999: loss = 2.0264 (1.195 sec/step)\n",
            "I1229 11:29:01.132976 140034317584256 learning.py:507] global step 1999: loss = 2.0264 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2000: loss = 2.8810 (1.210 sec/step)\n",
            "I1229 11:29:02.344150 140034317584256 learning.py:507] global step 2000: loss = 2.8810 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2001: loss = 1.9869 (1.227 sec/step)\n",
            "I1229 11:29:03.573059 140034317584256 learning.py:507] global step 2001: loss = 1.9869 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2002: loss = 1.8130 (1.246 sec/step)\n",
            "I1229 11:29:04.821173 140034317584256 learning.py:507] global step 2002: loss = 1.8130 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2003: loss = 2.1798 (1.220 sec/step)\n",
            "I1229 11:29:06.043094 140034317584256 learning.py:507] global step 2003: loss = 2.1798 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2004: loss = 3.0861 (1.208 sec/step)\n",
            "I1229 11:29:07.253157 140034317584256 learning.py:507] global step 2004: loss = 3.0861 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2005: loss = 2.3618 (1.211 sec/step)\n",
            "I1229 11:29:08.465607 140034317584256 learning.py:507] global step 2005: loss = 2.3618 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2006: loss = 2.0785 (1.222 sec/step)\n",
            "I1229 11:29:09.689054 140034317584256 learning.py:507] global step 2006: loss = 2.0785 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2007: loss = 3.1444 (1.237 sec/step)\n",
            "I1229 11:29:10.927324 140034317584256 learning.py:507] global step 2007: loss = 3.1444 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2008: loss = 3.0204 (1.225 sec/step)\n",
            "I1229 11:29:12.154519 140034317584256 learning.py:507] global step 2008: loss = 3.0204 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2009: loss = 3.0019 (1.215 sec/step)\n",
            "I1229 11:29:13.371580 140034317584256 learning.py:507] global step 2009: loss = 3.0019 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2010: loss = 3.3057 (1.203 sec/step)\n",
            "I1229 11:29:14.576438 140034317584256 learning.py:507] global step 2010: loss = 3.3057 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2011: loss = 2.1056 (1.272 sec/step)\n",
            "I1229 11:29:15.850406 140034317584256 learning.py:507] global step 2011: loss = 2.1056 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2012: loss = 2.9827 (1.210 sec/step)\n",
            "I1229 11:29:17.062387 140034317584256 learning.py:507] global step 2012: loss = 2.9827 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2013: loss = 1.2829 (2.053 sec/step)\n",
            "I1229 11:29:19.116771 140034317584256 learning.py:507] global step 2013: loss = 1.2829 (2.053 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2013.\n",
            "I1229 11:29:19.817515 140030580766464 supervisor.py:1050] Recording summary at step 2013.\n",
            "INFO:tensorflow:global step 2014: loss = 1.9141 (1.314 sec/step)\n",
            "I1229 11:29:20.433117 140034317584256 learning.py:507] global step 2014: loss = 1.9141 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 2015: loss = 2.7270 (1.202 sec/step)\n",
            "I1229 11:29:21.637105 140034317584256 learning.py:507] global step 2015: loss = 2.7270 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 2016: loss = 2.0060 (1.243 sec/step)\n",
            "I1229 11:29:22.881548 140034317584256 learning.py:507] global step 2016: loss = 2.0060 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2017: loss = 2.1051 (1.199 sec/step)\n",
            "I1229 11:29:24.081869 140034317584256 learning.py:507] global step 2017: loss = 2.1051 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 2018: loss = 2.3418 (1.207 sec/step)\n",
            "I1229 11:29:25.290941 140034317584256 learning.py:507] global step 2018: loss = 2.3418 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2019: loss = 1.4443 (1.247 sec/step)\n",
            "I1229 11:29:26.539189 140034317584256 learning.py:507] global step 2019: loss = 1.4443 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2020: loss = 2.5443 (1.249 sec/step)\n",
            "I1229 11:29:27.789965 140034317584256 learning.py:507] global step 2020: loss = 2.5443 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2021: loss = 2.7075 (1.226 sec/step)\n",
            "I1229 11:29:29.017989 140034317584256 learning.py:507] global step 2021: loss = 2.7075 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2022: loss = 2.2663 (1.235 sec/step)\n",
            "I1229 11:29:30.254542 140034317584256 learning.py:507] global step 2022: loss = 2.2663 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2023: loss = 2.8176 (1.274 sec/step)\n",
            "I1229 11:29:31.530452 140034317584256 learning.py:507] global step 2023: loss = 2.8176 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2024: loss = 1.6789 (1.236 sec/step)\n",
            "I1229 11:29:32.768555 140034317584256 learning.py:507] global step 2024: loss = 1.6789 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2025: loss = 2.1458 (1.212 sec/step)\n",
            "I1229 11:29:33.982331 140034317584256 learning.py:507] global step 2025: loss = 2.1458 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2026: loss = 2.5726 (1.244 sec/step)\n",
            "I1229 11:29:35.228413 140034317584256 learning.py:507] global step 2026: loss = 2.5726 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2027: loss = 2.6393 (1.222 sec/step)\n",
            "I1229 11:29:36.451841 140034317584256 learning.py:507] global step 2027: loss = 2.6393 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2028: loss = 2.3965 (1.234 sec/step)\n",
            "I1229 11:29:37.687705 140034317584256 learning.py:507] global step 2028: loss = 2.3965 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2029: loss = 2.4588 (1.279 sec/step)\n",
            "I1229 11:29:38.968101 140034317584256 learning.py:507] global step 2029: loss = 2.4588 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 2030: loss = 3.4678 (1.294 sec/step)\n",
            "I1229 11:29:40.264327 140034317584256 learning.py:507] global step 2030: loss = 3.4678 (1.294 sec/step)\n",
            "INFO:tensorflow:global step 2031: loss = 1.9367 (1.300 sec/step)\n",
            "I1229 11:29:41.565737 140034317584256 learning.py:507] global step 2031: loss = 1.9367 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 2032: loss = 2.6150 (1.238 sec/step)\n",
            "I1229 11:29:42.805339 140034317584256 learning.py:507] global step 2032: loss = 2.6150 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2033: loss = 2.8671 (1.239 sec/step)\n",
            "I1229 11:29:44.046190 140034317584256 learning.py:507] global step 2033: loss = 2.8671 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2034: loss = 2.9867 (1.215 sec/step)\n",
            "I1229 11:29:45.262927 140034317584256 learning.py:507] global step 2034: loss = 2.9867 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2035: loss = 2.2644 (1.250 sec/step)\n",
            "I1229 11:29:46.514493 140034317584256 learning.py:507] global step 2035: loss = 2.2644 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2036: loss = 3.2008 (1.254 sec/step)\n",
            "I1229 11:29:47.770596 140034317584256 learning.py:507] global step 2036: loss = 3.2008 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2037: loss = 2.2029 (1.236 sec/step)\n",
            "I1229 11:29:49.008324 140034317584256 learning.py:507] global step 2037: loss = 2.2029 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2038: loss = 2.4695 (1.260 sec/step)\n",
            "I1229 11:29:50.270633 140034317584256 learning.py:507] global step 2038: loss = 2.4695 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2039: loss = 3.0471 (1.281 sec/step)\n",
            "I1229 11:29:51.553190 140034317584256 learning.py:507] global step 2039: loss = 3.0471 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 2040: loss = 3.4395 (1.284 sec/step)\n",
            "I1229 11:29:52.839135 140034317584256 learning.py:507] global step 2040: loss = 3.4395 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 2041: loss = 2.7409 (1.258 sec/step)\n",
            "I1229 11:29:54.099481 140034317584256 learning.py:507] global step 2041: loss = 2.7409 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2042: loss = 1.5908 (1.253 sec/step)\n",
            "I1229 11:29:55.354712 140034317584256 learning.py:507] global step 2042: loss = 1.5908 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 2043: loss = 2.1457 (1.277 sec/step)\n",
            "I1229 11:29:56.633069 140034317584256 learning.py:507] global step 2043: loss = 2.1457 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 2044: loss = 1.4148 (1.218 sec/step)\n",
            "I1229 11:29:57.852627 140034317584256 learning.py:507] global step 2044: loss = 1.4148 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2045: loss = 2.6156 (1.176 sec/step)\n",
            "I1229 11:29:59.030777 140034317584256 learning.py:507] global step 2045: loss = 2.6156 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 2046: loss = 3.2918 (1.239 sec/step)\n",
            "I1229 11:30:00.271314 140034317584256 learning.py:507] global step 2046: loss = 3.2918 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2047: loss = 2.1771 (1.250 sec/step)\n",
            "I1229 11:30:01.523993 140034317584256 learning.py:507] global step 2047: loss = 2.1771 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2048: loss = 2.4619 (1.294 sec/step)\n",
            "I1229 11:30:02.819607 140034317584256 learning.py:507] global step 2048: loss = 2.4619 (1.294 sec/step)\n",
            "INFO:tensorflow:global step 2049: loss = 2.6761 (1.236 sec/step)\n",
            "I1229 11:30:04.057086 140034317584256 learning.py:507] global step 2049: loss = 2.6761 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2050: loss = 2.0614 (1.265 sec/step)\n",
            "I1229 11:30:05.323460 140034317584256 learning.py:507] global step 2050: loss = 2.0614 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 2051: loss = 2.2328 (1.261 sec/step)\n",
            "I1229 11:30:06.585882 140034317584256 learning.py:507] global step 2051: loss = 2.2328 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2052: loss = 2.7599 (1.289 sec/step)\n",
            "I1229 11:30:07.876832 140034317584256 learning.py:507] global step 2052: loss = 2.7599 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 2053: loss = 2.3914 (1.301 sec/step)\n",
            "I1229 11:30:09.179681 140034317584256 learning.py:507] global step 2053: loss = 2.3914 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 2054: loss = 1.5578 (1.271 sec/step)\n",
            "I1229 11:30:10.453317 140034317584256 learning.py:507] global step 2054: loss = 1.5578 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2055: loss = 2.8153 (1.225 sec/step)\n",
            "I1229 11:30:11.680274 140034317584256 learning.py:507] global step 2055: loss = 2.8153 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2056: loss = 2.6081 (1.268 sec/step)\n",
            "I1229 11:30:12.950426 140034317584256 learning.py:507] global step 2056: loss = 2.6081 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2057: loss = 2.2584 (1.223 sec/step)\n",
            "I1229 11:30:14.175127 140034317584256 learning.py:507] global step 2057: loss = 2.2584 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2058: loss = 3.0047 (1.233 sec/step)\n",
            "I1229 11:30:15.410165 140034317584256 learning.py:507] global step 2058: loss = 3.0047 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2059: loss = 2.3161 (1.262 sec/step)\n",
            "I1229 11:30:16.673976 140034317584256 learning.py:507] global step 2059: loss = 2.3161 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2060: loss = 1.8694 (1.248 sec/step)\n",
            "I1229 11:30:17.924258 140034317584256 learning.py:507] global step 2060: loss = 1.8694 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2061: loss = 3.3969 (1.224 sec/step)\n",
            "I1229 11:30:19.149415 140034317584256 learning.py:507] global step 2061: loss = 3.3969 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2062: loss = 2.6192 (1.207 sec/step)\n",
            "I1229 11:30:20.358473 140034317584256 learning.py:507] global step 2062: loss = 2.6192 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2063: loss = 1.8383 (1.240 sec/step)\n",
            "I1229 11:30:21.600571 140034317584256 learning.py:507] global step 2063: loss = 1.8383 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2064: loss = 1.5598 (1.217 sec/step)\n",
            "I1229 11:30:22.819764 140034317584256 learning.py:507] global step 2064: loss = 1.5598 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 2065: loss = 2.1317 (1.239 sec/step)\n",
            "I1229 11:30:24.060919 140034317584256 learning.py:507] global step 2065: loss = 2.1317 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2066: loss = 2.3537 (1.236 sec/step)\n",
            "I1229 11:30:25.298465 140034317584256 learning.py:507] global step 2066: loss = 2.3537 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2067: loss = 2.5365 (1.239 sec/step)\n",
            "I1229 11:30:26.539176 140034317584256 learning.py:507] global step 2067: loss = 2.5365 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2068: loss = 2.1872 (1.202 sec/step)\n",
            "I1229 11:30:27.742761 140034317584256 learning.py:507] global step 2068: loss = 2.1872 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 2069: loss = 2.0009 (1.198 sec/step)\n",
            "I1229 11:30:28.942729 140034317584256 learning.py:507] global step 2069: loss = 2.0009 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2070: loss = 2.5172 (1.203 sec/step)\n",
            "I1229 11:30:30.147402 140034317584256 learning.py:507] global step 2070: loss = 2.5172 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2071: loss = 2.3085 (1.247 sec/step)\n",
            "I1229 11:30:31.396384 140034317584256 learning.py:507] global step 2071: loss = 2.3085 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2072: loss = 2.0001 (1.237 sec/step)\n",
            "I1229 11:30:32.634607 140034317584256 learning.py:507] global step 2072: loss = 2.0001 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2073: loss = 3.1755 (1.268 sec/step)\n",
            "I1229 11:30:33.904758 140034317584256 learning.py:507] global step 2073: loss = 3.1755 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2074: loss = 2.6285 (1.245 sec/step)\n",
            "I1229 11:30:35.151867 140034317584256 learning.py:507] global step 2074: loss = 2.6285 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2075: loss = 1.7670 (1.218 sec/step)\n",
            "I1229 11:30:36.371583 140034317584256 learning.py:507] global step 2075: loss = 1.7670 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2076: loss = 2.1262 (1.223 sec/step)\n",
            "I1229 11:30:37.597246 140034317584256 learning.py:507] global step 2076: loss = 2.1262 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2077: loss = 1.7241 (1.183 sec/step)\n",
            "I1229 11:30:38.782525 140034317584256 learning.py:507] global step 2077: loss = 1.7241 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 2078: loss = 4.3197 (1.296 sec/step)\n",
            "I1229 11:30:40.080180 140034317584256 learning.py:507] global step 2078: loss = 4.3197 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 2079: loss = 2.4452 (1.323 sec/step)\n",
            "I1229 11:30:41.405385 140034317584256 learning.py:507] global step 2079: loss = 2.4452 (1.323 sec/step)\n",
            "INFO:tensorflow:global step 2080: loss = 1.6709 (1.214 sec/step)\n",
            "I1229 11:30:42.621351 140034317584256 learning.py:507] global step 2080: loss = 1.6709 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2081: loss = 4.2663 (1.203 sec/step)\n",
            "I1229 11:30:43.825806 140034317584256 learning.py:507] global step 2081: loss = 4.2663 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2082: loss = 1.7215 (1.234 sec/step)\n",
            "I1229 11:30:45.061704 140034317584256 learning.py:507] global step 2082: loss = 1.7215 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2083: loss = 2.7281 (1.222 sec/step)\n",
            "I1229 11:30:46.285393 140034317584256 learning.py:507] global step 2083: loss = 2.7281 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2084: loss = 2.1621 (1.202 sec/step)\n",
            "I1229 11:30:47.488839 140034317584256 learning.py:507] global step 2084: loss = 2.1621 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 2085: loss = 1.5815 (1.249 sec/step)\n",
            "I1229 11:30:48.739247 140034317584256 learning.py:507] global step 2085: loss = 1.5815 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2086: loss = 3.5404 (1.212 sec/step)\n",
            "I1229 11:30:49.952621 140034317584256 learning.py:507] global step 2086: loss = 3.5404 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2087: loss = 2.8248 (1.191 sec/step)\n",
            "I1229 11:30:51.145595 140034317584256 learning.py:507] global step 2087: loss = 2.8248 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 2088: loss = 1.9903 (1.194 sec/step)\n",
            "I1229 11:30:52.341279 140034317584256 learning.py:507] global step 2088: loss = 1.9903 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 2089: loss = 2.3489 (1.186 sec/step)\n",
            "I1229 11:30:53.529060 140034317584256 learning.py:507] global step 2089: loss = 2.3489 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2090: loss = 2.4346 (1.247 sec/step)\n",
            "I1229 11:30:54.778313 140034317584256 learning.py:507] global step 2090: loss = 2.4346 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2091: loss = 2.3678 (1.204 sec/step)\n",
            "I1229 11:30:55.983842 140034317584256 learning.py:507] global step 2091: loss = 2.3678 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 2092: loss = 1.6646 (1.199 sec/step)\n",
            "I1229 11:30:57.184391 140034317584256 learning.py:507] global step 2092: loss = 1.6646 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 2093: loss = 2.1228 (1.248 sec/step)\n",
            "I1229 11:30:58.433935 140034317584256 learning.py:507] global step 2093: loss = 2.1228 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2094: loss = 4.8427 (1.206 sec/step)\n",
            "I1229 11:30:59.641357 140034317584256 learning.py:507] global step 2094: loss = 4.8427 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2095: loss = 3.5448 (1.218 sec/step)\n",
            "I1229 11:31:00.861612 140034317584256 learning.py:507] global step 2095: loss = 3.5448 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2096: loss = 1.9291 (1.194 sec/step)\n",
            "I1229 11:31:02.057464 140034317584256 learning.py:507] global step 2096: loss = 1.9291 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 2097: loss = 2.7077 (1.192 sec/step)\n",
            "I1229 11:31:03.250867 140034317584256 learning.py:507] global step 2097: loss = 2.7077 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 2098: loss = 3.1702 (1.235 sec/step)\n",
            "I1229 11:31:04.487721 140034317584256 learning.py:507] global step 2098: loss = 3.1702 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2099: loss = 3.0129 (1.268 sec/step)\n",
            "I1229 11:31:05.757667 140034317584256 learning.py:507] global step 2099: loss = 3.0129 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2100: loss = 2.8688 (1.194 sec/step)\n",
            "I1229 11:31:06.953775 140034317584256 learning.py:507] global step 2100: loss = 2.8688 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 2101: loss = 2.8305 (1.212 sec/step)\n",
            "I1229 11:31:08.167756 140034317584256 learning.py:507] global step 2101: loss = 2.8305 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2102: loss = 2.1537 (1.243 sec/step)\n",
            "I1229 11:31:09.412666 140034317584256 learning.py:507] global step 2102: loss = 2.1537 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2103: loss = 3.0543 (1.238 sec/step)\n",
            "I1229 11:31:10.652193 140034317584256 learning.py:507] global step 2103: loss = 3.0543 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2104: loss = 2.3304 (1.240 sec/step)\n",
            "I1229 11:31:11.894557 140034317584256 learning.py:507] global step 2104: loss = 2.3304 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2105: loss = 2.9254 (1.246 sec/step)\n",
            "I1229 11:31:13.142035 140034317584256 learning.py:507] global step 2105: loss = 2.9254 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2106: loss = 1.6999 (1.204 sec/step)\n",
            "I1229 11:31:14.347401 140034317584256 learning.py:507] global step 2106: loss = 1.6999 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 2107: loss = 2.3936 (1.223 sec/step)\n",
            "I1229 11:31:15.571753 140034317584256 learning.py:507] global step 2107: loss = 2.3936 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2108: loss = 3.3562 (1.237 sec/step)\n",
            "I1229 11:31:16.810851 140034317584256 learning.py:507] global step 2108: loss = 3.3562 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2109: loss = 3.5216 (1.896 sec/step)\n",
            "I1229 11:31:18.711744 140034317584256 learning.py:507] global step 2109: loss = 3.5216 (1.896 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2109.\n",
            "I1229 11:31:19.872751 140030580766464 supervisor.py:1050] Recording summary at step 2109.\n",
            "INFO:tensorflow:global step 2110: loss = 2.4653 (1.489 sec/step)\n",
            "I1229 11:31:20.202674 140034317584256 learning.py:507] global step 2110: loss = 2.4653 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 2111: loss = 1.8505 (1.251 sec/step)\n",
            "I1229 11:31:21.457118 140034317584256 learning.py:507] global step 2111: loss = 1.8505 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2112: loss = 2.8019 (1.223 sec/step)\n",
            "I1229 11:31:22.684256 140034317584256 learning.py:507] global step 2112: loss = 2.8019 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2113: loss = 2.5160 (1.261 sec/step)\n",
            "I1229 11:31:23.947305 140034317584256 learning.py:507] global step 2113: loss = 2.5160 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2114: loss = 2.5503 (1.177 sec/step)\n",
            "I1229 11:31:25.125732 140034317584256 learning.py:507] global step 2114: loss = 2.5503 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 2115: loss = 3.0611 (1.267 sec/step)\n",
            "I1229 11:31:26.394606 140034317584256 learning.py:507] global step 2115: loss = 3.0611 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2116: loss = 2.2218 (1.247 sec/step)\n",
            "I1229 11:31:27.644141 140034317584256 learning.py:507] global step 2116: loss = 2.2218 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2117: loss = 1.6537 (1.274 sec/step)\n",
            "I1229 11:31:28.920427 140034317584256 learning.py:507] global step 2117: loss = 1.6537 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2118: loss = 2.9229 (1.236 sec/step)\n",
            "I1229 11:31:30.158181 140034317584256 learning.py:507] global step 2118: loss = 2.9229 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2119: loss = 2.8912 (1.244 sec/step)\n",
            "I1229 11:31:31.403754 140034317584256 learning.py:507] global step 2119: loss = 2.8912 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2120: loss = 2.4334 (1.196 sec/step)\n",
            "I1229 11:31:32.601644 140034317584256 learning.py:507] global step 2120: loss = 2.4334 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2121: loss = 1.6004 (1.236 sec/step)\n",
            "I1229 11:31:33.839089 140034317584256 learning.py:507] global step 2121: loss = 1.6004 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2122: loss = 1.8336 (1.184 sec/step)\n",
            "I1229 11:31:35.024637 140034317584256 learning.py:507] global step 2122: loss = 1.8336 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 2123: loss = 3.1603 (1.236 sec/step)\n",
            "I1229 11:31:36.262684 140034317584256 learning.py:507] global step 2123: loss = 3.1603 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2124: loss = 4.2539 (1.240 sec/step)\n",
            "I1229 11:31:37.504988 140034317584256 learning.py:507] global step 2124: loss = 4.2539 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2125: loss = 2.1567 (1.262 sec/step)\n",
            "I1229 11:31:38.768398 140034317584256 learning.py:507] global step 2125: loss = 2.1567 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2126: loss = 1.8213 (1.279 sec/step)\n",
            "I1229 11:31:40.049263 140034317584256 learning.py:507] global step 2126: loss = 1.8213 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 2127: loss = 2.0281 (1.273 sec/step)\n",
            "I1229 11:31:41.324266 140034317584256 learning.py:507] global step 2127: loss = 2.0281 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 2128: loss = 2.6527 (1.289 sec/step)\n",
            "I1229 11:31:42.614727 140034317584256 learning.py:507] global step 2128: loss = 2.6527 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 2129: loss = 3.3487 (1.251 sec/step)\n",
            "I1229 11:31:43.869173 140034317584256 learning.py:507] global step 2129: loss = 3.3487 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2130: loss = 2.1338 (1.280 sec/step)\n",
            "I1229 11:31:45.151682 140034317584256 learning.py:507] global step 2130: loss = 2.1338 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2131: loss = 2.7804 (1.262 sec/step)\n",
            "I1229 11:31:46.415968 140034317584256 learning.py:507] global step 2131: loss = 2.7804 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2132: loss = 2.0877 (1.252 sec/step)\n",
            "I1229 11:31:47.669697 140034317584256 learning.py:507] global step 2132: loss = 2.0877 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2133: loss = 1.6987 (1.242 sec/step)\n",
            "I1229 11:31:48.913728 140034317584256 learning.py:507] global step 2133: loss = 1.6987 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2134: loss = 2.0025 (1.237 sec/step)\n",
            "I1229 11:31:50.152841 140034317584256 learning.py:507] global step 2134: loss = 2.0025 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2135: loss = 1.8945 (1.208 sec/step)\n",
            "I1229 11:31:51.363085 140034317584256 learning.py:507] global step 2135: loss = 1.8945 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2136: loss = 3.8561 (1.211 sec/step)\n",
            "I1229 11:31:52.577349 140034317584256 learning.py:507] global step 2136: loss = 3.8561 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2137: loss = 2.9327 (1.204 sec/step)\n",
            "I1229 11:31:53.783571 140034317584256 learning.py:507] global step 2137: loss = 2.9327 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 2138: loss = 4.0446 (1.250 sec/step)\n",
            "I1229 11:31:55.035257 140034317584256 learning.py:507] global step 2138: loss = 4.0446 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2139: loss = 2.2957 (1.252 sec/step)\n",
            "I1229 11:31:56.288671 140034317584256 learning.py:507] global step 2139: loss = 2.2957 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2140: loss = 2.1856 (1.231 sec/step)\n",
            "I1229 11:31:57.521149 140034317584256 learning.py:507] global step 2140: loss = 2.1856 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2141: loss = 1.9349 (1.225 sec/step)\n",
            "I1229 11:31:58.748240 140034317584256 learning.py:507] global step 2141: loss = 1.9349 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2142: loss = 1.9998 (1.205 sec/step)\n",
            "I1229 11:31:59.954637 140034317584256 learning.py:507] global step 2142: loss = 1.9998 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 2143: loss = 2.5135 (1.234 sec/step)\n",
            "I1229 11:32:01.190339 140034317584256 learning.py:507] global step 2143: loss = 2.5135 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2144: loss = 2.7377 (1.218 sec/step)\n",
            "I1229 11:32:02.410554 140034317584256 learning.py:507] global step 2144: loss = 2.7377 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2145: loss = 1.8528 (1.262 sec/step)\n",
            "I1229 11:32:03.674372 140034317584256 learning.py:507] global step 2145: loss = 1.8528 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2146: loss = 2.8409 (1.231 sec/step)\n",
            "I1229 11:32:04.907607 140034317584256 learning.py:507] global step 2146: loss = 2.8409 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2147: loss = 2.3266 (1.198 sec/step)\n",
            "I1229 11:32:06.106998 140034317584256 learning.py:507] global step 2147: loss = 2.3266 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2148: loss = 2.3032 (1.231 sec/step)\n",
            "I1229 11:32:07.340110 140034317584256 learning.py:507] global step 2148: loss = 2.3032 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2149: loss = 2.8851 (1.271 sec/step)\n",
            "I1229 11:32:08.613125 140034317584256 learning.py:507] global step 2149: loss = 2.8851 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2150: loss = 2.0601 (1.245 sec/step)\n",
            "I1229 11:32:09.860042 140034317584256 learning.py:507] global step 2150: loss = 2.0601 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2151: loss = 1.9443 (1.250 sec/step)\n",
            "I1229 11:32:11.111990 140034317584256 learning.py:507] global step 2151: loss = 1.9443 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2152: loss = 1.9050 (1.293 sec/step)\n",
            "I1229 11:32:12.406853 140034317584256 learning.py:507] global step 2152: loss = 1.9050 (1.293 sec/step)\n",
            "INFO:tensorflow:global step 2153: loss = 2.2175 (1.277 sec/step)\n",
            "I1229 11:32:13.686402 140034317584256 learning.py:507] global step 2153: loss = 2.2175 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 2154: loss = 2.6882 (1.276 sec/step)\n",
            "I1229 11:32:14.964341 140034317584256 learning.py:507] global step 2154: loss = 2.6882 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 2155: loss = 2.2737 (1.324 sec/step)\n",
            "I1229 11:32:16.290813 140034317584256 learning.py:507] global step 2155: loss = 2.2737 (1.324 sec/step)\n",
            "INFO:tensorflow:global step 2156: loss = 3.0853 (1.241 sec/step)\n",
            "I1229 11:32:17.533681 140034317584256 learning.py:507] global step 2156: loss = 3.0853 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2157: loss = 1.9320 (1.253 sec/step)\n",
            "I1229 11:32:18.789482 140034317584256 learning.py:507] global step 2157: loss = 1.9320 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 2158: loss = 2.4924 (1.256 sec/step)\n",
            "I1229 11:32:20.047359 140034317584256 learning.py:507] global step 2158: loss = 2.4924 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 2159: loss = 2.3611 (1.274 sec/step)\n",
            "I1229 11:32:21.323361 140034317584256 learning.py:507] global step 2159: loss = 2.3611 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2160: loss = 2.2190 (1.235 sec/step)\n",
            "I1229 11:32:22.560426 140034317584256 learning.py:507] global step 2160: loss = 2.2190 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2161: loss = 2.7033 (1.209 sec/step)\n",
            "I1229 11:32:23.771713 140034317584256 learning.py:507] global step 2161: loss = 2.7033 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 2162: loss = 3.3100 (1.228 sec/step)\n",
            "I1229 11:32:25.001363 140034317584256 learning.py:507] global step 2162: loss = 3.3100 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2163: loss = 3.1213 (1.221 sec/step)\n",
            "I1229 11:32:26.224019 140034317584256 learning.py:507] global step 2163: loss = 3.1213 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2164: loss = 2.0958 (1.239 sec/step)\n",
            "I1229 11:32:27.465031 140034317584256 learning.py:507] global step 2164: loss = 2.0958 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2165: loss = 3.6630 (1.229 sec/step)\n",
            "I1229 11:32:28.695615 140034317584256 learning.py:507] global step 2165: loss = 3.6630 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2166: loss = 1.9298 (1.238 sec/step)\n",
            "I1229 11:32:29.936063 140034317584256 learning.py:507] global step 2166: loss = 1.9298 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2167: loss = 3.8416 (1.211 sec/step)\n",
            "I1229 11:32:31.148367 140034317584256 learning.py:507] global step 2167: loss = 3.8416 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2168: loss = 2.0876 (1.257 sec/step)\n",
            "I1229 11:32:32.406824 140034317584256 learning.py:507] global step 2168: loss = 2.0876 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2169: loss = 2.3955 (1.224 sec/step)\n",
            "I1229 11:32:33.632519 140034317584256 learning.py:507] global step 2169: loss = 2.3955 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2170: loss = 2.4148 (1.238 sec/step)\n",
            "I1229 11:32:34.871989 140034317584256 learning.py:507] global step 2170: loss = 2.4148 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2171: loss = 1.6879 (1.222 sec/step)\n",
            "I1229 11:32:36.095813 140034317584256 learning.py:507] global step 2171: loss = 1.6879 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2172: loss = 2.7983 (1.198 sec/step)\n",
            "I1229 11:32:37.295680 140034317584256 learning.py:507] global step 2172: loss = 2.7983 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2173: loss = 2.7205 (1.197 sec/step)\n",
            "I1229 11:32:38.494100 140034317584256 learning.py:507] global step 2173: loss = 2.7205 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 2174: loss = 1.9304 (1.234 sec/step)\n",
            "I1229 11:32:39.729678 140034317584256 learning.py:507] global step 2174: loss = 1.9304 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2175: loss = 1.8760 (1.266 sec/step)\n",
            "I1229 11:32:40.997589 140034317584256 learning.py:507] global step 2175: loss = 1.8760 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 2176: loss = 3.2552 (1.345 sec/step)\n",
            "I1229 11:32:42.344179 140034317584256 learning.py:507] global step 2176: loss = 3.2552 (1.345 sec/step)\n",
            "INFO:tensorflow:global step 2177: loss = 2.8043 (1.239 sec/step)\n",
            "I1229 11:32:43.585024 140034317584256 learning.py:507] global step 2177: loss = 2.8043 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2178: loss = 3.2008 (1.251 sec/step)\n",
            "I1229 11:32:44.837935 140034317584256 learning.py:507] global step 2178: loss = 3.2008 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2179: loss = 2.9362 (1.291 sec/step)\n",
            "I1229 11:32:46.131071 140034317584256 learning.py:507] global step 2179: loss = 2.9362 (1.291 sec/step)\n",
            "INFO:tensorflow:global step 2180: loss = 3.1308 (1.274 sec/step)\n",
            "I1229 11:32:47.406541 140034317584256 learning.py:507] global step 2180: loss = 3.1308 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2181: loss = 2.3407 (1.233 sec/step)\n",
            "I1229 11:32:48.641320 140034317584256 learning.py:507] global step 2181: loss = 2.3407 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2182: loss = 2.8038 (1.246 sec/step)\n",
            "I1229 11:32:49.889296 140034317584256 learning.py:507] global step 2182: loss = 2.8038 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2183: loss = 2.3255 (1.262 sec/step)\n",
            "I1229 11:32:51.155139 140034317584256 learning.py:507] global step 2183: loss = 2.3255 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2184: loss = 1.8373 (1.277 sec/step)\n",
            "I1229 11:32:52.434271 140034317584256 learning.py:507] global step 2184: loss = 1.8373 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 2185: loss = 3.8002 (1.251 sec/step)\n",
            "I1229 11:32:53.687579 140034317584256 learning.py:507] global step 2185: loss = 3.8002 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2186: loss = 3.0610 (1.250 sec/step)\n",
            "I1229 11:32:54.939589 140034317584256 learning.py:507] global step 2186: loss = 3.0610 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2187: loss = 1.8932 (1.241 sec/step)\n",
            "I1229 11:32:56.182581 140034317584256 learning.py:507] global step 2187: loss = 1.8932 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2188: loss = 2.2694 (1.302 sec/step)\n",
            "I1229 11:32:57.486627 140034317584256 learning.py:507] global step 2188: loss = 2.2694 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 2189: loss = 1.3309 (1.251 sec/step)\n",
            "I1229 11:32:58.739325 140034317584256 learning.py:507] global step 2189: loss = 1.3309 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2190: loss = 1.9037 (1.237 sec/step)\n",
            "I1229 11:32:59.977876 140034317584256 learning.py:507] global step 2190: loss = 1.9037 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2191: loss = 3.8290 (1.267 sec/step)\n",
            "I1229 11:33:01.246273 140034317584256 learning.py:507] global step 2191: loss = 3.8290 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2192: loss = 1.9142 (1.210 sec/step)\n",
            "I1229 11:33:02.458050 140034317584256 learning.py:507] global step 2192: loss = 1.9142 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2193: loss = 2.5093 (1.241 sec/step)\n",
            "I1229 11:33:03.700908 140034317584256 learning.py:507] global step 2193: loss = 2.5093 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2194: loss = 2.9491 (1.240 sec/step)\n",
            "I1229 11:33:04.942487 140034317584256 learning.py:507] global step 2194: loss = 2.9491 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2195: loss = 2.4178 (1.239 sec/step)\n",
            "I1229 11:33:06.183311 140034317584256 learning.py:507] global step 2195: loss = 2.4178 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2196: loss = 2.3715 (1.240 sec/step)\n",
            "I1229 11:33:07.425573 140034317584256 learning.py:507] global step 2196: loss = 2.3715 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2197: loss = 2.5150 (1.271 sec/step)\n",
            "I1229 11:33:08.698010 140034317584256 learning.py:507] global step 2197: loss = 2.5150 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2198: loss = 1.9787 (1.254 sec/step)\n",
            "I1229 11:33:09.954234 140034317584256 learning.py:507] global step 2198: loss = 1.9787 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2199: loss = 2.6801 (1.240 sec/step)\n",
            "I1229 11:33:11.195692 140034317584256 learning.py:507] global step 2199: loss = 2.6801 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2200: loss = 1.9225 (1.231 sec/step)\n",
            "I1229 11:33:12.428816 140034317584256 learning.py:507] global step 2200: loss = 1.9225 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2201: loss = 2.3001 (1.241 sec/step)\n",
            "I1229 11:33:13.671446 140034317584256 learning.py:507] global step 2201: loss = 2.3001 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2202: loss = 2.0966 (1.242 sec/step)\n",
            "I1229 11:33:14.915541 140034317584256 learning.py:507] global step 2202: loss = 2.0966 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2203: loss = 2.1923 (1.292 sec/step)\n",
            "I1229 11:33:16.209511 140034317584256 learning.py:507] global step 2203: loss = 2.1923 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 2204: loss = 2.0295 (1.223 sec/step)\n",
            "I1229 11:33:17.433727 140034317584256 learning.py:507] global step 2204: loss = 2.0295 (1.223 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2204.\n",
            "I1229 11:33:19.492830 140030580766464 supervisor.py:1050] Recording summary at step 2204.\n",
            "INFO:tensorflow:global step 2205: loss = 2.0653 (2.204 sec/step)\n",
            "I1229 11:33:19.639757 140034317584256 learning.py:507] global step 2205: loss = 2.0653 (2.204 sec/step)\n",
            "INFO:tensorflow:global step 2206: loss = 2.4483 (1.272 sec/step)\n",
            "I1229 11:33:20.913477 140034317584256 learning.py:507] global step 2206: loss = 2.4483 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2207: loss = 1.6693 (1.261 sec/step)\n",
            "I1229 11:33:22.176741 140034317584256 learning.py:507] global step 2207: loss = 1.6693 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2208: loss = 2.3576 (1.261 sec/step)\n",
            "I1229 11:33:23.439637 140034317584256 learning.py:507] global step 2208: loss = 2.3576 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2209: loss = 2.9164 (1.247 sec/step)\n",
            "I1229 11:33:24.688341 140034317584256 learning.py:507] global step 2209: loss = 2.9164 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2210: loss = 1.4341 (1.250 sec/step)\n",
            "I1229 11:33:25.939804 140034317584256 learning.py:507] global step 2210: loss = 1.4341 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2211: loss = 1.9652 (1.258 sec/step)\n",
            "I1229 11:33:27.199632 140034317584256 learning.py:507] global step 2211: loss = 1.9652 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2212: loss = 1.8466 (1.258 sec/step)\n",
            "I1229 11:33:28.459834 140034317584256 learning.py:507] global step 2212: loss = 1.8466 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2213: loss = 2.5263 (1.233 sec/step)\n",
            "I1229 11:33:29.694585 140034317584256 learning.py:507] global step 2213: loss = 2.5263 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2214: loss = 1.7187 (1.246 sec/step)\n",
            "I1229 11:33:30.942528 140034317584256 learning.py:507] global step 2214: loss = 1.7187 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2215: loss = 2.8948 (1.227 sec/step)\n",
            "I1229 11:33:32.171441 140034317584256 learning.py:507] global step 2215: loss = 2.8948 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2216: loss = 2.0886 (1.260 sec/step)\n",
            "I1229 11:33:33.433600 140034317584256 learning.py:507] global step 2216: loss = 2.0886 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2217: loss = 1.7227 (1.275 sec/step)\n",
            "I1229 11:33:34.711375 140034317584256 learning.py:507] global step 2217: loss = 1.7227 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 2218: loss = 2.8632 (1.254 sec/step)\n",
            "I1229 11:33:35.967415 140034317584256 learning.py:507] global step 2218: loss = 2.8632 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2219: loss = 1.4794 (1.231 sec/step)\n",
            "I1229 11:33:37.200069 140034317584256 learning.py:507] global step 2219: loss = 1.4794 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2220: loss = 1.8214 (1.201 sec/step)\n",
            "I1229 11:33:38.403288 140034317584256 learning.py:507] global step 2220: loss = 1.8214 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2221: loss = 3.1503 (1.251 sec/step)\n",
            "I1229 11:33:39.655577 140034317584256 learning.py:507] global step 2221: loss = 3.1503 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2222: loss = 2.0144 (1.299 sec/step)\n",
            "I1229 11:33:40.956550 140034317584256 learning.py:507] global step 2222: loss = 2.0144 (1.299 sec/step)\n",
            "INFO:tensorflow:global step 2223: loss = 1.7496 (1.261 sec/step)\n",
            "I1229 11:33:42.219007 140034317584256 learning.py:507] global step 2223: loss = 1.7496 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2224: loss = 1.9037 (1.252 sec/step)\n",
            "I1229 11:33:43.472386 140034317584256 learning.py:507] global step 2224: loss = 1.9037 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2225: loss = 2.3186 (1.207 sec/step)\n",
            "I1229 11:33:44.680428 140034317584256 learning.py:507] global step 2225: loss = 2.3186 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2226: loss = 2.5644 (1.188 sec/step)\n",
            "I1229 11:33:45.870133 140034317584256 learning.py:507] global step 2226: loss = 2.5644 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 2227: loss = 1.8606 (1.250 sec/step)\n",
            "I1229 11:33:47.121927 140034317584256 learning.py:507] global step 2227: loss = 1.8606 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2228: loss = 2.4263 (1.270 sec/step)\n",
            "I1229 11:33:48.393459 140034317584256 learning.py:507] global step 2228: loss = 2.4263 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 2229: loss = 2.3818 (1.218 sec/step)\n",
            "I1229 11:33:49.613150 140034317584256 learning.py:507] global step 2229: loss = 2.3818 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2230: loss = 2.7833 (1.188 sec/step)\n",
            "I1229 11:33:50.802716 140034317584256 learning.py:507] global step 2230: loss = 2.7833 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 2231: loss = 2.8086 (1.233 sec/step)\n",
            "I1229 11:33:52.037624 140034317584256 learning.py:507] global step 2231: loss = 2.8086 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2232: loss = 1.7549 (1.241 sec/step)\n",
            "I1229 11:33:53.280891 140034317584256 learning.py:507] global step 2232: loss = 1.7549 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2233: loss = 2.3811 (1.248 sec/step)\n",
            "I1229 11:33:54.530344 140034317584256 learning.py:507] global step 2233: loss = 2.3811 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2234: loss = 1.6137 (1.265 sec/step)\n",
            "I1229 11:33:55.797367 140034317584256 learning.py:507] global step 2234: loss = 1.6137 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 2235: loss = 2.9296 (1.219 sec/step)\n",
            "I1229 11:33:57.017958 140034317584256 learning.py:507] global step 2235: loss = 2.9296 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2236: loss = 2.5492 (1.242 sec/step)\n",
            "I1229 11:33:58.262065 140034317584256 learning.py:507] global step 2236: loss = 2.5492 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2237: loss = 3.7860 (1.219 sec/step)\n",
            "I1229 11:33:59.482910 140034317584256 learning.py:507] global step 2237: loss = 3.7860 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2238: loss = 2.7787 (1.252 sec/step)\n",
            "I1229 11:34:00.736810 140034317584256 learning.py:507] global step 2238: loss = 2.7787 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2239: loss = 2.0671 (1.219 sec/step)\n",
            "I1229 11:34:01.957504 140034317584256 learning.py:507] global step 2239: loss = 2.0671 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2240: loss = 1.8837 (1.204 sec/step)\n",
            "I1229 11:34:03.163057 140034317584256 learning.py:507] global step 2240: loss = 1.8837 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 2241: loss = 2.5642 (1.243 sec/step)\n",
            "I1229 11:34:04.407540 140034317584256 learning.py:507] global step 2241: loss = 2.5642 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2242: loss = 3.0395 (1.224 sec/step)\n",
            "I1229 11:34:05.632943 140034317584256 learning.py:507] global step 2242: loss = 3.0395 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2243: loss = 2.3152 (1.201 sec/step)\n",
            "I1229 11:34:06.835451 140034317584256 learning.py:507] global step 2243: loss = 2.3152 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2244: loss = 2.3405 (1.197 sec/step)\n",
            "I1229 11:34:08.034670 140034317584256 learning.py:507] global step 2244: loss = 2.3405 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 2245: loss = 2.7002 (1.221 sec/step)\n",
            "I1229 11:34:09.257405 140034317584256 learning.py:507] global step 2245: loss = 2.7002 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2246: loss = 1.7096 (1.196 sec/step)\n",
            "I1229 11:34:10.455384 140034317584256 learning.py:507] global step 2246: loss = 1.7096 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2247: loss = 2.8070 (1.204 sec/step)\n",
            "I1229 11:34:11.661236 140034317584256 learning.py:507] global step 2247: loss = 2.8070 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 2248: loss = 2.2528 (1.196 sec/step)\n",
            "I1229 11:34:12.859332 140034317584256 learning.py:507] global step 2248: loss = 2.2528 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2249: loss = 2.3301 (1.282 sec/step)\n",
            "I1229 11:34:14.143185 140034317584256 learning.py:507] global step 2249: loss = 2.3301 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 2250: loss = 3.1088 (1.229 sec/step)\n",
            "I1229 11:34:15.374058 140034317584256 learning.py:507] global step 2250: loss = 3.1088 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2251: loss = 2.5707 (1.265 sec/step)\n",
            "I1229 11:34:16.641102 140034317584256 learning.py:507] global step 2251: loss = 2.5707 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 2252: loss = 2.8696 (1.197 sec/step)\n",
            "I1229 11:34:17.839853 140034317584256 learning.py:507] global step 2252: loss = 2.8696 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 2253: loss = 2.6410 (1.243 sec/step)\n",
            "I1229 11:34:19.084398 140034317584256 learning.py:507] global step 2253: loss = 2.6410 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2254: loss = 1.8059 (1.215 sec/step)\n",
            "I1229 11:34:20.301089 140034317584256 learning.py:507] global step 2254: loss = 1.8059 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2255: loss = 2.2782 (1.244 sec/step)\n",
            "I1229 11:34:21.546903 140034317584256 learning.py:507] global step 2255: loss = 2.2782 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2256: loss = 2.7410 (1.247 sec/step)\n",
            "I1229 11:34:22.795744 140034317584256 learning.py:507] global step 2256: loss = 2.7410 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2257: loss = 1.5791 (1.250 sec/step)\n",
            "I1229 11:34:24.047181 140034317584256 learning.py:507] global step 2257: loss = 1.5791 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2258: loss = 2.4981 (1.264 sec/step)\n",
            "I1229 11:34:25.312739 140034317584256 learning.py:507] global step 2258: loss = 2.4981 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 2259: loss = 3.5351 (1.258 sec/step)\n",
            "I1229 11:34:26.571992 140034317584256 learning.py:507] global step 2259: loss = 3.5351 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2260: loss = 1.8908 (1.236 sec/step)\n",
            "I1229 11:34:27.809596 140034317584256 learning.py:507] global step 2260: loss = 1.8908 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2261: loss = 2.0426 (1.233 sec/step)\n",
            "I1229 11:34:29.044855 140034317584256 learning.py:507] global step 2261: loss = 2.0426 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2262: loss = 1.8193 (1.267 sec/step)\n",
            "I1229 11:34:30.314119 140034317584256 learning.py:507] global step 2262: loss = 1.8193 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2263: loss = 2.7307 (1.263 sec/step)\n",
            "I1229 11:34:31.578926 140034317584256 learning.py:507] global step 2263: loss = 2.7307 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2264: loss = 2.3399 (1.260 sec/step)\n",
            "I1229 11:34:32.840845 140034317584256 learning.py:507] global step 2264: loss = 2.3399 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2265: loss = 2.8809 (1.223 sec/step)\n",
            "I1229 11:34:34.065656 140034317584256 learning.py:507] global step 2265: loss = 2.8809 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2266: loss = 2.8061 (1.280 sec/step)\n",
            "I1229 11:34:35.347447 140034317584256 learning.py:507] global step 2266: loss = 2.8061 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2267: loss = 2.0791 (1.219 sec/step)\n",
            "I1229 11:34:36.567684 140034317584256 learning.py:507] global step 2267: loss = 2.0791 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2268: loss = 4.2328 (1.233 sec/step)\n",
            "I1229 11:34:37.802737 140034317584256 learning.py:507] global step 2268: loss = 4.2328 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2269: loss = 2.7353 (1.245 sec/step)\n",
            "I1229 11:34:39.049071 140034317584256 learning.py:507] global step 2269: loss = 2.7353 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2270: loss = 3.4982 (1.246 sec/step)\n",
            "I1229 11:34:40.297126 140034317584256 learning.py:507] global step 2270: loss = 3.4982 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2271: loss = 1.7345 (1.209 sec/step)\n",
            "I1229 11:34:41.508139 140034317584256 learning.py:507] global step 2271: loss = 1.7345 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 2272: loss = 3.3406 (1.200 sec/step)\n",
            "I1229 11:34:42.710391 140034317584256 learning.py:507] global step 2272: loss = 3.3406 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2273: loss = 2.0597 (1.233 sec/step)\n",
            "I1229 11:34:43.945996 140034317584256 learning.py:507] global step 2273: loss = 2.0597 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2274: loss = 2.4477 (1.189 sec/step)\n",
            "I1229 11:34:45.137304 140034317584256 learning.py:507] global step 2274: loss = 2.4477 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 2275: loss = 1.9727 (1.238 sec/step)\n",
            "I1229 11:34:46.376978 140034317584256 learning.py:507] global step 2275: loss = 1.9727 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2276: loss = 2.8186 (1.244 sec/step)\n",
            "I1229 11:34:47.622383 140034317584256 learning.py:507] global step 2276: loss = 2.8186 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2277: loss = 2.9546 (1.227 sec/step)\n",
            "I1229 11:34:48.851587 140034317584256 learning.py:507] global step 2277: loss = 2.9546 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2278: loss = 2.9835 (1.212 sec/step)\n",
            "I1229 11:34:50.065659 140034317584256 learning.py:507] global step 2278: loss = 2.9835 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2279: loss = 2.0710 (1.224 sec/step)\n",
            "I1229 11:34:51.291496 140034317584256 learning.py:507] global step 2279: loss = 2.0710 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2280: loss = 2.3936 (1.251 sec/step)\n",
            "I1229 11:34:52.544652 140034317584256 learning.py:507] global step 2280: loss = 2.3936 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2281: loss = 1.4022 (1.239 sec/step)\n",
            "I1229 11:34:53.785049 140034317584256 learning.py:507] global step 2281: loss = 1.4022 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2282: loss = 2.3401 (1.231 sec/step)\n",
            "I1229 11:34:55.017598 140034317584256 learning.py:507] global step 2282: loss = 2.3401 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2283: loss = 2.1011 (1.229 sec/step)\n",
            "I1229 11:34:56.248357 140034317584256 learning.py:507] global step 2283: loss = 2.1011 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2284: loss = 2.4813 (1.280 sec/step)\n",
            "I1229 11:34:57.530855 140034317584256 learning.py:507] global step 2284: loss = 2.4813 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2285: loss = 1.8018 (1.263 sec/step)\n",
            "I1229 11:34:58.795996 140034317584256 learning.py:507] global step 2285: loss = 1.8018 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2286: loss = 2.3280 (1.230 sec/step)\n",
            "I1229 11:35:00.027419 140034317584256 learning.py:507] global step 2286: loss = 2.3280 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 2287: loss = 2.3848 (1.239 sec/step)\n",
            "I1229 11:35:01.268445 140034317584256 learning.py:507] global step 2287: loss = 2.3848 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2288: loss = 3.1428 (1.274 sec/step)\n",
            "I1229 11:35:02.544842 140034317584256 learning.py:507] global step 2288: loss = 3.1428 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2289: loss = 2.0987 (1.221 sec/step)\n",
            "I1229 11:35:03.768079 140034317584256 learning.py:507] global step 2289: loss = 2.0987 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2290: loss = 1.9226 (1.255 sec/step)\n",
            "I1229 11:35:05.024482 140034317584256 learning.py:507] global step 2290: loss = 1.9226 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2291: loss = 1.8992 (1.274 sec/step)\n",
            "I1229 11:35:06.300426 140034317584256 learning.py:507] global step 2291: loss = 1.8992 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2292: loss = 2.9041 (1.255 sec/step)\n",
            "I1229 11:35:07.557156 140034317584256 learning.py:507] global step 2292: loss = 2.9041 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2293: loss = 2.8615 (1.276 sec/step)\n",
            "I1229 11:35:08.835325 140034317584256 learning.py:507] global step 2293: loss = 2.8615 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 2294: loss = 1.8515 (1.191 sec/step)\n",
            "I1229 11:35:10.027956 140034317584256 learning.py:507] global step 2294: loss = 1.8515 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 2295: loss = 3.5147 (1.206 sec/step)\n",
            "I1229 11:35:11.235568 140034317584256 learning.py:507] global step 2295: loss = 3.5147 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2296: loss = 1.4156 (1.280 sec/step)\n",
            "I1229 11:35:12.517374 140034317584256 learning.py:507] global step 2296: loss = 1.4156 (1.280 sec/step)\n",
            "INFO:tensorflow:global step 2297: loss = 1.7125 (1.213 sec/step)\n",
            "I1229 11:35:13.732285 140034317584256 learning.py:507] global step 2297: loss = 1.7125 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 2298: loss = 1.9185 (1.226 sec/step)\n",
            "I1229 11:35:14.960180 140034317584256 learning.py:507] global step 2298: loss = 1.9185 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2299: loss = 1.7857 (1.269 sec/step)\n",
            "I1229 11:35:16.231307 140034317584256 learning.py:507] global step 2299: loss = 1.7857 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 2300: loss = 2.7178 (1.227 sec/step)\n",
            "I1229 11:35:17.460574 140034317584256 learning.py:507] global step 2300: loss = 2.7178 (1.227 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2300.\n",
            "I1229 11:35:19.508906 140030580766464 supervisor.py:1050] Recording summary at step 2300.\n",
            "INFO:tensorflow:global step 2301: loss = 2.4359 (2.225 sec/step)\n",
            "I1229 11:35:19.687309 140034317584256 learning.py:507] global step 2301: loss = 2.4359 (2.225 sec/step)\n",
            "INFO:tensorflow:global step 2302: loss = 3.1709 (1.206 sec/step)\n",
            "I1229 11:35:20.895649 140034317584256 learning.py:507] global step 2302: loss = 3.1709 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2303: loss = 2.6513 (1.209 sec/step)\n",
            "I1229 11:35:22.106539 140034317584256 learning.py:507] global step 2303: loss = 2.6513 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 2304: loss = 3.5605 (1.267 sec/step)\n",
            "I1229 11:35:23.375332 140034317584256 learning.py:507] global step 2304: loss = 3.5605 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2305: loss = 2.0602 (1.242 sec/step)\n",
            "I1229 11:35:24.618676 140034317584256 learning.py:507] global step 2305: loss = 2.0602 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2306: loss = 2.9238 (1.249 sec/step)\n",
            "I1229 11:35:25.869646 140034317584256 learning.py:507] global step 2306: loss = 2.9238 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2307: loss = 2.2923 (1.201 sec/step)\n",
            "I1229 11:35:27.072308 140034317584256 learning.py:507] global step 2307: loss = 2.2923 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2308: loss = 2.5461 (1.270 sec/step)\n",
            "I1229 11:35:28.344904 140034317584256 learning.py:507] global step 2308: loss = 2.5461 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 2309: loss = 2.4601 (1.217 sec/step)\n",
            "I1229 11:35:29.563484 140034317584256 learning.py:507] global step 2309: loss = 2.4601 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 2310: loss = 2.7576 (1.238 sec/step)\n",
            "I1229 11:35:30.803050 140034317584256 learning.py:507] global step 2310: loss = 2.7576 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2311: loss = 3.0615 (1.255 sec/step)\n",
            "I1229 11:35:32.060255 140034317584256 learning.py:507] global step 2311: loss = 3.0615 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2312: loss = 3.2275 (1.248 sec/step)\n",
            "I1229 11:35:33.310133 140034317584256 learning.py:507] global step 2312: loss = 3.2275 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2313: loss = 2.6673 (1.203 sec/step)\n",
            "I1229 11:35:34.515191 140034317584256 learning.py:507] global step 2313: loss = 2.6673 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2314: loss = 1.8032 (1.222 sec/step)\n",
            "I1229 11:35:35.739241 140034317584256 learning.py:507] global step 2314: loss = 1.8032 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2315: loss = 2.7347 (1.219 sec/step)\n",
            "I1229 11:35:36.960297 140034317584256 learning.py:507] global step 2315: loss = 2.7347 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2316: loss = 2.1562 (1.271 sec/step)\n",
            "I1229 11:35:38.232777 140034317584256 learning.py:507] global step 2316: loss = 2.1562 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2317: loss = 2.0019 (1.239 sec/step)\n",
            "I1229 11:35:39.473885 140034317584256 learning.py:507] global step 2317: loss = 2.0019 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2318: loss = 1.4763 (1.226 sec/step)\n",
            "I1229 11:35:40.702020 140034317584256 learning.py:507] global step 2318: loss = 1.4763 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2319: loss = 1.3474 (1.258 sec/step)\n",
            "I1229 11:35:41.961500 140034317584256 learning.py:507] global step 2319: loss = 1.3474 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2320: loss = 2.1937 (1.249 sec/step)\n",
            "I1229 11:35:43.211673 140034317584256 learning.py:507] global step 2320: loss = 2.1937 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2321: loss = 2.6581 (1.270 sec/step)\n",
            "I1229 11:35:44.483658 140034317584256 learning.py:507] global step 2321: loss = 2.6581 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 2322: loss = 1.8471 (1.272 sec/step)\n",
            "I1229 11:35:45.757878 140034317584256 learning.py:507] global step 2322: loss = 1.8471 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2323: loss = 1.9576 (1.272 sec/step)\n",
            "I1229 11:35:47.031266 140034317584256 learning.py:507] global step 2323: loss = 1.9576 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2324: loss = 2.2239 (1.233 sec/step)\n",
            "I1229 11:35:48.265293 140034317584256 learning.py:507] global step 2324: loss = 2.2239 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2325: loss = 1.6444 (1.253 sec/step)\n",
            "I1229 11:35:49.520224 140034317584256 learning.py:507] global step 2325: loss = 1.6444 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 2326: loss = 1.5696 (1.240 sec/step)\n",
            "I1229 11:35:50.762514 140034317584256 learning.py:507] global step 2326: loss = 1.5696 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2327: loss = 2.9341 (1.249 sec/step)\n",
            "I1229 11:35:52.013239 140034317584256 learning.py:507] global step 2327: loss = 2.9341 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2328: loss = 2.1166 (1.224 sec/step)\n",
            "I1229 11:35:53.239230 140034317584256 learning.py:507] global step 2328: loss = 2.1166 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2329: loss = 1.8384 (1.260 sec/step)\n",
            "I1229 11:35:54.500513 140034317584256 learning.py:507] global step 2329: loss = 1.8384 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2330: loss = 2.0457 (1.201 sec/step)\n",
            "I1229 11:35:55.703018 140034317584256 learning.py:507] global step 2330: loss = 2.0457 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2331: loss = 2.6520 (1.239 sec/step)\n",
            "I1229 11:35:56.943613 140034317584256 learning.py:507] global step 2331: loss = 2.6520 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2332: loss = 3.1222 (1.193 sec/step)\n",
            "I1229 11:35:58.138654 140034317584256 learning.py:507] global step 2332: loss = 3.1222 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 2333: loss = 2.2633 (1.257 sec/step)\n",
            "I1229 11:35:59.397332 140034317584256 learning.py:507] global step 2333: loss = 2.2633 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2334: loss = 2.1313 (1.257 sec/step)\n",
            "I1229 11:36:00.656178 140034317584256 learning.py:507] global step 2334: loss = 2.1313 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2335: loss = 1.7849 (1.258 sec/step)\n",
            "I1229 11:36:01.916571 140034317584256 learning.py:507] global step 2335: loss = 1.7849 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2336: loss = 2.8152 (1.246 sec/step)\n",
            "I1229 11:36:03.164091 140034317584256 learning.py:507] global step 2336: loss = 2.8152 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2337: loss = 2.0839 (1.265 sec/step)\n",
            "I1229 11:36:04.430895 140034317584256 learning.py:507] global step 2337: loss = 2.0839 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 2338: loss = 2.5496 (1.257 sec/step)\n",
            "I1229 11:36:05.689677 140034317584256 learning.py:507] global step 2338: loss = 2.5496 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2339: loss = 2.9740 (1.228 sec/step)\n",
            "I1229 11:36:06.919011 140034317584256 learning.py:507] global step 2339: loss = 2.9740 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2340: loss = 2.8028 (1.241 sec/step)\n",
            "I1229 11:36:08.161967 140034317584256 learning.py:507] global step 2340: loss = 2.8028 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2341: loss = 2.4498 (1.210 sec/step)\n",
            "I1229 11:36:09.373604 140034317584256 learning.py:507] global step 2341: loss = 2.4498 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2342: loss = 2.9772 (1.265 sec/step)\n",
            "I1229 11:36:10.640511 140034317584256 learning.py:507] global step 2342: loss = 2.9772 (1.265 sec/step)\n",
            "INFO:tensorflow:global step 2343: loss = 2.4330 (1.266 sec/step)\n",
            "I1229 11:36:11.908540 140034317584256 learning.py:507] global step 2343: loss = 2.4330 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 2344: loss = 2.8388 (1.198 sec/step)\n",
            "I1229 11:36:13.108470 140034317584256 learning.py:507] global step 2344: loss = 2.8388 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2345: loss = 1.9383 (1.257 sec/step)\n",
            "I1229 11:36:14.367764 140034317584256 learning.py:507] global step 2345: loss = 1.9383 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2346: loss = 2.0398 (1.237 sec/step)\n",
            "I1229 11:36:15.606602 140034317584256 learning.py:507] global step 2346: loss = 2.0398 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2347: loss = 2.7307 (1.195 sec/step)\n",
            "I1229 11:36:16.804038 140034317584256 learning.py:507] global step 2347: loss = 2.7307 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2348: loss = 2.5386 (1.216 sec/step)\n",
            "I1229 11:36:18.022253 140034317584256 learning.py:507] global step 2348: loss = 2.5386 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2349: loss = 2.7333 (1.208 sec/step)\n",
            "I1229 11:36:19.232487 140034317584256 learning.py:507] global step 2349: loss = 2.7333 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2350: loss = 2.4893 (1.228 sec/step)\n",
            "I1229 11:36:20.462464 140034317584256 learning.py:507] global step 2350: loss = 2.4893 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2351: loss = 2.0549 (1.251 sec/step)\n",
            "I1229 11:36:21.715347 140034317584256 learning.py:507] global step 2351: loss = 2.0549 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2352: loss = 4.7230 (1.236 sec/step)\n",
            "I1229 11:36:22.953106 140034317584256 learning.py:507] global step 2352: loss = 4.7230 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2353: loss = 2.5787 (1.212 sec/step)\n",
            "I1229 11:36:24.166378 140034317584256 learning.py:507] global step 2353: loss = 2.5787 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2354: loss = 2.9357 (1.194 sec/step)\n",
            "I1229 11:36:25.362203 140034317584256 learning.py:507] global step 2354: loss = 2.9357 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 2355: loss = 3.5754 (1.215 sec/step)\n",
            "I1229 11:36:26.579141 140034317584256 learning.py:507] global step 2355: loss = 3.5754 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2356: loss = 2.4553 (1.239 sec/step)\n",
            "I1229 11:36:27.819477 140034317584256 learning.py:507] global step 2356: loss = 2.4553 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2357: loss = 2.3459 (1.216 sec/step)\n",
            "I1229 11:36:29.037953 140034317584256 learning.py:507] global step 2357: loss = 2.3459 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2358: loss = 2.5136 (1.256 sec/step)\n",
            "I1229 11:36:30.295836 140034317584256 learning.py:507] global step 2358: loss = 2.5136 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 2359: loss = 2.4825 (1.239 sec/step)\n",
            "I1229 11:36:31.536498 140034317584256 learning.py:507] global step 2359: loss = 2.4825 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2360: loss = 3.1541 (1.228 sec/step)\n",
            "I1229 11:36:32.766231 140034317584256 learning.py:507] global step 2360: loss = 3.1541 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2361: loss = 2.5502 (1.255 sec/step)\n",
            "I1229 11:36:34.023122 140034317584256 learning.py:507] global step 2361: loss = 2.5502 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2362: loss = 2.8270 (1.228 sec/step)\n",
            "I1229 11:36:35.253014 140034317584256 learning.py:507] global step 2362: loss = 2.8270 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2363: loss = 2.3253 (1.281 sec/step)\n",
            "I1229 11:36:36.536140 140034317584256 learning.py:507] global step 2363: loss = 2.3253 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 2364: loss = 1.8372 (1.233 sec/step)\n",
            "I1229 11:36:37.771347 140034317584256 learning.py:507] global step 2364: loss = 1.8372 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2365: loss = 3.2808 (1.182 sec/step)\n",
            "I1229 11:36:38.955752 140034317584256 learning.py:507] global step 2365: loss = 3.2808 (1.182 sec/step)\n",
            "INFO:tensorflow:global step 2366: loss = 2.2597 (1.218 sec/step)\n",
            "I1229 11:36:40.175011 140034317584256 learning.py:507] global step 2366: loss = 2.2597 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2367: loss = 1.7003 (1.219 sec/step)\n",
            "I1229 11:36:41.395556 140034317584256 learning.py:507] global step 2367: loss = 1.7003 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2368: loss = 1.0172 (1.254 sec/step)\n",
            "I1229 11:36:42.650955 140034317584256 learning.py:507] global step 2368: loss = 1.0172 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2369: loss = 2.6266 (1.205 sec/step)\n",
            "I1229 11:36:43.857931 140034317584256 learning.py:507] global step 2369: loss = 2.6266 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 2370: loss = 2.1370 (1.237 sec/step)\n",
            "I1229 11:36:45.098864 140034317584256 learning.py:507] global step 2370: loss = 2.1370 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2371: loss = 2.7573 (1.215 sec/step)\n",
            "I1229 11:36:46.316444 140034317584256 learning.py:507] global step 2371: loss = 2.7573 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2372: loss = 2.6837 (1.200 sec/step)\n",
            "I1229 11:36:47.519123 140034317584256 learning.py:507] global step 2372: loss = 2.6837 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2373: loss = 3.1640 (1.226 sec/step)\n",
            "I1229 11:36:48.746687 140034317584256 learning.py:507] global step 2373: loss = 3.1640 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2374: loss = 2.5606 (1.192 sec/step)\n",
            "I1229 11:36:49.940447 140034317584256 learning.py:507] global step 2374: loss = 2.5606 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 2375: loss = 4.6715 (1.256 sec/step)\n",
            "I1229 11:36:51.198046 140034317584256 learning.py:507] global step 2375: loss = 4.6715 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 2376: loss = 1.9642 (1.264 sec/step)\n",
            "I1229 11:36:52.463707 140034317584256 learning.py:507] global step 2376: loss = 1.9642 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 2377: loss = 2.9772 (1.230 sec/step)\n",
            "I1229 11:36:53.695530 140034317584256 learning.py:507] global step 2377: loss = 2.9772 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 2378: loss = 2.4689 (1.248 sec/step)\n",
            "I1229 11:36:54.944887 140034317584256 learning.py:507] global step 2378: loss = 2.4689 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2379: loss = 2.3364 (1.236 sec/step)\n",
            "I1229 11:36:56.182307 140034317584256 learning.py:507] global step 2379: loss = 2.3364 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2380: loss = 3.5868 (1.212 sec/step)\n",
            "I1229 11:36:57.396381 140034317584256 learning.py:507] global step 2380: loss = 3.5868 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2381: loss = 2.1431 (1.248 sec/step)\n",
            "I1229 11:36:58.646546 140034317584256 learning.py:507] global step 2381: loss = 2.1431 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2382: loss = 2.2067 (1.258 sec/step)\n",
            "I1229 11:36:59.906605 140034317584256 learning.py:507] global step 2382: loss = 2.2067 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2383: loss = 2.0219 (1.212 sec/step)\n",
            "I1229 11:37:01.120299 140034317584256 learning.py:507] global step 2383: loss = 2.0219 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2384: loss = 2.0450 (1.274 sec/step)\n",
            "I1229 11:37:02.396175 140034317584256 learning.py:507] global step 2384: loss = 2.0450 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2385: loss = 3.0679 (1.302 sec/step)\n",
            "I1229 11:37:03.699978 140034317584256 learning.py:507] global step 2385: loss = 3.0679 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 2386: loss = 2.3736 (1.242 sec/step)\n",
            "I1229 11:37:04.944322 140034317584256 learning.py:507] global step 2386: loss = 2.3736 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2387: loss = 2.6879 (1.250 sec/step)\n",
            "I1229 11:37:06.195981 140034317584256 learning.py:507] global step 2387: loss = 2.6879 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2388: loss = 2.9219 (1.272 sec/step)\n",
            "I1229 11:37:07.469870 140034317584256 learning.py:507] global step 2388: loss = 2.9219 (1.272 sec/step)\n",
            "INFO:tensorflow:global step 2389: loss = 2.0396 (1.216 sec/step)\n",
            "I1229 11:37:08.688195 140034317584256 learning.py:507] global step 2389: loss = 2.0396 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2390: loss = 1.8933 (1.224 sec/step)\n",
            "I1229 11:37:09.914048 140034317584256 learning.py:507] global step 2390: loss = 1.8933 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2391: loss = 1.7540 (1.229 sec/step)\n",
            "I1229 11:37:11.144293 140034317584256 learning.py:507] global step 2391: loss = 1.7540 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2392: loss = 1.5187 (1.266 sec/step)\n",
            "I1229 11:37:12.411807 140034317584256 learning.py:507] global step 2392: loss = 1.5187 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 2393: loss = 2.3164 (1.234 sec/step)\n",
            "I1229 11:37:13.647600 140034317584256 learning.py:507] global step 2393: loss = 2.3164 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2394: loss = 2.6747 (1.262 sec/step)\n",
            "I1229 11:37:14.911248 140034317584256 learning.py:507] global step 2394: loss = 2.6747 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2395: loss = 2.5973 (1.228 sec/step)\n",
            "I1229 11:37:16.140743 140034317584256 learning.py:507] global step 2395: loss = 2.5973 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2396: loss = 2.7821 (1.257 sec/step)\n",
            "I1229 11:37:17.399469 140034317584256 learning.py:507] global step 2396: loss = 2.7821 (1.257 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1229 11:37:17.668316 140030547195648 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W1229 11:37:17.915992 140030547195648 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Recording summary at step 2396.\n",
            "I1229 11:37:20.027394 140030580766464 supervisor.py:1050] Recording summary at step 2396.\n",
            "INFO:tensorflow:global step 2397: loss = 2.9216 (2.824 sec/step)\n",
            "I1229 11:37:20.230528 140034317584256 learning.py:507] global step 2397: loss = 2.9216 (2.824 sec/step)\n",
            "INFO:tensorflow:global step 2398: loss = 2.5508 (1.766 sec/step)\n",
            "I1229 11:37:22.009164 140034317584256 learning.py:507] global step 2398: loss = 2.5508 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 2399: loss = 2.4266 (2.436 sec/step)\n",
            "I1229 11:37:24.457717 140034317584256 learning.py:507] global step 2399: loss = 2.4266 (2.436 sec/step)\n",
            "INFO:tensorflow:global step 2400: loss = 7.4097 (1.611 sec/step)\n",
            "I1229 11:37:26.071325 140034317584256 learning.py:507] global step 2400: loss = 7.4097 (1.611 sec/step)\n",
            "INFO:tensorflow:global step 2401: loss = 2.1820 (1.301 sec/step)\n",
            "I1229 11:37:27.374287 140034317584256 learning.py:507] global step 2401: loss = 2.1820 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 2402: loss = 2.3019 (1.257 sec/step)\n",
            "I1229 11:37:28.632707 140034317584256 learning.py:507] global step 2402: loss = 2.3019 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2403: loss = 1.9829 (1.218 sec/step)\n",
            "I1229 11:37:29.852703 140034317584256 learning.py:507] global step 2403: loss = 1.9829 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2404: loss = 2.1203 (1.252 sec/step)\n",
            "I1229 11:37:31.106645 140034317584256 learning.py:507] global step 2404: loss = 2.1203 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2405: loss = 1.5044 (1.277 sec/step)\n",
            "I1229 11:37:32.385692 140034317584256 learning.py:507] global step 2405: loss = 1.5044 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 2406: loss = 2.2296 (1.284 sec/step)\n",
            "I1229 11:37:33.671784 140034317584256 learning.py:507] global step 2406: loss = 2.2296 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 2407: loss = 3.1759 (1.287 sec/step)\n",
            "I1229 11:37:34.961082 140034317584256 learning.py:507] global step 2407: loss = 3.1759 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 2408: loss = 3.9049 (1.250 sec/step)\n",
            "I1229 11:37:36.213468 140034317584256 learning.py:507] global step 2408: loss = 3.9049 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2409: loss = 1.9927 (1.226 sec/step)\n",
            "I1229 11:37:37.440878 140034317584256 learning.py:507] global step 2409: loss = 1.9927 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2410: loss = 1.4857 (1.250 sec/step)\n",
            "I1229 11:37:38.692359 140034317584256 learning.py:507] global step 2410: loss = 1.4857 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2411: loss = 2.0976 (1.237 sec/step)\n",
            "I1229 11:37:39.931290 140034317584256 learning.py:507] global step 2411: loss = 2.0976 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2412: loss = 2.2077 (1.273 sec/step)\n",
            "I1229 11:37:41.206475 140034317584256 learning.py:507] global step 2412: loss = 2.2077 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 2413: loss = 3.0669 (1.195 sec/step)\n",
            "I1229 11:37:42.403144 140034317584256 learning.py:507] global step 2413: loss = 3.0669 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2414: loss = 2.9151 (1.244 sec/step)\n",
            "I1229 11:37:43.649390 140034317584256 learning.py:507] global step 2414: loss = 2.9151 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2415: loss = 1.8972 (1.208 sec/step)\n",
            "I1229 11:37:44.859523 140034317584256 learning.py:507] global step 2415: loss = 1.8972 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2416: loss = 2.4780 (1.219 sec/step)\n",
            "I1229 11:37:46.080975 140034317584256 learning.py:507] global step 2416: loss = 2.4780 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2417: loss = 2.4715 (1.243 sec/step)\n",
            "I1229 11:37:47.326050 140034317584256 learning.py:507] global step 2417: loss = 2.4715 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2418: loss = 2.4303 (1.261 sec/step)\n",
            "I1229 11:37:48.588803 140034317584256 learning.py:507] global step 2418: loss = 2.4303 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2419: loss = 2.1036 (1.201 sec/step)\n",
            "I1229 11:37:49.791357 140034317584256 learning.py:507] global step 2419: loss = 2.1036 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2420: loss = 3.5789 (1.222 sec/step)\n",
            "I1229 11:37:51.015377 140034317584256 learning.py:507] global step 2420: loss = 3.5789 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2421: loss = 3.6471 (1.262 sec/step)\n",
            "I1229 11:37:52.279555 140034317584256 learning.py:507] global step 2421: loss = 3.6471 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2422: loss = 2.7943 (1.261 sec/step)\n",
            "I1229 11:37:53.542513 140034317584256 learning.py:507] global step 2422: loss = 2.7943 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2423: loss = 2.4555 (1.251 sec/step)\n",
            "I1229 11:37:54.795299 140034317584256 learning.py:507] global step 2423: loss = 2.4555 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2424: loss = 2.3619 (1.235 sec/step)\n",
            "I1229 11:37:56.031947 140034317584256 learning.py:507] global step 2424: loss = 2.3619 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2425: loss = 2.0204 (1.255 sec/step)\n",
            "I1229 11:37:57.289025 140034317584256 learning.py:507] global step 2425: loss = 2.0204 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2426: loss = 2.4956 (1.240 sec/step)\n",
            "I1229 11:37:58.530619 140034317584256 learning.py:507] global step 2426: loss = 2.4956 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2427: loss = 2.4475 (1.222 sec/step)\n",
            "I1229 11:37:59.754059 140034317584256 learning.py:507] global step 2427: loss = 2.4475 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2428: loss = 1.9232 (1.213 sec/step)\n",
            "I1229 11:38:00.968909 140034317584256 learning.py:507] global step 2428: loss = 1.9232 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 2429: loss = 2.5264 (1.208 sec/step)\n",
            "I1229 11:38:02.178187 140034317584256 learning.py:507] global step 2429: loss = 2.5264 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2430: loss = 2.3581 (1.258 sec/step)\n",
            "I1229 11:38:03.437420 140034317584256 learning.py:507] global step 2430: loss = 2.3581 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2431: loss = 2.4785 (1.241 sec/step)\n",
            "I1229 11:38:04.679521 140034317584256 learning.py:507] global step 2431: loss = 2.4785 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2432: loss = 3.0114 (1.223 sec/step)\n",
            "I1229 11:38:05.903659 140034317584256 learning.py:507] global step 2432: loss = 3.0114 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2433: loss = 2.9416 (1.227 sec/step)\n",
            "I1229 11:38:07.132005 140034317584256 learning.py:507] global step 2433: loss = 2.9416 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2434: loss = 3.0751 (1.244 sec/step)\n",
            "I1229 11:38:08.377611 140034317584256 learning.py:507] global step 2434: loss = 3.0751 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2435: loss = 2.5341 (1.273 sec/step)\n",
            "I1229 11:38:09.652451 140034317584256 learning.py:507] global step 2435: loss = 2.5341 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 2436: loss = 2.3631 (1.248 sec/step)\n",
            "I1229 11:38:10.901936 140034317584256 learning.py:507] global step 2436: loss = 2.3631 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2437: loss = 3.3693 (1.180 sec/step)\n",
            "I1229 11:38:12.083168 140034317584256 learning.py:507] global step 2437: loss = 3.3693 (1.180 sec/step)\n",
            "INFO:tensorflow:global step 2438: loss = 1.6709 (1.207 sec/step)\n",
            "I1229 11:38:13.292377 140034317584256 learning.py:507] global step 2438: loss = 1.6709 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2439: loss = 2.5525 (1.223 sec/step)\n",
            "I1229 11:38:14.518690 140034317584256 learning.py:507] global step 2439: loss = 2.5525 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2440: loss = 1.6691 (1.222 sec/step)\n",
            "I1229 11:38:15.742759 140034317584256 learning.py:507] global step 2440: loss = 1.6691 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2441: loss = 2.4223 (1.218 sec/step)\n",
            "I1229 11:38:16.962343 140034317584256 learning.py:507] global step 2441: loss = 2.4223 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2442: loss = 2.0413 (1.225 sec/step)\n",
            "I1229 11:38:18.188867 140034317584256 learning.py:507] global step 2442: loss = 2.0413 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2443: loss = 1.8885 (1.203 sec/step)\n",
            "I1229 11:38:19.393525 140034317584256 learning.py:507] global step 2443: loss = 1.8885 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2444: loss = 2.5405 (1.211 sec/step)\n",
            "I1229 11:38:20.606039 140034317584256 learning.py:507] global step 2444: loss = 2.5405 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2445: loss = 1.6809 (1.252 sec/step)\n",
            "I1229 11:38:21.859553 140034317584256 learning.py:507] global step 2445: loss = 1.6809 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2446: loss = 2.6395 (1.276 sec/step)\n",
            "I1229 11:38:23.137878 140034317584256 learning.py:507] global step 2446: loss = 2.6395 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 2447: loss = 2.0482 (1.245 sec/step)\n",
            "I1229 11:38:24.384983 140034317584256 learning.py:507] global step 2447: loss = 2.0482 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2448: loss = 2.2475 (1.207 sec/step)\n",
            "I1229 11:38:25.593617 140034317584256 learning.py:507] global step 2448: loss = 2.2475 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2449: loss = 2.1326 (1.241 sec/step)\n",
            "I1229 11:38:26.836815 140034317584256 learning.py:507] global step 2449: loss = 2.1326 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2450: loss = 2.6561 (1.217 sec/step)\n",
            "I1229 11:38:28.055782 140034317584256 learning.py:507] global step 2450: loss = 2.6561 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 2451: loss = 2.0746 (1.212 sec/step)\n",
            "I1229 11:38:29.269740 140034317584256 learning.py:507] global step 2451: loss = 2.0746 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2452: loss = 2.3037 (1.237 sec/step)\n",
            "I1229 11:38:30.508269 140034317584256 learning.py:507] global step 2452: loss = 2.3037 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2453: loss = 1.8896 (1.210 sec/step)\n",
            "I1229 11:38:31.720171 140034317584256 learning.py:507] global step 2453: loss = 1.8896 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2454: loss = 2.0425 (1.240 sec/step)\n",
            "I1229 11:38:32.962084 140034317584256 learning.py:507] global step 2454: loss = 2.0425 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2455: loss = 2.9423 (1.237 sec/step)\n",
            "I1229 11:38:34.201145 140034317584256 learning.py:507] global step 2455: loss = 2.9423 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2456: loss = 2.6547 (1.239 sec/step)\n",
            "I1229 11:38:35.442160 140034317584256 learning.py:507] global step 2456: loss = 2.6547 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2457: loss = 3.0525 (1.227 sec/step)\n",
            "I1229 11:38:36.671405 140034317584256 learning.py:507] global step 2457: loss = 3.0525 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2458: loss = 2.1969 (1.213 sec/step)\n",
            "I1229 11:38:37.885653 140034317584256 learning.py:507] global step 2458: loss = 2.1969 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 2459: loss = 4.1428 (1.196 sec/step)\n",
            "I1229 11:38:39.083302 140034317584256 learning.py:507] global step 2459: loss = 4.1428 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2460: loss = 3.0180 (1.247 sec/step)\n",
            "I1229 11:38:40.331917 140034317584256 learning.py:507] global step 2460: loss = 3.0180 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2461: loss = 2.6799 (1.200 sec/step)\n",
            "I1229 11:38:41.534176 140034317584256 learning.py:507] global step 2461: loss = 2.6799 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2462: loss = 2.3298 (1.186 sec/step)\n",
            "I1229 11:38:42.722159 140034317584256 learning.py:507] global step 2462: loss = 2.3298 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2463: loss = 2.3920 (1.255 sec/step)\n",
            "I1229 11:38:43.979357 140034317584256 learning.py:507] global step 2463: loss = 2.3920 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2464: loss = 1.8269 (1.212 sec/step)\n",
            "I1229 11:38:45.193190 140034317584256 learning.py:507] global step 2464: loss = 1.8269 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2465: loss = 2.9221 (1.234 sec/step)\n",
            "I1229 11:38:46.429530 140034317584256 learning.py:507] global step 2465: loss = 2.9221 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2466: loss = 1.1597 (1.248 sec/step)\n",
            "I1229 11:38:47.679755 140034317584256 learning.py:507] global step 2466: loss = 1.1597 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2467: loss = 2.2373 (1.247 sec/step)\n",
            "I1229 11:38:48.928529 140034317584256 learning.py:507] global step 2467: loss = 2.2373 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2468: loss = 2.6487 (1.198 sec/step)\n",
            "I1229 11:38:50.128281 140034317584256 learning.py:507] global step 2468: loss = 2.6487 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2469: loss = 2.3073 (1.209 sec/step)\n",
            "I1229 11:38:51.338628 140034317584256 learning.py:507] global step 2469: loss = 2.3073 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 2470: loss = 3.9552 (1.207 sec/step)\n",
            "I1229 11:38:52.547689 140034317584256 learning.py:507] global step 2470: loss = 3.9552 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2471: loss = 2.9956 (1.241 sec/step)\n",
            "I1229 11:38:53.790447 140034317584256 learning.py:507] global step 2471: loss = 2.9956 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2472: loss = 2.3771 (1.222 sec/step)\n",
            "I1229 11:38:55.014282 140034317584256 learning.py:507] global step 2472: loss = 2.3771 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2473: loss = 1.1702 (1.212 sec/step)\n",
            "I1229 11:38:56.228055 140034317584256 learning.py:507] global step 2473: loss = 1.1702 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2474: loss = 3.8864 (1.229 sec/step)\n",
            "I1229 11:38:57.458883 140034317584256 learning.py:507] global step 2474: loss = 3.8864 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2475: loss = 1.4777 (1.246 sec/step)\n",
            "I1229 11:38:58.706334 140034317584256 learning.py:507] global step 2475: loss = 1.4777 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2476: loss = 1.9674 (1.186 sec/step)\n",
            "I1229 11:38:59.893909 140034317584256 learning.py:507] global step 2476: loss = 1.9674 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2477: loss = 2.0214 (1.205 sec/step)\n",
            "I1229 11:39:01.100447 140034317584256 learning.py:507] global step 2477: loss = 2.0214 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 2478: loss = 3.2534 (1.205 sec/step)\n",
            "I1229 11:39:02.307297 140034317584256 learning.py:507] global step 2478: loss = 3.2534 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 2479: loss = 1.8756 (1.214 sec/step)\n",
            "I1229 11:39:03.523540 140034317584256 learning.py:507] global step 2479: loss = 1.8756 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2480: loss = 1.9322 (1.255 sec/step)\n",
            "I1229 11:39:04.781071 140034317584256 learning.py:507] global step 2480: loss = 1.9322 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 2481: loss = 1.7405 (1.235 sec/step)\n",
            "I1229 11:39:06.017879 140034317584256 learning.py:507] global step 2481: loss = 1.7405 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2482: loss = 1.8236 (1.200 sec/step)\n",
            "I1229 11:39:07.219635 140034317584256 learning.py:507] global step 2482: loss = 1.8236 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2483: loss = 3.3506 (1.194 sec/step)\n",
            "I1229 11:39:08.415806 140034317584256 learning.py:507] global step 2483: loss = 3.3506 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 2484: loss = 2.4442 (1.206 sec/step)\n",
            "I1229 11:39:09.623257 140034317584256 learning.py:507] global step 2484: loss = 2.4442 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2485: loss = 3.2275 (1.250 sec/step)\n",
            "I1229 11:39:10.874582 140034317584256 learning.py:507] global step 2485: loss = 3.2275 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2486: loss = 2.6440 (1.238 sec/step)\n",
            "I1229 11:39:12.114454 140034317584256 learning.py:507] global step 2486: loss = 2.6440 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2487: loss = 2.5147 (1.179 sec/step)\n",
            "I1229 11:39:13.295423 140034317584256 learning.py:507] global step 2487: loss = 2.5147 (1.179 sec/step)\n",
            "INFO:tensorflow:global step 2488: loss = 1.8943 (1.231 sec/step)\n",
            "I1229 11:39:14.527877 140034317584256 learning.py:507] global step 2488: loss = 1.8943 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2489: loss = 2.5010 (1.249 sec/step)\n",
            "I1229 11:39:15.778748 140034317584256 learning.py:507] global step 2489: loss = 2.5010 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2490: loss = 1.6990 (1.247 sec/step)\n",
            "I1229 11:39:17.027893 140034317584256 learning.py:507] global step 2490: loss = 1.6990 (1.247 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2491.\n",
            "I1229 11:39:19.256490 140030580766464 supervisor.py:1050] Recording summary at step 2491.\n",
            "INFO:tensorflow:global step 2491: loss = 2.3087 (2.228 sec/step)\n",
            "I1229 11:39:19.264766 140034317584256 learning.py:507] global step 2491: loss = 2.3087 (2.228 sec/step)\n",
            "INFO:tensorflow:global step 2492: loss = 1.7765 (1.246 sec/step)\n",
            "I1229 11:39:20.515049 140034317584256 learning.py:507] global step 2492: loss = 1.7765 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2493: loss = 2.0401 (1.248 sec/step)\n",
            "I1229 11:39:21.764578 140034317584256 learning.py:507] global step 2493: loss = 2.0401 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2494: loss = 2.3229 (1.203 sec/step)\n",
            "I1229 11:39:22.969683 140034317584256 learning.py:507] global step 2494: loss = 2.3229 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2495: loss = 1.7445 (1.195 sec/step)\n",
            "I1229 11:39:24.166328 140034317584256 learning.py:507] global step 2495: loss = 1.7445 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2496: loss = 3.4666 (1.222 sec/step)\n",
            "I1229 11:39:25.389784 140034317584256 learning.py:507] global step 2496: loss = 3.4666 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2497: loss = 2.9000 (1.206 sec/step)\n",
            "I1229 11:39:26.597666 140034317584256 learning.py:507] global step 2497: loss = 2.9000 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2498: loss = 2.3889 (1.200 sec/step)\n",
            "I1229 11:39:27.799584 140034317584256 learning.py:507] global step 2498: loss = 2.3889 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2499: loss = 1.7906 (1.186 sec/step)\n",
            "I1229 11:39:28.987288 140034317584256 learning.py:507] global step 2499: loss = 1.7906 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2500: loss = 1.8968 (1.226 sec/step)\n",
            "I1229 11:39:30.215516 140034317584256 learning.py:507] global step 2500: loss = 1.8968 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2501: loss = 2.8663 (1.251 sec/step)\n",
            "I1229 11:39:31.468616 140034317584256 learning.py:507] global step 2501: loss = 2.8663 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2502: loss = 2.0918 (1.212 sec/step)\n",
            "I1229 11:39:32.682782 140034317584256 learning.py:507] global step 2502: loss = 2.0918 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2503: loss = 1.9500 (1.201 sec/step)\n",
            "I1229 11:39:33.885914 140034317584256 learning.py:507] global step 2503: loss = 1.9500 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2504: loss = 2.7014 (1.206 sec/step)\n",
            "I1229 11:39:35.093478 140034317584256 learning.py:507] global step 2504: loss = 2.7014 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2505: loss = 1.9395 (1.208 sec/step)\n",
            "I1229 11:39:36.302695 140034317584256 learning.py:507] global step 2505: loss = 1.9395 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2506: loss = 2.6119 (1.200 sec/step)\n",
            "I1229 11:39:37.504530 140034317584256 learning.py:507] global step 2506: loss = 2.6119 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2507: loss = 2.0205 (1.203 sec/step)\n",
            "I1229 11:39:38.708670 140034317584256 learning.py:507] global step 2507: loss = 2.0205 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2508: loss = 1.7088 (1.244 sec/step)\n",
            "I1229 11:39:39.954024 140034317584256 learning.py:507] global step 2508: loss = 1.7088 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2509: loss = 3.3819 (1.203 sec/step)\n",
            "I1229 11:39:41.158146 140034317584256 learning.py:507] global step 2509: loss = 3.3819 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2510: loss = 2.6253 (1.232 sec/step)\n",
            "I1229 11:39:42.392246 140034317584256 learning.py:507] global step 2510: loss = 2.6253 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 2511: loss = 2.6871 (1.187 sec/step)\n",
            "I1229 11:39:43.581001 140034317584256 learning.py:507] global step 2511: loss = 2.6871 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 2512: loss = 2.6820 (1.191 sec/step)\n",
            "I1229 11:39:44.773614 140034317584256 learning.py:507] global step 2512: loss = 2.6820 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 2513: loss = 2.3894 (1.189 sec/step)\n",
            "I1229 11:39:45.964285 140034317584256 learning.py:507] global step 2513: loss = 2.3894 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 2514: loss = 2.6278 (1.206 sec/step)\n",
            "I1229 11:39:47.172137 140034317584256 learning.py:507] global step 2514: loss = 2.6278 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2515: loss = 1.9803 (1.254 sec/step)\n",
            "I1229 11:39:48.428393 140034317584256 learning.py:507] global step 2515: loss = 1.9803 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2516: loss = 2.3872 (1.183 sec/step)\n",
            "I1229 11:39:49.613228 140034317584256 learning.py:507] global step 2516: loss = 2.3872 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 2517: loss = 1.9653 (1.275 sec/step)\n",
            "I1229 11:39:50.890545 140034317584256 learning.py:507] global step 2517: loss = 1.9653 (1.275 sec/step)\n",
            "INFO:tensorflow:global step 2518: loss = 2.7015 (1.211 sec/step)\n",
            "I1229 11:39:52.103492 140034317584256 learning.py:507] global step 2518: loss = 2.7015 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2519: loss = 2.4175 (1.164 sec/step)\n",
            "I1229 11:39:53.269968 140034317584256 learning.py:507] global step 2519: loss = 2.4175 (1.164 sec/step)\n",
            "INFO:tensorflow:global step 2520: loss = 2.0011 (1.220 sec/step)\n",
            "I1229 11:39:54.491255 140034317584256 learning.py:507] global step 2520: loss = 2.0011 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2521: loss = 2.3861 (1.185 sec/step)\n",
            "I1229 11:39:55.677639 140034317584256 learning.py:507] global step 2521: loss = 2.3861 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 2522: loss = 1.8199 (1.231 sec/step)\n",
            "I1229 11:39:56.910379 140034317584256 learning.py:507] global step 2522: loss = 1.8199 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2523: loss = 2.3878 (1.226 sec/step)\n",
            "I1229 11:39:58.137825 140034317584256 learning.py:507] global step 2523: loss = 2.3878 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2524: loss = 2.3969 (1.259 sec/step)\n",
            "I1229 11:39:59.398697 140034317584256 learning.py:507] global step 2524: loss = 2.3969 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 2525: loss = 2.3360 (1.227 sec/step)\n",
            "I1229 11:40:00.626793 140034317584256 learning.py:507] global step 2525: loss = 2.3360 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2526: loss = 2.0400 (1.251 sec/step)\n",
            "I1229 11:40:01.879282 140034317584256 learning.py:507] global step 2526: loss = 2.0400 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2527: loss = 1.9832 (1.254 sec/step)\n",
            "I1229 11:40:03.134661 140034317584256 learning.py:507] global step 2527: loss = 1.9832 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2528: loss = 2.6542 (1.228 sec/step)\n",
            "I1229 11:40:04.365396 140034317584256 learning.py:507] global step 2528: loss = 2.6542 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2529: loss = 1.8049 (1.302 sec/step)\n",
            "I1229 11:40:05.668736 140034317584256 learning.py:507] global step 2529: loss = 1.8049 (1.302 sec/step)\n",
            "INFO:tensorflow:global step 2530: loss = 2.3718 (1.251 sec/step)\n",
            "I1229 11:40:06.922064 140034317584256 learning.py:507] global step 2530: loss = 2.3718 (1.251 sec/step)\n",
            "INFO:tensorflow:global step 2531: loss = 1.6303 (1.226 sec/step)\n",
            "I1229 11:40:08.149662 140034317584256 learning.py:507] global step 2531: loss = 1.6303 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2532: loss = 2.3314 (1.279 sec/step)\n",
            "I1229 11:40:09.430339 140034317584256 learning.py:507] global step 2532: loss = 2.3314 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 2533: loss = 3.1849 (1.227 sec/step)\n",
            "I1229 11:40:10.659407 140034317584256 learning.py:507] global step 2533: loss = 3.1849 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2534: loss = 2.5803 (1.247 sec/step)\n",
            "I1229 11:40:11.908572 140034317584256 learning.py:507] global step 2534: loss = 2.5803 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 2535: loss = 2.1154 (1.224 sec/step)\n",
            "I1229 11:40:13.133852 140034317584256 learning.py:507] global step 2535: loss = 2.1154 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2536: loss = 2.3132 (1.222 sec/step)\n",
            "I1229 11:40:14.357147 140034317584256 learning.py:507] global step 2536: loss = 2.3132 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2537: loss = 2.8347 (1.276 sec/step)\n",
            "I1229 11:40:15.634553 140034317584256 learning.py:507] global step 2537: loss = 2.8347 (1.276 sec/step)\n",
            "INFO:tensorflow:global step 2538: loss = 2.4817 (1.234 sec/step)\n",
            "I1229 11:40:16.870150 140034317584256 learning.py:507] global step 2538: loss = 2.4817 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2539: loss = 2.7578 (1.241 sec/step)\n",
            "I1229 11:40:18.112786 140034317584256 learning.py:507] global step 2539: loss = 2.7578 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2540: loss = 2.4954 (1.220 sec/step)\n",
            "I1229 11:40:19.334519 140034317584256 learning.py:507] global step 2540: loss = 2.4954 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2541: loss = 2.7522 (1.200 sec/step)\n",
            "I1229 11:40:20.537040 140034317584256 learning.py:507] global step 2541: loss = 2.7522 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2542: loss = 2.8456 (1.281 sec/step)\n",
            "I1229 11:40:21.819529 140034317584256 learning.py:507] global step 2542: loss = 2.8456 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 2543: loss = 3.3305 (1.221 sec/step)\n",
            "I1229 11:40:23.042308 140034317584256 learning.py:507] global step 2543: loss = 3.3305 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2544: loss = 1.9607 (1.221 sec/step)\n",
            "I1229 11:40:24.265448 140034317584256 learning.py:507] global step 2544: loss = 1.9607 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2545: loss = 1.9374 (1.203 sec/step)\n",
            "I1229 11:40:25.470431 140034317584256 learning.py:507] global step 2545: loss = 1.9374 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2546: loss = 2.5987 (1.211 sec/step)\n",
            "I1229 11:40:26.682875 140034317584256 learning.py:507] global step 2546: loss = 2.5987 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2547: loss = 2.9462 (1.215 sec/step)\n",
            "I1229 11:40:27.899743 140034317584256 learning.py:507] global step 2547: loss = 2.9462 (1.215 sec/step)\n",
            "INFO:tensorflow:global step 2548: loss = 2.5591 (1.199 sec/step)\n",
            "I1229 11:40:29.101334 140034317584256 learning.py:507] global step 2548: loss = 2.5591 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 2549: loss = 3.2843 (1.223 sec/step)\n",
            "I1229 11:40:30.326023 140034317584256 learning.py:507] global step 2549: loss = 3.2843 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2550: loss = 3.0264 (1.253 sec/step)\n",
            "I1229 11:40:31.580781 140034317584256 learning.py:507] global step 2550: loss = 3.0264 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 2551: loss = 1.8543 (1.238 sec/step)\n",
            "I1229 11:40:32.820262 140034317584256 learning.py:507] global step 2551: loss = 1.8543 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2552: loss = 2.6041 (1.237 sec/step)\n",
            "I1229 11:40:34.059279 140034317584256 learning.py:507] global step 2552: loss = 2.6041 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2553: loss = 2.4348 (1.243 sec/step)\n",
            "I1229 11:40:35.305127 140034317584256 learning.py:507] global step 2553: loss = 2.4348 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2554: loss = 3.2462 (1.260 sec/step)\n",
            "I1229 11:40:36.566494 140034317584256 learning.py:507] global step 2554: loss = 3.2462 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2555: loss = 2.7369 (1.263 sec/step)\n",
            "I1229 11:40:37.831113 140034317584256 learning.py:507] global step 2555: loss = 2.7369 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2556: loss = 1.7017 (1.212 sec/step)\n",
            "I1229 11:40:39.045303 140034317584256 learning.py:507] global step 2556: loss = 1.7017 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2557: loss = 1.9489 (1.248 sec/step)\n",
            "I1229 11:40:40.295350 140034317584256 learning.py:507] global step 2557: loss = 1.9489 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2558: loss = 2.7312 (1.239 sec/step)\n",
            "I1229 11:40:41.536225 140034317584256 learning.py:507] global step 2558: loss = 2.7312 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2559: loss = 2.7949 (1.220 sec/step)\n",
            "I1229 11:40:42.757928 140034317584256 learning.py:507] global step 2559: loss = 2.7949 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2560: loss = 2.4966 (1.232 sec/step)\n",
            "I1229 11:40:43.992174 140034317584256 learning.py:507] global step 2560: loss = 2.4966 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 2561: loss = 2.2854 (1.263 sec/step)\n",
            "I1229 11:40:45.257533 140034317584256 learning.py:507] global step 2561: loss = 2.2854 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2562: loss = 2.4975 (1.271 sec/step)\n",
            "I1229 11:40:46.529828 140034317584256 learning.py:507] global step 2562: loss = 2.4975 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2563: loss = 2.4825 (1.245 sec/step)\n",
            "I1229 11:40:47.776473 140034317584256 learning.py:507] global step 2563: loss = 2.4825 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2564: loss = 2.8897 (1.231 sec/step)\n",
            "I1229 11:40:49.009189 140034317584256 learning.py:507] global step 2564: loss = 2.8897 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2565: loss = 3.3707 (1.198 sec/step)\n",
            "I1229 11:40:50.209305 140034317584256 learning.py:507] global step 2565: loss = 3.3707 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2566: loss = 2.8478 (1.224 sec/step)\n",
            "I1229 11:40:51.434878 140034317584256 learning.py:507] global step 2566: loss = 2.8478 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2567: loss = 2.5364 (1.229 sec/step)\n",
            "I1229 11:40:52.665947 140034317584256 learning.py:507] global step 2567: loss = 2.5364 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2568: loss = 1.9580 (1.232 sec/step)\n",
            "I1229 11:40:53.899366 140034317584256 learning.py:507] global step 2568: loss = 1.9580 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 2569: loss = 2.3014 (1.264 sec/step)\n",
            "I1229 11:40:55.164640 140034317584256 learning.py:507] global step 2569: loss = 2.3014 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 2570: loss = 3.1688 (1.219 sec/step)\n",
            "I1229 11:40:56.385527 140034317584256 learning.py:507] global step 2570: loss = 3.1688 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2571: loss = 4.2094 (1.271 sec/step)\n",
            "I1229 11:40:57.658359 140034317584256 learning.py:507] global step 2571: loss = 4.2094 (1.271 sec/step)\n",
            "INFO:tensorflow:global step 2572: loss = 2.9849 (1.218 sec/step)\n",
            "I1229 11:40:58.877969 140034317584256 learning.py:507] global step 2572: loss = 2.9849 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2573: loss = 2.4615 (1.246 sec/step)\n",
            "I1229 11:41:00.125643 140034317584256 learning.py:507] global step 2573: loss = 2.4615 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2574: loss = 3.0377 (1.261 sec/step)\n",
            "I1229 11:41:01.388278 140034317584256 learning.py:507] global step 2574: loss = 3.0377 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2575: loss = 1.6899 (1.252 sec/step)\n",
            "I1229 11:41:02.642084 140034317584256 learning.py:507] global step 2575: loss = 1.6899 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2576: loss = 3.1472 (1.226 sec/step)\n",
            "I1229 11:41:03.870174 140034317584256 learning.py:507] global step 2576: loss = 3.1472 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2577: loss = 2.1840 (1.231 sec/step)\n",
            "I1229 11:41:05.103035 140034317584256 learning.py:507] global step 2577: loss = 2.1840 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2578: loss = 1.7731 (1.238 sec/step)\n",
            "I1229 11:41:06.342591 140034317584256 learning.py:507] global step 2578: loss = 1.7731 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2579: loss = 1.7854 (1.232 sec/step)\n",
            "I1229 11:41:07.576592 140034317584256 learning.py:507] global step 2579: loss = 1.7854 (1.232 sec/step)\n",
            "INFO:tensorflow:global step 2580: loss = 1.9509 (1.234 sec/step)\n",
            "I1229 11:41:08.811754 140034317584256 learning.py:507] global step 2580: loss = 1.9509 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2581: loss = 2.0148 (1.221 sec/step)\n",
            "I1229 11:41:10.034064 140034317584256 learning.py:507] global step 2581: loss = 2.0148 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2582: loss = 2.2634 (1.225 sec/step)\n",
            "I1229 11:41:11.261196 140034317584256 learning.py:507] global step 2582: loss = 2.2634 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2583: loss = 2.7681 (1.196 sec/step)\n",
            "I1229 11:41:12.458766 140034317584256 learning.py:507] global step 2583: loss = 2.7681 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2584: loss = 1.6324 (1.239 sec/step)\n",
            "I1229 11:41:13.699618 140034317584256 learning.py:507] global step 2584: loss = 1.6324 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2585: loss = 2.0094 (1.225 sec/step)\n",
            "I1229 11:41:14.926361 140034317584256 learning.py:507] global step 2585: loss = 2.0094 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2586: loss = 2.0367 (1.268 sec/step)\n",
            "I1229 11:41:16.195499 140034317584256 learning.py:507] global step 2586: loss = 2.0367 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2587: loss = 1.5908 (1.209 sec/step)\n",
            "I1229 11:41:17.406523 140034317584256 learning.py:507] global step 2587: loss = 1.5908 (1.209 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2587.\n",
            "I1229 11:41:19.412962 140030580766464 supervisor.py:1050] Recording summary at step 2587.\n",
            "INFO:tensorflow:global step 2588: loss = 1.9811 (2.122 sec/step)\n",
            "I1229 11:41:19.530557 140034317584256 learning.py:507] global step 2588: loss = 1.9811 (2.122 sec/step)\n",
            "INFO:tensorflow:global step 2589: loss = 1.8077 (1.208 sec/step)\n",
            "I1229 11:41:20.740589 140034317584256 learning.py:507] global step 2589: loss = 1.8077 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2590: loss = 1.6403 (1.202 sec/step)\n",
            "I1229 11:41:21.944528 140034317584256 learning.py:507] global step 2590: loss = 1.6403 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 2591: loss = 2.3487 (1.263 sec/step)\n",
            "I1229 11:41:23.209470 140034317584256 learning.py:507] global step 2591: loss = 2.3487 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2592: loss = 2.5160 (1.244 sec/step)\n",
            "I1229 11:41:24.455543 140034317584256 learning.py:507] global step 2592: loss = 2.5160 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2593: loss = 2.5700 (1.249 sec/step)\n",
            "I1229 11:41:25.706116 140034317584256 learning.py:507] global step 2593: loss = 2.5700 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2594: loss = 2.8094 (1.227 sec/step)\n",
            "I1229 11:41:26.934462 140034317584256 learning.py:507] global step 2594: loss = 2.8094 (1.227 sec/step)\n",
            "INFO:tensorflow:global step 2595: loss = 1.8043 (1.199 sec/step)\n",
            "I1229 11:41:28.135528 140034317584256 learning.py:507] global step 2595: loss = 1.8043 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 2596: loss = 1.9629 (1.213 sec/step)\n",
            "I1229 11:41:29.349895 140034317584256 learning.py:507] global step 2596: loss = 1.9629 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 2597: loss = 2.0894 (1.195 sec/step)\n",
            "I1229 11:41:30.546016 140034317584256 learning.py:507] global step 2597: loss = 2.0894 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2598: loss = 1.6251 (1.217 sec/step)\n",
            "I1229 11:41:31.764288 140034317584256 learning.py:507] global step 2598: loss = 1.6251 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 2599: loss = 2.1072 (1.242 sec/step)\n",
            "I1229 11:41:33.008428 140034317584256 learning.py:507] global step 2599: loss = 2.1072 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2600: loss = 2.9562 (1.246 sec/step)\n",
            "I1229 11:41:34.256486 140034317584256 learning.py:507] global step 2600: loss = 2.9562 (1.246 sec/step)\n",
            "INFO:tensorflow:global step 2601: loss = 1.1892 (1.186 sec/step)\n",
            "I1229 11:41:35.443871 140034317584256 learning.py:507] global step 2601: loss = 1.1892 (1.186 sec/step)\n",
            "INFO:tensorflow:global step 2602: loss = 1.7989 (1.229 sec/step)\n",
            "I1229 11:41:36.675011 140034317584256 learning.py:507] global step 2602: loss = 1.7989 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2603: loss = 3.3881 (1.184 sec/step)\n",
            "I1229 11:41:37.861014 140034317584256 learning.py:507] global step 2603: loss = 3.3881 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 2604: loss = 2.3425 (1.260 sec/step)\n",
            "I1229 11:41:39.123195 140034317584256 learning.py:507] global step 2604: loss = 2.3425 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2605: loss = 2.2074 (1.243 sec/step)\n",
            "I1229 11:41:40.368246 140034317584256 learning.py:507] global step 2605: loss = 2.2074 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2606: loss = 1.7504 (1.234 sec/step)\n",
            "I1229 11:41:41.603991 140034317584256 learning.py:507] global step 2606: loss = 1.7504 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2607: loss = 3.1527 (1.198 sec/step)\n",
            "I1229 11:41:42.803814 140034317584256 learning.py:507] global step 2607: loss = 3.1527 (1.198 sec/step)\n",
            "INFO:tensorflow:global step 2608: loss = 2.7300 (1.234 sec/step)\n",
            "I1229 11:41:44.039780 140034317584256 learning.py:507] global step 2608: loss = 2.7300 (1.234 sec/step)\n",
            "INFO:tensorflow:global step 2609: loss = 2.7323 (1.273 sec/step)\n",
            "I1229 11:41:45.314173 140034317584256 learning.py:507] global step 2609: loss = 2.7323 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 2610: loss = 3.0952 (1.237 sec/step)\n",
            "I1229 11:41:46.553015 140034317584256 learning.py:507] global step 2610: loss = 3.0952 (1.237 sec/step)\n",
            "INFO:tensorflow:global step 2611: loss = 1.7290 (1.257 sec/step)\n",
            "I1229 11:41:47.811604 140034317584256 learning.py:507] global step 2611: loss = 1.7290 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2612: loss = 2.5003 (1.228 sec/step)\n",
            "I1229 11:41:49.041201 140034317584256 learning.py:507] global step 2612: loss = 2.5003 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2613: loss = 2.8659 (1.208 sec/step)\n",
            "I1229 11:41:50.250541 140034317584256 learning.py:507] global step 2613: loss = 2.8659 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2614: loss = 1.5181 (1.207 sec/step)\n",
            "I1229 11:41:51.459348 140034317584256 learning.py:507] global step 2614: loss = 1.5181 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2615: loss = 2.2050 (1.228 sec/step)\n",
            "I1229 11:41:52.688851 140034317584256 learning.py:507] global step 2615: loss = 2.2050 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2616: loss = 1.9620 (1.214 sec/step)\n",
            "I1229 11:41:53.904296 140034317584256 learning.py:507] global step 2616: loss = 1.9620 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2617: loss = 2.7410 (1.243 sec/step)\n",
            "I1229 11:41:55.149888 140034317584256 learning.py:507] global step 2617: loss = 2.7410 (1.243 sec/step)\n",
            "INFO:tensorflow:global step 2618: loss = 2.0091 (1.230 sec/step)\n",
            "I1229 11:41:56.382115 140034317584256 learning.py:507] global step 2618: loss = 2.0091 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 2619: loss = 2.4705 (1.219 sec/step)\n",
            "I1229 11:41:57.603096 140034317584256 learning.py:507] global step 2619: loss = 2.4705 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2620: loss = 3.7452 (1.176 sec/step)\n",
            "I1229 11:41:58.780735 140034317584256 learning.py:507] global step 2620: loss = 3.7452 (1.176 sec/step)\n",
            "INFO:tensorflow:global step 2621: loss = 2.6541 (1.223 sec/step)\n",
            "I1229 11:42:00.005912 140034317584256 learning.py:507] global step 2621: loss = 2.6541 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2622: loss = 2.6135 (1.238 sec/step)\n",
            "I1229 11:42:01.245671 140034317584256 learning.py:507] global step 2622: loss = 2.6135 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2623: loss = 1.8832 (1.241 sec/step)\n",
            "I1229 11:42:02.488140 140034317584256 learning.py:507] global step 2623: loss = 1.8832 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2624: loss = 2.9262 (1.207 sec/step)\n",
            "I1229 11:42:03.697373 140034317584256 learning.py:507] global step 2624: loss = 2.9262 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2625: loss = 1.7531 (1.214 sec/step)\n",
            "I1229 11:42:04.913630 140034317584256 learning.py:507] global step 2625: loss = 1.7531 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2626: loss = 1.2656 (1.206 sec/step)\n",
            "I1229 11:42:06.121376 140034317584256 learning.py:507] global step 2626: loss = 1.2656 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2627: loss = 2.0058 (1.231 sec/step)\n",
            "I1229 11:42:07.354462 140034317584256 learning.py:507] global step 2627: loss = 2.0058 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2628: loss = 1.7347 (1.231 sec/step)\n",
            "I1229 11:42:08.587595 140034317584256 learning.py:507] global step 2628: loss = 1.7347 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2629: loss = 2.5357 (1.242 sec/step)\n",
            "I1229 11:42:09.831449 140034317584256 learning.py:507] global step 2629: loss = 2.5357 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2630: loss = 4.0599 (1.191 sec/step)\n",
            "I1229 11:42:11.024287 140034317584256 learning.py:507] global step 2630: loss = 4.0599 (1.191 sec/step)\n",
            "INFO:tensorflow:global step 2631: loss = 2.6006 (1.212 sec/step)\n",
            "I1229 11:42:12.237760 140034317584256 learning.py:507] global step 2631: loss = 2.6006 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2632: loss = 2.8726 (1.241 sec/step)\n",
            "I1229 11:42:13.480303 140034317584256 learning.py:507] global step 2632: loss = 2.8726 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2633: loss = 1.7037 (1.210 sec/step)\n",
            "I1229 11:42:14.692757 140034317584256 learning.py:507] global step 2633: loss = 1.7037 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2634: loss = 1.7314 (1.267 sec/step)\n",
            "I1229 11:42:15.961827 140034317584256 learning.py:507] global step 2634: loss = 1.7314 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 2635: loss = 1.6026 (1.236 sec/step)\n",
            "I1229 11:42:17.199365 140034317584256 learning.py:507] global step 2635: loss = 1.6026 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2636: loss = 2.8058 (1.250 sec/step)\n",
            "I1229 11:42:18.451532 140034317584256 learning.py:507] global step 2636: loss = 2.8058 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2637: loss = 2.5349 (1.181 sec/step)\n",
            "I1229 11:42:19.634319 140034317584256 learning.py:507] global step 2637: loss = 2.5349 (1.181 sec/step)\n",
            "INFO:tensorflow:global step 2638: loss = 3.3695 (1.218 sec/step)\n",
            "I1229 11:42:20.854858 140034317584256 learning.py:507] global step 2638: loss = 3.3695 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2639: loss = 3.0225 (1.210 sec/step)\n",
            "I1229 11:42:22.066341 140034317584256 learning.py:507] global step 2639: loss = 3.0225 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2640: loss = 1.8583 (1.214 sec/step)\n",
            "I1229 11:42:23.281770 140034317584256 learning.py:507] global step 2640: loss = 1.8583 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2641: loss = 2.7738 (1.236 sec/step)\n",
            "I1229 11:42:24.519514 140034317584256 learning.py:507] global step 2641: loss = 2.7738 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2642: loss = 2.3238 (1.239 sec/step)\n",
            "I1229 11:42:25.760546 140034317584256 learning.py:507] global step 2642: loss = 2.3238 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2643: loss = 4.2263 (1.225 sec/step)\n",
            "I1229 11:42:26.987178 140034317584256 learning.py:507] global step 2643: loss = 4.2263 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2644: loss = 2.2943 (1.260 sec/step)\n",
            "I1229 11:42:28.248725 140034317584256 learning.py:507] global step 2644: loss = 2.2943 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 2645: loss = 2.8410 (1.268 sec/step)\n",
            "I1229 11:42:29.519283 140034317584256 learning.py:507] global step 2645: loss = 2.8410 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2646: loss = 2.0960 (1.274 sec/step)\n",
            "I1229 11:42:30.795683 140034317584256 learning.py:507] global step 2646: loss = 2.0960 (1.274 sec/step)\n",
            "INFO:tensorflow:global step 2647: loss = 2.1341 (1.240 sec/step)\n",
            "I1229 11:42:32.037296 140034317584256 learning.py:507] global step 2647: loss = 2.1341 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2648: loss = 3.2768 (1.221 sec/step)\n",
            "I1229 11:42:33.259970 140034317584256 learning.py:507] global step 2648: loss = 3.2768 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2649: loss = 1.6290 (1.216 sec/step)\n",
            "I1229 11:42:34.477890 140034317584256 learning.py:507] global step 2649: loss = 1.6290 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2650: loss = 2.3828 (1.287 sec/step)\n",
            "I1229 11:42:35.766421 140034317584256 learning.py:507] global step 2650: loss = 2.3828 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 2651: loss = 1.9115 (1.254 sec/step)\n",
            "I1229 11:42:37.022773 140034317584256 learning.py:507] global step 2651: loss = 1.9115 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2652: loss = 2.4649 (1.233 sec/step)\n",
            "I1229 11:42:38.257108 140034317584256 learning.py:507] global step 2652: loss = 2.4649 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2653: loss = 3.3508 (1.197 sec/step)\n",
            "I1229 11:42:39.455823 140034317584256 learning.py:507] global step 2653: loss = 3.3508 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 2654: loss = 2.7599 (1.222 sec/step)\n",
            "I1229 11:42:40.679736 140034317584256 learning.py:507] global step 2654: loss = 2.7599 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2655: loss = 2.9748 (1.250 sec/step)\n",
            "I1229 11:42:41.931795 140034317584256 learning.py:507] global step 2655: loss = 2.9748 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2656: loss = 1.7468 (1.200 sec/step)\n",
            "I1229 11:42:43.132964 140034317584256 learning.py:507] global step 2656: loss = 1.7468 (1.200 sec/step)\n",
            "INFO:tensorflow:global step 2657: loss = 2.1043 (1.238 sec/step)\n",
            "I1229 11:42:44.372734 140034317584256 learning.py:507] global step 2657: loss = 2.1043 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2658: loss = 2.9173 (1.236 sec/step)\n",
            "I1229 11:42:45.610622 140034317584256 learning.py:507] global step 2658: loss = 2.9173 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2659: loss = 3.2204 (1.207 sec/step)\n",
            "I1229 11:42:46.819409 140034317584256 learning.py:507] global step 2659: loss = 3.2204 (1.207 sec/step)\n",
            "INFO:tensorflow:global step 2660: loss = 2.7166 (1.236 sec/step)\n",
            "I1229 11:42:48.058116 140034317584256 learning.py:507] global step 2660: loss = 2.7166 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2661: loss = 1.8136 (1.236 sec/step)\n",
            "I1229 11:42:49.296628 140034317584256 learning.py:507] global step 2661: loss = 1.8136 (1.236 sec/step)\n",
            "INFO:tensorflow:global step 2662: loss = 3.1156 (1.220 sec/step)\n",
            "I1229 11:42:50.518608 140034317584256 learning.py:507] global step 2662: loss = 3.1156 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2663: loss = 3.7044 (1.263 sec/step)\n",
            "I1229 11:42:51.784023 140034317584256 learning.py:507] global step 2663: loss = 3.7044 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 2664: loss = 2.3087 (1.262 sec/step)\n",
            "I1229 11:42:53.048202 140034317584256 learning.py:507] global step 2664: loss = 2.3087 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 2665: loss = 2.6869 (1.172 sec/step)\n",
            "I1229 11:42:54.222471 140034317584256 learning.py:507] global step 2665: loss = 2.6869 (1.172 sec/step)\n",
            "INFO:tensorflow:global step 2666: loss = 1.8743 (1.220 sec/step)\n",
            "I1229 11:42:55.445109 140034317584256 learning.py:507] global step 2666: loss = 1.8743 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2667: loss = 2.4165 (1.242 sec/step)\n",
            "I1229 11:42:56.689041 140034317584256 learning.py:507] global step 2667: loss = 2.4165 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2668: loss = 3.3264 (1.240 sec/step)\n",
            "I1229 11:42:57.931572 140034317584256 learning.py:507] global step 2668: loss = 3.3264 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2669: loss = 2.7809 (1.242 sec/step)\n",
            "I1229 11:42:59.175203 140034317584256 learning.py:507] global step 2669: loss = 2.7809 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2670: loss = 2.2951 (1.242 sec/step)\n",
            "I1229 11:43:00.418803 140034317584256 learning.py:507] global step 2670: loss = 2.2951 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2671: loss = 3.7512 (1.212 sec/step)\n",
            "I1229 11:43:01.633179 140034317584256 learning.py:507] global step 2671: loss = 3.7512 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2672: loss = 2.7980 (1.229 sec/step)\n",
            "I1229 11:43:02.864231 140034317584256 learning.py:507] global step 2672: loss = 2.7980 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2673: loss = 2.2901 (1.224 sec/step)\n",
            "I1229 11:43:04.090400 140034317584256 learning.py:507] global step 2673: loss = 2.2901 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2674: loss = 2.1950 (1.193 sec/step)\n",
            "I1229 11:43:05.285792 140034317584256 learning.py:507] global step 2674: loss = 2.1950 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 2675: loss = 2.3780 (1.239 sec/step)\n",
            "I1229 11:43:06.526910 140034317584256 learning.py:507] global step 2675: loss = 2.3780 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 2676: loss = 2.4225 (1.221 sec/step)\n",
            "I1229 11:43:07.749256 140034317584256 learning.py:507] global step 2676: loss = 2.4225 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2677: loss = 1.8060 (1.228 sec/step)\n",
            "I1229 11:43:08.979132 140034317584256 learning.py:507] global step 2677: loss = 1.8060 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2678: loss = 1.5494 (1.229 sec/step)\n",
            "I1229 11:43:10.210028 140034317584256 learning.py:507] global step 2678: loss = 1.5494 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2679: loss = 1.8021 (1.201 sec/step)\n",
            "I1229 11:43:11.412397 140034317584256 learning.py:507] global step 2679: loss = 1.8021 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2680: loss = 1.7600 (1.212 sec/step)\n",
            "I1229 11:43:12.626845 140034317584256 learning.py:507] global step 2680: loss = 1.7600 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 2681: loss = 3.3403 (1.241 sec/step)\n",
            "I1229 11:43:13.870457 140034317584256 learning.py:507] global step 2681: loss = 3.3403 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2682: loss = 1.9844 (1.235 sec/step)\n",
            "I1229 11:43:15.107100 140034317584256 learning.py:507] global step 2682: loss = 1.9844 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2683: loss = 2.0190 (1.261 sec/step)\n",
            "I1229 11:43:16.370063 140034317584256 learning.py:507] global step 2683: loss = 2.0190 (1.261 sec/step)\n",
            "INFO:tensorflow:global step 2684: loss = 2.1617 (1.233 sec/step)\n",
            "I1229 11:43:17.605540 140034317584256 learning.py:507] global step 2684: loss = 2.1617 (1.233 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2684.\n",
            "I1229 11:43:19.623178 140030580766464 supervisor.py:1050] Recording summary at step 2684.\n",
            "INFO:tensorflow:global step 2685: loss = 2.3237 (2.226 sec/step)\n",
            "I1229 11:43:19.833439 140034317584256 learning.py:507] global step 2685: loss = 2.3237 (2.226 sec/step)\n",
            "INFO:tensorflow:global step 2686: loss = 3.2186 (1.205 sec/step)\n",
            "I1229 11:43:21.042350 140034317584256 learning.py:507] global step 2686: loss = 3.2186 (1.205 sec/step)\n",
            "INFO:tensorflow:global step 2687: loss = 2.7251 (1.218 sec/step)\n",
            "I1229 11:43:22.263082 140034317584256 learning.py:507] global step 2687: loss = 2.7251 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2688: loss = 2.4343 (1.216 sec/step)\n",
            "I1229 11:43:23.481232 140034317584256 learning.py:507] global step 2688: loss = 2.4343 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2689: loss = 3.0099 (1.242 sec/step)\n",
            "I1229 11:43:24.725090 140034317584256 learning.py:507] global step 2689: loss = 3.0099 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2690: loss = 2.0777 (1.214 sec/step)\n",
            "I1229 11:43:25.941249 140034317584256 learning.py:507] global step 2690: loss = 2.0777 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 2691: loss = 2.4543 (1.201 sec/step)\n",
            "I1229 11:43:27.144030 140034317584256 learning.py:507] global step 2691: loss = 2.4543 (1.201 sec/step)\n",
            "INFO:tensorflow:global step 2692: loss = 2.6688 (1.235 sec/step)\n",
            "I1229 11:43:28.380624 140034317584256 learning.py:507] global step 2692: loss = 2.6688 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2693: loss = 4.4868 (1.252 sec/step)\n",
            "I1229 11:43:29.634572 140034317584256 learning.py:507] global step 2693: loss = 4.4868 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2694: loss = 2.6061 (1.231 sec/step)\n",
            "I1229 11:43:30.867490 140034317584256 learning.py:507] global step 2694: loss = 2.6061 (1.231 sec/step)\n",
            "INFO:tensorflow:global step 2695: loss = 2.1829 (1.250 sec/step)\n",
            "I1229 11:43:32.119732 140034317584256 learning.py:507] global step 2695: loss = 2.1829 (1.250 sec/step)\n",
            "INFO:tensorflow:global step 2696: loss = 2.4116 (1.222 sec/step)\n",
            "I1229 11:43:33.343298 140034317584256 learning.py:507] global step 2696: loss = 2.4116 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 2697: loss = 3.4611 (1.196 sec/step)\n",
            "I1229 11:43:34.540745 140034317584256 learning.py:507] global step 2697: loss = 3.4611 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2698: loss = 1.9384 (1.188 sec/step)\n",
            "I1229 11:43:35.730797 140034317584256 learning.py:507] global step 2698: loss = 1.9384 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 2699: loss = 1.4242 (1.160 sec/step)\n",
            "I1229 11:43:36.892662 140034317584256 learning.py:507] global step 2699: loss = 1.4242 (1.160 sec/step)\n",
            "INFO:tensorflow:global step 2700: loss = 1.8733 (1.248 sec/step)\n",
            "I1229 11:43:38.142872 140034317584256 learning.py:507] global step 2700: loss = 1.8733 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2701: loss = 2.0284 (1.257 sec/step)\n",
            "I1229 11:43:39.401987 140034317584256 learning.py:507] global step 2701: loss = 2.0284 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2702: loss = 2.0410 (1.206 sec/step)\n",
            "I1229 11:43:40.610076 140034317584256 learning.py:507] global step 2702: loss = 2.0410 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2703: loss = 1.6254 (1.252 sec/step)\n",
            "I1229 11:43:41.864069 140034317584256 learning.py:507] global step 2703: loss = 1.6254 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2704: loss = 3.6979 (1.245 sec/step)\n",
            "I1229 11:43:43.110952 140034317584256 learning.py:507] global step 2704: loss = 3.6979 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2705: loss = 2.1147 (1.167 sec/step)\n",
            "I1229 11:43:44.280038 140034317584256 learning.py:507] global step 2705: loss = 2.1147 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 2706: loss = 1.8999 (1.177 sec/step)\n",
            "I1229 11:43:45.458857 140034317584256 learning.py:507] global step 2706: loss = 1.8999 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 2707: loss = 1.6444 (1.223 sec/step)\n",
            "I1229 11:43:46.684083 140034317584256 learning.py:507] global step 2707: loss = 1.6444 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 2708: loss = 1.7873 (1.230 sec/step)\n",
            "I1229 11:43:47.916572 140034317584256 learning.py:507] global step 2708: loss = 1.7873 (1.230 sec/step)\n",
            "INFO:tensorflow:global step 2709: loss = 3.7900 (1.211 sec/step)\n",
            "I1229 11:43:49.129197 140034317584256 learning.py:507] global step 2709: loss = 3.7900 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2710: loss = 1.9294 (1.235 sec/step)\n",
            "I1229 11:43:50.365922 140034317584256 learning.py:507] global step 2710: loss = 1.9294 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2711: loss = 1.9779 (1.238 sec/step)\n",
            "I1229 11:43:51.606047 140034317584256 learning.py:507] global step 2711: loss = 1.9779 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2712: loss = 1.9025 (1.240 sec/step)\n",
            "I1229 11:43:52.847788 140034317584256 learning.py:507] global step 2712: loss = 1.9025 (1.240 sec/step)\n",
            "INFO:tensorflow:global step 2713: loss = 2.3766 (1.254 sec/step)\n",
            "I1229 11:43:54.103695 140034317584256 learning.py:507] global step 2713: loss = 2.3766 (1.254 sec/step)\n",
            "INFO:tensorflow:global step 2714: loss = 2.6272 (1.199 sec/step)\n",
            "I1229 11:43:55.304503 140034317584256 learning.py:507] global step 2714: loss = 2.6272 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 2715: loss = 5.1782 (1.178 sec/step)\n",
            "I1229 11:43:56.484303 140034317584256 learning.py:507] global step 2715: loss = 5.1782 (1.178 sec/step)\n",
            "INFO:tensorflow:global step 2716: loss = 1.7867 (1.248 sec/step)\n",
            "I1229 11:43:57.734320 140034317584256 learning.py:507] global step 2716: loss = 1.7867 (1.248 sec/step)\n",
            "INFO:tensorflow:global step 2717: loss = 2.5017 (1.220 sec/step)\n",
            "I1229 11:43:58.956387 140034317584256 learning.py:507] global step 2717: loss = 2.5017 (1.220 sec/step)\n",
            "INFO:tensorflow:global step 2718: loss = 1.9485 (1.188 sec/step)\n",
            "I1229 11:44:00.146179 140034317584256 learning.py:507] global step 2718: loss = 1.9485 (1.188 sec/step)\n",
            "INFO:tensorflow:global step 2719: loss = 2.2063 (1.217 sec/step)\n",
            "I1229 11:44:01.364521 140034317584256 learning.py:507] global step 2719: loss = 2.2063 (1.217 sec/step)\n",
            "INFO:tensorflow:global step 2720: loss = 2.5592 (1.195 sec/step)\n",
            "I1229 11:44:02.561522 140034317584256 learning.py:507] global step 2720: loss = 2.5592 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2721: loss = 2.8125 (1.206 sec/step)\n",
            "I1229 11:44:03.769076 140034317584256 learning.py:507] global step 2721: loss = 2.8125 (1.206 sec/step)\n",
            "INFO:tensorflow:global step 2722: loss = 4.7766 (1.197 sec/step)\n",
            "I1229 11:44:04.967948 140034317584256 learning.py:507] global step 2722: loss = 4.7766 (1.197 sec/step)\n",
            "INFO:tensorflow:global step 2723: loss = 2.0684 (1.174 sec/step)\n",
            "I1229 11:44:06.143450 140034317584256 learning.py:507] global step 2723: loss = 2.0684 (1.174 sec/step)\n",
            "INFO:tensorflow:global step 2724: loss = 2.8825 (1.210 sec/step)\n",
            "I1229 11:44:07.355850 140034317584256 learning.py:507] global step 2724: loss = 2.8825 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2725: loss = 2.0683 (1.311 sec/step)\n",
            "I1229 11:44:08.668569 140034317584256 learning.py:507] global step 2725: loss = 2.0683 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 2726: loss = 1.8352 (1.210 sec/step)\n",
            "I1229 11:44:09.880733 140034317584256 learning.py:507] global step 2726: loss = 1.8352 (1.210 sec/step)\n",
            "INFO:tensorflow:global step 2727: loss = 2.3104 (1.249 sec/step)\n",
            "I1229 11:44:11.131342 140034317584256 learning.py:507] global step 2727: loss = 2.3104 (1.249 sec/step)\n",
            "INFO:tensorflow:global step 2728: loss = 2.1958 (1.238 sec/step)\n",
            "I1229 11:44:12.371168 140034317584256 learning.py:507] global step 2728: loss = 2.1958 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 2729: loss = 2.1224 (1.252 sec/step)\n",
            "I1229 11:44:13.625274 140034317584256 learning.py:507] global step 2729: loss = 2.1224 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 2730: loss = 1.1676 (1.800 sec/step)\n",
            "I1229 11:44:15.428148 140034317584256 learning.py:507] global step 2730: loss = 1.1676 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 2731: loss = 2.6046 (1.844 sec/step)\n",
            "I1229 11:44:17.274852 140034317584256 learning.py:507] global step 2731: loss = 2.6046 (1.844 sec/step)\n",
            "INFO:tensorflow:global step 2732: loss = 2.7640 (1.304 sec/step)\n",
            "I1229 11:44:18.580582 140034317584256 learning.py:507] global step 2732: loss = 2.7640 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 2733: loss = 2.6802 (1.203 sec/step)\n",
            "I1229 11:44:19.785078 140034317584256 learning.py:507] global step 2733: loss = 2.6802 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 2734: loss = 2.9702 (1.224 sec/step)\n",
            "I1229 11:44:21.011021 140034317584256 learning.py:507] global step 2734: loss = 2.9702 (1.224 sec/step)\n",
            "INFO:tensorflow:global step 2735: loss = 2.2877 (1.233 sec/step)\n",
            "I1229 11:44:22.245715 140034317584256 learning.py:507] global step 2735: loss = 2.2877 (1.233 sec/step)\n",
            "INFO:tensorflow:global step 2736: loss = 1.6552 (1.219 sec/step)\n",
            "I1229 11:44:23.466194 140034317584256 learning.py:507] global step 2736: loss = 1.6552 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2737: loss = 3.3110 (1.256 sec/step)\n",
            "I1229 11:44:24.723569 140034317584256 learning.py:507] global step 2737: loss = 3.3110 (1.256 sec/step)\n",
            "INFO:tensorflow:global step 2738: loss = 2.8712 (1.245 sec/step)\n",
            "I1229 11:44:25.970529 140034317584256 learning.py:507] global step 2738: loss = 2.8712 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 2739: loss = 2.8196 (1.258 sec/step)\n",
            "I1229 11:44:27.230585 140034317584256 learning.py:507] global step 2739: loss = 2.8196 (1.258 sec/step)\n",
            "INFO:tensorflow:global step 2740: loss = 2.9912 (1.187 sec/step)\n",
            "I1229 11:44:28.419826 140034317584256 learning.py:507] global step 2740: loss = 2.9912 (1.187 sec/step)\n",
            "INFO:tensorflow:global step 2741: loss = 2.3301 (1.192 sec/step)\n",
            "I1229 11:44:29.613196 140034317584256 learning.py:507] global step 2741: loss = 2.3301 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 2742: loss = 2.1747 (1.259 sec/step)\n",
            "I1229 11:44:30.874266 140034317584256 learning.py:507] global step 2742: loss = 2.1747 (1.259 sec/step)\n",
            "INFO:tensorflow:global step 2743: loss = 2.4041 (1.226 sec/step)\n",
            "I1229 11:44:32.101804 140034317584256 learning.py:507] global step 2743: loss = 2.4041 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2744: loss = 3.2662 (1.192 sec/step)\n",
            "I1229 11:44:33.295736 140034317584256 learning.py:507] global step 2744: loss = 3.2662 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 2745: loss = 2.5633 (1.211 sec/step)\n",
            "I1229 11:44:34.509433 140034317584256 learning.py:507] global step 2745: loss = 2.5633 (1.211 sec/step)\n",
            "INFO:tensorflow:global step 2746: loss = 1.7113 (1.270 sec/step)\n",
            "I1229 11:44:35.781275 140034317584256 learning.py:507] global step 2746: loss = 1.7113 (1.270 sec/step)\n",
            "INFO:tensorflow:global step 2747: loss = 3.5543 (1.209 sec/step)\n",
            "I1229 11:44:36.991832 140034317584256 learning.py:507] global step 2747: loss = 3.5543 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 2748: loss = 1.7737 (1.228 sec/step)\n",
            "I1229 11:44:38.221733 140034317584256 learning.py:507] global step 2748: loss = 1.7737 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2749: loss = 2.7269 (1.218 sec/step)\n",
            "I1229 11:44:39.444249 140034317584256 learning.py:507] global step 2749: loss = 2.7269 (1.218 sec/step)\n",
            "INFO:tensorflow:global step 2750: loss = 2.0120 (1.216 sec/step)\n",
            "I1229 11:44:40.664492 140034317584256 learning.py:507] global step 2750: loss = 2.0120 (1.216 sec/step)\n",
            "INFO:tensorflow:global step 2751: loss = 1.8992 (1.195 sec/step)\n",
            "I1229 11:44:41.861172 140034317584256 learning.py:507] global step 2751: loss = 1.8992 (1.195 sec/step)\n",
            "INFO:tensorflow:global step 2752: loss = 2.4455 (1.226 sec/step)\n",
            "I1229 11:44:43.088980 140034317584256 learning.py:507] global step 2752: loss = 2.4455 (1.226 sec/step)\n",
            "INFO:tensorflow:global step 2753: loss = 3.5935 (1.208 sec/step)\n",
            "I1229 11:44:44.298488 140034317584256 learning.py:507] global step 2753: loss = 3.5935 (1.208 sec/step)\n",
            "INFO:tensorflow:global step 2754: loss = 1.9658 (1.241 sec/step)\n",
            "I1229 11:44:45.541129 140034317584256 learning.py:507] global step 2754: loss = 1.9658 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 2755: loss = 2.4701 (1.225 sec/step)\n",
            "I1229 11:44:46.768089 140034317584256 learning.py:507] global step 2755: loss = 2.4701 (1.225 sec/step)\n",
            "INFO:tensorflow:global step 2756: loss = 3.3507 (1.244 sec/step)\n",
            "I1229 11:44:48.013476 140034317584256 learning.py:507] global step 2756: loss = 3.3507 (1.244 sec/step)\n",
            "INFO:tensorflow:global step 2757: loss = 2.4866 (1.196 sec/step)\n",
            "I1229 11:44:49.211356 140034317584256 learning.py:507] global step 2757: loss = 2.4866 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 2758: loss = 2.6411 (1.221 sec/step)\n",
            "I1229 11:44:50.433627 140034317584256 learning.py:507] global step 2758: loss = 2.6411 (1.221 sec/step)\n",
            "INFO:tensorflow:global step 2759: loss = 1.5192 (1.242 sec/step)\n",
            "I1229 11:44:51.677085 140034317584256 learning.py:507] global step 2759: loss = 1.5192 (1.242 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 417, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRLRRHPeYZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir TFLite_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "26014033-b12e-445c-a705-1da29f3af3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_quantized_300x300_coco.config --trained_checkpoint_prefix training/model.ckpt-2396 --output_directory inference_graph"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1229 12:08:24.618248 140677372331904 module_wrapper.py:139] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1229 12:08:24.624768 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1229 12:08:24.624981 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1229 12:08:24.668811 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1229 12:08:24.705165 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1229 12:08:24.708629 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1229 12:08:27.430653 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1229 12:08:27.445631 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.446025 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.498000 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.667744 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.725027 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.772484 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 12:08:27.824892 140677372331904 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1229 12:08:28.170541 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1229 12:08:28.616619 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W1229 12:08:28.616950 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1229 12:08:28.620888 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
            "I1229 12:08:30.427521 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
            "I1229 12:08:30.427896 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
            "I1229 12:08:30.428252 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
            "I1229 12:08:30.428526 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
            "I1229 12:08:30.428846 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
            "I1229 12:08:30.429030 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
            "I1229 12:08:30.429312 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
            "I1229 12:08:30.429500 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
            "I1229 12:08:30.429745 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
            "I1229 12:08:30.429923 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
            "I1229 12:08:30.430186 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
            "I1229 12:08:30.430384 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
            "I1229 12:08:30.430638 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
            "I1229 12:08:30.430860 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
            "I1229 12:08:30.431141 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
            "I1229 12:08:30.431333 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
            "I1229 12:08:30.431572 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
            "I1229 12:08:30.431746 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
            "I1229 12:08:30.432050 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
            "I1229 12:08:30.432286 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
            "I1229 12:08:30.432531 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
            "I1229 12:08:30.432702 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
            "I1229 12:08:30.432940 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
            "I1229 12:08:30.433120 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
            "I1229 12:08:30.433550 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
            "I1229 12:08:30.433820 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
            "I1229 12:08:30.434077 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
            "I1229 12:08:30.434285 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
            "I1229 12:08:30.434530 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
            "I1229 12:08:30.434703 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
            "I1229 12:08:30.434958 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
            "I1229 12:08:30.435170 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
            "I1229 12:08:30.435499 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
            "I1229 12:08:30.435749 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
            "I1229 12:08:30.436072 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
            "I1229 12:08:30.436366 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
            "I1229 12:08:30.436539 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
            "I1229 12:08:30.436762 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
            "I1229 12:08:30.436974 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
            "I1229 12:08:30.437154 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
            "I1229 12:08:30.437349 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
            "I1229 12:08:30.437519 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
            "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
            "I1229 12:08:30.437689 140677372331904 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W1229 12:08:30.439859 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W1229 12:08:30.441387 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "253 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/4.57m params)\n",
            "  BoxPredictor_0 (--/10.39k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "  BoxPredictor_1 (--/46.12k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "  BoxPredictor_2 (--/18.47k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "  BoxPredictor_3 (--/9.25k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_4 (--/9.25k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_5 (--/4.64k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "  FeatureExtractor (--/4.48m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/4.48m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "253 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/4.49m flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/mul_fold (1.18m/1.18m flops)\n",
            "  FeatureExtractor/MobilenetV2/Conv_1/mul_fold (409.60k/409.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/mul_fold (327.68k/327.68k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_16/project/mul_fold (307.20k/307.20k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_15/expand/mul_fold (153.60k/153.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_16/expand/mul_fold (153.60k/153.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_15/project/mul_fold (153.60k/153.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_14/project/mul_fold (153.60k/153.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_14/expand/mul_fold (153.60k/153.60k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_13/project/mul_fold (92.16k/92.16k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/mul_fold (73.73k/73.73k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/mul_fold (65.54k/65.54k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_11/project/mul_fold (55.30k/55.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_13/expand/mul_fold (55.30k/55.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_12/project/mul_fold (55.30k/55.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_12/expand/mul_fold (55.30k/55.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_11/expand/mul_fold (55.30k/55.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_10/project/mul_fold (36.86k/36.86k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/mul_fold (32.77k/32.77k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_7/expand/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_7/project/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_8/expand/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_8/project/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_9/expand/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_9/project/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_10/expand/mul_fold (24.58k/24.58k flops)\n",
            "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/mul_fold (16.38k/16.38k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_6/project/mul_fold (12.29k/12.29k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/mul_fold (8.64k/8.64k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/mul_fold (8.64k/8.64k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/mul_fold (8.64k/8.64k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_4/project/mul_fold (6.14k/6.14k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_6/expand/mul_fold (6.14k/6.14k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_5/project/mul_fold (6.14k/6.14k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_5/expand/mul_fold (6.14k/6.14k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_4/expand/mul_fold (6.14k/6.14k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/mul_fold (5.18k/5.18k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/mul_fold (5.18k/5.18k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/mul_fold (5.18k/5.18k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_3/project/mul_fold (4.61k/4.61k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_2/project/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_3/expand/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_2/expand/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_1/project/mul_fold (2.30k/2.30k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/mul_fold (1.73k/1.73k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/mul_fold (1.73k/1.73k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/mul_fold (1.73k/1.73k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_1/expand/mul_fold (1.54k/1.54k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/mul_fold (1.30k/1.30k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/mul_fold (1.30k/1.30k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/mul_fold (864/864 flops)\n",
            "  FeatureExtractor/MobilenetV2/Conv/mul_fold (864/864 flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv/project/mul_fold (512/512 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  FeatureExtractor/MobilenetV2/expanded_conv/depthwise/mul_fold (288/288 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1229 12:08:32.219306 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1229 12:08:34.009873 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-12-29 12:08:34.011612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-29 12:08:34.027043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.027967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-29 12:08:34.028328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 12:08:34.030424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-29 12:08:34.044618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-29 12:08:34.045004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-29 12:08:34.051348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-29 12:08:34.052602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-29 12:08:34.058164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 12:08:34.058343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.059256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.060062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-29 12:08:34.066092: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-12-29 12:08:34.066332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cbf100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-29 12:08:34.066360: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-29 12:08:34.161054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.162123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cbf640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-29 12:08:34.162166: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-29 12:08:34.162497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.163389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-29 12:08:34.163510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 12:08:34.163539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-29 12:08:34.163566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-29 12:08:34.163590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-29 12:08:34.163614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-29 12:08:34.163640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-29 12:08:34.163668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 12:08:34.163758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.164620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.165411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-29 12:08:34.165475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 12:08:34.166968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-29 12:08:34.166998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-29 12:08:34.167011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-29 12:08:34.167238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.168221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:34.169126: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-29 12:08:34.169176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2396\n",
            "I1229 12:08:34.171014 140677372331904 saver.py:1284] Restoring parameters from training/model.ckpt-2396\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W1229 12:08:37.014955 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-12-29 12:08:38.103379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:38.104457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-29 12:08:38.104573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 12:08:38.104634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-29 12:08:38.104675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-29 12:08:38.104699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-29 12:08:38.104736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-29 12:08:38.104774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-29 12:08:38.104798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 12:08:38.104888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:38.105772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:38.106578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-29 12:08:38.106623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-29 12:08:38.106637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-29 12:08:38.106647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-29 12:08:38.106761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:38.107703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:38.108505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2396\n",
            "I1229 12:08:38.109722 140677372331904 saver.py:1284] Restoring parameters from training/model.ckpt-2396\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W1229 12:08:39.286547 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W1229 12:08:39.286844 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 632 variables.\n",
            "I1229 12:08:39.964917 140677372331904 graph_util_impl.py:334] Froze 632 variables.\n",
            "INFO:tensorflow:Converted 632 variables to const ops.\n",
            "I1229 12:08:40.103285 140677372331904 graph_util_impl.py:394] Converted 632 variables to const ops.\n",
            "2019-12-29 12:08:40.312430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:40.313348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-29 12:08:40.313441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-29 12:08:40.313486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-29 12:08:40.313527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-29 12:08:40.313569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-29 12:08:40.313617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-29 12:08:40.313654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-29 12:08:40.313692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-29 12:08:40.313815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:40.314804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:40.315692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-29 12:08:40.315746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-29 12:08:40.315769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-29 12:08:40.315785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-29 12:08:40.315931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:40.316902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-29 12:08:40.317786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W1229 12:08:41.301421 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W1229 12:08:41.302085 140677372331904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W1229 12:08:41.302893 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W1229 12:08:41.303156 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W1229 12:08:41.303564 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W1229 12:08:41.303792 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I1229 12:08:41.304171 140677372331904 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I1229 12:08:41.304346 140677372331904 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "I1229 12:08:41.749897 140677372331904 builder_impl.py:425] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1229 12:08:41.789075 140677372331904 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to inference_graph/pipeline.config\n",
            "I1229 12:08:41.789310 140677372331904 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -r /content/models/research/object_detection/inference_graph /gdrive/My\\ Drive/colabfiles/inference_graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasenbilder190726_01/bild101.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8-djxitLZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasen.mov /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9-03ftAhVmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/models/research/object_detection/Object_detection_image.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMnwzxh-MLlQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "outputId": "e226e695-fc59-423d-ce92-f21f5d365600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = 'bild101.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEsCAIAAABi1XKVAAEAAElEQVR4nFT923IkSZIlCDLLTVVN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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x300 at 0x7F2306C69D68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs2cfgcr1AZ",
        "colab_type": "code",
        "outputId": "f0b6db40-99e7-4d56-91bb-5e35b956d49f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "######## Video Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/16/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on a video.\n",
        "# It draws boxes and scores around the objects of interest in each frame\n",
        "# of the video.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import imutils\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "VIDEO_NAME = 'rasen.mov'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "print(CWD_PATH)\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to video\n",
        "PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Open video file\n",
        "video = cv2.VideoCapture(PATH_TO_VIDEO)\n",
        "print(\"ok\")\n",
        "i = 0\n",
        "while(video.isOpened()):\n",
        "\n",
        "    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]\n",
        "    # i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "    ret, frame = video.read()\n",
        "    if ret==True:\n",
        "        #frame = cv2.imread(\"/gdrive/My Drive/colabfiles/lektion20/DSC01019.JPG\")\n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={image_tensor: frame_expanded})\n",
        "\n",
        "        # Draw the results of the detection (aka 'visulaize the results')\n",
        "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            frame,\n",
        "            np.squeeze(boxes),\n",
        "            np.squeeze(classes).astype(np.int32),\n",
        "            np.squeeze(scores),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.70)\n",
        "\n",
        "        # All the results have been drawn on the frame, so it's time to display it.\n",
        "        \n",
        "        frame = imutils.resize(frame, 400)\n",
        "        cv2_imshow(frame)\n",
        "        #print(boxes)\n",
        "        time.sleep(1)\n",
        "        clear_output()\n",
        "        \n",
        "\n",
        "        # Press 'q' to quit\n",
        "        #print(i)\n",
        "        #i = i + 1\n",
        "        #if cv2.waitKey(1) == ord('q'):\n",
        "        #    break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Clean up\n",
        "print(\"end1\")\n",
        "#video.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(\"end2\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAADhCAIAAABp1HRLAAEAAElEQVR4nCz8x7JuW46liQGYeqlf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\nhusSl30AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x225 at 0x7F23070214E0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-814086d696e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m#print(boxes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}