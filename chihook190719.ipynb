{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/chihook190719.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "e6277acd-4d46-4ef6-fea3-6a4991229e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -R /models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/models': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "8a6978d0-38f2-4fd4-fcea-adbbfe90fc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 28659 (delta 10), reused 0 (delta 0), pack-reused 28641\u001b[K\n",
            "Receiving objects: 100% (28659/28659), 509.55 MiB | 15.73 MiB/s, done.\n",
            "Resolving deltas: 100% (17736/17736), done.\n",
            "Checking out files: 100% (3038/3038), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_lt0De55L-Q",
        "colab_type": "code",
        "outputId": "4669c6ee-5cdd-43b4-9f0c-5749082fd1ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-19 09:14:20--  https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘detection_model_zoo.md’\n",
            "\n",
            "detection_model_zoo     [  <=>               ] 101.11K   182KB/s    in 0.6s    \n",
            "\n",
            "2019-07-19 09:14:22 (182 KB/s) - ‘detection_model_zoo.md’ saved [103539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "a3abd4e8-5640-4b39-d000-5137cf348f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "17da15e7-e19c-4ccb-fba1-910d263f4543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-19 09:14:24--  http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.204.128, 2404:6800:4008:c04::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.204.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149119618 (142M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’\n",
            "\n",
            "faster_rcnn_incepti 100%[===================>] 142.21M   246MB/s    in 0.6s    \n",
            "\n",
            "2019-07-19 09:14:25 (246 MB/s) - ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’ saved [149119618/149119618]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf faster_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "9b0b0941-547b-413c-ab93-90de1951f4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!git clone https://github.com/seraj94ai/Train-Object-Detection-Classifier.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Train-Object-Detection-Classifier'...\n",
            "remote: Enumerating objects: 482, done.\u001b[K\n",
            "remote: Total 482 (delta 0), reused 0 (delta 0), pack-reused 482\u001b[K\n",
            "Receiving objects: 100% (482/482), 30.90 MiB | 9.21 MiB/s, done.\n",
            "Resolving deltas: 100% (234/234), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnJcNmcI8ALN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGihW7NO8Cvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjBVHu28HdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DGe7nAW9BSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "b72cef90-abce-461f-8c47-3cfac8df3b89"
      },
      "source": [
        "ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34manchor_generators\u001b[0m/\n",
            "\u001b[01;34mbox_coders\u001b[0m/\n",
            "\u001b[01;34mbuilders\u001b[0m/\n",
            "CONTRIBUTING.md\n",
            "\u001b[01;34mcore\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/\n",
            "\u001b[01;34mdata_decoders\u001b[0m/\n",
            "\u001b[01;34mdataset_tools\u001b[0m/\n",
            "\u001b[01;34mdockerfiles\u001b[0m/\n",
            "eval_util.py\n",
            "eval_util_test.py\n",
            "exporter.py\n",
            "exporter_test.py\n",
            "export_inference_graph.py\n",
            "export_tflite_ssd_graph_lib.py\n",
            "export_tflite_ssd_graph_lib_test.py\n",
            "export_tflite_ssd_graph.py\n",
            "\u001b[01;34mfaster_rcnn_inception_v2_coco_2018_01_28\u001b[0m/\n",
            "faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
            "\u001b[01;34mg3doc\u001b[0m/\n",
            "\u001b[01;34mimages\u001b[0m/\n",
            "\u001b[01;34minference\u001b[0m/\n",
            "\u001b[01;34minference_graph\u001b[0m/\n",
            "__init__.py\n",
            "inputs.py\n",
            "inputs_test.py\n",
            "\u001b[01;34mlegacy\u001b[0m/\n",
            "\u001b[01;34mmatchers\u001b[0m/\n",
            "\u001b[01;34mmeta_architectures\u001b[0m/\n",
            "\u001b[01;34mmetrics\u001b[0m/\n",
            "model_hparams.py\n",
            "model_lib.py\n",
            "model_lib_test.py\n",
            "model_lib_v2.py\n",
            "model_lib_v2_test.py\n",
            "model_main.py\n",
            "\u001b[01;34mmodels\u001b[0m/\n",
            "model_tpu_main.py\n",
            "object_detection_tutorial.ipynb\n",
            "\u001b[01;34mpredictors\u001b[0m/\n",
            "\u001b[01;34mprotos\u001b[0m/\n",
            "README.md\n",
            "\u001b[01;34msamples\u001b[0m/\n",
            "\u001b[01;34mtest_ckpt\u001b[0m/\n",
            "\u001b[01;34mtest_data\u001b[0m/\n",
            "\u001b[01;34mtest_images\u001b[0m/\n",
            "\u001b[01;34mtpu_exporters\u001b[0m/\n",
            "\u001b[01;34mtraining\u001b[0m/\n",
            "\u001b[01;34mTrain-Object-Detection-Classifier\u001b[0m/\n",
            "\u001b[01;34mutils\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/errs /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/images /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/in /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/Train-Object-Detection-Classifier/out /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/renameFiles.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/Train-Object-Detection-Classifier/xml_to_csv.py /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnSvHWz-3X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!set PYTHONPATH=/content/models:/content/models/research:/content/models/research/slim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEMhWLvdLXKO",
        "colab_type": "code",
        "outputId": "5e519a3b-a059-4d02-8b89-613f5f23f6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo %PYTHONPATH%"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "%PYTHONPATH%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G61sEdQ-Lkaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!set PATH=%PATH%,PYTHONPATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LutDU_-ME6G",
        "colab_type": "code",
        "outputId": "1398b67c-33a1-4934-ef8c-4ba8d28cb915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo %PATH%"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "%PATH%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "19ae0e45-6d23-494b-f0ea-d2dc61725f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "outputId": "8a8e492f-eab9-48fb-f061-818cf71690d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/object_detection\n",
            "copying object_detection/xml_to_csv.py -> build/lib/object_detection\n",
            "copying object_detection/Object_detection_video.py -> build/lib/object_detection\n",
            "copying object_detection/generate_tfrecord.py -> build/lib/object_detection\n",
            "copying object_detection/export_tflite_ssd_graph_lib_test.py -> build/lib/object_detection\n",
            "copying object_detection/model_hparams.py -> build/lib/object_detection\n",
            "copying object_detection/eval_util_test.py -> build/lib/object_detection\n",
            "copying object_detection/model_lib_test.py -> build/lib/object_detection\n",
            "copying object_detection/__init__.py -> build/lib/object_detection\n",
            "copying object_detection/Object_detection_image.py -> build/lib/object_detection\n",
            "copying object_detection/renameFiles.py -> build/lib/object_detection\n",
            "copying object_detection/inputs_test.py -> build/lib/object_detection\n",
            "copying object_detection/exporter.py -> build/lib/object_detection\n",
            "copying object_detection/model_main.py -> build/lib/object_detection\n",
            "copying object_detection/model_lib_v2.py -> build/lib/object_detection\n",
            "copying object_detection/model_lib_v2_test.py -> build/lib/object_detection\n",
            "copying object_detection/export_tflite_ssd_graph.py -> build/lib/object_detection\n",
            "copying object_detection/export_inference_graph.py -> build/lib/object_detection\n",
            "copying object_detection/export_tflite_ssd_graph_lib.py -> build/lib/object_detection\n",
            "copying object_detection/model_tpu_main.py -> build/lib/object_detection\n",
            "copying object_detection/model_lib.py -> build/lib/object_detection\n",
            "copying object_detection/exporter_test.py -> build/lib/object_detection\n",
            "copying object_detection/eval_util.py -> build/lib/object_detection\n",
            "copying object_detection/inputs.py -> build/lib/object_detection\n",
            "creating build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/square_box_coder_test.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/keypoint_box_coder.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/keypoint_box_coder_test.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/__init__.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/mean_stddev_box_coder.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/faster_rcnn_box_coder.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/faster_rcnn_box_coder_test.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/square_box_coder.py -> build/lib/object_detection/box_coders\n",
            "copying object_detection/box_coders/mean_stddev_box_coder_test.py -> build/lib/object_detection/box_coders\n",
            "creating build/lib/object_detection/protos\n",
            "copying object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/keypoint_box_coder_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/anchor_generator_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/graph_rewriter_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/calibration_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/faster_rcnn_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/bipartite_matcher_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/argmax_matcher_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/region_similarity_calculator_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/box_coder_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/matcher_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/string_int_label_map_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/__init__.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/multiscale_anchor_generator_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/box_predictor_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/grid_anchor_generator_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/mean_stddev_box_coder_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/model_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/image_resizer_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/losses_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/hyperparams_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/input_reader_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/square_box_coder_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/preprocessor_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/optimizer_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/ssd_anchor_generator_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/eval_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/ssd_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/post_processing_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/pipeline_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/train_pb2.py -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/lib/object_detection/protos\n",
            "creating build/lib/object_detection/core\n",
            "copying object_detection/core/preprocessor_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/anchor_generator.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_list_ops_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_list.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/batch_multiclass_nms_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/class_agnostic_nms_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/matcher_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_coder.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/standard_fields.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/prefetcher.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/batcher.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/losses.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/data_parser.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_list_ops.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/preprocessor_cache.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/batcher_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/__init__.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/keypoint_ops.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/freezable_batch_norm_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/data_decoder.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/freezable_batch_norm.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/balanced_positive_negative_sampler_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/losses_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/region_similarity_calculator_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/post_processing.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_list_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/multiclass_nms_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_coder_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/keypoint_ops_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/preprocessor.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/balanced_positive_negative_sampler.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/matcher.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/model.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/prefetcher_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/region_similarity_calculator.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/target_assigner.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/minibatch_sampler_test.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/minibatch_sampler.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/box_predictor.py -> build/lib/object_detection/core\n",
            "copying object_detection/core/target_assigner_test.py -> build/lib/object_detection/core\n",
            "creating build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/mask_rcnn_keras_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/__init__.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/convolutional_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/rfcn_keras_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/convolutional_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/convolutional_keras_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/convolutional_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/mask_rcnn_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/mask_rcnn_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/rfcn_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/rfcn_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/rfcn_box_predictor.py -> build/lib/object_detection/predictors\n",
            "copying object_detection/predictors/mask_rcnn_keras_box_predictor_test.py -> build/lib/object_detection/predictors\n",
            "creating build/lib/object_detection/matchers\n",
            "copying object_detection/matchers/argmax_matcher.py -> build/lib/object_detection/matchers\n",
            "copying object_detection/matchers/__init__.py -> build/lib/object_detection/matchers\n",
            "copying object_detection/matchers/argmax_matcher_test.py -> build/lib/object_detection/matchers\n",
            "copying object_detection/matchers/bipartite_matcher.py -> build/lib/object_detection/matchers\n",
            "copying object_detection/matchers/bipartite_matcher_test.py -> build/lib/object_detection/matchers\n",
            "creating build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/__init__.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/multiple_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/multiple_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/flexible_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/flexible_grid_anchor_generator_test.py -> build/lib/object_detection/anchor_generators\n",
            "copying object_detection/anchor_generators/multiscale_grid_anchor_generator.py -> build/lib/object_detection/anchor_generators\n",
            "creating build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/ssd_meta_arch_test_lib.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/ssd_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/rfcn_meta_arch.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/__init__.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/ssd_meta_arch.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/rfcn_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/faster_rcnn_meta_arch_test.py -> build/lib/object_detection/meta_architectures\n",
            "copying object_detection/meta_architectures/faster_rcnn_meta_arch.py -> build/lib/object_detection/meta_architectures\n",
            "creating build/lib/object_detection/utils\n",
            "copying object_detection/utils/dataset_util.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/learning_schedules.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/metrics_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/per_image_evaluation_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/model_util.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/static_shape.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_mask_list_ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/label_map_util_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/test_utils.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_mask_list_ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_list_ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_mask_ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/spatial_transform_ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/category_util.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/config_util.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/context_manager.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/vrd_evaluation_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/label_map_util.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/visualization_utils_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/config_util_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_list.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/object_detection_evaluation_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/json_utils_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/__init__.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_list_ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/per_image_evaluation.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/ops.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_mask_list.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/dataset_util_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/model_util_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/vrd_evaluation.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/learning_schedules_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/shape_utils.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_list_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_mask_ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/np_box_mask_list_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/object_detection_evaluation.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/shape_utils_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/metrics.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/per_image_vrd_evaluation.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/visualization_utils.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/test_case.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/variables_helper_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/context_manager_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/json_utils.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/variables_helper.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/spatial_transform_ops_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/static_shape_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/autoaugment_utils.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/test_utils_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/per_image_vrd_evaluation_test.py -> build/lib/object_detection/utils\n",
            "copying object_detection/utils/category_util_test.py -> build/lib/object_detection/utils\n",
            "creating build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_inception_v3_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_inception_v2_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_nas_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/__init__.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_pnasnet_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v2_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_pnasnet_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_v2_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/feature_map_generators.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_inception_v2_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_inception_v3_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_nas_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_pnas_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/feature_map_generators_test.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_pnas_feature_extractor.py -> build/lib/object_detection/models\n",
            "copying object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py -> build/lib/object_detection/models\n",
            "creating build/lib/object_detection/inference\n",
            "copying object_detection/inference/__init__.py -> build/lib/object_detection/inference\n",
            "copying object_detection/inference/detection_inference.py -> build/lib/object_detection/inference\n",
            "copying object_detection/inference/infer_detections.py -> build/lib/object_detection/inference\n",
            "copying object_detection/inference/detection_inference_test.py -> build/lib/object_detection/inference\n",
            "creating build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/evaluator.py -> build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/__init__.py -> build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/eval.py -> build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/trainer.py -> build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/train.py -> build/lib/object_detection/legacy\n",
            "copying object_detection/legacy/trainer_test.py -> build/lib/object_detection/legacy\n",
            "creating build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/utils.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/__init__.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/faster_rcnn.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/export_saved_model_tpu.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/utils_test.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/ssd.py -> build/lib/object_detection/tpu_exporters\n",
            "copying object_detection/tpu_exporters/export_saved_model_tpu_lib.py -> build/lib/object_detection/tpu_exporters\n",
            "creating build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/tf_example_parser.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/io_utils.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/coco_evaluation.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/__init__.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/tf_example_parser_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/coco_tools.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/calibration_evaluation.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/offline_eval_map_corloc_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/offline_eval_map_corloc.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_vrd_challenge_evaluation.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/coco_tools_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_challenge_evaluation_utils.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_challenge_evaluation_utils_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/coco_evaluation_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_challenge_evaluation.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/calibration_metrics.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/calibration_metrics_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/calibration_evaluation_test.py -> build/lib/object_detection/metrics\n",
            "copying object_detection/metrics/oid_vrd_challenge_evaluation_utils.py -> build/lib/object_detection/metrics\n",
            "creating build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/tf_record_creation_util_test.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_kitti_tf_record_test.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_kitti_tf_record.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/oid_tfrecord_creation_test.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/__init__.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_pascal_tf_record.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/tf_record_creation_util.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_coco_tf_record.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_pet_tf_record.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/oid_hierarchical_labels_expansion.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_oid_tf_record.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/oid_tfrecord_creation.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_pascal_tf_record_test.py -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/create_coco_tf_record_test.py -> build/lib/object_detection/dataset_tools\n",
            "creating build/lib/object_detection/builders\n",
            "copying object_detection/builders/model_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/losses_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/image_resizer_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/region_similarity_calculator_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/box_coder_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/region_similarity_calculator_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/hyperparams_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/anchor_generator_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/calibration_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/optimizer_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/box_predictor_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/post_processing_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/__init__.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/preprocessor_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/box_coder_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/image_resizer_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/optimizer_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/input_reader_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/dataset_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/input_reader_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/box_predictor_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/model_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/graph_rewriter_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/dataset_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/hyperparams_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/preprocessor_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/graph_rewriter_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/calibration_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/anchor_generator_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/losses_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/post_processing_builder_test.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/matcher_builder.py -> build/lib/object_detection/builders\n",
            "copying object_detection/builders/matcher_builder_test.py -> build/lib/object_detection/builders\n",
            "creating build/lib/object_detection/data_decoders\n",
            "copying object_detection/data_decoders/tf_example_decoder_test.py -> build/lib/object_detection/data_decoders\n",
            "copying object_detection/data_decoders/__init__.py -> build/lib/object_detection/data_decoders\n",
            "copying object_detection/data_decoders/tf_example_decoder.py -> build/lib/object_detection/data_decoders\n",
            "creating build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_box_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keypoint_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keypoint_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/box_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_mask_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/__init__.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_mask_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/box_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/class_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/class_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_class_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_box_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/mask_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/mask_head.py -> build/lib/object_detection/predictors/heads\n",
            "copying object_detection/predictors/heads/keras_class_head_test.py -> build/lib/object_detection/predictors/heads\n",
            "creating build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/mobilenet_v1.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/test_utils.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/inception_resnet_v2_test.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/__init__.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/inception_resnet_v2.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/mobilenet_v1_test.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/resnet_v1.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/model_utils.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/resnet_v1_test.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/mobilenet_v2_test.py -> build/lib/object_detection/models/keras_models\n",
            "copying object_detection/models/keras_models/mobilenet_v2.py -> build/lib/object_detection/models/keras_models\n",
            "creating build/lib/object_detection/tpu_exporters/testdata\n",
            "copying object_detection/tpu_exporters/testdata/__init__.py -> build/lib/object_detection/tpu_exporters/testdata\n",
            "running egg_info\n",
            "creating object_detection.egg-info\n",
            "writing object_detection.egg-info/PKG-INFO\n",
            "writing dependency_links to object_detection.egg-info/dependency_links.txt\n",
            "writing requirements to object_detection.egg-info/requires.txt\n",
            "writing top-level names to object_detection.egg-info/top_level.txt\n",
            "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
            "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
            "copying object_detection/CONTRIBUTING.md -> build/lib/object_detection\n",
            "copying object_detection/README.md -> build/lib/object_detection\n",
            "copying object_detection/object_detection_tutorial.ipynb -> build/lib/object_detection\n",
            "creating build/lib/object_detection/data\n",
            "copying object_detection/data/ava_label_map_v2.1.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/face_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/fgvc_2854_classes_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/kitti_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/mscoco_complete_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/mscoco_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/mscoco_minival_ids.txt -> build/lib/object_detection/data\n",
            "copying object_detection/data/oid_bbox_trainable_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/oid_v4_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/pascal_label_map.pbtxt -> build/lib/object_detection/data\n",
            "copying object_detection/data/pet_label_map.pbtxt -> build/lib/object_detection/data\n",
            "creating build/lib/object_detection/dockerfiles\n",
            "creating build/lib/object_detection/dockerfiles/android\n",
            "copying object_detection/dockerfiles/android/Dockerfile -> build/lib/object_detection/dockerfiles/android\n",
            "copying object_detection/dockerfiles/android/README.md -> build/lib/object_detection/dockerfiles/android\n",
            "creating build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/challenge_evaluation.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/configuring_jobs.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/defining_your_own_model.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/detection_model_zoo.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/evaluation_protocols.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/exporting_models.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/faq.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/installation.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/instance_segmentation.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/oid_inference_and_evaluation.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/preparing_inputs.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/running_locally.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/running_notebook.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/running_on_cloud.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/running_on_mobile_tensorflowlite.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/running_pets.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/tpu_compatibility.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/tpu_exporters.md -> build/lib/object_detection/g3doc\n",
            "copying object_detection/g3doc/using_your_own_dataset.md -> build/lib/object_detection/g3doc\n",
            "creating build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/dataset_explorer.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/dogs_detections_output.jpg -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/example_cat.jpg -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/groupof_case_eval.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/kites_detections_output.jpg -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/kites_with_segment_overlay.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/nongroupof_case_eval.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/oid_bus_72e19c28aac34ed8.jpg -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/oid_monkey_3b4168c89cecbc5b.jpg -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/oxford_pet.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/tensorboard.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/tensorboard2.png -> build/lib/object_detection/g3doc/img\n",
            "copying object_detection/g3doc/img/tf-od-api-logo.png -> build/lib/object_detection/g3doc/img\n",
            "creating build/lib/object_detection/samples\n",
            "creating build/lib/object_detection/samples/cloud\n",
            "copying object_detection/samples/cloud/cloud.yml -> build/lib/object_detection/samples/cloud\n",
            "creating build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/facessd_mobilenet_v2_quantized_320x320_open_image_v4.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_inception_v2_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_nas_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_ava_v2.1.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_fgvc.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_kitti.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet101_voc07.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet152_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet152_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet50_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet50_fgvc.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/faster_rcnn_resnet50_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/mask_rcnn_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/mask_rcnn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/rfcn_resnet101_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/rfcn_resnet101_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_inception_v2_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_inception_v2_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_inception_v3_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_pets.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_pets_keras.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config -> build/lib/object_detection/samples/configs\n",
            "copying object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config -> build/lib/object_detection/samples/configs\n",
            "creating build/lib/object_detection/test_ckpt\n",
            "copying object_detection/test_ckpt/ssd_inception_v2.pb -> build/lib/object_detection/test_ckpt\n",
            "creating build/lib/object_detection/test_data\n",
            "copying object_detection/test_data/pets_examples.record -> build/lib/object_detection/test_data\n",
            "creating build/lib/object_detection/test_images\n",
            "copying object_detection/test_images/image1.jpg -> build/lib/object_detection/test_images\n",
            "copying object_detection/test_images/image2.jpg -> build/lib/object_detection/test_images\n",
            "copying object_detection/test_images/image_info.txt -> build/lib/object_detection/test_images\n",
            "copying object_detection/protos/anchor_generator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/argmax_matcher.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/bipartite_matcher.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/box_coder.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/box_predictor.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/calibration.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/eval.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/faster_rcnn.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/faster_rcnn_box_coder.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/flexible_grid_anchor_generator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/graph_rewriter.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/grid_anchor_generator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/hyperparams.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/image_resizer.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/input_reader.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/keypoint_box_coder.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/losses.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/matcher.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/mean_stddev_box_coder.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/model.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/multiscale_anchor_generator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/optimizer.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/pipeline.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/post_processing.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/preprocessor.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/region_similarity_calculator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/square_box_coder.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/ssd.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/ssd_anchor_generator.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/string_int_label_map.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/protos/train.proto -> build/lib/object_detection/protos\n",
            "copying object_detection/dataset_tools/create_pycocotools_package.sh -> build/lib/object_detection/dataset_tools\n",
            "copying object_detection/dataset_tools/download_and_preprocess_mscoco.sh -> build/lib/object_detection/dataset_tools\n",
            "creating build/lib/object_detection/models/keras_models/base_models\n",
            "copying object_detection/models/keras_models/base_models/original_mobilenet_v2.py -> build/lib/object_detection/models/keras_models/base_models\n",
            "creating build/lib/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "copying object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config -> build/lib/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "creating build/lib/object_detection/tpu_exporters/testdata/ssd\n",
            "copying object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config -> build/lib/object_detection/tpu_exporters/testdata/ssd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "outputId": "28892fbb-c134-4fb3-d9b0-86baf1f77de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing object_detection.egg-info/PKG-INFO\n",
            "writing dependency_links to object_detection.egg-info/dependency_links.txt\n",
            "writing requirements to object_detection.egg-info/requires.txt\n",
            "writing top-level names to object_detection.egg-info/top_level.txt\n",
            "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/square_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/keypoint_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/keypoint_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/mean_stddev_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/faster_rcnn_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/faster_rcnn_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/square_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib/object_detection/box_coders/mean_stddev_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/input_reader.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/flexible_grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/mean_stddev_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/keypoint_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/post_processing.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/graph_rewriter_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/calibration_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/faster_rcnn_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/hyperparams.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/bipartite_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/graph_rewriter.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/multiscale_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/preprocessor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/argmax_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/region_similarity_calculator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/keypoint_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/box_predictor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/region_similarity_calculator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/string_int_label_map_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/multiscale_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/square_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/pipeline.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/image_resizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/losses.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/box_predictor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/mean_stddev_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/model_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/faster_rcnn.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/image_resizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/faster_rcnn_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/losses_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/calibration.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/argmax_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/hyperparams_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/input_reader_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/square_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/eval.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/ssd.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/preprocessor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/optimizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/optimizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/ssd_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/ssd_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/eval_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/ssd_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/string_int_label_map.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/model.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/post_processing_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/train.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/bipartite_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/pipeline_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/train_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib/object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/preprocessor_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_list.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/batch_multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/class_agnostic_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/standard_fields.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/prefetcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/batcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/losses.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/data_parser.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/preprocessor_cache.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/batcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/keypoint_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/freezable_batch_norm_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/data_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/freezable_batch_norm.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/balanced_positive_negative_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/losses_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/region_similarity_calculator_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/post_processing.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/keypoint_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/preprocessor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/balanced_positive_negative_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/matcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/model.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/prefetcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/region_similarity_calculator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/target_assigner.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/minibatch_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/minibatch_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/core/target_assigner_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib/object_detection/xml_to_csv.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/Object_detection_video.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/mask_rcnn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/convolutional_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/rfcn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/convolutional_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/convolutional_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keypoint_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keypoint_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/heads/keras_class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib/object_detection/predictors/convolutional_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/mask_rcnn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/mask_rcnn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/rfcn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/rfcn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/rfcn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib/object_detection/generate_tfrecord.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_data\n",
            "copying build/lib/object_detection/test_data/pets_examples.record -> build/bdist.linux-x86_64/egg/object_detection/test_data\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib/object_detection/matchers/argmax_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib/object_detection/matchers/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib/object_detection/matchers/argmax_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib/object_detection/matchers/bipartite_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib/object_detection/matchers/bipartite_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/multiple_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/flexible_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/anchor_generators/multiscale_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib/object_detection/export_tflite_ssd_graph_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/ssd_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/ssd_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/rfcn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/ssd_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/rfcn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/meta_architectures/faster_rcnn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib/object_detection/model_hparams.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib/object_detection/dockerfiles/android/Dockerfile -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib/object_detection/dockerfiles/android/README.md -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib/object_detection/eval_util_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/model_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib/object_detection/test_images/image_info.txt -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib/object_detection/test_images/image1.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib/object_detection/test_images/image2.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib/object_detection/__init__.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/Object_detection_image.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/renameFiles.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/inputs_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/exporter.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/challenge_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/detection_model_zoo.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/oid_inference_and_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/preparing_inputs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/running_pets.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/running_on_mobile_tensorflowlite.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/faq.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/tpu_compatibility.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/evaluation_protocols.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/tpu_exporters.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/instance_segmentation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/dogs_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/tf-od-api-logo.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/kites_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/tensorboard2.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/dataset_explorer.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/tensorboard.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/oid_bus_72e19c28aac34ed8.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/kites_with_segment_overlay.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/oxford_pet.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/oid_monkey_3b4168c89cecbc5b.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/groupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/nongroupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/img/example_cat.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib/object_detection/g3doc/defining_your_own_model.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/configuring_jobs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/running_locally.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/installation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/using_your_own_dataset.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/running_on_cloud.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/exporting_models.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib/object_detection/g3doc/running_notebook.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/dataset_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/learning_schedules.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/per_image_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/model_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/static_shape.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_mask_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/label_map_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_mask_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_mask_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/spatial_transform_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/category_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/config_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/context_manager.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/label_map_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/visualization_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/config_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/object_detection_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/json_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/per_image_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_mask_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/dataset_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/model_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/learning_schedules_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/shape_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_mask_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/np_box_mask_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/object_detection_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/shape_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/metrics.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/per_image_vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/visualization_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/test_case.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/variables_helper_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/context_manager_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/json_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/variables_helper.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/spatial_transform_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/static_shape_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/autoaugment_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/test_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/per_image_vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib/object_detection/utils/category_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_inception_v3_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_nas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_pnasnet_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_pnasnet_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/feature_map_generators.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_inception_v3_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/mobilenet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/inception_resnet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/mobilenet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/resnet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/model_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n",
            "copying build/lib/object_detection/models/keras_models/base_models/original_mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n",
            "copying build/lib/object_detection/models/keras_models/resnet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/mobilenet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/keras_models/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib/object_detection/models/faster_rcnn_nas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/feature_map_generators_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_pnas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib/object_detection/model_main.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/model_lib_v2.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/CONTRIBUTING.md -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib/object_detection/inference/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib/object_detection/inference/detection_inference.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib/object_detection/inference/infer_detections.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib/object_detection/inference/detection_inference_test.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib/object_detection/model_lib_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/export_tflite_ssd_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/export_inference_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/evaluator.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/eval.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/trainer.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/train.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib/object_detection/legacy/trainer_test.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_nas_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_kitti.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet152_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_pets_keras.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_ava_v2.1.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/rfcn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_inception_v3_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/facessd_mobilenet_v2_quantized_320x320_open_image_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet152_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/mask_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/rfcn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_voc07.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet50_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib/object_detection/samples/configs/faster_rcnn_resnet101_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n",
            "copying build/lib/object_detection/samples/cloud/cloud.yml -> build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n",
            "copying build/lib/object_detection/export_tflite_ssd_graph_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/model_tpu_main.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/pascal_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/kitti_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/mscoco_minival_ids.txt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/oid_bbox_trainable_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/mscoco_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/mscoco_complete_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/face_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/ava_label_map_v2.1.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/fgvc_2854_classes_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/pet_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/data/oid_v4_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib/object_detection/model_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/utils.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/faster_rcnn.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/ssd.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib/object_detection/tpu_exporters/export_saved_model_tpu_lib.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "copying build/lib/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "copying build/lib/object_detection/tpu_exporters/testdata/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n",
            "copying build/lib/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n",
            "copying build/lib/object_detection/object_detection_tutorial.ipynb -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n",
            "copying build/lib/object_detection/test_ckpt/ssd_inception_v2.pb -> build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n",
            "copying build/lib/object_detection/exporter_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/eval_util.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/tf_example_parser.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/io_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/coco_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/tf_example_parser_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/coco_tools.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/calibration_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/offline_eval_map_corloc_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/offline_eval_map_corloc.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/coco_tools_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/coco_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/calibration_metrics.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/calibration_metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/calibration_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/tf_record_creation_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_pycocotools_package.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_kitti_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_kitti_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/oid_tfrecord_creation_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_pascal_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/tf_record_creation_util.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_coco_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_pet_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_oid_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/oid_tfrecord_creation.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_pascal_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/create_coco_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib/object_detection/dataset_tools/download_and_preprocess_mscoco.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/model_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/losses_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/image_resizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/region_similarity_calculator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/box_coder_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/region_similarity_calculator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/hyperparams_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/anchor_generator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/calibration_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/optimizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/box_predictor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/post_processing_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/preprocessor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/box_coder_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/image_resizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/optimizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/input_reader_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/dataset_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/input_reader_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/box_predictor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/model_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/graph_rewriter_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/dataset_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/hyperparams_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/preprocessor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/graph_rewriter_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/calibration_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/anchor_generator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/losses_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/post_processing_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/matcher_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib/object_detection/builders/matcher_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib/object_detection/data_decoders/tf_example_decoder_test.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib/object_detection/data_decoders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib/object_detection/data_decoders/tf_example_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib/object_detection/README.md -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib/object_detection/inputs.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder_test.py to square_box_coder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder.py to keypoint_box_coder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder_test.py to keypoint_box_coder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder.py to mean_stddev_box_coder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder.py to faster_rcnn_box_coder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder_test.py to faster_rcnn_box_coder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder.py to square_box_coder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder_test.py to mean_stddev_box_coder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_box_coder_pb2.py to faster_rcnn_box_coder_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/keypoint_box_coder_pb2.py to keypoint_box_coder_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/anchor_generator_pb2.py to anchor_generator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/graph_rewriter_pb2.py to graph_rewriter_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/calibration_pb2.py to calibration_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_pb2.py to faster_rcnn_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/bipartite_matcher_pb2.py to bipartite_matcher_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/argmax_matcher_pb2.py to argmax_matcher_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/region_similarity_calculator_pb2.py to region_similarity_calculator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_coder_pb2.py to box_coder_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/matcher_pb2.py to matcher_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/string_int_label_map_pb2.py to string_int_label_map_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/multiscale_anchor_generator_pb2.py to multiscale_anchor_generator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_predictor_pb2.py to box_predictor_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/grid_anchor_generator_pb2.py to grid_anchor_generator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/mean_stddev_box_coder_pb2.py to mean_stddev_box_coder_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/model_pb2.py to model_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/image_resizer_pb2.py to image_resizer_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/losses_pb2.py to losses_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/hyperparams_pb2.py to hyperparams_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/input_reader_pb2.py to input_reader_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/square_box_coder_pb2.py to square_box_coder_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/preprocessor_pb2.py to preprocessor_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/optimizer_pb2.py to optimizer_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_anchor_generator_pb2.py to ssd_anchor_generator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/eval_pb2.py to eval_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_pb2.py to ssd_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/post_processing_pb2.py to post_processing_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/pipeline_pb2.py to pipeline_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/train_pb2.py to train_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/flexible_grid_anchor_generator_pb2.py to flexible_grid_anchor_generator_pb2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_test.py to preprocessor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/anchor_generator.py to anchor_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops_test.py to box_list_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list.py to box_list.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batch_multiclass_nms_test.py to batch_multiclass_nms_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/class_agnostic_nms_test.py to class_agnostic_nms_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher_test.py to matcher_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder.py to box_coder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/standard_fields.py to standard_fields.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher.py to prefetcher.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher.py to batcher.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses.py to losses.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_parser.py to data_parser.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops.py to box_list_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_cache.py to preprocessor_cache.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher_test.py to batcher_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops.py to keypoint_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm_test.py to freezable_batch_norm_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_decoder.py to data_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm.py to freezable_batch_norm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler_test.py to balanced_positive_negative_sampler_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses_test.py to losses_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator_test.py to region_similarity_calculator_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/post_processing.py to post_processing.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_test.py to box_list_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/multiclass_nms_test.py to multiclass_nms_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder_test.py to box_coder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops_test.py to keypoint_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor.py to preprocessor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler.py to balanced_positive_negative_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher.py to matcher.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/model.py to model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher_test.py to prefetcher_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator.py to region_similarity_calculator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner.py to target_assigner.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler_test.py to minibatch_sampler_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler.py to minibatch_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_predictor.py to box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner_test.py to target_assigner_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/xml_to_csv.py to xml_to_csv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/Object_detection_video.py to Object_detection_video.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor.py to mask_rcnn_keras_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor.py to convolutional_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor.py to rfcn_keras_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor_test.py to convolutional_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor.py to convolutional_keras_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head.py to keras_box_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head.py to keypoint_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head_test.py to keypoint_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head_test.py to box_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head.py to keras_mask_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head_test.py to keras_mask_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head.py to box_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head_test.py to class_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head.py to class_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head.py to keras_class_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head_test.py to keras_box_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/head.py to head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head_test.py to mask_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head.py to mask_head.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head_test.py to keras_class_head_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor_test.py to convolutional_keras_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor.py to mask_rcnn_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor_test.py to mask_rcnn_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor_test.py to rfcn_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor_test.py to rfcn_keras_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor.py to rfcn_box_predictor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py to mask_rcnn_keras_box_predictor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/generate_tfrecord.py to generate_tfrecord.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher.py to argmax_matcher.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher_test.py to argmax_matcher_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher.py to bipartite_matcher.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher_test.py to bipartite_matcher_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator.py to grid_anchor_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator_test.py to grid_anchor_generator_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py to multiple_grid_anchor_generator_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator.py to multiple_grid_anchor_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator.py to flexible_grid_anchor_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py to multiscale_grid_anchor_generator_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py to flexible_grid_anchor_generator_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator.py to multiscale_grid_anchor_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib_test.py to export_tflite_ssd_graph_lib_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test_lib.py to ssd_meta_arch_test_lib.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test.py to ssd_meta_arch_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch.py to rfcn_meta_arch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch.py to ssd_meta_arch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch_test.py to rfcn_meta_arch_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py to faster_rcnn_meta_arch_test_lib.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py to faster_rcnn_meta_arch_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py to faster_rcnn_meta_arch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_hparams.py to model_hparams.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util_test.py to eval_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_test.py to model_lib_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/Object_detection_image.py to Object_detection_image.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/renameFiles.py to renameFiles.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs_test.py to inputs_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter.py to exporter.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util.py to dataset_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules.py to learning_schedules.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics_test.py to metrics_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation_test.py to per_image_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops_test.py to ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util.py to model_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape.py to static_shape.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops_test.py to np_box_mask_list_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util_test.py to label_map_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops.py to np_box_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils.py to test_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops.py to np_box_mask_list_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops.py to np_box_list_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops.py to np_mask_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops.py to spatial_transform_ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util.py to category_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util.py to config_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager.py to context_manager.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation_test.py to vrd_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util.py to label_map_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils_test.py to visualization_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util_test.py to config_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list.py to np_box_list.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation_test.py to object_detection_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils_test.py to json_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops_test.py to np_box_list_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation.py to per_image_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops.py to ops.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list.py to np_box_mask_list.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util_test.py to dataset_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util_test.py to model_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation.py to vrd_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules_test.py to learning_schedules_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils.py to shape_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_test.py to np_box_list_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops_test.py to np_box_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops_test.py to np_mask_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_test.py to np_box_mask_list_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation.py to object_detection_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils_test.py to shape_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation.py to per_image_vrd_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils.py to visualization_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_case.py to test_case.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper_test.py to variables_helper_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager_test.py to context_manager_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils.py to json_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper.py to variables_helper.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops_test.py to spatial_transform_ops_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape_test.py to static_shape_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/autoaugment_utils.py to autoaugment_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils_test.py to test_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation_test.py to per_image_vrd_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util_test.py to category_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py to ssd_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py to ssd_mobilenet_v1_fpn_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py to ssd_resnet_v1_ppn_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor.py to ssd_inception_v3_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor.py to ssd_mobilenet_v1_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor_test.py to ssd_inception_v2_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py to ssd_resnet_v1_ppn_feature_extractor_testbase.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py to ssd_mobilenet_v1_fpn_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor.py to faster_rcnn_nas_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_feature_extractor_test.py to ssd_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py to faster_rcnn_resnet_v1_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py to ssd_mobilenet_v2_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py to faster_rcnn_mobilenet_v1_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor.py to ssd_pnasnet_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py to embedded_ssd_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py to ssd_mobilenet_v2_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py to ssd_mobilenet_v1_ppn_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py to ssd_resnet_v1_fpn_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py to ssd_mobilenet_v2_fpn_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py to ssd_mobilenet_v2_fpn_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py to ssd_mobilenet_v2_fpn_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py to ssd_resnet_v1_fpn_feature_extractor_testbase.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor.py to ssd_mobilenet_v2_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor_test.py to ssd_pnasnet_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py to ssd_mobilenet_v1_ppn_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py to ssd_mobilenet_v1_fpn_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py to faster_rcnn_resnet_v1_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py to faster_rcnn_inception_v2_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py to embedded_ssd_mobilenet_v1_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py to ssd_resnet_v1_ppn_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators.py to feature_map_generators.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor.py to ssd_inception_v2_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py to faster_rcnn_mobilenet_v1_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor_test.py to ssd_inception_v3_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py to ssd_resnet_v1_fpn_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py to ssd_mobilenet_v1_keras_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1.py to mobilenet_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/test_utils.py to test_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2_test.py to inception_resnet_v2_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2.py to inception_resnet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1_test.py to mobilenet_v1_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1.py to resnet_v1.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/model_utils.py to model_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models/original_mobilenet_v2.py to original_mobilenet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1_test.py to resnet_v1_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2_test.py to mobilenet_v2_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2.py to mobilenet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor_test.py to faster_rcnn_nas_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py to faster_rcnn_pnas_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py to ssd_resnet_v1_fpn_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py to faster_rcnn_inception_v2_feature_extractor_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators_test.py to feature_map_generators_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor.py to faster_rcnn_pnas_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py to faster_rcnn_inception_resnet_v2_feature_extractor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_main.py to model_main.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2.py to model_lib_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference.py to detection_inference.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/infer_detections.py to infer_detections.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference_test.py to detection_inference_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2_test.py to model_lib_v2_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph.py to export_tflite_ssd_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_inference_graph.py to export_inference_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/evaluator.py to evaluator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/eval.py to eval.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer.py to trainer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer_test.py to trainer_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib.py to export_tflite_ssd_graph_lib.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_tpu_main.py to model_tpu_main.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib.py to model_lib.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py to export_saved_model_tpu_lib_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/faster_rcnn.py to faster_rcnn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu.py to export_saved_model_tpu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils_test.py to utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/ssd.py to ssd.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib.py to export_saved_model_tpu_lib.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter_test.py to exporter_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util.py to eval_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser.py to tf_example_parser.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/io_utils.py to io_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation.py to coco_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py to oid_vrd_challenge_evaluation_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser_test.py to tf_example_parser_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools.py to coco_tools.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation.py to calibration_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc_test.py to offline_eval_map_corloc_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc.py to offline_eval_map_corloc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation.py to oid_vrd_challenge_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools_test.py to coco_tools_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils.py to oid_challenge_evaluation_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils_test.py to oid_challenge_evaluation_utils_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation_test.py to coco_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation.py to oid_challenge_evaluation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics.py to calibration_metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics_test.py to calibration_metrics_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation_test.py to calibration_evaluation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py to oid_vrd_challenge_evaluation_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util_test.py to tf_record_creation_util_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record_test.py to create_kitti_tf_record_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record.py to create_kitti_tf_record.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation_test.py to oid_tfrecord_creation_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py to oid_hierarchical_labels_expansion_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record.py to create_pascal_tf_record.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util.py to tf_record_creation_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record.py to create_coco_tf_record.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pet_tf_record.py to create_pet_tf_record.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py to oid_hierarchical_labels_expansion.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_oid_tf_record.py to create_oid_tf_record.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation.py to oid_tfrecord_creation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record_test.py to create_pascal_tf_record_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record_test.py to create_coco_tf_record_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder_test.py to model_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder_test.py to losses_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder_test.py to image_resizer_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder_test.py to region_similarity_calculator_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder_test.py to box_coder_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder.py to region_similarity_calculator_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder_test.py to hyperparams_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder_test.py to anchor_generator_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder_test.py to calibration_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder.py to optimizer_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder_test.py to box_predictor_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder.py to post_processing_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder.py to preprocessor_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder.py to box_coder_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder.py to image_resizer_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder_test.py to optimizer_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder_test.py to input_reader_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder.py to dataset_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder.py to input_reader_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder.py to box_predictor_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder.py to model_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder_test.py to graph_rewriter_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder_test.py to dataset_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder.py to hyperparams_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder_test.py to preprocessor_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder.py to graph_rewriter_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder.py to calibration_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder.py to anchor_generator_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder.py to losses_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder_test.py to post_processing_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder.py to matcher_builder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder_test.py to matcher_builder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder_test.py to tf_example_decoder_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder.py to tf_example_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs.py to inputs.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "object_detection.core.__pycache__.preprocessor.cpython-36: module MAY be using inspect.stack\n",
            "object_detection.utils.__pycache__.autoaugment_utils.cpython-36: module MAY be using inspect.stack\n",
            "creating dist\n",
            "creating 'dist/object_detection-0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing object_detection-0.1-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg\n",
            "Extracting object_detection-0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding object-detection 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg\n",
            "Processing dependencies for object-detection==0.1\n",
            "Searching for Cython==0.29.12\n",
            "Best match: Cython 0.29.12\n",
            "Adding Cython 0.29.12 to easy-install.pth file\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.0.3\n",
            "Best match: matplotlib 3.0.3\n",
            "Adding matplotlib 3.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Pillow==4.3.0\n",
            "Best match: Pillow 4.3.0\n",
            "Adding Pillow 4.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.4\n",
            "Best match: numpy 1.16.4\n",
            "Adding numpy 1.16.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.0\n",
            "Best match: pyparsing 2.4.0\n",
            "Adding pyparsing 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.5.3\n",
            "Best match: python-dateutil 2.5.3\n",
            "Adding python-dateutil 2.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.1.0\n",
            "Best match: kiwisolver 1.1.0\n",
            "Adding kiwisolver 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for olefile==0.46\n",
            "Best match: olefile 0.46\n",
            "Adding olefile 0.46 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==41.0.1\n",
            "Best match: setuptools 41.0.1\n",
            "Adding setuptools 41.0.1 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for object-detection==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "outputId": "f356c149-61f5-4df9-c1ae-012e19ffe219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLGxacwFARW",
        "colab_type": "code",
        "outputId": "840162e3-10eb-4b6a-ffbc-83eb8c086855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!python3 xml_to_csv.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VfI6dZGHeFp",
        "colab_type": "code",
        "outputId": "7d570d6a-9878-4a7a-a893-bb236baebcf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'chinook':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "ac60df6e-5b4c-471d-eb42-5488f68da104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 09:15:42.017018 139745660753792 deprecation_wrapper.py:119] From generate_tfrecord.py:101: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0719 09:15:42.017667 139745660753792 deprecation_wrapper.py:119] From generate_tfrecord.py:87: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0719 09:15:42.100731 139745660753792 deprecation_wrapper.py:119] From generate_tfrecord.py:46: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "29ae5351-e0e4-4f08-e9c0-63b141c121a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 09:15:46.267346 140051359754112 deprecation_wrapper.py:119] From generate_tfrecord.py:101: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0719 09:15:46.267997 140051359754112 deprecation_wrapper.py:119] From generate_tfrecord.py:87: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0719 09:15:46.315664 140051359754112 deprecation_wrapper.py:119] From generate_tfrecord.py:46: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "89806fe1-6675-4f20-f50a-6d2c93462556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'chinook'\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyuEP-WUUF3y",
        "colab_type": "code",
        "outputId": "759e3eb5-89c7-42fe-e31a-d908549a6a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/faster_rcnn_inception_v2_pets.config\n",
        "\n",
        "# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 1\n",
        "    image_resizer {\n",
        "      keep_aspect_ratio_resizer {\n",
        "        min_dimension: 600\n",
        "        max_dimension: 1024\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'faster_rcnn_inception_v2'\n",
        "      first_stage_features_stride: 16\n",
        "    }\n",
        "    first_stage_anchor_generator {\n",
        "      grid_anchor_generator {\n",
        "        scales: [0.25, 0.5, 1.0, 2.0]\n",
        "        aspect_ratios: [0.5, 1.0, 2.0]\n",
        "        height_stride: 16\n",
        "        width_stride: 16\n",
        "      }\n",
        "    }\n",
        "    first_stage_box_predictor_conv_hyperparams {\n",
        "      op: CONV\n",
        "      regularizer {\n",
        "        l2_regularizer {\n",
        "          weight: 0.0\n",
        "        }\n",
        "      }\n",
        "      initializer {\n",
        "        truncated_normal_initializer {\n",
        "          stddev: 0.01\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    first_stage_nms_score_threshold: 0.0\n",
        "    first_stage_nms_iou_threshold: 0.7\n",
        "    first_stage_max_proposals: 300\n",
        "    first_stage_localization_loss_weight: 2.0\n",
        "    first_stage_objectness_loss_weight: 1.0\n",
        "    initial_crop_size: 14\n",
        "    maxpool_kernel_size: 2\n",
        "    maxpool_stride: 2\n",
        "    second_stage_box_predictor {\n",
        "      mask_rcnn_box_predictor {\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 1.0\n",
        "        fc_hyperparams {\n",
        "          op: FC\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.0\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            variance_scaling_initializer {\n",
        "              factor: 1.0\n",
        "              uniform: true\n",
        "              mode: FAN_AVG\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    second_stage_post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 0.0\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 300\n",
        "      }\n",
        "      score_converter: SOFTMAX\n",
        "    }\n",
        "    second_stage_localization_loss_weight: 2.0\n",
        "    second_stage_classification_loss_weight: 1.0\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 1\n",
        "  optimizer {\n",
        "    momentum_optimizer: {\n",
        "      learning_rate: {\n",
        "        manual_step_learning_rate {\n",
        "          initial_learning_rate: 0.0002\n",
        "          schedule {\n",
        "            step: 900000\n",
        "            learning_rate: .00002\n",
        "          }\n",
        "          schedule {\n",
        "            step: 1200000\n",
        "            learning_rate: .000002\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  gradient_clipping_by_norm: 10.0\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt\"\n",
        "  from_detection_checkpoint: true\n",
        "  load_all_detection_checkpoint_vars: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  num_examples: 70\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxtt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/faster_rcnn_inception_v2_pets.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "ae596b44-5abe-4338-b550-1d8072b41ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "0370e435-ab33-412e-d3fb-932955a331b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 09:15:57.531402 140481689634688 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0719 09:15:57.822804 140481689634688 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0719 09:15:57.855991 140481689634688 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0719 09:15:57.873682 140481689634688 deprecation_wrapper.py:119] From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0719 09:15:57.873854 140481689634688 deprecation_wrapper.py:119] From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0719 09:15:57.874567 140481689634688 deprecation_wrapper.py:119] From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0719 09:15:57.875065 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0719 09:15:57.875233 140481689634688 deprecation_wrapper.py:119] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0719 09:15:57.875564 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0719 09:15:57.880427 140481689634688 deprecation_wrapper.py:119] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0719 09:15:57.888516 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0719 09:15:57.898431 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0719 09:15:57.898647 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0719 09:15:57.920519 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0719 09:15:57.925822 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0719 09:15:57.925948 140481689634688 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "W0719 09:15:57.933627 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0719 09:15:57.933781 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0719 09:15:57.964363 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0719 09:15:58.206903 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0719 09:15:58.215085 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0719 09:15:58.220477 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0719 09:15:58.275558 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0719 09:15:58.280933 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0719 09:15:58.282228 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0719 09:15:58.293500 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0719 09:15:58.372548 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2412: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0719 09:15:58.437994 140481689634688 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "I0719 09:16:01.482003 140481689634688 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "I0719 09:16:01.504580 140481689634688 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "W0719 09:16:01.504971 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0719 09:16:01.505100 140481689634688 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0719 09:16:01.567666 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:174: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0719 09:16:02.810455 140481689634688 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/spatial_transform_ops.py:418: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "W0719 09:16:03.706837 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "I0719 09:16:03.954126 140481689634688 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "I0719 09:16:04.285114 140481689634688 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "W0719 09:16:04.653125 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0719 09:16:04.654821 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0719 09:16:04.721627 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:350: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0719 09:16:05.067853 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W0719 09:16:05.068367 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0719 09:16:05.078888 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "W0719 09:16:11.747335 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0719 09:16:12.228839 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0719 09:16:12.234358 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0719 09:16:12.238020 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0719 09:16:12.249398 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0719 09:16:12.856522 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2650: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0719 09:16:12.858121 140481689634688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0719 09:16:12.860561 140481689634688 variables_helper.py:157] Variable [Conv/biases/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.860694 140481689634688 variables_helper.py:157] Variable [Conv/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.860796 140481689634688 variables_helper.py:157] Variable [FirstStageBoxPredictor/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.860893 140481689634688 variables_helper.py:157] Variable [FirstStageBoxPredictor/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.860986 140481689634688 variables_helper.py:157] Variable [FirstStageBoxPredictor/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861101 140481689634688 variables_helper.py:157] Variable [FirstStageBoxPredictor/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861205 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861323 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861430 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861525 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861640 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861733 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861834 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.861932 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862023 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862126 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862219 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862338 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862443 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862536 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862638 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862739 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862831 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.862921 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863022 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863116 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863208 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863323 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863420 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863512 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863619 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863712 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863804 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863902 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.863994 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864085 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864200 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864328 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864422 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864536 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864636 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864727 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864841 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.864932 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865024 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865129 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865233 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865357 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865461 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865563 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865658 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865761 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865854 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.865947 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866052 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866147 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866239 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866364 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866458 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866621 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866722 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866864 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.866976 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867077 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867172 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867440 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867569 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867660 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867751 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867852 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.867947 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868044 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868155 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868252 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868367 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868469 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868586 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868678 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868775 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868884 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.868989 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.869088 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.869195 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923142 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923325 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923460 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923604 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923744 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923871 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.923999 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924139 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924288 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924417 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924574 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924706 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924834 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.924985 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925110 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925251 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925420 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925558 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925687 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925823 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.925951 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926076 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926214 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926361 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926487 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926633 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926760 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.926882 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927166 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927315 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927457 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927602 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927729 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.927853 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928003 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928132 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928294 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928449 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928586 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928714 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928848 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.928975 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929103 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929240 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929390 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929530 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929675 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929798 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.929920 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930069 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930195 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930339 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930475 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930611 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930765 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.930899 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931030 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931158 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931311 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931441 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931591 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931742 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.931885 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932088 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932239 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932387 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932512 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932672 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932796 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.932916 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933066 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933192 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933336 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933471 140481689634688 variables_helper.py:157] Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933611 140481689634688 variables_helper.py:154] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[360]], model variable shape: [[4]]. This variable will not be initialized from the checkpoint.\n",
            "W0719 09:16:12.933730 140481689634688 variables_helper.py:157] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.933859 140481689634688 variables_helper.py:154] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 360]], model variable shape: [[1024, 4]]. This variable will not be initialized from the checkpoint.\n",
            "W0719 09:16:12.933975 140481689634688 variables_helper.py:157] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.934102 140481689634688 variables_helper.py:154] Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[91]], model variable shape: [[2]]. This variable will not be initialized from the checkpoint.\n",
            "W0719 09:16:12.934247 140481689634688 variables_helper.py:157] Variable [SecondStageBoxPredictor/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.934401 140481689634688 variables_helper.py:154] Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 91]], model variable shape: [[1024, 2]]. This variable will not be initialized from the checkpoint.\n",
            "W0719 09:16:12.934518 140481689634688 variables_helper.py:157] Variable [SecondStageBoxPredictor/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.934656 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.934783 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.934920 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935047 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935173 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935325 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935456 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935620 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935792 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.935918 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936042 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936178 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936321 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936466 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936643 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936769 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.936892 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937039 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937167 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937309 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937447 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937584 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937708 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937841 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.937966 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938090 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938241 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938406 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938529 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938691 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938815 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.938937 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939069 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939195 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939386 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939521 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939659 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939785 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.939934 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940102 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940227 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940402 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940526 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940659 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940791 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.940916 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941055 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941189 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941333 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941460 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941610 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941737 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941860 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.941992 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.942117 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.942256 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.942410 140481689634688 variables_helper.py:157] Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\n",
            "W0719 09:16:12.942525 140481689634688 variables_helper.py:157] Variable [global_step] is not available in checkpoint\n",
            "W0719 09:16:13.826511 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-07-19 09:16:14.901212: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-19 09:16:14.903370: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xe9b2d80 executing computations on platform Host. Devices:\n",
            "2019-07-19 09:16:14.903408: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-19 09:16:14.909787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-19 09:16:15.079339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:15.079886: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xe9b3100 executing computations on platform CUDA. Devices:\n",
            "2019-07-19 09:16:15.079924: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-19 09:16:15.080175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:15.080582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-19 09:16:15.095658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:16:15.327467: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:16:15.414238: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-19 09:16:15.442180: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-19 09:16:15.677107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-19 09:16:15.801490: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-19 09:16:16.205457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-19 09:16:16.205718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:16.206236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:16.206621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-19 09:16:16.208907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:16:16.211380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-19 09:16:16.211417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-19 09:16:16.211436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-19 09:16:16.213985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:16.214521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:16:16.214905: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-19 09:16:16.214964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-07-19 09:16:17.697524: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0719 09:16:18.052624 140481689634688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0719 09:16:18.054146 140481689634688 saver.py:1280] Restoring parameters from /content/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt\n",
            "I0719 09:16:18.617043 140481689634688 session_manager.py:500] Running local_init_op.\n",
            "I0719 09:16:18.981481 140481689634688 session_manager.py:502] Done running local_init_op.\n",
            "I0719 09:16:25.722736 140481689634688 learning.py:754] Starting Session.\n",
            "I0719 09:16:25.906422 140479034988288 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0719 09:16:25.910284 140481689634688 learning.py:768] Starting Queues.\n",
            "2019-07-19 09:16:30.015993: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:16:31.131885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "I0719 09:16:35.190800 140479026595584 supervisor.py:1099] global_step/sec: 0\n",
            "I0719 09:16:35.546827 140479018202880 supervisor.py:1050] Recording summary at step 0.\n",
            "I0719 09:16:41.713805 140481689634688 learning.py:507] global step 1: loss = 1.8246 (15.610 sec/step)\n",
            "I0719 09:16:45.039655 140481689634688 learning.py:507] global step 2: loss = 1.8439 (3.133 sec/step)\n",
            "I0719 09:16:48.500701 140481689634688 learning.py:507] global step 3: loss = 0.8222 (3.459 sec/step)\n",
            "I0719 09:16:52.879146 140481689634688 learning.py:507] global step 4: loss = 0.6569 (4.376 sec/step)\n",
            "I0719 09:16:54.669702 140481689634688 learning.py:507] global step 5: loss = 0.5591 (1.789 sec/step)\n",
            "I0719 09:16:59.045224 140481689634688 learning.py:507] global step 6: loss = 1.1725 (4.374 sec/step)\n",
            "I0719 09:17:02.470323 140481689634688 learning.py:507] global step 7: loss = 1.4181 (3.423 sec/step)\n",
            "I0719 09:17:07.550560 140481689634688 learning.py:507] global step 8: loss = 1.0779 (5.079 sec/step)\n",
            "I0719 09:17:08.296135 140481689634688 learning.py:507] global step 9: loss = 1.7765 (0.744 sec/step)\n",
            "I0719 09:17:13.346714 140481689634688 learning.py:507] global step 10: loss = 0.5711 (5.048 sec/step)\n",
            "I0719 09:17:13.695103 140481689634688 learning.py:507] global step 11: loss = 0.4372 (0.347 sec/step)\n",
            "I0719 09:17:15.777399 140481689634688 learning.py:507] global step 12: loss = 1.1742 (2.080 sec/step)\n",
            "I0719 09:17:17.840180 140481689634688 learning.py:507] global step 13: loss = 1.1895 (2.061 sec/step)\n",
            "I0719 09:17:21.180764 140481689634688 learning.py:507] global step 14: loss = 1.6172 (3.339 sec/step)\n",
            "I0719 09:17:24.604234 140481689634688 learning.py:507] global step 15: loss = 0.4408 (3.422 sec/step)\n",
            "I0719 09:17:24.958377 140481689634688 learning.py:507] global step 16: loss = 0.9598 (0.352 sec/step)\n",
            "I0719 09:17:28.410820 140481689634688 learning.py:507] global step 17: loss = 1.0009 (3.451 sec/step)\n",
            "I0719 09:17:28.762996 140481689634688 learning.py:507] global step 18: loss = 0.4150 (0.350 sec/step)\n",
            "I0719 09:17:29.106961 140481689634688 learning.py:507] global step 19: loss = 1.0506 (0.342 sec/step)\n",
            "I0719 09:17:29.457435 140481689634688 learning.py:507] global step 20: loss = 0.9992 (0.349 sec/step)\n",
            "I0719 09:17:29.794399 140481689634688 learning.py:507] global step 21: loss = 0.9712 (0.335 sec/step)\n",
            "I0719 09:17:33.227499 140481689634688 learning.py:507] global step 22: loss = 0.9565 (3.431 sec/step)\n",
            "I0719 09:17:34.953363 140481689634688 learning.py:507] global step 23: loss = 1.0026 (1.724 sec/step)\n",
            "I0719 09:17:38.768884 140481689634688 learning.py:507] global step 24: loss = 0.4022 (3.814 sec/step)\n",
            "I0719 09:17:42.594046 140481689634688 learning.py:507] global step 25: loss = 1.4596 (3.823 sec/step)\n",
            "I0719 09:17:42.984926 140481689634688 learning.py:507] global step 26: loss = 0.6843 (0.389 sec/step)\n",
            "I0719 09:17:44.742795 140481689634688 learning.py:507] global step 27: loss = 0.7824 (1.756 sec/step)\n",
            "I0719 09:17:46.792488 140481689634688 learning.py:507] global step 28: loss = 1.4702 (2.048 sec/step)\n",
            "I0719 09:17:50.306949 140481689634688 learning.py:507] global step 29: loss = 0.7613 (3.513 sec/step)\n",
            "I0719 09:17:52.050018 140481689634688 learning.py:507] global step 30: loss = 0.8088 (1.741 sec/step)\n",
            "I0719 09:17:55.382371 140481689634688 learning.py:507] global step 31: loss = 0.6779 (3.331 sec/step)\n",
            "I0719 09:17:56.950009 140481689634688 learning.py:507] global step 32: loss = 0.8642 (1.566 sec/step)\n",
            "I0719 09:18:00.352652 140481689634688 learning.py:507] global step 33: loss = 0.2924 (3.401 sec/step)\n",
            "I0719 09:18:00.711730 140481689634688 learning.py:507] global step 34: loss = 0.8390 (0.357 sec/step)\n",
            "I0719 09:18:01.062581 140481689634688 learning.py:507] global step 35: loss = 0.8553 (0.349 sec/step)\n",
            "I0719 09:18:06.077057 140481689634688 learning.py:507] global step 36: loss = 0.8481 (5.012 sec/step)\n",
            "I0719 09:18:06.417184 140481689634688 learning.py:507] global step 37: loss = 0.2136 (0.338 sec/step)\n",
            "I0719 09:18:08.431038 140481689634688 learning.py:507] global step 38: loss = 0.5848 (2.012 sec/step)\n",
            "I0719 09:18:08.793614 140481689634688 learning.py:507] global step 39: loss = 0.9317 (0.361 sec/step)\n",
            "I0719 09:18:09.117468 140481689634688 learning.py:507] global step 40: loss = 0.7612 (0.322 sec/step)\n",
            "I0719 09:18:09.447799 140481689634688 learning.py:507] global step 41: loss = 0.5407 (0.329 sec/step)\n",
            "I0719 09:18:09.795767 140481689634688 learning.py:507] global step 42: loss = 0.5741 (0.346 sec/step)\n",
            "I0719 09:18:10.163001 140481689634688 learning.py:507] global step 43: loss = 0.6331 (0.366 sec/step)\n",
            "I0719 09:18:10.476481 140481689634688 learning.py:507] global step 44: loss = 0.7848 (0.312 sec/step)\n",
            "I0719 09:18:10.812921 140481689634688 learning.py:507] global step 45: loss = 0.4572 (0.335 sec/step)\n",
            "I0719 09:18:11.148125 140481689634688 learning.py:507] global step 46: loss = 0.8488 (0.333 sec/step)\n",
            "I0719 09:18:11.479850 140481689634688 learning.py:507] global step 47: loss = 0.9008 (0.330 sec/step)\n",
            "I0719 09:18:11.798481 140481689634688 learning.py:507] global step 48: loss = 0.5143 (0.317 sec/step)\n",
            "I0719 09:18:12.149508 140481689634688 learning.py:507] global step 49: loss = 1.2312 (0.349 sec/step)\n",
            "I0719 09:18:12.504845 140481689634688 learning.py:507] global step 50: loss = 0.8197 (0.354 sec/step)\n",
            "I0719 09:18:12.802678 140481689634688 learning.py:507] global step 51: loss = 0.8492 (0.296 sec/step)\n",
            "I0719 09:18:16.048059 140481689634688 learning.py:507] global step 52: loss = 0.1711 (3.243 sec/step)\n",
            "I0719 09:18:16.801449 140481689634688 learning.py:507] global step 53: loss = 0.3609 (0.752 sec/step)\n",
            "I0719 09:18:17.154448 140481689634688 learning.py:507] global step 54: loss = 0.6579 (0.351 sec/step)\n",
            "I0719 09:18:20.474180 140481689634688 learning.py:507] global step 55: loss = 0.3413 (3.318 sec/step)\n",
            "I0719 09:18:20.826394 140481689634688 learning.py:507] global step 56: loss = 0.7289 (0.351 sec/step)\n",
            "I0719 09:18:21.158351 140481689634688 learning.py:507] global step 57: loss = 0.7361 (0.330 sec/step)\n",
            "I0719 09:18:21.495486 140481689634688 learning.py:507] global step 58: loss = 0.6747 (0.335 sec/step)\n",
            "I0719 09:18:21.832522 140481689634688 learning.py:507] global step 59: loss = 1.1079 (0.335 sec/step)\n",
            "I0719 09:18:22.158192 140481689634688 learning.py:507] global step 60: loss = 0.7068 (0.324 sec/step)\n",
            "I0719 09:18:22.519824 140481689634688 learning.py:507] global step 61: loss = 0.7479 (0.360 sec/step)\n",
            "I0719 09:18:22.831624 140481689634688 learning.py:507] global step 62: loss = 0.3569 (0.310 sec/step)\n",
            "I0719 09:18:23.169341 140481689634688 learning.py:507] global step 63: loss = 0.8531 (0.336 sec/step)\n",
            "I0719 09:18:23.502969 140481689634688 learning.py:507] global step 64: loss = 1.2736 (0.332 sec/step)\n",
            "I0719 09:18:28.106616 140479026595584 supervisor.py:1099] global_step/sec: 0.566794\n",
            "I0719 09:18:28.142783 140481689634688 learning.py:507] global step 65: loss = 0.2458 (4.636 sec/step)\n",
            "I0719 09:18:28.205208 140479018202880 supervisor.py:1050] Recording summary at step 65.\n",
            "I0719 09:18:28.543193 140481689634688 learning.py:507] global step 66: loss = 0.5403 (0.396 sec/step)\n",
            "I0719 09:18:28.859711 140481689634688 learning.py:507] global step 67: loss = 0.5832 (0.315 sec/step)\n",
            "I0719 09:18:29.184463 140481689634688 learning.py:507] global step 68: loss = 0.7263 (0.323 sec/step)\n",
            "I0719 09:18:29.532346 140481689634688 learning.py:507] global step 69: loss = 0.4831 (0.346 sec/step)\n",
            "I0719 09:18:29.844913 140481689634688 learning.py:507] global step 70: loss = 0.5978 (0.311 sec/step)\n",
            "I0719 09:18:30.189517 140481689634688 learning.py:507] global step 71: loss = 0.9059 (0.343 sec/step)\n",
            "I0719 09:18:30.517816 140481689634688 learning.py:507] global step 72: loss = 0.4769 (0.326 sec/step)\n",
            "I0719 09:18:30.866316 140481689634688 learning.py:507] global step 73: loss = 0.4514 (0.347 sec/step)\n",
            "I0719 09:18:31.207061 140481689634688 learning.py:507] global step 74: loss = 0.7254 (0.339 sec/step)\n",
            "I0719 09:18:32.903571 140481689634688 learning.py:507] global step 75: loss = 1.0534 (1.695 sec/step)\n",
            "I0719 09:18:33.264142 140481689634688 learning.py:507] global step 76: loss = 0.5713 (0.359 sec/step)\n",
            "I0719 09:18:37.562619 140481689634688 learning.py:507] global step 77: loss = 0.1697 (4.297 sec/step)\n",
            "I0719 09:18:37.896102 140481689634688 learning.py:507] global step 78: loss = 0.9441 (0.331 sec/step)\n",
            "I0719 09:18:38.426788 140481689634688 learning.py:507] global step 79: loss = 0.3767 (0.529 sec/step)\n",
            "I0719 09:18:41.897537 140481689634688 learning.py:507] global step 80: loss = 0.5011 (3.469 sec/step)\n",
            "I0719 09:18:42.260508 140481689634688 learning.py:507] global step 81: loss = 0.7095 (0.361 sec/step)\n",
            "I0719 09:18:42.582021 140481689634688 learning.py:507] global step 82: loss = 0.2895 (0.319 sec/step)\n",
            "I0719 09:18:42.936194 140481689634688 learning.py:507] global step 83: loss = 0.7730 (0.352 sec/step)\n",
            "I0719 09:18:43.271556 140481689634688 learning.py:507] global step 84: loss = 0.6769 (0.334 sec/step)\n",
            "I0719 09:18:43.609611 140481689634688 learning.py:507] global step 85: loss = 0.4307 (0.337 sec/step)\n",
            "I0719 09:18:43.942797 140481689634688 learning.py:507] global step 86: loss = 0.4313 (0.331 sec/step)\n",
            "I0719 09:18:48.344799 140481689634688 learning.py:507] global step 87: loss = 0.6473 (4.400 sec/step)\n",
            "I0719 09:18:48.655144 140481689634688 learning.py:507] global step 88: loss = 1.1339 (0.309 sec/step)\n",
            "I0719 09:18:48.960863 140481689634688 learning.py:507] global step 89: loss = 0.2953 (0.304 sec/step)\n",
            "I0719 09:18:49.295839 140481689634688 learning.py:507] global step 90: loss = 0.5238 (0.333 sec/step)\n",
            "I0719 09:18:49.650959 140481689634688 learning.py:507] global step 91: loss = 0.5493 (0.353 sec/step)\n",
            "I0719 09:18:49.987080 140481689634688 learning.py:507] global step 92: loss = 0.5010 (0.334 sec/step)\n",
            "I0719 09:18:50.288683 140481689634688 learning.py:507] global step 93: loss = 0.3960 (0.300 sec/step)\n",
            "I0719 09:18:50.641350 140481689634688 learning.py:507] global step 94: loss = 0.7684 (0.351 sec/step)\n",
            "I0719 09:18:51.365459 140481689634688 learning.py:507] global step 95: loss = 0.4018 (0.722 sec/step)\n",
            "I0719 09:18:51.650525 140481689634688 learning.py:507] global step 96: loss = 0.1962 (0.283 sec/step)\n",
            "I0719 09:18:54.814736 140481689634688 learning.py:507] global step 97: loss = 0.1889 (3.162 sec/step)\n",
            "I0719 09:18:55.144771 140481689634688 learning.py:507] global step 98: loss = 0.1573 (0.328 sec/step)\n",
            "I0719 09:18:55.520792 140481689634688 learning.py:507] global step 99: loss = 0.6553 (0.374 sec/step)\n",
            "I0719 09:18:55.853886 140481689634688 learning.py:507] global step 100: loss = 0.1664 (0.331 sec/step)\n",
            "I0719 09:18:56.191657 140481689634688 learning.py:507] global step 101: loss = 0.2200 (0.336 sec/step)\n",
            "I0719 09:18:56.567358 140481689634688 learning.py:507] global step 102: loss = 0.2904 (0.374 sec/step)\n",
            "I0719 09:18:56.913774 140481689634688 learning.py:507] global step 103: loss = 0.4764 (0.345 sec/step)\n",
            "I0719 09:18:57.219510 140481689634688 learning.py:507] global step 104: loss = 0.4707 (0.304 sec/step)\n",
            "I0719 09:18:57.568156 140481689634688 learning.py:507] global step 105: loss = 0.5368 (0.347 sec/step)\n",
            "I0719 09:18:57.900132 140481689634688 learning.py:507] global step 106: loss = 0.4710 (0.330 sec/step)\n",
            "I0719 09:18:58.208619 140481689634688 learning.py:507] global step 107: loss = 1.5097 (0.305 sec/step)\n",
            "I0719 09:18:58.516571 140481689634688 learning.py:507] global step 108: loss = 0.6407 (0.306 sec/step)\n",
            "I0719 09:18:58.839022 140481689634688 learning.py:507] global step 109: loss = 0.4345 (0.321 sec/step)\n",
            "I0719 09:18:59.176559 140481689634688 learning.py:507] global step 110: loss = 0.6196 (0.331 sec/step)\n",
            "I0719 09:18:59.511304 140481689634688 learning.py:507] global step 111: loss = 0.5266 (0.332 sec/step)\n",
            "I0719 09:18:59.841388 140481689634688 learning.py:507] global step 112: loss = 0.4512 (0.328 sec/step)\n",
            "I0719 09:19:00.180652 140481689634688 learning.py:507] global step 113: loss = 0.2666 (0.337 sec/step)\n",
            "I0719 09:19:00.528184 140481689634688 learning.py:507] global step 114: loss = 0.3640 (0.346 sec/step)\n",
            "I0719 09:19:00.829301 140481689634688 learning.py:507] global step 115: loss = 0.3354 (0.299 sec/step)\n",
            "I0719 09:19:01.538857 140481689634688 learning.py:507] global step 116: loss = 0.3106 (0.705 sec/step)\n",
            "I0719 09:19:01.897498 140481689634688 learning.py:507] global step 117: loss = 0.7137 (0.357 sec/step)\n",
            "I0719 09:19:02.230742 140481689634688 learning.py:507] global step 118: loss = 0.3948 (0.331 sec/step)\n",
            "I0719 09:19:02.579692 140481689634688 learning.py:507] global step 119: loss = 0.9481 (0.347 sec/step)\n",
            "I0719 09:19:02.886095 140481689634688 learning.py:507] global step 120: loss = 0.1117 (0.304 sec/step)\n",
            "I0719 09:19:03.225968 140481689634688 learning.py:507] global step 121: loss = 0.4198 (0.338 sec/step)\n",
            "I0719 09:19:03.533504 140481689634688 learning.py:507] global step 122: loss = 0.0938 (0.306 sec/step)\n",
            "I0719 09:19:03.866872 140481689634688 learning.py:507] global step 123: loss = 0.1920 (0.331 sec/step)\n",
            "I0719 09:19:04.201060 140481689634688 learning.py:507] global step 124: loss = 0.2741 (0.332 sec/step)\n",
            "I0719 09:19:04.536576 140481689634688 learning.py:507] global step 125: loss = 0.2069 (0.334 sec/step)\n",
            "I0719 09:19:04.872000 140481689634688 learning.py:507] global step 126: loss = 0.7395 (0.334 sec/step)\n",
            "I0719 09:19:05.214638 140481689634688 learning.py:507] global step 127: loss = 0.4925 (0.341 sec/step)\n",
            "I0719 09:19:05.947128 140481689634688 learning.py:507] global step 128: loss = 0.4818 (0.731 sec/step)\n",
            "I0719 09:19:06.245117 140481689634688 learning.py:507] global step 129: loss = 0.1752 (0.296 sec/step)\n",
            "I0719 09:19:06.566354 140481689634688 learning.py:507] global step 130: loss = 0.1172 (0.319 sec/step)\n",
            "I0719 09:19:07.346610 140481689634688 learning.py:507] global step 131: loss = 0.3461 (0.779 sec/step)\n",
            "I0719 09:19:07.683700 140481689634688 learning.py:507] global step 132: loss = 0.5415 (0.335 sec/step)\n",
            "I0719 09:19:08.007960 140481689634688 learning.py:507] global step 133: loss = 0.8369 (0.322 sec/step)\n",
            "I0719 09:19:11.494645 140481689634688 learning.py:507] global step 134: loss = 0.3657 (3.485 sec/step)\n",
            "I0719 09:19:11.832548 140481689634688 learning.py:507] global step 135: loss = 0.6308 (0.336 sec/step)\n",
            "I0719 09:19:12.119193 140481689634688 learning.py:507] global step 136: loss = 1.0811 (0.285 sec/step)\n",
            "I0719 09:19:12.414875 140481689634688 learning.py:507] global step 137: loss = 0.2679 (0.294 sec/step)\n",
            "I0719 09:19:12.719454 140481689634688 learning.py:507] global step 138: loss = 0.9363 (0.303 sec/step)\n",
            "I0719 09:19:13.080721 140481689634688 learning.py:507] global step 139: loss = 0.4639 (0.360 sec/step)\n",
            "I0719 09:19:13.411632 140481689634688 learning.py:507] global step 140: loss = 0.4168 (0.329 sec/step)\n",
            "I0719 09:19:13.764156 140481689634688 learning.py:507] global step 141: loss = 0.2244 (0.351 sec/step)\n",
            "I0719 09:19:14.079663 140481689634688 learning.py:507] global step 142: loss = 0.3445 (0.314 sec/step)\n",
            "I0719 09:19:14.363599 140481689634688 learning.py:507] global step 143: loss = 0.6514 (0.280 sec/step)\n",
            "I0719 09:19:14.728723 140481689634688 learning.py:507] global step 144: loss = 0.6411 (0.363 sec/step)\n",
            "I0719 09:19:15.049415 140481689634688 learning.py:507] global step 145: loss = 0.4119 (0.319 sec/step)\n",
            "I0719 09:19:15.385532 140481689634688 learning.py:507] global step 146: loss = 0.2864 (0.334 sec/step)\n",
            "I0719 09:19:15.746411 140481689634688 learning.py:507] global step 147: loss = 0.4185 (0.359 sec/step)\n",
            "I0719 09:19:20.738447 140481689634688 learning.py:507] global step 148: loss = 0.3007 (4.989 sec/step)\n",
            "I0719 09:19:21.147988 140481689634688 learning.py:507] global step 149: loss = 0.3302 (0.407 sec/step)\n",
            "I0719 09:19:21.470909 140481689634688 learning.py:507] global step 150: loss = 0.3041 (0.321 sec/step)\n",
            "I0719 09:19:21.804160 140481689634688 learning.py:507] global step 151: loss = 0.3866 (0.332 sec/step)\n",
            "I0719 09:19:22.147562 140481689634688 learning.py:507] global step 152: loss = 0.4186 (0.342 sec/step)\n",
            "I0719 09:19:22.488013 140481689634688 learning.py:507] global step 153: loss = 0.3550 (0.339 sec/step)\n",
            "I0719 09:19:24.269293 140481689634688 learning.py:507] global step 154: loss = 0.5290 (1.780 sec/step)\n",
            "I0719 09:19:29.361390 140481689634688 learning.py:507] global step 155: loss = 0.2670 (5.090 sec/step)\n",
            "I0719 09:19:29.738864 140481689634688 learning.py:507] global step 156: loss = 0.1669 (0.376 sec/step)\n",
            "I0719 09:19:30.094038 140481689634688 learning.py:507] global step 157: loss = 0.4516 (0.354 sec/step)\n",
            "I0719 09:19:31.813131 140481689634688 learning.py:507] global step 158: loss = 0.2454 (1.718 sec/step)\n",
            "I0719 09:19:32.167246 140481689634688 learning.py:507] global step 159: loss = 0.6678 (0.352 sec/step)\n",
            "I0719 09:19:32.501090 140481689634688 learning.py:507] global step 160: loss = 0.3442 (0.332 sec/step)\n",
            "I0719 09:19:32.838407 140481689634688 learning.py:507] global step 161: loss = 0.2905 (0.335 sec/step)\n",
            "I0719 09:19:33.195224 140481689634688 learning.py:507] global step 162: loss = 0.3728 (0.355 sec/step)\n",
            "I0719 09:19:33.510151 140481689634688 learning.py:507] global step 163: loss = 0.1756 (0.313 sec/step)\n",
            "I0719 09:19:33.852758 140481689634688 learning.py:507] global step 164: loss = 0.2440 (0.341 sec/step)\n",
            "I0719 09:19:34.181460 140481689634688 learning.py:507] global step 165: loss = 0.4286 (0.327 sec/step)\n",
            "I0719 09:19:34.534233 140481689634688 learning.py:507] global step 166: loss = 0.3468 (0.351 sec/step)\n",
            "I0719 09:19:34.813592 140481689634688 learning.py:507] global step 167: loss = 0.6200 (0.278 sec/step)\n",
            "I0719 09:19:35.191109 140481689634688 learning.py:507] global step 168: loss = 0.4345 (0.376 sec/step)\n",
            "I0719 09:19:35.513073 140481689634688 learning.py:507] global step 169: loss = 0.2925 (0.320 sec/step)\n",
            "I0719 09:19:35.850792 140481689634688 learning.py:507] global step 170: loss = 0.4460 (0.336 sec/step)\n",
            "I0719 09:19:36.159938 140481689634688 learning.py:507] global step 171: loss = 0.0750 (0.307 sec/step)\n",
            "I0719 09:19:36.495674 140481689634688 learning.py:507] global step 172: loss = 0.3474 (0.334 sec/step)\n",
            "I0719 09:19:36.833683 140481689634688 learning.py:507] global step 173: loss = 0.4331 (0.336 sec/step)\n",
            "I0719 09:19:41.787012 140481689634688 learning.py:507] global step 174: loss = 0.3959 (4.952 sec/step)\n",
            "I0719 09:19:42.135404 140481689634688 learning.py:507] global step 175: loss = 0.3898 (0.347 sec/step)\n",
            "I0719 09:19:42.421637 140481689634688 learning.py:507] global step 176: loss = 0.2308 (0.285 sec/step)\n",
            "I0719 09:19:42.762297 140481689634688 learning.py:507] global step 177: loss = 0.3903 (0.339 sec/step)\n",
            "I0719 09:19:43.081591 140481689634688 learning.py:507] global step 178: loss = 0.3511 (0.318 sec/step)\n",
            "I0719 09:19:43.422535 140481689634688 learning.py:507] global step 179: loss = 0.2889 (0.339 sec/step)\n",
            "I0719 09:19:43.766220 140481689634688 learning.py:507] global step 180: loss = 0.4523 (0.342 sec/step)\n",
            "I0719 09:19:44.101256 140481689634688 learning.py:507] global step 181: loss = 1.0857 (0.333 sec/step)\n",
            "I0719 09:19:44.439009 140481689634688 learning.py:507] global step 182: loss = 0.2571 (0.336 sec/step)\n",
            "I0719 09:19:44.790401 140481689634688 learning.py:507] global step 183: loss = 0.3934 (0.349 sec/step)\n",
            "I0719 09:19:45.091723 140481689634688 learning.py:507] global step 184: loss = 0.1575 (0.299 sec/step)\n",
            "I0719 09:19:45.456006 140481689634688 learning.py:507] global step 185: loss = 0.3793 (0.362 sec/step)\n",
            "I0719 09:19:45.794664 140481689634688 learning.py:507] global step 186: loss = 0.2504 (0.337 sec/step)\n",
            "I0719 09:19:46.135454 140481689634688 learning.py:507] global step 187: loss = 0.3224 (0.339 sec/step)\n",
            "I0719 09:19:46.466872 140481689634688 learning.py:507] global step 188: loss = 0.2976 (0.329 sec/step)\n",
            "I0719 09:19:46.779350 140481689634688 learning.py:507] global step 189: loss = 0.2448 (0.311 sec/step)\n",
            "I0719 09:19:47.124535 140481689634688 learning.py:507] global step 190: loss = 0.1892 (0.343 sec/step)\n",
            "I0719 09:19:47.444971 140481689634688 learning.py:507] global step 191: loss = 0.2297 (0.318 sec/step)\n",
            "I0719 09:19:47.796427 140481689634688 learning.py:507] global step 192: loss = 0.2959 (0.350 sec/step)\n",
            "I0719 09:19:48.110429 140481689634688 learning.py:507] global step 193: loss = 0.5146 (0.312 sec/step)\n",
            "I0719 09:19:48.421069 140481689634688 learning.py:507] global step 194: loss = 0.2773 (0.309 sec/step)\n",
            "I0719 09:19:48.703286 140481689634688 learning.py:507] global step 195: loss = 0.1180 (0.280 sec/step)\n",
            "I0719 09:19:49.029339 140481689634688 learning.py:507] global step 196: loss = 0.3741 (0.324 sec/step)\n",
            "I0719 09:19:49.359345 140481689634688 learning.py:507] global step 197: loss = 0.4472 (0.328 sec/step)\n",
            "I0719 09:19:49.700145 140481689634688 learning.py:507] global step 198: loss = 0.3174 (0.339 sec/step)\n",
            "I0719 09:19:50.046367 140481689634688 learning.py:507] global step 199: loss = 0.2154 (0.344 sec/step)\n",
            "I0719 09:19:50.379714 140481689634688 learning.py:507] global step 200: loss = 0.1806 (0.332 sec/step)\n",
            "I0719 09:19:50.730838 140481689634688 learning.py:507] global step 201: loss = 0.3847 (0.349 sec/step)\n",
            "I0719 09:19:52.443512 140481689634688 learning.py:507] global step 202: loss = 0.1831 (1.711 sec/step)\n",
            "I0719 09:19:52.807716 140481689634688 learning.py:507] global step 203: loss = 0.3330 (0.363 sec/step)\n",
            "I0719 09:19:53.148792 140481689634688 learning.py:507] global step 204: loss = 0.1753 (0.339 sec/step)\n",
            "I0719 09:19:53.475788 140481689634688 learning.py:507] global step 205: loss = 0.1780 (0.325 sec/step)\n",
            "I0719 09:19:53.805763 140481689634688 learning.py:507] global step 206: loss = 0.1409 (0.328 sec/step)\n",
            "I0719 09:19:54.116258 140481689634688 learning.py:507] global step 207: loss = 0.3781 (0.309 sec/step)\n",
            "I0719 09:19:54.441297 140481689634688 learning.py:507] global step 208: loss = 0.8669 (0.323 sec/step)\n",
            "I0719 09:19:54.777062 140481689634688 learning.py:507] global step 209: loss = 0.3840 (0.334 sec/step)\n",
            "I0719 09:19:55.071523 140481689634688 learning.py:507] global step 210: loss = 0.1370 (0.293 sec/step)\n",
            "I0719 09:19:55.401286 140481689634688 learning.py:507] global step 211: loss = 0.1255 (0.328 sec/step)\n",
            "I0719 09:19:55.701788 140481689634688 learning.py:507] global step 212: loss = 0.8260 (0.299 sec/step)\n",
            "I0719 09:19:56.053692 140481689634688 learning.py:507] global step 213: loss = 0.2851 (0.350 sec/step)\n",
            "I0719 09:19:56.383064 140481689634688 learning.py:507] global step 214: loss = 0.2344 (0.327 sec/step)\n",
            "I0719 09:19:59.614218 140481689634688 learning.py:507] global step 215: loss = 0.1983 (3.229 sec/step)\n",
            "I0719 09:19:59.959348 140481689634688 learning.py:507] global step 216: loss = 0.0558 (0.343 sec/step)\n",
            "I0719 09:20:00.312327 140481689634688 learning.py:507] global step 217: loss = 0.1465 (0.351 sec/step)\n",
            "I0719 09:20:00.638026 140481689634688 learning.py:507] global step 218: loss = 0.2805 (0.324 sec/step)\n",
            "I0719 09:20:00.949817 140481689634688 learning.py:507] global step 219: loss = 0.2107 (0.310 sec/step)\n",
            "I0719 09:20:05.866791 140481689634688 learning.py:507] global step 220: loss = 0.3149 (4.915 sec/step)\n",
            "I0719 09:20:06.200082 140481689634688 learning.py:507] global step 221: loss = 0.0763 (0.332 sec/step)\n",
            "I0719 09:20:06.535367 140481689634688 learning.py:507] global step 222: loss = 0.3162 (0.334 sec/step)\n",
            "I0719 09:20:06.854724 140481689634688 learning.py:507] global step 223: loss = 0.3731 (0.318 sec/step)\n",
            "I0719 09:20:07.232867 140481689634688 learning.py:507] global step 224: loss = 0.2761 (0.376 sec/step)\n",
            "I0719 09:20:07.558826 140481689634688 learning.py:507] global step 225: loss = 0.1347 (0.324 sec/step)\n",
            "I0719 09:20:07.883837 140481689634688 learning.py:507] global step 226: loss = 0.2924 (0.323 sec/step)\n",
            "I0719 09:20:08.219246 140481689634688 learning.py:507] global step 227: loss = 0.2980 (0.334 sec/step)\n",
            "I0719 09:20:08.567888 140481689634688 learning.py:507] global step 228: loss = 0.1187 (0.347 sec/step)\n",
            "I0719 09:20:08.890398 140481689634688 learning.py:507] global step 229: loss = 0.5825 (0.321 sec/step)\n",
            "I0719 09:20:09.218338 140481689634688 learning.py:507] global step 230: loss = 0.3981 (0.326 sec/step)\n",
            "I0719 09:20:09.546206 140481689634688 learning.py:507] global step 231: loss = 0.4236 (0.326 sec/step)\n",
            "I0719 09:20:09.886907 140481689634688 learning.py:507] global step 232: loss = 1.4019 (0.339 sec/step)\n",
            "I0719 09:20:10.247225 140481689634688 learning.py:507] global step 233: loss = 0.0817 (0.359 sec/step)\n",
            "I0719 09:20:10.589430 140481689634688 learning.py:507] global step 234: loss = 0.3706 (0.341 sec/step)\n",
            "I0719 09:20:10.873623 140481689634688 learning.py:507] global step 235: loss = 0.3385 (0.282 sec/step)\n",
            "I0719 09:20:11.215893 140481689634688 learning.py:507] global step 236: loss = 0.1934 (0.341 sec/step)\n",
            "I0719 09:20:11.508878 140481689634688 learning.py:507] global step 237: loss = 0.3559 (0.291 sec/step)\n",
            "I0719 09:20:11.838175 140481689634688 learning.py:507] global step 238: loss = 0.1845 (0.327 sec/step)\n",
            "I0719 09:20:12.184498 140481689634688 learning.py:507] global step 239: loss = 0.1652 (0.344 sec/step)\n",
            "I0719 09:20:12.507203 140481689634688 learning.py:507] global step 240: loss = 0.0997 (0.321 sec/step)\n",
            "I0719 09:20:12.836374 140481689634688 learning.py:507] global step 241: loss = 0.3333 (0.327 sec/step)\n",
            "I0719 09:20:13.180312 140481689634688 learning.py:507] global step 242: loss = 0.3400 (0.342 sec/step)\n",
            "I0719 09:20:13.489647 140481689634688 learning.py:507] global step 243: loss = 0.2391 (0.307 sec/step)\n",
            "I0719 09:20:13.836851 140481689634688 learning.py:507] global step 244: loss = 0.3897 (0.345 sec/step)\n",
            "I0719 09:20:14.186431 140481689634688 learning.py:507] global step 245: loss = 0.2604 (0.348 sec/step)\n",
            "I0719 09:20:14.504499 140481689634688 learning.py:507] global step 246: loss = 0.1792 (0.316 sec/step)\n",
            "I0719 09:20:14.816458 140481689634688 learning.py:507] global step 247: loss = 0.7732 (0.310 sec/step)\n",
            "I0719 09:20:15.152881 140481689634688 learning.py:507] global step 248: loss = 0.3447 (0.334 sec/step)\n",
            "I0719 09:20:15.484018 140481689634688 learning.py:507] global step 249: loss = 0.3232 (0.329 sec/step)\n",
            "I0719 09:20:15.823609 140481689634688 learning.py:507] global step 250: loss = 0.2446 (0.338 sec/step)\n",
            "I0719 09:20:16.124337 140481689634688 learning.py:507] global step 251: loss = 0.6554 (0.299 sec/step)\n",
            "I0719 09:20:16.422593 140481689634688 learning.py:507] global step 252: loss = 0.5091 (0.297 sec/step)\n",
            "I0719 09:20:16.734330 140481689634688 learning.py:507] global step 253: loss = 0.1435 (0.310 sec/step)\n",
            "I0719 09:20:17.082933 140481689634688 learning.py:507] global step 254: loss = 0.2099 (0.347 sec/step)\n",
            "I0719 09:20:17.417965 140481689634688 learning.py:507] global step 255: loss = 0.1598 (0.333 sec/step)\n",
            "I0719 09:20:17.742928 140481689634688 learning.py:507] global step 256: loss = 0.9406 (0.323 sec/step)\n",
            "I0719 09:20:18.121591 140481689634688 learning.py:507] global step 257: loss = 0.3900 (0.376 sec/step)\n",
            "I0719 09:20:18.476913 140481689634688 learning.py:507] global step 258: loss = 0.4012 (0.354 sec/step)\n",
            "I0719 09:20:18.790038 140481689634688 learning.py:507] global step 259: loss = 0.5421 (0.311 sec/step)\n",
            "I0719 09:20:19.117814 140481689634688 learning.py:507] global step 260: loss = 0.4227 (0.326 sec/step)\n",
            "I0719 09:20:19.421581 140481689634688 learning.py:507] global step 261: loss = 0.4623 (0.302 sec/step)\n",
            "I0719 09:20:19.738428 140481689634688 learning.py:507] global step 262: loss = 0.2683 (0.315 sec/step)\n",
            "I0719 09:20:20.081538 140481689634688 learning.py:507] global step 263: loss = 0.2558 (0.341 sec/step)\n",
            "I0719 09:20:20.411466 140481689634688 learning.py:507] global step 264: loss = 0.2661 (0.328 sec/step)\n",
            "I0719 09:20:20.758174 140481689634688 learning.py:507] global step 265: loss = 0.3208 (0.345 sec/step)\n",
            "I0719 09:20:21.131672 140481689634688 learning.py:507] global step 266: loss = 0.2478 (0.372 sec/step)\n",
            "I0719 09:20:21.468225 140481689634688 learning.py:507] global step 267: loss = 0.4070 (0.335 sec/step)\n",
            "I0719 09:20:21.836546 140481689634688 learning.py:507] global step 268: loss = 0.3589 (0.367 sec/step)\n",
            "I0719 09:20:22.158758 140481689634688 learning.py:507] global step 269: loss = 0.2736 (0.320 sec/step)\n",
            "I0719 09:20:22.459183 140481689634688 learning.py:507] global step 270: loss = 0.4092 (0.298 sec/step)\n",
            "I0719 09:20:22.748101 140481689634688 learning.py:507] global step 271: loss = 0.2604 (0.287 sec/step)\n",
            "I0719 09:20:23.025873 140481689634688 learning.py:507] global step 272: loss = 0.2176 (0.276 sec/step)\n",
            "I0719 09:20:23.372991 140481689634688 learning.py:507] global step 273: loss = 0.3271 (0.345 sec/step)\n",
            "I0719 09:20:23.709584 140481689634688 learning.py:507] global step 274: loss = 0.3407 (0.334 sec/step)\n",
            "I0719 09:20:24.043870 140481689634688 learning.py:507] global step 275: loss = 0.4169 (0.332 sec/step)\n",
            "I0719 09:20:24.387062 140481689634688 learning.py:507] global step 276: loss = 0.4572 (0.341 sec/step)\n",
            "I0719 09:20:24.727918 140481689634688 learning.py:507] global step 277: loss = 0.2872 (0.339 sec/step)\n",
            "I0719 09:20:25.068183 140481689634688 learning.py:507] global step 278: loss = 0.3370 (0.338 sec/step)\n",
            "I0719 09:20:25.388939 140481689634688 learning.py:507] global step 279: loss = 0.2539 (0.317 sec/step)\n",
            "I0719 09:20:25.725800 140481689634688 learning.py:507] global step 280: loss = 0.2254 (0.335 sec/step)\n",
            "I0719 09:20:26.088299 140481689634688 learning.py:507] global step 281: loss = 0.4261 (0.360 sec/step)\n",
            "I0719 09:20:26.654802 140481689634688 learning.py:507] global step 282: loss = 0.3251 (0.528 sec/step)\n",
            "I0719 09:20:27.150084 140481689634688 learning.py:507] global step 283: loss = 0.3300 (0.493 sec/step)\n",
            "I0719 09:20:27.205913 140479018202880 supervisor.py:1050] Recording summary at step 283.\n",
            "I0719 09:20:27.514554 140481689634688 learning.py:507] global step 284: loss = 0.2794 (0.362 sec/step)\n",
            "I0719 09:20:27.661933 140479026595584 supervisor.py:1099] global_step/sec: 1.84015\n",
            "I0719 09:20:27.838351 140481689634688 learning.py:507] global step 285: loss = 0.2486 (0.322 sec/step)\n",
            "I0719 09:20:28.139287 140481689634688 learning.py:507] global step 286: loss = 0.1320 (0.299 sec/step)\n",
            "I0719 09:20:28.478413 140481689634688 learning.py:507] global step 287: loss = 0.3727 (0.337 sec/step)\n",
            "I0719 09:20:28.823292 140481689634688 learning.py:507] global step 288: loss = 0.2983 (0.343 sec/step)\n",
            "I0719 09:20:29.158359 140481689634688 learning.py:507] global step 289: loss = 0.3526 (0.333 sec/step)\n",
            "I0719 09:20:29.448692 140481689634688 learning.py:507] global step 290: loss = 0.1404 (0.289 sec/step)\n",
            "I0719 09:20:29.784968 140481689634688 learning.py:507] global step 291: loss = 0.2813 (0.334 sec/step)\n",
            "I0719 09:20:30.137481 140481689634688 learning.py:507] global step 292: loss = 0.4359 (0.351 sec/step)\n",
            "I0719 09:20:30.405700 140481689634688 learning.py:507] global step 293: loss = 0.4003 (0.267 sec/step)\n",
            "I0719 09:20:30.745121 140481689634688 learning.py:507] global step 294: loss = 0.2303 (0.338 sec/step)\n",
            "I0719 09:20:31.083024 140481689634688 learning.py:507] global step 295: loss = 0.2784 (0.336 sec/step)\n",
            "I0719 09:20:31.409490 140481689634688 learning.py:507] global step 296: loss = 0.1210 (0.325 sec/step)\n",
            "I0719 09:20:31.736402 140481689634688 learning.py:507] global step 297: loss = 0.5016 (0.325 sec/step)\n",
            "I0719 09:20:32.062715 140481689634688 learning.py:507] global step 298: loss = 0.2612 (0.325 sec/step)\n",
            "I0719 09:20:32.395548 140481689634688 learning.py:507] global step 299: loss = 0.2317 (0.331 sec/step)\n",
            "I0719 09:20:32.724261 140481689634688 learning.py:507] global step 300: loss = 0.2306 (0.327 sec/step)\n",
            "I0719 09:20:33.058368 140481689634688 learning.py:507] global step 301: loss = 0.1580 (0.332 sec/step)\n",
            "I0719 09:20:33.401010 140481689634688 learning.py:507] global step 302: loss = 0.4253 (0.341 sec/step)\n",
            "I0719 09:20:33.706257 140481689634688 learning.py:507] global step 303: loss = 0.1363 (0.304 sec/step)\n",
            "I0719 09:20:34.064063 140481689634688 learning.py:507] global step 304: loss = 0.2799 (0.356 sec/step)\n",
            "I0719 09:20:34.379530 140481689634688 learning.py:507] global step 305: loss = 0.7634 (0.314 sec/step)\n",
            "I0719 09:20:34.698611 140481689634688 learning.py:507] global step 306: loss = 0.0479 (0.317 sec/step)\n",
            "I0719 09:20:35.034059 140481689634688 learning.py:507] global step 307: loss = 0.1794 (0.333 sec/step)\n",
            "I0719 09:20:35.364128 140481689634688 learning.py:507] global step 308: loss = 0.3266 (0.328 sec/step)\n",
            "I0719 09:20:35.679188 140481689634688 learning.py:507] global step 309: loss = 0.7090 (0.313 sec/step)\n",
            "I0719 09:20:36.046682 140481689634688 learning.py:507] global step 310: loss = 0.4043 (0.366 sec/step)\n",
            "I0719 09:20:36.393422 140481689634688 learning.py:507] global step 311: loss = 0.1964 (0.345 sec/step)\n",
            "I0719 09:20:36.728063 140481689634688 learning.py:507] global step 312: loss = 0.3509 (0.333 sec/step)\n",
            "I0719 09:20:37.079392 140481689634688 learning.py:507] global step 313: loss = 0.2298 (0.350 sec/step)\n",
            "I0719 09:20:37.410262 140481689634688 learning.py:507] global step 314: loss = 0.5560 (0.329 sec/step)\n",
            "I0719 09:20:37.749622 140481689634688 learning.py:507] global step 315: loss = 0.5032 (0.338 sec/step)\n",
            "I0719 09:20:38.098014 140481689634688 learning.py:507] global step 316: loss = 0.2831 (0.347 sec/step)\n",
            "I0719 09:20:38.428912 140481689634688 learning.py:507] global step 317: loss = 0.3918 (0.329 sec/step)\n",
            "I0719 09:20:38.768224 140481689634688 learning.py:507] global step 318: loss = 0.1951 (0.337 sec/step)\n",
            "I0719 09:20:39.117306 140481689634688 learning.py:507] global step 319: loss = 0.3224 (0.347 sec/step)\n",
            "I0719 09:20:39.439291 140481689634688 learning.py:507] global step 320: loss = 0.3649 (0.320 sec/step)\n",
            "I0719 09:20:39.827455 140481689634688 learning.py:507] global step 321: loss = 0.2631 (0.386 sec/step)\n",
            "I0719 09:20:40.158722 140481689634688 learning.py:507] global step 322: loss = 0.7087 (0.329 sec/step)\n",
            "I0719 09:20:40.439915 140481689634688 learning.py:507] global step 323: loss = 0.2096 (0.279 sec/step)\n",
            "I0719 09:20:40.733918 140481689634688 learning.py:507] global step 324: loss = 0.2443 (0.292 sec/step)\n",
            "I0719 09:20:41.058479 140481689634688 learning.py:507] global step 325: loss = 0.1419 (0.323 sec/step)\n",
            "I0719 09:20:41.353976 140481689634688 learning.py:507] global step 326: loss = 0.2003 (0.294 sec/step)\n",
            "I0719 09:20:41.717644 140481689634688 learning.py:507] global step 327: loss = 0.2545 (0.362 sec/step)\n",
            "I0719 09:20:42.066646 140481689634688 learning.py:507] global step 328: loss = 0.3600 (0.347 sec/step)\n",
            "I0719 09:20:42.424570 140481689634688 learning.py:507] global step 329: loss = 0.4599 (0.356 sec/step)\n",
            "I0719 09:20:42.752504 140481689634688 learning.py:507] global step 330: loss = 0.1075 (0.326 sec/step)\n",
            "I0719 09:20:43.086730 140481689634688 learning.py:507] global step 331: loss = 0.0764 (0.332 sec/step)\n",
            "I0719 09:20:43.401292 140481689634688 learning.py:507] global step 332: loss = 0.4233 (0.313 sec/step)\n",
            "I0719 09:20:43.732642 140481689634688 learning.py:507] global step 333: loss = 0.2144 (0.330 sec/step)\n",
            "I0719 09:20:44.062607 140481689634688 learning.py:507] global step 334: loss = 0.2031 (0.328 sec/step)\n",
            "I0719 09:20:44.388439 140481689634688 learning.py:507] global step 335: loss = 0.1556 (0.324 sec/step)\n",
            "I0719 09:20:44.733587 140481689634688 learning.py:507] global step 336: loss = 0.4383 (0.343 sec/step)\n",
            "I0719 09:20:45.079543 140481689634688 learning.py:507] global step 337: loss = 0.3453 (0.344 sec/step)\n",
            "I0719 09:20:45.418108 140481689634688 learning.py:507] global step 338: loss = 1.1128 (0.337 sec/step)\n",
            "I0719 09:20:45.733444 140481689634688 learning.py:507] global step 339: loss = 0.2789 (0.314 sec/step)\n",
            "I0719 09:20:46.084106 140481689634688 learning.py:507] global step 340: loss = 0.2493 (0.349 sec/step)\n",
            "I0719 09:20:46.422817 140481689634688 learning.py:507] global step 341: loss = 0.3407 (0.337 sec/step)\n",
            "I0719 09:20:47.179770 140481689634688 learning.py:507] global step 342: loss = 0.4530 (0.755 sec/step)\n",
            "I0719 09:20:47.546089 140481689634688 learning.py:507] global step 343: loss = 0.3821 (0.365 sec/step)\n",
            "I0719 09:20:47.842461 140481689634688 learning.py:507] global step 344: loss = 0.1414 (0.294 sec/step)\n",
            "I0719 09:20:48.180749 140481689634688 learning.py:507] global step 345: loss = 0.0990 (0.336 sec/step)\n",
            "I0719 09:20:48.529957 140481689634688 learning.py:507] global step 346: loss = 0.1294 (0.347 sec/step)\n",
            "I0719 09:20:48.863116 140481689634688 learning.py:507] global step 347: loss = 0.3532 (0.331 sec/step)\n",
            "I0719 09:20:49.211952 140481689634688 learning.py:507] global step 348: loss = 0.1914 (0.347 sec/step)\n",
            "I0719 09:20:49.560686 140481689634688 learning.py:507] global step 349: loss = 0.3002 (0.347 sec/step)\n",
            "I0719 09:20:49.908328 140481689634688 learning.py:507] global step 350: loss = 0.3558 (0.345 sec/step)\n",
            "I0719 09:20:50.262006 140481689634688 learning.py:507] global step 351: loss = 0.2013 (0.352 sec/step)\n",
            "I0719 09:20:50.591455 140481689634688 learning.py:507] global step 352: loss = 0.2161 (0.327 sec/step)\n",
            "I0719 09:20:50.925005 140481689634688 learning.py:507] global step 353: loss = 0.1719 (0.332 sec/step)\n",
            "I0719 09:20:51.262552 140481689634688 learning.py:507] global step 354: loss = 0.3998 (0.336 sec/step)\n",
            "I0719 09:20:51.607127 140481689634688 learning.py:507] global step 355: loss = 0.5191 (0.342 sec/step)\n",
            "I0719 09:20:51.955959 140481689634688 learning.py:507] global step 356: loss = 0.2678 (0.347 sec/step)\n",
            "I0719 09:20:52.305378 140481689634688 learning.py:507] global step 357: loss = 0.2101 (0.347 sec/step)\n",
            "I0719 09:20:52.642206 140481689634688 learning.py:507] global step 358: loss = 0.3675 (0.335 sec/step)\n",
            "I0719 09:20:52.932421 140481689634688 learning.py:507] global step 359: loss = 0.3383 (0.288 sec/step)\n",
            "I0719 09:20:53.244084 140481689634688 learning.py:507] global step 360: loss = 0.2572 (0.310 sec/step)\n",
            "I0719 09:20:53.534021 140481689634688 learning.py:507] global step 361: loss = 0.2648 (0.287 sec/step)\n",
            "I0719 09:20:53.858920 140481689634688 learning.py:507] global step 362: loss = 0.0439 (0.323 sec/step)\n",
            "I0719 09:20:54.196417 140481689634688 learning.py:507] global step 363: loss = 0.2940 (0.336 sec/step)\n",
            "I0719 09:20:54.517138 140481689634688 learning.py:507] global step 364: loss = 0.2975 (0.319 sec/step)\n",
            "I0719 09:20:54.833514 140481689634688 learning.py:507] global step 365: loss = 0.6860 (0.314 sec/step)\n",
            "I0719 09:20:55.194768 140481689634688 learning.py:507] global step 366: loss = 0.1740 (0.359 sec/step)\n",
            "I0719 09:20:55.566465 140481689634688 learning.py:507] global step 367: loss = 0.2182 (0.370 sec/step)\n",
            "I0719 09:20:55.899930 140481689634688 learning.py:507] global step 368: loss = 0.2429 (0.332 sec/step)\n",
            "I0719 09:20:56.223112 140481689634688 learning.py:507] global step 369: loss = 0.3575 (0.321 sec/step)\n",
            "I0719 09:20:56.552062 140481689634688 learning.py:507] global step 370: loss = 0.2988 (0.327 sec/step)\n",
            "I0719 09:20:56.870976 140481689634688 learning.py:507] global step 371: loss = 0.2951 (0.317 sec/step)\n",
            "I0719 09:20:57.190791 140481689634688 learning.py:507] global step 372: loss = 0.2909 (0.318 sec/step)\n",
            "I0719 09:20:57.522773 140481689634688 learning.py:507] global step 373: loss = 0.4445 (0.330 sec/step)\n",
            "I0719 09:20:57.791376 140481689634688 learning.py:507] global step 374: loss = 0.2609 (0.267 sec/step)\n",
            "I0719 09:20:58.119657 140481689634688 learning.py:507] global step 375: loss = 0.3823 (0.327 sec/step)\n",
            "I0719 09:20:58.463164 140481689634688 learning.py:507] global step 376: loss = 0.2662 (0.342 sec/step)\n",
            "I0719 09:20:58.808354 140481689634688 learning.py:507] global step 377: loss = 0.4596 (0.343 sec/step)\n",
            "I0719 09:20:59.149454 140481689634688 learning.py:507] global step 378: loss = 0.3150 (0.339 sec/step)\n",
            "I0719 09:20:59.504354 140481689634688 learning.py:507] global step 379: loss = 0.3733 (0.353 sec/step)\n",
            "I0719 09:20:59.875415 140481689634688 learning.py:507] global step 380: loss = 0.2679 (0.370 sec/step)\n",
            "I0719 09:21:00.211535 140481689634688 learning.py:507] global step 381: loss = 0.5275 (0.334 sec/step)\n",
            "I0719 09:21:00.551761 140481689634688 learning.py:507] global step 382: loss = 0.3442 (0.338 sec/step)\n",
            "I0719 09:21:00.891141 140481689634688 learning.py:507] global step 383: loss = 0.1933 (0.338 sec/step)\n",
            "I0719 09:21:01.237448 140481689634688 learning.py:507] global step 384: loss = 0.2904 (0.344 sec/step)\n",
            "I0719 09:21:01.583508 140481689634688 learning.py:507] global step 385: loss = 0.0848 (0.344 sec/step)\n",
            "I0719 09:21:01.930167 140481689634688 learning.py:507] global step 386: loss = 0.2613 (0.345 sec/step)\n",
            "I0719 09:21:02.309533 140481689634688 learning.py:507] global step 387: loss = 0.2079 (0.377 sec/step)\n",
            "I0719 09:21:02.652441 140481689634688 learning.py:507] global step 388: loss = 0.2230 (0.340 sec/step)\n",
            "I0719 09:21:02.957681 140481689634688 learning.py:507] global step 389: loss = 0.4445 (0.304 sec/step)\n",
            "I0719 09:21:03.261693 140481689634688 learning.py:507] global step 390: loss = 0.6378 (0.302 sec/step)\n",
            "I0719 09:21:03.609144 140481689634688 learning.py:507] global step 391: loss = 0.3658 (0.346 sec/step)\n",
            "I0719 09:21:03.951302 140481689634688 learning.py:507] global step 392: loss = 0.3212 (0.340 sec/step)\n",
            "I0719 09:21:04.282207 140481689634688 learning.py:507] global step 393: loss = 0.2952 (0.329 sec/step)\n",
            "I0719 09:21:04.636877 140481689634688 learning.py:507] global step 394: loss = 0.4211 (0.353 sec/step)\n",
            "I0719 09:21:04.978172 140481689634688 learning.py:507] global step 395: loss = 0.2976 (0.339 sec/step)\n",
            "I0719 09:21:05.289386 140481689634688 learning.py:507] global step 396: loss = 0.2186 (0.309 sec/step)\n",
            "I0719 09:21:05.630644 140481689634688 learning.py:507] global step 397: loss = 0.2408 (0.340 sec/step)\n",
            "I0719 09:21:05.985707 140481689634688 learning.py:507] global step 398: loss = 0.1987 (0.353 sec/step)\n",
            "I0719 09:21:06.316725 140481689634688 learning.py:507] global step 399: loss = 0.0620 (0.329 sec/step)\n",
            "I0719 09:21:06.652953 140481689634688 learning.py:507] global step 400: loss = 0.2163 (0.335 sec/step)\n",
            "I0719 09:21:06.989931 140481689634688 learning.py:507] global step 401: loss = 0.4023 (0.335 sec/step)\n",
            "I0719 09:21:07.331154 140481689634688 learning.py:507] global step 402: loss = 0.2997 (0.339 sec/step)\n",
            "I0719 09:21:07.674756 140481689634688 learning.py:507] global step 403: loss = 0.1856 (0.342 sec/step)\n",
            "I0719 09:21:07.967710 140481689634688 learning.py:507] global step 404: loss = 0.1353 (0.291 sec/step)\n",
            "I0719 09:21:08.322582 140481689634688 learning.py:507] global step 405: loss = 0.2893 (0.353 sec/step)\n",
            "I0719 09:21:08.650151 140481689634688 learning.py:507] global step 406: loss = 0.5189 (0.323 sec/step)\n",
            "I0719 09:21:08.996872 140481689634688 learning.py:507] global step 407: loss = 0.1572 (0.345 sec/step)\n",
            "I0719 09:21:09.339111 140481689634688 learning.py:507] global step 408: loss = 0.2715 (0.341 sec/step)\n",
            "I0719 09:21:09.698596 140481689634688 learning.py:507] global step 409: loss = 0.3964 (0.357 sec/step)\n",
            "I0719 09:21:10.042342 140481689634688 learning.py:507] global step 410: loss = 0.2622 (0.342 sec/step)\n",
            "I0719 09:21:10.391428 140481689634688 learning.py:507] global step 411: loss = 0.1566 (0.347 sec/step)\n",
            "I0719 09:21:10.771116 140481689634688 learning.py:507] global step 412: loss = 0.0551 (0.378 sec/step)\n",
            "I0719 09:21:11.088947 140481689634688 learning.py:507] global step 413: loss = 0.1996 (0.316 sec/step)\n",
            "I0719 09:21:11.397138 140481689634688 learning.py:507] global step 414: loss = 0.2213 (0.306 sec/step)\n",
            "I0719 09:21:11.753539 140481689634688 learning.py:507] global step 415: loss = 0.4948 (0.351 sec/step)\n",
            "I0719 09:21:12.087072 140481689634688 learning.py:507] global step 416: loss = 0.2412 (0.332 sec/step)\n",
            "I0719 09:21:12.413900 140481689634688 learning.py:507] global step 417: loss = 0.0631 (0.325 sec/step)\n",
            "I0719 09:21:12.757401 140481689634688 learning.py:507] global step 418: loss = 0.2657 (0.342 sec/step)\n",
            "I0719 09:21:13.087038 140481689634688 learning.py:507] global step 419: loss = 0.2564 (0.328 sec/step)\n",
            "I0719 09:21:13.419679 140481689634688 learning.py:507] global step 420: loss = 0.1851 (0.331 sec/step)\n",
            "I0719 09:21:13.762589 140481689634688 learning.py:507] global step 421: loss = 0.2467 (0.341 sec/step)\n",
            "I0719 09:21:14.119523 140481689634688 learning.py:507] global step 422: loss = 0.2796 (0.355 sec/step)\n",
            "I0719 09:21:14.409691 140481689634688 learning.py:507] global step 423: loss = 0.2554 (0.288 sec/step)\n",
            "I0719 09:21:14.737982 140481689634688 learning.py:507] global step 424: loss = 0.1705 (0.327 sec/step)\n",
            "I0719 09:21:15.087087 140481689634688 learning.py:507] global step 425: loss = 0.3207 (0.347 sec/step)\n",
            "I0719 09:21:15.396178 140481689634688 learning.py:507] global step 426: loss = 0.2007 (0.307 sec/step)\n",
            "I0719 09:21:15.697546 140481689634688 learning.py:507] global step 427: loss = 0.6550 (0.300 sec/step)\n",
            "I0719 09:21:16.033470 140481689634688 learning.py:507] global step 428: loss = 0.2103 (0.334 sec/step)\n",
            "I0719 09:21:16.384079 140481689634688 learning.py:507] global step 429: loss = 0.4025 (0.348 sec/step)\n",
            "I0719 09:21:16.736044 140481689634688 learning.py:507] global step 430: loss = 0.1903 (0.350 sec/step)\n",
            "I0719 09:21:17.084068 140481689634688 learning.py:507] global step 431: loss = 0.1791 (0.346 sec/step)\n",
            "I0719 09:21:17.382015 140481689634688 learning.py:507] global step 432: loss = 0.2445 (0.296 sec/step)\n",
            "I0719 09:21:17.701841 140481689634688 learning.py:507] global step 433: loss = 0.2263 (0.318 sec/step)\n",
            "I0719 09:21:18.054895 140481689634688 learning.py:507] global step 434: loss = 0.1895 (0.351 sec/step)\n",
            "I0719 09:21:18.401527 140481689634688 learning.py:507] global step 435: loss = 0.3460 (0.345 sec/step)\n",
            "I0719 09:21:18.734993 140481689634688 learning.py:507] global step 436: loss = 0.4127 (0.332 sec/step)\n",
            "I0719 09:21:19.098416 140481689634688 learning.py:507] global step 437: loss = 0.3371 (0.361 sec/step)\n",
            "I0719 09:21:19.386025 140481689634688 learning.py:507] global step 438: loss = 0.1414 (0.286 sec/step)\n",
            "I0719 09:21:19.704058 140481689634688 learning.py:507] global step 439: loss = 0.0549 (0.317 sec/step)\n",
            "I0719 09:21:20.039165 140481689634688 learning.py:507] global step 440: loss = 0.2459 (0.333 sec/step)\n",
            "I0719 09:21:20.318043 140481689634688 learning.py:507] global step 441: loss = 0.3090 (0.277 sec/step)\n",
            "I0719 09:21:20.670731 140481689634688 learning.py:507] global step 442: loss = 0.2981 (0.351 sec/step)\n",
            "I0719 09:21:21.007236 140481689634688 learning.py:507] global step 443: loss = 0.4597 (0.335 sec/step)\n",
            "I0719 09:21:21.328434 140481689634688 learning.py:507] global step 444: loss = 0.2101 (0.319 sec/step)\n",
            "I0719 09:21:21.656804 140481689634688 learning.py:507] global step 445: loss = 0.2114 (0.326 sec/step)\n",
            "I0719 09:21:22.033434 140481689634688 learning.py:507] global step 446: loss = 0.1659 (0.375 sec/step)\n",
            "I0719 09:21:22.368843 140481689634688 learning.py:507] global step 447: loss = 0.3186 (0.334 sec/step)\n",
            "I0719 09:21:22.720117 140481689634688 learning.py:507] global step 448: loss = 0.2683 (0.349 sec/step)\n",
            "I0719 09:21:23.039256 140481689634688 learning.py:507] global step 449: loss = 0.0533 (0.317 sec/step)\n",
            "I0719 09:21:23.398109 140481689634688 learning.py:507] global step 450: loss = 0.3045 (0.357 sec/step)\n",
            "I0719 09:21:23.706324 140481689634688 learning.py:507] global step 451: loss = 0.1968 (0.306 sec/step)\n",
            "I0719 09:21:24.057239 140481689634688 learning.py:507] global step 452: loss = 0.1763 (0.349 sec/step)\n",
            "I0719 09:21:24.405261 140481689634688 learning.py:507] global step 453: loss = 0.4621 (0.347 sec/step)\n",
            "I0719 09:21:24.763925 140481689634688 learning.py:507] global step 454: loss = 0.2895 (0.355 sec/step)\n",
            "I0719 09:21:25.098921 140481689634688 learning.py:507] global step 455: loss = 0.3579 (0.333 sec/step)\n",
            "I0719 09:21:25.443988 140481689634688 learning.py:507] global step 456: loss = 0.3234 (0.343 sec/step)\n",
            "I0719 09:21:25.782615 140481689634688 learning.py:507] global step 457: loss = 0.4024 (0.337 sec/step)\n",
            "I0719 09:21:26.125066 140481689634688 learning.py:507] global step 458: loss = 0.3164 (0.340 sec/step)\n",
            "I0719 09:21:26.472688 140481689634688 learning.py:507] global step 459: loss = 0.2718 (0.346 sec/step)\n",
            "I0719 09:21:26.809118 140481689634688 learning.py:507] global step 460: loss = 0.1008 (0.334 sec/step)\n",
            "I0719 09:21:27.142404 140481689634688 learning.py:507] global step 461: loss = 0.2042 (0.331 sec/step)\n",
            "I0719 09:21:27.464959 140481689634688 learning.py:507] global step 462: loss = 0.2718 (0.321 sec/step)\n",
            "I0719 09:21:27.795476 140481689634688 learning.py:507] global step 463: loss = 0.1924 (0.329 sec/step)\n",
            "I0719 09:21:28.116224 140481689634688 learning.py:507] global step 464: loss = 0.1645 (0.319 sec/step)\n",
            "I0719 09:21:28.463898 140481689634688 learning.py:507] global step 465: loss = 0.2444 (0.346 sec/step)\n",
            "I0719 09:21:28.775213 140481689634688 learning.py:507] global step 466: loss = 0.2006 (0.310 sec/step)\n",
            "I0719 09:21:29.060961 140481689634688 learning.py:507] global step 467: loss = 0.0732 (0.284 sec/step)\n",
            "I0719 09:21:29.388427 140481689634688 learning.py:507] global step 468: loss = 0.1504 (0.326 sec/step)\n",
            "I0719 09:21:29.737157 140481689634688 learning.py:507] global step 469: loss = 0.0887 (0.347 sec/step)\n",
            "I0719 09:21:30.074363 140481689634688 learning.py:507] global step 470: loss = 0.4921 (0.336 sec/step)\n",
            "I0719 09:21:30.419109 140481689634688 learning.py:507] global step 471: loss = 0.2579 (0.343 sec/step)\n",
            "I0719 09:21:30.724915 140481689634688 learning.py:507] global step 472: loss = 0.1693 (0.304 sec/step)\n",
            "I0719 09:21:31.056812 140481689634688 learning.py:507] global step 473: loss = 0.1335 (0.330 sec/step)\n",
            "I0719 09:21:31.411926 140481689634688 learning.py:507] global step 474: loss = 0.2335 (0.353 sec/step)\n",
            "I0719 09:21:31.804823 140481689634688 learning.py:507] global step 475: loss = 0.2163 (0.391 sec/step)\n",
            "I0719 09:21:32.121978 140481689634688 learning.py:507] global step 476: loss = 0.3315 (0.315 sec/step)\n",
            "I0719 09:21:32.477940 140481689634688 learning.py:507] global step 477: loss = 0.1959 (0.354 sec/step)\n",
            "I0719 09:21:32.820420 140481689634688 learning.py:507] global step 478: loss = 0.1837 (0.341 sec/step)\n",
            "I0719 09:21:33.142219 140481689634688 learning.py:507] global step 479: loss = 0.2977 (0.320 sec/step)\n",
            "I0719 09:21:33.493231 140481689634688 learning.py:507] global step 480: loss = 0.3946 (0.349 sec/step)\n",
            "I0719 09:21:33.823668 140481689634688 learning.py:507] global step 481: loss = 0.0561 (0.329 sec/step)\n",
            "I0719 09:21:34.157379 140481689634688 learning.py:507] global step 482: loss = 0.2055 (0.332 sec/step)\n",
            "I0719 09:21:34.522757 140481689634688 learning.py:507] global step 483: loss = 0.2642 (0.364 sec/step)\n",
            "I0719 09:21:34.901001 140481689634688 learning.py:507] global step 484: loss = 0.1914 (0.376 sec/step)\n",
            "I0719 09:21:35.233355 140481689634688 learning.py:507] global step 485: loss = 0.1911 (0.330 sec/step)\n",
            "I0719 09:21:35.579332 140481689634688 learning.py:507] global step 486: loss = 0.2565 (0.344 sec/step)\n",
            "I0719 09:21:35.929671 140481689634688 learning.py:507] global step 487: loss = 0.1144 (0.348 sec/step)\n",
            "I0719 09:21:36.267659 140481689634688 learning.py:507] global step 488: loss = 0.1089 (0.336 sec/step)\n",
            "I0719 09:21:36.630793 140481689634688 learning.py:507] global step 489: loss = 0.4865 (0.361 sec/step)\n",
            "I0719 09:21:36.997994 140481689634688 learning.py:507] global step 490: loss = 0.2038 (0.365 sec/step)\n",
            "I0719 09:21:37.354449 140481689634688 learning.py:507] global step 491: loss = 0.1182 (0.355 sec/step)\n",
            "I0719 09:21:37.688797 140481689634688 learning.py:507] global step 492: loss = 0.4453 (0.333 sec/step)\n",
            "I0719 09:21:38.036917 140481689634688 learning.py:507] global step 493: loss = 0.3563 (0.346 sec/step)\n",
            "I0719 09:21:38.369040 140481689634688 learning.py:507] global step 494: loss = 0.1578 (0.330 sec/step)\n",
            "I0719 09:21:38.720574 140481689634688 learning.py:507] global step 495: loss = 0.4364 (0.350 sec/step)\n",
            "I0719 09:21:39.072626 140481689634688 learning.py:507] global step 496: loss = 0.1974 (0.350 sec/step)\n",
            "I0719 09:21:39.414777 140481689634688 learning.py:507] global step 497: loss = 0.2596 (0.340 sec/step)\n",
            "I0719 09:21:39.770001 140481689634688 learning.py:507] global step 498: loss = 0.2972 (0.353 sec/step)\n",
            "I0719 09:21:40.053877 140481689634688 learning.py:507] global step 499: loss = 0.7261 (0.282 sec/step)\n",
            "I0719 09:21:40.386742 140481689634688 learning.py:507] global step 500: loss = 0.1116 (0.331 sec/step)\n",
            "I0719 09:21:40.721733 140481689634688 learning.py:507] global step 501: loss = 0.2084 (0.333 sec/step)\n",
            "I0719 09:21:41.047088 140481689634688 learning.py:507] global step 502: loss = 0.0759 (0.323 sec/step)\n",
            "I0719 09:21:41.385190 140481689634688 learning.py:507] global step 503: loss = 0.3792 (0.336 sec/step)\n",
            "I0719 09:21:41.720918 140481689634688 learning.py:507] global step 504: loss = 0.3368 (0.334 sec/step)\n",
            "I0719 09:21:42.027595 140481689634688 learning.py:507] global step 505: loss = 0.0889 (0.305 sec/step)\n",
            "I0719 09:21:42.370198 140481689634688 learning.py:507] global step 506: loss = 0.2511 (0.341 sec/step)\n",
            "I0719 09:21:42.684250 140481689634688 learning.py:507] global step 507: loss = 0.3099 (0.312 sec/step)\n",
            "I0719 09:21:42.975691 140481689634688 learning.py:507] global step 508: loss = 0.2665 (0.290 sec/step)\n",
            "I0719 09:21:43.318001 140481689634688 learning.py:507] global step 509: loss = 0.1468 (0.340 sec/step)\n",
            "I0719 09:21:43.645357 140481689634688 learning.py:507] global step 510: loss = 0.0709 (0.326 sec/step)\n",
            "I0719 09:21:43.997821 140481689634688 learning.py:507] global step 511: loss = 0.0895 (0.351 sec/step)\n",
            "I0719 09:21:44.326221 140481689634688 learning.py:507] global step 512: loss = 0.3130 (0.326 sec/step)\n",
            "I0719 09:21:44.648344 140481689634688 learning.py:507] global step 513: loss = 0.1516 (0.320 sec/step)\n",
            "I0719 09:21:44.963087 140481689634688 learning.py:507] global step 514: loss = 0.3577 (0.313 sec/step)\n",
            "I0719 09:21:45.319613 140481689634688 learning.py:507] global step 515: loss = 0.3573 (0.355 sec/step)\n",
            "I0719 09:21:45.611716 140481689634688 learning.py:507] global step 516: loss = 0.2202 (0.290 sec/step)\n",
            "I0719 09:21:45.934055 140481689634688 learning.py:507] global step 517: loss = 0.2694 (0.320 sec/step)\n",
            "I0719 09:21:46.297496 140481689634688 learning.py:507] global step 518: loss = 0.2852 (0.361 sec/step)\n",
            "I0719 09:21:46.648118 140481689634688 learning.py:507] global step 519: loss = 0.1076 (0.349 sec/step)\n",
            "I0719 09:21:46.991044 140481689634688 learning.py:507] global step 520: loss = 0.1058 (0.340 sec/step)\n",
            "I0719 09:21:47.332532 140481689634688 learning.py:507] global step 521: loss = 0.1474 (0.339 sec/step)\n",
            "I0719 09:21:47.674668 140481689634688 learning.py:507] global step 522: loss = 0.3174 (0.340 sec/step)\n",
            "I0719 09:21:48.018703 140481689634688 learning.py:507] global step 523: loss = 0.4232 (0.342 sec/step)\n",
            "I0719 09:21:48.367151 140481689634688 learning.py:507] global step 524: loss = 0.1891 (0.347 sec/step)\n",
            "I0719 09:21:48.714965 140481689634688 learning.py:507] global step 525: loss = 0.2720 (0.346 sec/step)\n",
            "I0719 09:21:49.064137 140481689634688 learning.py:507] global step 526: loss = 0.0972 (0.348 sec/step)\n",
            "I0719 09:21:49.408778 140481689634688 learning.py:507] global step 527: loss = 0.3662 (0.343 sec/step)\n",
            "I0719 09:21:49.720761 140481689634688 learning.py:507] global step 528: loss = 0.1724 (0.310 sec/step)\n",
            "I0719 09:21:50.063259 140481689634688 learning.py:507] global step 529: loss = 0.3946 (0.341 sec/step)\n",
            "I0719 09:21:50.415258 140481689634688 learning.py:507] global step 530: loss = 0.2531 (0.350 sec/step)\n",
            "I0719 09:21:50.763783 140481689634688 learning.py:507] global step 531: loss = 0.2901 (0.347 sec/step)\n",
            "I0719 09:21:51.107727 140481689634688 learning.py:507] global step 532: loss = 0.1666 (0.342 sec/step)\n",
            "I0719 09:21:51.448193 140481689634688 learning.py:507] global step 533: loss = 0.3099 (0.339 sec/step)\n",
            "I0719 09:21:51.781166 140481689634688 learning.py:507] global step 534: loss = 0.2849 (0.331 sec/step)\n",
            "I0719 09:21:52.118421 140481689634688 learning.py:507] global step 535: loss = 0.2011 (0.335 sec/step)\n",
            "I0719 09:21:52.451551 140481689634688 learning.py:507] global step 536: loss = 0.0383 (0.331 sec/step)\n",
            "I0719 09:21:52.763930 140481689634688 learning.py:507] global step 537: loss = 0.1129 (0.310 sec/step)\n",
            "I0719 09:21:53.114601 140481689634688 learning.py:507] global step 538: loss = 0.2539 (0.349 sec/step)\n",
            "I0719 09:21:53.440645 140481689634688 learning.py:507] global step 539: loss = 0.2208 (0.324 sec/step)\n",
            "I0719 09:21:53.787193 140481689634688 learning.py:507] global step 540: loss = 0.0962 (0.345 sec/step)\n",
            "I0719 09:21:54.131634 140481689634688 learning.py:507] global step 541: loss = 0.1684 (0.343 sec/step)\n",
            "I0719 09:21:54.469613 140481689634688 learning.py:507] global step 542: loss = 0.2975 (0.336 sec/step)\n",
            "I0719 09:21:54.811857 140481689634688 learning.py:507] global step 543: loss = 0.2169 (0.340 sec/step)\n",
            "I0719 09:21:55.136288 140481689634688 learning.py:507] global step 544: loss = 0.4857 (0.323 sec/step)\n",
            "I0719 09:21:55.486201 140481689634688 learning.py:507] global step 545: loss = 0.2169 (0.348 sec/step)\n",
            "I0719 09:21:55.797766 140481689634688 learning.py:507] global step 546: loss = 0.6332 (0.310 sec/step)\n",
            "I0719 09:21:56.144481 140481689634688 learning.py:507] global step 547: loss = 0.3070 (0.345 sec/step)\n",
            "I0719 09:21:56.479768 140481689634688 learning.py:507] global step 548: loss = 0.3181 (0.333 sec/step)\n",
            "I0719 09:21:56.821794 140481689634688 learning.py:507] global step 549: loss = 1.1252 (0.340 sec/step)\n",
            "I0719 09:21:57.146914 140481689634688 learning.py:507] global step 550: loss = 0.1414 (0.323 sec/step)\n",
            "I0719 09:21:57.519693 140481689634688 learning.py:507] global step 551: loss = 0.1858 (0.371 sec/step)\n",
            "I0719 09:21:57.838214 140481689634688 learning.py:507] global step 552: loss = 0.0998 (0.317 sec/step)\n",
            "I0719 09:21:58.171364 140481689634688 learning.py:507] global step 553: loss = 0.3379 (0.331 sec/step)\n",
            "I0719 09:21:58.495532 140481689634688 learning.py:507] global step 554: loss = 0.2748 (0.322 sec/step)\n",
            "I0719 09:21:58.831954 140481689634688 learning.py:507] global step 555: loss = 0.0670 (0.334 sec/step)\n",
            "I0719 09:21:59.128239 140481689634688 learning.py:507] global step 556: loss = 0.5169 (0.294 sec/step)\n",
            "I0719 09:21:59.448526 140481689634688 learning.py:507] global step 557: loss = 0.2309 (0.318 sec/step)\n",
            "I0719 09:21:59.815512 140481689634688 learning.py:507] global step 558: loss = 0.3723 (0.365 sec/step)\n",
            "I0719 09:22:00.150461 140481689634688 learning.py:507] global step 559: loss = 0.1947 (0.333 sec/step)\n",
            "I0719 09:22:00.479240 140481689634688 learning.py:507] global step 560: loss = 0.0954 (0.327 sec/step)\n",
            "I0719 09:22:00.833831 140481689634688 learning.py:507] global step 561: loss = 0.2844 (0.353 sec/step)\n",
            "I0719 09:22:01.193459 140481689634688 learning.py:507] global step 562: loss = 0.1761 (0.358 sec/step)\n",
            "I0719 09:22:01.536527 140481689634688 learning.py:507] global step 563: loss = 0.2089 (0.341 sec/step)\n",
            "I0719 09:22:01.865046 140481689634688 learning.py:507] global step 564: loss = 0.0728 (0.327 sec/step)\n",
            "I0719 09:22:02.211390 140481689634688 learning.py:507] global step 565: loss = 0.2125 (0.344 sec/step)\n",
            "I0719 09:22:02.539909 140481689634688 learning.py:507] global step 566: loss = 0.1021 (0.327 sec/step)\n",
            "I0719 09:22:02.885574 140481689634688 learning.py:507] global step 567: loss = 0.4150 (0.344 sec/step)\n",
            "I0719 09:22:03.207373 140481689634688 learning.py:507] global step 568: loss = 0.0768 (0.318 sec/step)\n",
            "I0719 09:22:03.560706 140481689634688 learning.py:507] global step 569: loss = 0.2181 (0.351 sec/step)\n",
            "I0719 09:22:03.932709 140481689634688 learning.py:507] global step 570: loss = 0.0997 (0.370 sec/step)\n",
            "I0719 09:22:04.276910 140481689634688 learning.py:507] global step 571: loss = 0.1830 (0.342 sec/step)\n",
            "I0719 09:22:04.650716 140481689634688 learning.py:507] global step 572: loss = 0.3086 (0.372 sec/step)\n",
            "I0719 09:22:04.983351 140481689634688 learning.py:507] global step 573: loss = 0.1801 (0.330 sec/step)\n",
            "I0719 09:22:05.304517 140481689634688 learning.py:507] global step 574: loss = 0.2124 (0.319 sec/step)\n",
            "I0719 09:22:05.620439 140481689634688 learning.py:507] global step 575: loss = 0.1196 (0.314 sec/step)\n",
            "I0719 09:22:05.969747 140481689634688 learning.py:507] global step 576: loss = 0.1780 (0.348 sec/step)\n",
            "I0719 09:22:06.305597 140481689634688 learning.py:507] global step 577: loss = 0.4659 (0.334 sec/step)\n",
            "I0719 09:22:06.650686 140481689634688 learning.py:507] global step 578: loss = 0.2385 (0.343 sec/step)\n",
            "I0719 09:22:07.008139 140481689634688 learning.py:507] global step 579: loss = 0.7983 (0.355 sec/step)\n",
            "I0719 09:22:07.352681 140481689634688 learning.py:507] global step 580: loss = 0.0901 (0.343 sec/step)\n",
            "I0719 09:22:07.689485 140481689634688 learning.py:507] global step 581: loss = 0.2068 (0.335 sec/step)\n",
            "I0719 09:22:08.036395 140481689634688 learning.py:507] global step 582: loss = 0.3418 (0.345 sec/step)\n",
            "I0719 09:22:08.341362 140481689634688 learning.py:507] global step 583: loss = 0.1680 (0.303 sec/step)\n",
            "I0719 09:22:08.669509 140481689634688 learning.py:507] global step 584: loss = 0.1807 (0.326 sec/step)\n",
            "I0719 09:22:08.976560 140481689634688 learning.py:507] global step 585: loss = 0.4550 (0.305 sec/step)\n",
            "I0719 09:22:09.322818 140481689634688 learning.py:507] global step 586: loss = 0.2010 (0.345 sec/step)\n",
            "I0719 09:22:09.650777 140481689634688 learning.py:507] global step 587: loss = 0.2101 (0.326 sec/step)\n",
            "I0719 09:22:10.000757 140481689634688 learning.py:507] global step 588: loss = 0.2314 (0.348 sec/step)\n",
            "I0719 09:22:10.302693 140481689634688 learning.py:507] global step 589: loss = 0.0861 (0.300 sec/step)\n",
            "I0719 09:22:10.654121 140481689634688 learning.py:507] global step 590: loss = 0.2766 (0.348 sec/step)\n",
            "I0719 09:22:10.963137 140481689634688 learning.py:507] global step 591: loss = 0.2103 (0.307 sec/step)\n",
            "I0719 09:22:11.304110 140481689634688 learning.py:507] global step 592: loss = 0.4903 (0.339 sec/step)\n",
            "I0719 09:22:11.697932 140481689634688 learning.py:507] global step 593: loss = 0.3645 (0.392 sec/step)\n",
            "I0719 09:22:12.040513 140481689634688 learning.py:507] global step 594: loss = 0.1491 (0.341 sec/step)\n",
            "I0719 09:22:12.390258 140481689634688 learning.py:507] global step 595: loss = 0.2636 (0.348 sec/step)\n",
            "I0719 09:22:12.723083 140481689634688 learning.py:507] global step 596: loss = 0.0904 (0.331 sec/step)\n",
            "I0719 09:22:13.058145 140481689634688 learning.py:507] global step 597: loss = 0.1070 (0.334 sec/step)\n",
            "I0719 09:22:13.389284 140481689634688 learning.py:507] global step 598: loss = 0.4235 (0.329 sec/step)\n",
            "I0719 09:22:13.714738 140481689634688 learning.py:507] global step 599: loss = 0.1648 (0.324 sec/step)\n",
            "I0719 09:22:14.087840 140481689634688 learning.py:507] global step 600: loss = 0.1808 (0.371 sec/step)\n",
            "I0719 09:22:14.410678 140481689634688 learning.py:507] global step 601: loss = 0.2245 (0.321 sec/step)\n",
            "I0719 09:22:14.736469 140481689634688 learning.py:507] global step 602: loss = 0.2430 (0.324 sec/step)\n",
            "I0719 09:22:15.062469 140481689634688 learning.py:507] global step 603: loss = 0.2796 (0.324 sec/step)\n",
            "I0719 09:22:15.399172 140481689634688 learning.py:507] global step 604: loss = 0.6261 (0.335 sec/step)\n",
            "I0719 09:22:15.742589 140481689634688 learning.py:507] global step 605: loss = 0.0893 (0.342 sec/step)\n",
            "I0719 09:22:16.108690 140481689634688 learning.py:507] global step 606: loss = 0.4032 (0.364 sec/step)\n",
            "I0719 09:22:16.432555 140481689634688 learning.py:507] global step 607: loss = 0.2999 (0.322 sec/step)\n",
            "I0719 09:22:16.807239 140481689634688 learning.py:507] global step 608: loss = 0.5071 (0.373 sec/step)\n",
            "I0719 09:22:17.141308 140481689634688 learning.py:507] global step 609: loss = 0.0556 (0.332 sec/step)\n",
            "I0719 09:22:17.489318 140481689634688 learning.py:507] global step 610: loss = 0.1452 (0.346 sec/step)\n",
            "I0719 09:22:17.840926 140481689634688 learning.py:507] global step 611: loss = 0.3794 (0.350 sec/step)\n",
            "I0719 09:22:18.173069 140481689634688 learning.py:507] global step 612: loss = 0.4529 (0.330 sec/step)\n",
            "I0719 09:22:18.501587 140481689634688 learning.py:507] global step 613: loss = 0.2832 (0.327 sec/step)\n",
            "I0719 09:22:18.855632 140481689634688 learning.py:507] global step 614: loss = 0.1618 (0.352 sec/step)\n",
            "I0719 09:22:19.170672 140481689634688 learning.py:507] global step 615: loss = 0.4223 (0.313 sec/step)\n",
            "I0719 09:22:19.475322 140481689634688 learning.py:507] global step 616: loss = 0.2029 (0.303 sec/step)\n",
            "I0719 09:22:19.812167 140481689634688 learning.py:507] global step 617: loss = 0.2528 (0.335 sec/step)\n",
            "I0719 09:22:20.156538 140481689634688 learning.py:507] global step 618: loss = 0.1628 (0.343 sec/step)\n",
            "I0719 09:22:20.472360 140481689634688 learning.py:507] global step 619: loss = 0.3553 (0.314 sec/step)\n",
            "I0719 09:22:20.827619 140481689634688 learning.py:507] global step 620: loss = 0.0988 (0.353 sec/step)\n",
            "I0719 09:22:21.125959 140481689634688 learning.py:507] global step 621: loss = 0.1996 (0.297 sec/step)\n",
            "I0719 09:22:21.447733 140481689634688 learning.py:507] global step 622: loss = 0.3701 (0.320 sec/step)\n",
            "I0719 09:22:21.782447 140481689634688 learning.py:507] global step 623: loss = 0.0867 (0.333 sec/step)\n",
            "I0719 09:22:22.126167 140481689634688 learning.py:507] global step 624: loss = 0.2055 (0.342 sec/step)\n",
            "I0719 09:22:22.484478 140481689634688 learning.py:507] global step 625: loss = 0.2032 (0.357 sec/step)\n",
            "I0719 09:22:22.817356 140481689634688 learning.py:507] global step 626: loss = 0.1602 (0.331 sec/step)\n",
            "I0719 09:22:23.155057 140481689634688 learning.py:507] global step 627: loss = 0.1164 (0.336 sec/step)\n",
            "I0719 09:22:23.511320 140481689634688 learning.py:507] global step 628: loss = 0.1112 (0.354 sec/step)\n",
            "I0719 09:22:23.838249 140481689634688 learning.py:507] global step 629: loss = 0.4633 (0.325 sec/step)\n",
            "I0719 09:22:24.180168 140481689634688 learning.py:507] global step 630: loss = 0.2845 (0.340 sec/step)\n",
            "I0719 09:22:24.497347 140481689634688 learning.py:507] global step 631: loss = 0.0661 (0.315 sec/step)\n",
            "I0719 09:22:24.816145 140481689634688 learning.py:507] global step 632: loss = 0.3289 (0.317 sec/step)\n",
            "I0719 09:22:25.130577 140481689634688 learning.py:507] global step 633: loss = 0.1269 (0.313 sec/step)\n",
            "I0719 09:22:25.475019 140481689634688 learning.py:507] global step 634: loss = 0.2935 (0.343 sec/step)\n",
            "I0719 09:22:25.792225 140481689634688 learning.py:507] global step 635: loss = 0.2012 (0.315 sec/step)\n",
            "I0719 09:22:26.215149 140481689634688 learning.py:507] global step 636: loss = 0.1574 (0.401 sec/step)\n",
            "I0719 09:22:26.711776 140481689634688 learning.py:507] global step 637: loss = 0.1569 (0.455 sec/step)\n",
            "I0719 09:22:27.162487 140481689634688 learning.py:507] global step 638: loss = 0.2084 (0.437 sec/step)\n",
            "I0719 09:22:27.283716 140479018202880 supervisor.py:1050] Recording summary at step 638.\n",
            "I0719 09:22:27.507711 140481689634688 learning.py:507] global step 639: loss = 0.1628 (0.343 sec/step)\n",
            "I0719 09:22:27.662956 140479026595584 supervisor.py:1099] global_step/sec: 2.95831\n",
            "I0719 09:22:27.793296 140481689634688 learning.py:507] global step 640: loss = 0.0863 (0.284 sec/step)\n",
            "I0719 09:22:28.156396 140481689634688 learning.py:507] global step 641: loss = 0.2314 (0.361 sec/step)\n",
            "I0719 09:22:28.508167 140481689634688 learning.py:507] global step 642: loss = 0.1963 (0.350 sec/step)\n",
            "I0719 09:22:28.806376 140481689634688 learning.py:507] global step 643: loss = 0.4408 (0.297 sec/step)\n",
            "I0719 09:22:29.171263 140481689634688 learning.py:507] global step 644: loss = 0.3556 (0.363 sec/step)\n",
            "I0719 09:22:29.504564 140481689634688 learning.py:507] global step 645: loss = 0.3251 (0.332 sec/step)\n",
            "I0719 09:22:29.795350 140481689634688 learning.py:507] global step 646: loss = 0.0711 (0.289 sec/step)\n",
            "I0719 09:22:30.156016 140481689634688 learning.py:507] global step 647: loss = 0.2367 (0.359 sec/step)\n",
            "I0719 09:22:30.450442 140481689634688 learning.py:507] global step 648: loss = 0.4759 (0.293 sec/step)\n",
            "I0719 09:22:30.794567 140481689634688 learning.py:507] global step 649: loss = 0.1848 (0.342 sec/step)\n",
            "I0719 09:22:31.107524 140481689634688 learning.py:507] global step 650: loss = 0.1316 (0.311 sec/step)\n",
            "I0719 09:22:31.449613 140481689634688 learning.py:507] global step 651: loss = 0.1825 (0.340 sec/step)\n",
            "I0719 09:22:31.798774 140481689634688 learning.py:507] global step 652: loss = 0.3564 (0.347 sec/step)\n",
            "I0719 09:22:32.147419 140481689634688 learning.py:507] global step 653: loss = 0.5723 (0.346 sec/step)\n",
            "I0719 09:22:32.480082 140481689634688 learning.py:507] global step 654: loss = 0.2235 (0.331 sec/step)\n",
            "I0719 09:22:32.833354 140481689634688 learning.py:507] global step 655: loss = 0.2309 (0.352 sec/step)\n",
            "I0719 09:22:33.181538 140481689634688 learning.py:507] global step 656: loss = 0.1384 (0.347 sec/step)\n",
            "I0719 09:22:33.516643 140481689634688 learning.py:507] global step 657: loss = 0.8071 (0.333 sec/step)\n",
            "I0719 09:22:33.857530 140481689634688 learning.py:507] global step 658: loss = 0.1811 (0.339 sec/step)\n",
            "I0719 09:22:34.190531 140481689634688 learning.py:507] global step 659: loss = 0.1413 (0.331 sec/step)\n",
            "I0719 09:22:34.522575 140481689634688 learning.py:507] global step 660: loss = 0.1877 (0.329 sec/step)\n",
            "I0719 09:22:34.864239 140481689634688 learning.py:507] global step 661: loss = 0.6506 (0.340 sec/step)\n",
            "I0719 09:22:35.202426 140481689634688 learning.py:507] global step 662: loss = 0.3031 (0.335 sec/step)\n",
            "I0719 09:22:35.532815 140481689634688 learning.py:507] global step 663: loss = 0.3602 (0.328 sec/step)\n",
            "I0719 09:22:35.843868 140481689634688 learning.py:507] global step 664: loss = 0.1281 (0.308 sec/step)\n",
            "I0719 09:22:36.191576 140481689634688 learning.py:507] global step 665: loss = 0.4582 (0.346 sec/step)\n",
            "I0719 09:22:36.553757 140481689634688 learning.py:507] global step 666: loss = 0.5360 (0.360 sec/step)\n",
            "I0719 09:22:36.879491 140481689634688 learning.py:507] global step 667: loss = 0.2836 (0.324 sec/step)\n",
            "I0719 09:22:37.204747 140481689634688 learning.py:507] global step 668: loss = 0.1921 (0.324 sec/step)\n",
            "I0719 09:22:37.533635 140481689634688 learning.py:507] global step 669: loss = 0.1758 (0.327 sec/step)\n",
            "I0719 09:22:37.856338 140481689634688 learning.py:507] global step 670: loss = 0.2413 (0.321 sec/step)\n",
            "I0719 09:22:38.189232 140481689634688 learning.py:507] global step 671: loss = 0.1549 (0.331 sec/step)\n",
            "I0719 09:22:38.542406 140481689634688 learning.py:507] global step 672: loss = 0.2534 (0.351 sec/step)\n",
            "I0719 09:22:38.891213 140481689634688 learning.py:507] global step 673: loss = 0.1500 (0.347 sec/step)\n",
            "I0719 09:22:39.180472 140481689634688 learning.py:507] global step 674: loss = 0.8014 (0.285 sec/step)\n",
            "I0719 09:22:39.537899 140481689634688 learning.py:507] global step 675: loss = 0.1291 (0.355 sec/step)\n",
            "I0719 09:22:39.900774 140481689634688 learning.py:507] global step 676: loss = 0.2097 (0.361 sec/step)\n",
            "I0719 09:22:40.239311 140481689634688 learning.py:507] global step 677: loss = 0.1956 (0.337 sec/step)\n",
            "I0719 09:22:40.565235 140481689634688 learning.py:507] global step 678: loss = 0.1627 (0.324 sec/step)\n",
            "I0719 09:22:40.891776 140481689634688 learning.py:507] global step 679: loss = 0.3309 (0.325 sec/step)\n",
            "I0719 09:22:41.232577 140481689634688 learning.py:507] global step 680: loss = 0.1184 (0.339 sec/step)\n",
            "I0719 09:22:41.564914 140481689634688 learning.py:507] global step 681: loss = 0.0869 (0.330 sec/step)\n",
            "I0719 09:22:41.916600 140481689634688 learning.py:507] global step 682: loss = 0.1756 (0.350 sec/step)\n",
            "I0719 09:22:42.234694 140481689634688 learning.py:507] global step 683: loss = 0.1595 (0.316 sec/step)\n",
            "I0719 09:22:42.528709 140481689634688 learning.py:507] global step 684: loss = 0.3317 (0.292 sec/step)\n",
            "I0719 09:22:42.805452 140481689634688 learning.py:507] global step 685: loss = 0.0712 (0.275 sec/step)\n",
            "I0719 09:22:43.128545 140481689634688 learning.py:507] global step 686: loss = 0.2404 (0.321 sec/step)\n",
            "I0719 09:22:43.410966 140481689634688 learning.py:507] global step 687: loss = 0.1350 (0.281 sec/step)\n",
            "I0719 09:22:43.768535 140481689634688 learning.py:507] global step 688: loss = 0.3577 (0.356 sec/step)\n",
            "I0719 09:22:44.100116 140481689634688 learning.py:507] global step 689: loss = 0.1809 (0.330 sec/step)\n",
            "I0719 09:22:44.461505 140481689634688 learning.py:507] global step 690: loss = 0.2833 (0.359 sec/step)\n",
            "I0719 09:22:44.816630 140481689634688 learning.py:507] global step 691: loss = 0.4499 (0.353 sec/step)\n",
            "I0719 09:22:45.158622 140481689634688 learning.py:507] global step 692: loss = 0.2491 (0.340 sec/step)\n",
            "I0719 09:22:45.530435 140481689634688 learning.py:507] global step 693: loss = 0.1912 (0.370 sec/step)\n",
            "I0719 09:22:45.880093 140481689634688 learning.py:507] global step 694: loss = 0.2426 (0.348 sec/step)\n",
            "I0719 09:22:46.197497 140481689634688 learning.py:507] global step 695: loss = 0.1675 (0.316 sec/step)\n",
            "I0719 09:22:46.470027 140481689634688 learning.py:507] global step 696: loss = 0.2273 (0.271 sec/step)\n",
            "I0719 09:22:46.806657 140481689634688 learning.py:507] global step 697: loss = 0.0381 (0.333 sec/step)\n",
            "I0719 09:22:47.141709 140481689634688 learning.py:507] global step 698: loss = 0.1145 (0.333 sec/step)\n",
            "I0719 09:22:47.463494 140481689634688 learning.py:507] global step 699: loss = 0.4731 (0.320 sec/step)\n",
            "I0719 09:22:47.776723 140481689634688 learning.py:507] global step 700: loss = 0.1268 (0.311 sec/step)\n",
            "I0719 09:22:48.107953 140481689634688 learning.py:507] global step 701: loss = 0.3910 (0.329 sec/step)\n",
            "I0719 09:22:48.431520 140481689634688 learning.py:507] global step 702: loss = 0.1124 (0.322 sec/step)\n",
            "I0719 09:22:48.775839 140481689634688 learning.py:507] global step 703: loss = 0.0351 (0.343 sec/step)\n",
            "I0719 09:22:49.157037 140481689634688 learning.py:507] global step 704: loss = 0.1427 (0.379 sec/step)\n",
            "I0719 09:22:49.507114 140481689634688 learning.py:507] global step 705: loss = 0.1877 (0.348 sec/step)\n",
            "I0719 09:22:49.841165 140481689634688 learning.py:507] global step 706: loss = 0.3488 (0.332 sec/step)\n",
            "I0719 09:22:50.227945 140481689634688 learning.py:507] global step 707: loss = 0.3330 (0.385 sec/step)\n",
            "I0719 09:22:50.537627 140481689634688 learning.py:507] global step 708: loss = 0.2019 (0.308 sec/step)\n",
            "I0719 09:22:50.860210 140481689634688 learning.py:507] global step 709: loss = 0.1307 (0.321 sec/step)\n",
            "I0719 09:22:51.195501 140481689634688 learning.py:507] global step 710: loss = 0.1260 (0.333 sec/step)\n",
            "I0719 09:22:51.526478 140481689634688 learning.py:507] global step 711: loss = 0.0468 (0.329 sec/step)\n",
            "I0719 09:22:51.842991 140481689634688 learning.py:507] global step 712: loss = 0.3256 (0.315 sec/step)\n",
            "I0719 09:22:52.206136 140481689634688 learning.py:507] global step 713: loss = 0.3689 (0.361 sec/step)\n",
            "I0719 09:22:52.540368 140481689634688 learning.py:507] global step 714: loss = 0.0902 (0.332 sec/step)\n",
            "I0719 09:22:52.892088 140481689634688 learning.py:507] global step 715: loss = 0.3202 (0.350 sec/step)\n",
            "I0719 09:22:53.229030 140481689634688 learning.py:507] global step 716: loss = 0.3027 (0.335 sec/step)\n",
            "I0719 09:22:53.575961 140481689634688 learning.py:507] global step 717: loss = 0.0776 (0.345 sec/step)\n",
            "I0719 09:22:53.918667 140481689634688 learning.py:507] global step 718: loss = 0.1702 (0.341 sec/step)\n",
            "I0719 09:22:54.269183 140481689634688 learning.py:507] global step 719: loss = 0.0975 (0.349 sec/step)\n",
            "I0719 09:22:54.563036 140481689634688 learning.py:507] global step 720: loss = 0.3843 (0.292 sec/step)\n",
            "I0719 09:22:54.893989 140481689634688 learning.py:507] global step 721: loss = 0.0667 (0.329 sec/step)\n",
            "I0719 09:22:55.216197 140481689634688 learning.py:507] global step 722: loss = 0.3460 (0.320 sec/step)\n",
            "I0719 09:22:55.550650 140481689634688 learning.py:507] global step 723: loss = 0.2992 (0.333 sec/step)\n",
            "I0719 09:22:55.896333 140481689634688 learning.py:507] global step 724: loss = 0.1568 (0.344 sec/step)\n",
            "I0719 09:22:56.192656 140481689634688 learning.py:507] global step 725: loss = 0.2979 (0.294 sec/step)\n",
            "I0719 09:22:56.498633 140481689634688 learning.py:507] global step 726: loss = 0.4465 (0.304 sec/step)\n",
            "I0719 09:22:56.841626 140481689634688 learning.py:507] global step 727: loss = 0.2921 (0.340 sec/step)\n",
            "I0719 09:22:57.186466 140481689634688 learning.py:507] global step 728: loss = 0.1552 (0.342 sec/step)\n",
            "I0719 09:22:57.525671 140481689634688 learning.py:507] global step 729: loss = 0.5411 (0.337 sec/step)\n",
            "I0719 09:22:57.853095 140481689634688 learning.py:507] global step 730: loss = 0.1281 (0.325 sec/step)\n",
            "I0719 09:22:58.214143 140481689634688 learning.py:507] global step 731: loss = 0.1592 (0.359 sec/step)\n",
            "I0719 09:22:58.564836 140481689634688 learning.py:507] global step 732: loss = 0.0637 (0.349 sec/step)\n",
            "I0719 09:22:58.853864 140481689634688 learning.py:507] global step 733: loss = 0.0938 (0.287 sec/step)\n",
            "I0719 09:22:59.156597 140481689634688 learning.py:507] global step 734: loss = 0.1388 (0.301 sec/step)\n",
            "I0719 09:22:59.479099 140481689634688 learning.py:507] global step 735: loss = 0.1488 (0.321 sec/step)\n",
            "I0719 09:22:59.797508 140481689634688 learning.py:507] global step 736: loss = 0.3284 (0.317 sec/step)\n",
            "I0719 09:23:00.114430 140481689634688 learning.py:507] global step 737: loss = 0.0624 (0.315 sec/step)\n",
            "I0719 09:23:00.440602 140481689634688 learning.py:507] global step 738: loss = 0.1493 (0.324 sec/step)\n",
            "I0719 09:23:00.747149 140481689634688 learning.py:507] global step 739: loss = 0.1176 (0.305 sec/step)\n",
            "I0719 09:23:01.091226 140481689634688 learning.py:507] global step 740: loss = 0.3031 (0.342 sec/step)\n",
            "I0719 09:23:01.409427 140481689634688 learning.py:507] global step 741: loss = 0.1862 (0.316 sec/step)\n",
            "I0719 09:23:01.752539 140481689634688 learning.py:507] global step 742: loss = 0.0539 (0.341 sec/step)\n",
            "I0719 09:23:02.043843 140481689634688 learning.py:507] global step 743: loss = 0.1708 (0.289 sec/step)\n",
            "I0719 09:23:02.375181 140481689634688 learning.py:507] global step 744: loss = 0.1833 (0.330 sec/step)\n",
            "I0719 09:23:02.701185 140481689634688 learning.py:507] global step 745: loss = 0.2657 (0.324 sec/step)\n",
            "I0719 09:23:03.047353 140481689634688 learning.py:507] global step 746: loss = 0.1610 (0.344 sec/step)\n",
            "I0719 09:23:03.376651 140481689634688 learning.py:507] global step 747: loss = 0.2821 (0.327 sec/step)\n",
            "I0719 09:23:03.707446 140481689634688 learning.py:507] global step 748: loss = 0.1684 (0.329 sec/step)\n",
            "I0719 09:23:04.036363 140481689634688 learning.py:507] global step 749: loss = 0.0693 (0.326 sec/step)\n",
            "I0719 09:23:04.356825 140481689634688 learning.py:507] global step 750: loss = 0.2467 (0.318 sec/step)\n",
            "I0719 09:23:04.715452 140481689634688 learning.py:507] global step 751: loss = 0.2782 (0.357 sec/step)\n",
            "I0719 09:23:05.014632 140481689634688 learning.py:507] global step 752: loss = 0.2586 (0.297 sec/step)\n",
            "I0719 09:23:05.300038 140481689634688 learning.py:507] global step 753: loss = 0.2700 (0.284 sec/step)\n",
            "I0719 09:23:05.639294 140481689634688 learning.py:507] global step 754: loss = 0.2295 (0.338 sec/step)\n",
            "I0719 09:23:05.972546 140481689634688 learning.py:507] global step 755: loss = 0.7941 (0.332 sec/step)\n",
            "I0719 09:23:06.323141 140481689634688 learning.py:507] global step 756: loss = 0.1570 (0.349 sec/step)\n",
            "I0719 09:23:06.657500 140481689634688 learning.py:507] global step 757: loss = 0.1686 (0.333 sec/step)\n",
            "I0719 09:23:07.009575 140481689634688 learning.py:507] global step 758: loss = 0.4061 (0.350 sec/step)\n",
            "I0719 09:23:07.355001 140481689634688 learning.py:507] global step 759: loss = 0.1874 (0.344 sec/step)\n",
            "I0719 09:23:07.682537 140481689634688 learning.py:507] global step 760: loss = 0.2151 (0.325 sec/step)\n",
            "I0719 09:23:08.032611 140481689634688 learning.py:507] global step 761: loss = 0.1601 (0.349 sec/step)\n",
            "I0719 09:23:08.379248 140481689634688 learning.py:507] global step 762: loss = 0.6968 (0.345 sec/step)\n",
            "I0719 09:23:08.738656 140481689634688 learning.py:507] global step 763: loss = 0.0926 (0.358 sec/step)\n",
            "I0719 09:23:09.025626 140481689634688 learning.py:507] global step 764: loss = 0.1319 (0.285 sec/step)\n",
            "I0719 09:23:09.381469 140481689634688 learning.py:507] global step 765: loss = 0.1564 (0.354 sec/step)\n",
            "I0719 09:23:09.708485 140481689634688 learning.py:507] global step 766: loss = 0.0676 (0.325 sec/step)\n",
            "I0719 09:23:10.046708 140481689634688 learning.py:507] global step 767: loss = 0.1318 (0.336 sec/step)\n",
            "I0719 09:23:10.435686 140481689634688 learning.py:507] global step 768: loss = 0.2081 (0.387 sec/step)\n",
            "I0719 09:23:10.733211 140481689634688 learning.py:507] global step 769: loss = 0.0470 (0.296 sec/step)\n",
            "I0719 09:23:11.086305 140481689634688 learning.py:507] global step 770: loss = 0.3332 (0.351 sec/step)\n",
            "I0719 09:23:11.451135 140481689634688 learning.py:507] global step 771: loss = 0.2709 (0.363 sec/step)\n",
            "I0719 09:23:11.790879 140481689634688 learning.py:507] global step 772: loss = 0.2557 (0.338 sec/step)\n",
            "I0719 09:23:12.128407 140481689634688 learning.py:507] global step 773: loss = 0.2134 (0.335 sec/step)\n",
            "I0719 09:23:12.457939 140481689634688 learning.py:507] global step 774: loss = 0.3254 (0.327 sec/step)\n",
            "I0719 09:23:12.774608 140481689634688 learning.py:507] global step 775: loss = 0.2115 (0.315 sec/step)\n",
            "I0719 09:23:13.114627 140481689634688 learning.py:507] global step 776: loss = 0.4344 (0.338 sec/step)\n",
            "I0719 09:23:13.420302 140481689634688 learning.py:507] global step 777: loss = 0.1815 (0.304 sec/step)\n",
            "I0719 09:23:13.759098 140481689634688 learning.py:507] global step 778: loss = 0.2147 (0.337 sec/step)\n",
            "I0719 09:23:14.109816 140481689634688 learning.py:507] global step 779: loss = 0.3438 (0.349 sec/step)\n",
            "I0719 09:23:14.461039 140481689634688 learning.py:507] global step 780: loss = 0.2302 (0.349 sec/step)\n",
            "I0719 09:23:14.803773 140481689634688 learning.py:507] global step 781: loss = 0.2287 (0.341 sec/step)\n",
            "I0719 09:23:15.094623 140481689634688 learning.py:507] global step 782: loss = 0.2767 (0.287 sec/step)\n",
            "I0719 09:23:15.437339 140481689634688 learning.py:507] global step 783: loss = 0.2121 (0.341 sec/step)\n",
            "I0719 09:23:15.789934 140481689634688 learning.py:507] global step 784: loss = 0.1667 (0.351 sec/step)\n",
            "I0719 09:23:16.141575 140481689634688 learning.py:507] global step 785: loss = 0.2329 (0.350 sec/step)\n",
            "I0719 09:23:16.478310 140481689634688 learning.py:507] global step 786: loss = 0.3275 (0.335 sec/step)\n",
            "I0719 09:23:16.784754 140481689634688 learning.py:507] global step 787: loss = 0.2146 (0.301 sec/step)\n",
            "I0719 09:23:17.118382 140481689634688 learning.py:507] global step 788: loss = 0.1885 (0.331 sec/step)\n",
            "I0719 09:23:17.467764 140481689634688 learning.py:507] global step 789: loss = 0.2753 (0.348 sec/step)\n",
            "I0719 09:23:17.782536 140481689634688 learning.py:507] global step 790: loss = 0.2596 (0.313 sec/step)\n",
            "I0719 09:23:18.139047 140481689634688 learning.py:507] global step 791: loss = 0.2663 (0.355 sec/step)\n",
            "I0719 09:23:18.434024 140481689634688 learning.py:507] global step 792: loss = 0.1872 (0.293 sec/step)\n",
            "I0719 09:23:18.776420 140481689634688 learning.py:507] global step 793: loss = 0.2033 (0.340 sec/step)\n",
            "I0719 09:23:19.079067 140481689634688 learning.py:507] global step 794: loss = 0.1343 (0.301 sec/step)\n",
            "I0719 09:23:19.416166 140481689634688 learning.py:507] global step 795: loss = 0.2555 (0.335 sec/step)\n",
            "I0719 09:23:19.713597 140481689634688 learning.py:507] global step 796: loss = 0.1787 (0.293 sec/step)\n",
            "I0719 09:23:20.035676 140481689634688 learning.py:507] global step 797: loss = 0.1280 (0.319 sec/step)\n",
            "I0719 09:23:20.365144 140481689634688 learning.py:507] global step 798: loss = 0.2472 (0.328 sec/step)\n",
            "I0719 09:23:20.728093 140481689634688 learning.py:507] global step 799: loss = 0.1386 (0.361 sec/step)\n",
            "I0719 09:23:21.062289 140481689634688 learning.py:507] global step 800: loss = 0.1897 (0.333 sec/step)\n",
            "I0719 09:23:21.351679 140481689634688 learning.py:507] global step 801: loss = 0.2421 (0.288 sec/step)\n",
            "I0719 09:23:21.707205 140481689634688 learning.py:507] global step 802: loss = 0.1245 (0.354 sec/step)\n",
            "I0719 09:23:22.036421 140481689634688 learning.py:507] global step 803: loss = 0.2368 (0.327 sec/step)\n",
            "I0719 09:23:22.403619 140481689634688 learning.py:507] global step 804: loss = 0.0926 (0.365 sec/step)\n",
            "I0719 09:23:22.696591 140481689634688 learning.py:507] global step 805: loss = 0.1316 (0.291 sec/step)\n",
            "I0719 09:23:23.039989 140481689634688 learning.py:507] global step 806: loss = 0.1601 (0.341 sec/step)\n",
            "I0719 09:23:23.356579 140481689634688 learning.py:507] global step 807: loss = 0.1845 (0.315 sec/step)\n",
            "I0719 09:23:23.732578 140481689634688 learning.py:507] global step 808: loss = 0.3957 (0.374 sec/step)\n",
            "I0719 09:23:24.064111 140481689634688 learning.py:507] global step 809: loss = 0.0711 (0.330 sec/step)\n",
            "I0719 09:23:24.407732 140481689634688 learning.py:507] global step 810: loss = 0.2456 (0.342 sec/step)\n",
            "I0719 09:23:24.683086 140481689634688 learning.py:507] global step 811: loss = 0.0863 (0.274 sec/step)\n",
            "I0719 09:23:25.016493 140481689634688 learning.py:507] global step 812: loss = 0.1672 (0.332 sec/step)\n",
            "I0719 09:23:25.345054 140481689634688 learning.py:507] global step 813: loss = 0.4300 (0.327 sec/step)\n",
            "I0719 09:23:25.679615 140481689634688 learning.py:507] global step 814: loss = 0.1669 (0.333 sec/step)\n",
            "I0719 09:23:26.012936 140481689634688 learning.py:507] global step 815: loss = 0.2426 (0.331 sec/step)\n",
            "I0719 09:23:26.334719 140481689634688 learning.py:507] global step 816: loss = 0.0861 (0.320 sec/step)\n",
            "I0719 09:23:26.668181 140481689634688 learning.py:507] global step 817: loss = 0.0549 (0.332 sec/step)\n",
            "I0719 09:23:26.994618 140481689634688 learning.py:507] global step 818: loss = 0.1620 (0.324 sec/step)\n",
            "I0719 09:23:27.381582 140481689634688 learning.py:507] global step 819: loss = 0.2120 (0.385 sec/step)\n",
            "I0719 09:23:27.716716 140481689634688 learning.py:507] global step 820: loss = 0.3031 (0.332 sec/step)\n",
            "I0719 09:23:28.061718 140481689634688 learning.py:507] global step 821: loss = 0.2972 (0.342 sec/step)\n",
            "I0719 09:23:28.383241 140481689634688 learning.py:507] global step 822: loss = 0.0675 (0.320 sec/step)\n",
            "I0719 09:23:28.727694 140481689634688 learning.py:507] global step 823: loss = 0.2181 (0.343 sec/step)\n",
            "I0719 09:23:29.051987 140481689634688 learning.py:507] global step 824: loss = 0.1069 (0.322 sec/step)\n",
            "I0719 09:23:29.376600 140481689634688 learning.py:507] global step 825: loss = 0.2125 (0.323 sec/step)\n",
            "I0719 09:23:29.692369 140481689634688 learning.py:507] global step 826: loss = 0.2332 (0.314 sec/step)\n",
            "I0719 09:23:29.985570 140481689634688 learning.py:507] global step 827: loss = 0.5037 (0.291 sec/step)\n",
            "I0719 09:23:30.286757 140481689634688 learning.py:507] global step 828: loss = 0.1284 (0.299 sec/step)\n",
            "I0719 09:23:30.628593 140481689634688 learning.py:507] global step 829: loss = 0.1041 (0.340 sec/step)\n",
            "I0719 09:23:30.951296 140481689634688 learning.py:507] global step 830: loss = 0.2436 (0.321 sec/step)\n",
            "I0719 09:23:31.315902 140481689634688 learning.py:507] global step 831: loss = 0.2626 (0.362 sec/step)\n",
            "I0719 09:23:31.671057 140481689634688 learning.py:507] global step 832: loss = 0.3185 (0.353 sec/step)\n",
            "I0719 09:23:31.970909 140481689634688 learning.py:507] global step 833: loss = 0.1803 (0.298 sec/step)\n",
            "I0719 09:23:32.309205 140481689634688 learning.py:507] global step 834: loss = 0.1789 (0.336 sec/step)\n",
            "I0719 09:23:32.637745 140481689634688 learning.py:507] global step 835: loss = 0.2031 (0.327 sec/step)\n",
            "I0719 09:23:32.942723 140481689634688 learning.py:507] global step 836: loss = 0.2399 (0.303 sec/step)\n",
            "I0719 09:23:33.285093 140481689634688 learning.py:507] global step 837: loss = 0.0639 (0.341 sec/step)\n",
            "I0719 09:23:33.601077 140481689634688 learning.py:507] global step 838: loss = 0.1272 (0.314 sec/step)\n",
            "I0719 09:23:33.920918 140481689634688 learning.py:507] global step 839: loss = 0.2918 (0.318 sec/step)\n",
            "I0719 09:23:34.261955 140481689634688 learning.py:507] global step 840: loss = 0.2453 (0.339 sec/step)\n",
            "I0719 09:23:34.612360 140481689634688 learning.py:507] global step 841: loss = 0.1902 (0.349 sec/step)\n",
            "I0719 09:23:34.948829 140481689634688 learning.py:507] global step 842: loss = 0.1927 (0.335 sec/step)\n",
            "I0719 09:23:35.247703 140481689634688 learning.py:507] global step 843: loss = 0.1147 (0.297 sec/step)\n",
            "I0719 09:23:35.570556 140481689634688 learning.py:507] global step 844: loss = 0.1536 (0.321 sec/step)\n",
            "I0719 09:23:35.897794 140481689634688 learning.py:507] global step 845: loss = 0.2347 (0.325 sec/step)\n",
            "I0719 09:23:36.258173 140481689634688 learning.py:507] global step 846: loss = 0.7907 (0.359 sec/step)\n",
            "I0719 09:23:36.582631 140481689634688 learning.py:507] global step 847: loss = 0.2155 (0.323 sec/step)\n",
            "I0719 09:23:36.929093 140481689634688 learning.py:507] global step 848: loss = 0.2537 (0.345 sec/step)\n",
            "I0719 09:23:37.236800 140481689634688 learning.py:507] global step 849: loss = 0.5293 (0.306 sec/step)\n",
            "I0719 09:23:37.574321 140481689634688 learning.py:507] global step 850: loss = 0.1582 (0.336 sec/step)\n",
            "I0719 09:23:37.894290 140481689634688 learning.py:507] global step 851: loss = 0.1262 (0.318 sec/step)\n",
            "I0719 09:23:38.261099 140481689634688 learning.py:507] global step 852: loss = 0.2024 (0.365 sec/step)\n",
            "I0719 09:23:38.599475 140481689634688 learning.py:507] global step 853: loss = 0.2604 (0.336 sec/step)\n",
            "I0719 09:23:38.941236 140481689634688 learning.py:507] global step 854: loss = 0.4289 (0.340 sec/step)\n",
            "I0719 09:23:39.266211 140481689634688 learning.py:507] global step 855: loss = 0.3040 (0.323 sec/step)\n",
            "I0719 09:23:39.590981 140481689634688 learning.py:507] global step 856: loss = 0.4477 (0.323 sec/step)\n",
            "I0719 09:23:39.936245 140481689634688 learning.py:507] global step 857: loss = 0.1218 (0.343 sec/step)\n",
            "I0719 09:23:40.276718 140481689634688 learning.py:507] global step 858: loss = 0.2785 (0.338 sec/step)\n",
            "I0719 09:23:40.627527 140481689634688 learning.py:507] global step 859: loss = 0.2224 (0.349 sec/step)\n",
            "I0719 09:23:40.976627 140481689634688 learning.py:507] global step 860: loss = 0.0789 (0.348 sec/step)\n",
            "I0719 09:23:41.311978 140481689634688 learning.py:507] global step 861: loss = 0.3045 (0.333 sec/step)\n",
            "I0719 09:23:41.652781 140481689634688 learning.py:507] global step 862: loss = 0.2435 (0.339 sec/step)\n",
            "I0719 09:23:41.981981 140481689634688 learning.py:507] global step 863: loss = 0.2728 (0.327 sec/step)\n",
            "I0719 09:23:42.325893 140481689634688 learning.py:507] global step 864: loss = 0.2405 (0.341 sec/step)\n",
            "I0719 09:23:42.680139 140481689634688 learning.py:507] global step 865: loss = 0.2360 (0.353 sec/step)\n",
            "I0719 09:23:43.014607 140481689634688 learning.py:507] global step 866: loss = 0.3026 (0.333 sec/step)\n",
            "I0719 09:23:43.342091 140481689634688 learning.py:507] global step 867: loss = 0.1146 (0.326 sec/step)\n",
            "I0719 09:23:43.684582 140481689634688 learning.py:507] global step 868: loss = 0.3818 (0.341 sec/step)\n",
            "I0719 09:23:44.015336 140481689634688 learning.py:507] global step 869: loss = 0.2168 (0.329 sec/step)\n",
            "I0719 09:23:44.383782 140481689634688 learning.py:507] global step 870: loss = 0.1855 (0.366 sec/step)\n",
            "I0719 09:23:44.728933 140481689634688 learning.py:507] global step 871: loss = 0.2188 (0.343 sec/step)\n",
            "I0719 09:23:45.071751 140481689634688 learning.py:507] global step 872: loss = 0.0931 (0.341 sec/step)\n",
            "I0719 09:23:45.448096 140481689634688 learning.py:507] global step 873: loss = 0.2530 (0.375 sec/step)\n",
            "I0719 09:23:45.821444 140481689634688 learning.py:507] global step 874: loss = 0.5139 (0.371 sec/step)\n",
            "I0719 09:23:46.150846 140481689634688 learning.py:507] global step 875: loss = 0.0898 (0.328 sec/step)\n",
            "I0719 09:23:46.505848 140481689634688 learning.py:507] global step 876: loss = 0.2339 (0.353 sec/step)\n",
            "I0719 09:23:46.839430 140481689634688 learning.py:507] global step 877: loss = 0.2415 (0.332 sec/step)\n",
            "I0719 09:23:47.153864 140481689634688 learning.py:507] global step 878: loss = 0.0729 (0.313 sec/step)\n",
            "I0719 09:23:47.508429 140481689634688 learning.py:507] global step 879: loss = 0.3072 (0.353 sec/step)\n",
            "I0719 09:23:47.849443 140481689634688 learning.py:507] global step 880: loss = 0.3443 (0.339 sec/step)\n",
            "I0719 09:23:48.197261 140481689634688 learning.py:507] global step 881: loss = 0.2302 (0.346 sec/step)\n",
            "I0719 09:23:48.528296 140481689634688 learning.py:507] global step 882: loss = 0.2427 (0.329 sec/step)\n",
            "I0719 09:23:48.872171 140481689634688 learning.py:507] global step 883: loss = 0.1867 (0.342 sec/step)\n",
            "I0719 09:23:49.208606 140481689634688 learning.py:507] global step 884: loss = 0.1678 (0.335 sec/step)\n",
            "I0719 09:23:49.528178 140481689634688 learning.py:507] global step 885: loss = 0.1919 (0.318 sec/step)\n",
            "I0719 09:23:49.850507 140481689634688 learning.py:507] global step 886: loss = 0.2797 (0.321 sec/step)\n",
            "I0719 09:23:50.190906 140481689634688 learning.py:507] global step 887: loss = 0.2690 (0.339 sec/step)\n",
            "I0719 09:23:50.524510 140481689634688 learning.py:507] global step 888: loss = 0.3851 (0.332 sec/step)\n",
            "I0719 09:23:50.863174 140481689634688 learning.py:507] global step 889: loss = 0.3129 (0.337 sec/step)\n",
            "I0719 09:23:51.197839 140481689634688 learning.py:507] global step 890: loss = 0.0812 (0.333 sec/step)\n",
            "I0719 09:23:51.535299 140481689634688 learning.py:507] global step 891: loss = 0.3321 (0.336 sec/step)\n",
            "I0719 09:23:51.880602 140481689634688 learning.py:507] global step 892: loss = 0.1955 (0.344 sec/step)\n",
            "I0719 09:23:52.235842 140481689634688 learning.py:507] global step 893: loss = 0.2056 (0.353 sec/step)\n",
            "I0719 09:23:52.568767 140481689634688 learning.py:507] global step 894: loss = 0.3463 (0.331 sec/step)\n",
            "I0719 09:23:52.911670 140481689634688 learning.py:507] global step 895: loss = 0.1748 (0.341 sec/step)\n",
            "I0719 09:23:53.194821 140481689634688 learning.py:507] global step 896: loss = 0.1223 (0.281 sec/step)\n",
            "I0719 09:23:53.535314 140481689634688 learning.py:507] global step 897: loss = 0.1062 (0.339 sec/step)\n",
            "I0719 09:23:53.867109 140481689634688 learning.py:507] global step 898: loss = 0.2294 (0.330 sec/step)\n",
            "I0719 09:23:54.201517 140481689634688 learning.py:507] global step 899: loss = 0.1819 (0.332 sec/step)\n",
            "I0719 09:23:54.516300 140481689634688 learning.py:507] global step 900: loss = 0.6112 (0.313 sec/step)\n",
            "I0719 09:23:54.818470 140481689634688 learning.py:507] global step 901: loss = 0.0655 (0.300 sec/step)\n",
            "I0719 09:23:55.165056 140481689634688 learning.py:507] global step 902: loss = 0.1244 (0.345 sec/step)\n",
            "I0719 09:23:55.468133 140481689634688 learning.py:507] global step 903: loss = 0.3335 (0.301 sec/step)\n",
            "I0719 09:23:55.818387 140481689634688 learning.py:507] global step 904: loss = 0.2203 (0.349 sec/step)\n",
            "I0719 09:23:56.155336 140481689634688 learning.py:507] global step 905: loss = 0.2528 (0.335 sec/step)\n",
            "I0719 09:23:56.474616 140481689634688 learning.py:507] global step 906: loss = 0.1453 (0.318 sec/step)\n",
            "I0719 09:23:56.815135 140481689634688 learning.py:507] global step 907: loss = 0.2098 (0.339 sec/step)\n",
            "I0719 09:23:57.150425 140481689634688 learning.py:507] global step 908: loss = 0.5373 (0.333 sec/step)\n",
            "I0719 09:23:57.516243 140481689634688 learning.py:507] global step 909: loss = 0.2396 (0.364 sec/step)\n",
            "I0719 09:23:57.859040 140481689634688 learning.py:507] global step 910: loss = 0.2435 (0.341 sec/step)\n",
            "I0719 09:23:58.197924 140481689634688 learning.py:507] global step 911: loss = 0.2845 (0.337 sec/step)\n",
            "I0719 09:23:58.527468 140481689634688 learning.py:507] global step 912: loss = 0.1493 (0.328 sec/step)\n",
            "I0719 09:23:58.856423 140481689634688 learning.py:507] global step 913: loss = 0.2422 (0.327 sec/step)\n",
            "I0719 09:23:59.192725 140481689634688 learning.py:507] global step 914: loss = 0.0366 (0.335 sec/step)\n",
            "I0719 09:23:59.543780 140481689634688 learning.py:507] global step 915: loss = 0.2561 (0.349 sec/step)\n",
            "I0719 09:23:59.887770 140481689634688 learning.py:507] global step 916: loss = 0.3701 (0.342 sec/step)\n",
            "I0719 09:24:00.217238 140481689634688 learning.py:507] global step 917: loss = 0.1917 (0.328 sec/step)\n",
            "I0719 09:24:00.556919 140481689634688 learning.py:507] global step 918: loss = 0.0472 (0.338 sec/step)\n",
            "I0719 09:24:00.895808 140481689634688 learning.py:507] global step 919: loss = 0.1997 (0.337 sec/step)\n",
            "I0719 09:24:01.242152 140481689634688 learning.py:507] global step 920: loss = 0.2516 (0.345 sec/step)\n",
            "I0719 09:24:01.595349 140481689634688 learning.py:507] global step 921: loss = 0.0594 (0.351 sec/step)\n",
            "I0719 09:24:01.941066 140481689634688 learning.py:507] global step 922: loss = 0.3633 (0.344 sec/step)\n",
            "I0719 09:24:02.275035 140481689634688 learning.py:507] global step 923: loss = 0.3196 (0.332 sec/step)\n",
            "I0719 09:24:02.632255 140481689634688 learning.py:507] global step 924: loss = 0.2422 (0.355 sec/step)\n",
            "I0719 09:24:02.961146 140481689634688 learning.py:507] global step 925: loss = 0.1999 (0.327 sec/step)\n",
            "I0719 09:24:03.293450 140481689634688 learning.py:507] global step 926: loss = 0.1188 (0.330 sec/step)\n",
            "I0719 09:24:03.616050 140481689634688 learning.py:507] global step 927: loss = 0.2239 (0.321 sec/step)\n",
            "I0719 09:24:03.944034 140481689634688 learning.py:507] global step 928: loss = 0.0539 (0.326 sec/step)\n",
            "I0719 09:24:04.303862 140481689634688 learning.py:507] global step 929: loss = 0.1676 (0.358 sec/step)\n",
            "I0719 09:24:04.651446 140481689634688 learning.py:507] global step 930: loss = 0.2135 (0.346 sec/step)\n",
            "I0719 09:24:04.997341 140481689634688 learning.py:507] global step 931: loss = 0.2319 (0.344 sec/step)\n",
            "I0719 09:24:05.353012 140481689634688 learning.py:507] global step 932: loss = 0.1452 (0.354 sec/step)\n",
            "I0719 09:24:05.628922 140481689634688 learning.py:507] global step 933: loss = 0.2293 (0.274 sec/step)\n",
            "I0719 09:24:05.967420 140481689634688 learning.py:507] global step 934: loss = 0.1813 (0.337 sec/step)\n",
            "I0719 09:24:06.310427 140481689634688 learning.py:507] global step 935: loss = 0.2002 (0.341 sec/step)\n",
            "I0719 09:24:06.676852 140481689634688 learning.py:507] global step 936: loss = 0.2926 (0.365 sec/step)\n",
            "I0719 09:24:07.030017 140481689634688 learning.py:507] global step 937: loss = 0.2096 (0.352 sec/step)\n",
            "I0719 09:24:07.369653 140481689634688 learning.py:507] global step 938: loss = 0.2099 (0.338 sec/step)\n",
            "I0719 09:24:07.702850 140481689634688 learning.py:507] global step 939: loss = 0.2510 (0.332 sec/step)\n",
            "I0719 09:24:08.037350 140481689634688 learning.py:507] global step 940: loss = 0.0759 (0.333 sec/step)\n",
            "I0719 09:24:08.363862 140481689634688 learning.py:507] global step 941: loss = 0.1641 (0.325 sec/step)\n",
            "I0719 09:24:08.700288 140481689634688 learning.py:507] global step 942: loss = 0.2949 (0.335 sec/step)\n",
            "I0719 09:24:09.045665 140481689634688 learning.py:507] global step 943: loss = 0.2575 (0.344 sec/step)\n",
            "I0719 09:24:09.385331 140481689634688 learning.py:507] global step 944: loss = 0.2669 (0.338 sec/step)\n",
            "I0719 09:24:09.692174 140481689634688 learning.py:507] global step 945: loss = 0.5818 (0.305 sec/step)\n",
            "I0719 09:24:09.980984 140481689634688 learning.py:507] global step 946: loss = 0.1913 (0.287 sec/step)\n",
            "I0719 09:24:10.336152 140481689634688 learning.py:507] global step 947: loss = 0.1445 (0.351 sec/step)\n",
            "I0719 09:24:10.694146 140481689634688 learning.py:507] global step 948: loss = 0.1303 (0.356 sec/step)\n",
            "I0719 09:24:11.044630 140481689634688 learning.py:507] global step 949: loss = 0.3142 (0.349 sec/step)\n",
            "I0719 09:24:11.358455 140481689634688 learning.py:507] global step 950: loss = 0.3026 (0.312 sec/step)\n",
            "I0719 09:24:11.656526 140481689634688 learning.py:507] global step 951: loss = 0.1274 (0.296 sec/step)\n",
            "I0719 09:24:11.981261 140481689634688 learning.py:507] global step 952: loss = 0.3567 (0.323 sec/step)\n",
            "I0719 09:24:12.345916 140481689634688 learning.py:507] global step 953: loss = 0.1420 (0.363 sec/step)\n",
            "I0719 09:24:12.686118 140481689634688 learning.py:507] global step 954: loss = 0.2323 (0.338 sec/step)\n",
            "I0719 09:24:13.038699 140481689634688 learning.py:507] global step 955: loss = 0.1969 (0.351 sec/step)\n",
            "I0719 09:24:13.401599 140481689634688 learning.py:507] global step 956: loss = 0.2348 (0.361 sec/step)\n",
            "I0719 09:24:13.701224 140481689634688 learning.py:507] global step 957: loss = 0.3620 (0.298 sec/step)\n",
            "I0719 09:24:14.043685 140481689634688 learning.py:507] global step 958: loss = 0.2503 (0.341 sec/step)\n",
            "I0719 09:24:14.396718 140481689634688 learning.py:507] global step 959: loss = 0.1355 (0.350 sec/step)\n",
            "I0719 09:24:14.719476 140481689634688 learning.py:507] global step 960: loss = 0.2225 (0.321 sec/step)\n",
            "I0719 09:24:15.059240 140481689634688 learning.py:507] global step 961: loss = 0.4168 (0.338 sec/step)\n",
            "I0719 09:24:15.429537 140481689634688 learning.py:507] global step 962: loss = 0.1547 (0.368 sec/step)\n",
            "I0719 09:24:15.740522 140481689634688 learning.py:507] global step 963: loss = 0.0589 (0.309 sec/step)\n",
            "I0719 09:24:16.058060 140481689634688 learning.py:507] global step 964: loss = 0.2222 (0.316 sec/step)\n",
            "I0719 09:24:16.353974 140481689634688 learning.py:507] global step 965: loss = 0.2218 (0.294 sec/step)\n",
            "I0719 09:24:16.676944 140481689634688 learning.py:507] global step 966: loss = 0.1197 (0.321 sec/step)\n",
            "I0719 09:24:17.001660 140481689634688 learning.py:507] global step 967: loss = 0.1287 (0.323 sec/step)\n",
            "I0719 09:24:17.347458 140481689634688 learning.py:507] global step 968: loss = 0.1713 (0.344 sec/step)\n",
            "I0719 09:24:17.691453 140481689634688 learning.py:507] global step 969: loss = 0.1756 (0.342 sec/step)\n",
            "I0719 09:24:17.986617 140481689634688 learning.py:507] global step 970: loss = 0.0608 (0.293 sec/step)\n",
            "I0719 09:24:18.335582 140481689634688 learning.py:507] global step 971: loss = 0.1432 (0.347 sec/step)\n",
            "I0719 09:24:18.702439 140481689634688 learning.py:507] global step 972: loss = 0.1476 (0.365 sec/step)\n",
            "I0719 09:24:19.008084 140481689634688 learning.py:507] global step 973: loss = 0.0596 (0.304 sec/step)\n",
            "I0719 09:24:19.353765 140481689634688 learning.py:507] global step 974: loss = 0.1486 (0.344 sec/step)\n",
            "I0719 09:24:19.688101 140481689634688 learning.py:507] global step 975: loss = 0.6133 (0.332 sec/step)\n",
            "I0719 09:24:20.011847 140481689634688 learning.py:507] global step 976: loss = 0.3076 (0.322 sec/step)\n",
            "I0719 09:24:20.348318 140481689634688 learning.py:507] global step 977: loss = 0.1273 (0.335 sec/step)\n",
            "I0719 09:24:20.657977 140481689634688 learning.py:507] global step 978: loss = 0.1292 (0.308 sec/step)\n",
            "I0719 09:24:20.940359 140481689634688 learning.py:507] global step 979: loss = 0.0858 (0.281 sec/step)\n",
            "I0719 09:24:21.277298 140481689634688 learning.py:507] global step 980: loss = 0.0893 (0.335 sec/step)\n",
            "I0719 09:24:21.623492 140481689634688 learning.py:507] global step 981: loss = 0.3976 (0.345 sec/step)\n",
            "I0719 09:24:21.948696 140481689634688 learning.py:507] global step 982: loss = 0.2510 (0.323 sec/step)\n",
            "I0719 09:24:22.329253 140481689634688 learning.py:507] global step 983: loss = 0.4414 (0.379 sec/step)\n",
            "I0719 09:24:22.661583 140481689634688 learning.py:507] global step 984: loss = 0.1979 (0.330 sec/step)\n",
            "I0719 09:24:22.983139 140481689634688 learning.py:507] global step 985: loss = 0.2109 (0.320 sec/step)\n",
            "I0719 09:24:23.313055 140481689634688 learning.py:507] global step 986: loss = 0.0974 (0.328 sec/step)\n",
            "I0719 09:24:23.633428 140481689634688 learning.py:507] global step 987: loss = 0.2068 (0.319 sec/step)\n",
            "I0719 09:24:23.961470 140481689634688 learning.py:507] global step 988: loss = 0.3439 (0.326 sec/step)\n",
            "I0719 09:24:24.341088 140481689634688 learning.py:507] global step 989: loss = 0.1902 (0.378 sec/step)\n",
            "I0719 09:24:24.703234 140481689634688 learning.py:507] global step 990: loss = 0.1989 (0.360 sec/step)\n",
            "I0719 09:24:25.050090 140481689634688 learning.py:507] global step 991: loss = 0.2320 (0.345 sec/step)\n",
            "I0719 09:24:25.365906 140481689634688 learning.py:507] global step 992: loss = 0.1509 (0.314 sec/step)\n",
            "I0719 09:24:25.715858 140481689634688 learning.py:507] global step 993: loss = 0.1227 (0.348 sec/step)\n",
            "I0719 09:24:26.124139 140481689634688 learning.py:507] global step 994: loss = 0.0861 (0.406 sec/step)\n",
            "I0719 09:24:26.725507 140481689634688 learning.py:507] global step 995: loss = 0.1080 (0.563 sec/step)\n",
            "I0719 09:24:27.182323 140481689634688 learning.py:507] global step 996: loss = 0.4630 (0.435 sec/step)\n",
            "I0719 09:24:27.459696 140479018202880 supervisor.py:1050] Recording summary at step 996.\n",
            "I0719 09:24:27.561933 140481689634688 learning.py:507] global step 997: loss = 0.0591 (0.378 sec/step)\n",
            "I0719 09:24:27.665456 140479026595584 supervisor.py:1099] global_step/sec: 2.98327\n",
            "I0719 09:24:27.895803 140481689634688 learning.py:507] global step 998: loss = 0.1715 (0.332 sec/step)\n",
            "I0719 09:24:28.243870 140481689634688 learning.py:507] global step 999: loss = 0.1544 (0.346 sec/step)\n",
            "I0719 09:24:28.590197 140481689634688 learning.py:507] global step 1000: loss = 0.3853 (0.345 sec/step)\n",
            "I0719 09:24:28.922775 140481689634688 learning.py:507] global step 1001: loss = 0.1227 (0.331 sec/step)\n",
            "I0719 09:24:29.246203 140481689634688 learning.py:507] global step 1002: loss = 0.5470 (0.322 sec/step)\n",
            "I0719 09:24:29.605416 140481689634688 learning.py:507] global step 1003: loss = 0.0517 (0.357 sec/step)\n",
            "I0719 09:24:29.949634 140481689634688 learning.py:507] global step 1004: loss = 0.2702 (0.342 sec/step)\n",
            "I0719 09:24:30.238468 140481689634688 learning.py:507] global step 1005: loss = 0.2506 (0.287 sec/step)\n",
            "I0719 09:24:30.600931 140481689634688 learning.py:507] global step 1006: loss = 0.3110 (0.361 sec/step)\n",
            "I0719 09:24:30.935890 140481689634688 learning.py:507] global step 1007: loss = 0.1390 (0.333 sec/step)\n",
            "I0719 09:24:31.267244 140481689634688 learning.py:507] global step 1008: loss = 0.2979 (0.329 sec/step)\n",
            "I0719 09:24:31.611309 140481689634688 learning.py:507] global step 1009: loss = 0.1565 (0.342 sec/step)\n",
            "I0719 09:24:31.949732 140481689634688 learning.py:507] global step 1010: loss = 0.5650 (0.337 sec/step)\n",
            "I0719 09:24:32.252519 140481689634688 learning.py:507] global step 1011: loss = 0.1788 (0.301 sec/step)\n",
            "I0719 09:24:32.600190 140481689634688 learning.py:507] global step 1012: loss = 0.1511 (0.346 sec/step)\n",
            "I0719 09:24:32.914434 140481689634688 learning.py:507] global step 1013: loss = 0.1231 (0.312 sec/step)\n",
            "I0719 09:24:33.254823 140481689634688 learning.py:507] global step 1014: loss = 0.2354 (0.339 sec/step)\n",
            "I0719 09:24:33.611203 140481689634688 learning.py:507] global step 1015: loss = 0.1737 (0.354 sec/step)\n",
            "I0719 09:24:33.895515 140481689634688 learning.py:507] global step 1016: loss = 0.1314 (0.282 sec/step)\n",
            "I0719 09:24:34.251641 140481689634688 learning.py:507] global step 1017: loss = 0.2877 (0.355 sec/step)\n",
            "I0719 09:24:34.577794 140481689634688 learning.py:507] global step 1018: loss = 0.1752 (0.324 sec/step)\n",
            "I0719 09:24:34.907529 140481689634688 learning.py:507] global step 1019: loss = 0.2466 (0.328 sec/step)\n",
            "I0719 09:24:35.232220 140481689634688 learning.py:507] global step 1020: loss = 0.0906 (0.323 sec/step)\n",
            "I0719 09:24:35.559221 140481689634688 learning.py:507] global step 1021: loss = 0.1487 (0.325 sec/step)\n",
            "I0719 09:24:35.883862 140481689634688 learning.py:507] global step 1022: loss = 0.1460 (0.323 sec/step)\n",
            "I0719 09:24:36.226908 140481689634688 learning.py:507] global step 1023: loss = 0.1825 (0.341 sec/step)\n",
            "I0719 09:24:36.576072 140481689634688 learning.py:507] global step 1024: loss = 0.0696 (0.347 sec/step)\n",
            "I0719 09:24:36.958941 140481689634688 learning.py:507] global step 1025: loss = 0.2723 (0.381 sec/step)\n",
            "I0719 09:24:37.293313 140481689634688 learning.py:507] global step 1026: loss = 0.2551 (0.333 sec/step)\n",
            "I0719 09:24:37.631866 140481689634688 learning.py:507] global step 1027: loss = 0.2359 (0.337 sec/step)\n",
            "I0719 09:24:37.971214 140481689634688 learning.py:507] global step 1028: loss = 0.2895 (0.337 sec/step)\n",
            "I0719 09:24:38.301473 140481689634688 learning.py:507] global step 1029: loss = 0.0504 (0.328 sec/step)\n",
            "I0719 09:24:38.644452 140481689634688 learning.py:507] global step 1030: loss = 0.3686 (0.341 sec/step)\n",
            "I0719 09:24:39.013956 140481689634688 learning.py:507] global step 1031: loss = 0.1359 (0.368 sec/step)\n",
            "I0719 09:24:39.348767 140481689634688 learning.py:507] global step 1032: loss = 0.1975 (0.333 sec/step)\n",
            "I0719 09:24:39.642465 140481689634688 learning.py:507] global step 1033: loss = 0.2241 (0.292 sec/step)\n",
            "I0719 09:24:40.006862 140481689634688 learning.py:507] global step 1034: loss = 0.2048 (0.362 sec/step)\n",
            "I0719 09:24:40.294831 140481689634688 learning.py:507] global step 1035: loss = 0.0933 (0.286 sec/step)\n",
            "I0719 09:24:40.610952 140481689634688 learning.py:507] global step 1036: loss = 0.6186 (0.314 sec/step)\n",
            "I0719 09:24:40.949767 140481689634688 learning.py:507] global step 1037: loss = 0.0523 (0.337 sec/step)\n",
            "I0719 09:24:41.318210 140481689634688 learning.py:507] global step 1038: loss = 0.1904 (0.367 sec/step)\n",
            "I0719 09:24:41.670516 140481689634688 learning.py:507] global step 1039: loss = 0.2645 (0.351 sec/step)\n",
            "I0719 09:24:41.976654 140481689634688 learning.py:507] global step 1040: loss = 0.1970 (0.304 sec/step)\n",
            "I0719 09:24:42.313000 140481689634688 learning.py:507] global step 1041: loss = 0.1856 (0.334 sec/step)\n",
            "I0719 09:24:42.669691 140481689634688 learning.py:507] global step 1042: loss = 0.4886 (0.355 sec/step)\n",
            "I0719 09:24:42.995611 140481689634688 learning.py:507] global step 1043: loss = 0.2063 (0.324 sec/step)\n",
            "I0719 09:24:43.281209 140481689634688 learning.py:507] global step 1044: loss = 0.1354 (0.284 sec/step)\n",
            "I0719 09:24:43.663787 140481689634688 learning.py:507] global step 1045: loss = 0.2801 (0.381 sec/step)\n",
            "I0719 09:24:43.981288 140481689634688 learning.py:507] global step 1046: loss = 0.1811 (0.316 sec/step)\n",
            "I0719 09:24:44.311637 140481689634688 learning.py:507] global step 1047: loss = 0.1443 (0.328 sec/step)\n",
            "I0719 09:24:44.682708 140481689634688 learning.py:507] global step 1048: loss = 0.0806 (0.369 sec/step)\n",
            "I0719 09:24:45.022968 140481689634688 learning.py:507] global step 1049: loss = 0.1239 (0.338 sec/step)\n",
            "I0719 09:24:45.346478 140481689634688 learning.py:507] global step 1050: loss = 0.4833 (0.322 sec/step)\n",
            "I0719 09:24:45.695759 140481689634688 learning.py:507] global step 1051: loss = 0.2187 (0.347 sec/step)\n",
            "I0719 09:24:46.046099 140481689634688 learning.py:507] global step 1052: loss = 0.1750 (0.349 sec/step)\n",
            "I0719 09:24:46.383021 140481689634688 learning.py:507] global step 1053: loss = 0.1550 (0.335 sec/step)\n",
            "I0719 09:24:46.687355 140481689634688 learning.py:507] global step 1054: loss = 0.4665 (0.302 sec/step)\n",
            "I0719 09:24:47.009893 140481689634688 learning.py:507] global step 1055: loss = 0.0938 (0.321 sec/step)\n",
            "I0719 09:24:47.322180 140481689634688 learning.py:507] global step 1056: loss = 0.1534 (0.311 sec/step)\n",
            "I0719 09:24:47.644102 140481689634688 learning.py:507] global step 1057: loss = 0.2801 (0.320 sec/step)\n",
            "I0719 09:24:47.963595 140481689634688 learning.py:507] global step 1058: loss = 0.2576 (0.318 sec/step)\n",
            "I0719 09:24:48.295493 140481689634688 learning.py:507] global step 1059: loss = 0.1203 (0.330 sec/step)\n",
            "I0719 09:24:48.637123 140481689634688 learning.py:507] global step 1060: loss = 0.3225 (0.340 sec/step)\n",
            "I0719 09:24:48.989410 140481689634688 learning.py:507] global step 1061: loss = 0.0635 (0.350 sec/step)\n",
            "I0719 09:24:49.323728 140481689634688 learning.py:507] global step 1062: loss = 0.2242 (0.333 sec/step)\n",
            "I0719 09:24:49.623806 140481689634688 learning.py:507] global step 1063: loss = 0.1486 (0.298 sec/step)\n",
            "I0719 09:24:49.967717 140481689634688 learning.py:507] global step 1064: loss = 0.1403 (0.342 sec/step)\n",
            "I0719 09:24:50.327152 140481689634688 learning.py:507] global step 1065: loss = 0.0445 (0.357 sec/step)\n",
            "I0719 09:24:50.661084 140481689634688 learning.py:507] global step 1066: loss = 0.2839 (0.332 sec/step)\n",
            "I0719 09:24:51.012741 140481689634688 learning.py:507] global step 1067: loss = 0.4608 (0.350 sec/step)\n",
            "I0719 09:24:51.354423 140481689634688 learning.py:507] global step 1068: loss = 0.2225 (0.339 sec/step)\n",
            "I0719 09:24:51.686346 140481689634688 learning.py:507] global step 1069: loss = 0.0553 (0.330 sec/step)\n",
            "I0719 09:24:52.026652 140481689634688 learning.py:507] global step 1070: loss = 0.1222 (0.338 sec/step)\n",
            "I0719 09:24:52.329841 140481689634688 learning.py:507] global step 1071: loss = 0.2666 (0.300 sec/step)\n",
            "I0719 09:24:52.663220 140481689634688 learning.py:507] global step 1072: loss = 0.4384 (0.331 sec/step)\n",
            "I0719 09:24:53.000187 140481689634688 learning.py:507] global step 1073: loss = 0.3768 (0.335 sec/step)\n",
            "I0719 09:24:53.354450 140481689634688 learning.py:507] global step 1074: loss = 0.1319 (0.352 sec/step)\n",
            "I0719 09:24:53.727755 140481689634688 learning.py:507] global step 1075: loss = 0.1097 (0.371 sec/step)\n",
            "I0719 09:24:54.048497 140481689634688 learning.py:507] global step 1076: loss = 0.2826 (0.319 sec/step)\n",
            "I0719 09:24:54.401533 140481689634688 learning.py:507] global step 1077: loss = 0.0474 (0.351 sec/step)\n",
            "I0719 09:24:54.728262 140481689634688 learning.py:507] global step 1078: loss = 0.1862 (0.325 sec/step)\n",
            "I0719 09:24:55.062116 140481689634688 learning.py:507] global step 1079: loss = 0.1907 (0.332 sec/step)\n",
            "I0719 09:24:55.384041 140481689634688 learning.py:507] global step 1080: loss = 0.0528 (0.320 sec/step)\n",
            "I0719 09:24:55.734420 140481689634688 learning.py:507] global step 1081: loss = 0.2036 (0.348 sec/step)\n",
            "I0719 09:24:56.069732 140481689634688 learning.py:507] global step 1082: loss = 0.2241 (0.334 sec/step)\n",
            "I0719 09:24:56.423860 140481689634688 learning.py:507] global step 1083: loss = 0.3228 (0.352 sec/step)\n",
            "I0719 09:24:56.766053 140481689634688 learning.py:507] global step 1084: loss = 0.2012 (0.340 sec/step)\n",
            "I0719 09:24:57.111606 140481689634688 learning.py:507] global step 1085: loss = 0.1649 (0.344 sec/step)\n",
            "I0719 09:24:57.457096 140481689634688 learning.py:507] global step 1086: loss = 0.2990 (0.344 sec/step)\n",
            "I0719 09:24:57.780789 140481689634688 learning.py:507] global step 1087: loss = 0.1750 (0.322 sec/step)\n",
            "I0719 09:24:58.110342 140481689634688 learning.py:507] global step 1088: loss = 0.1815 (0.328 sec/step)\n",
            "I0719 09:24:58.397233 140481689634688 learning.py:507] global step 1089: loss = 0.0694 (0.285 sec/step)\n",
            "I0719 09:24:58.755262 140481689634688 learning.py:507] global step 1090: loss = 0.1708 (0.356 sec/step)\n",
            "I0719 09:24:59.107559 140481689634688 learning.py:507] global step 1091: loss = 0.2661 (0.351 sec/step)\n",
            "I0719 09:24:59.488825 140481689634688 learning.py:507] global step 1092: loss = 0.3590 (0.379 sec/step)\n",
            "I0719 09:24:59.808168 140481689634688 learning.py:507] global step 1093: loss = 0.3152 (0.317 sec/step)\n",
            "I0719 09:25:00.128528 140481689634688 learning.py:507] global step 1094: loss = 0.1785 (0.319 sec/step)\n",
            "I0719 09:25:00.473175 140481689634688 learning.py:507] global step 1095: loss = 0.3844 (0.343 sec/step)\n",
            "I0719 09:25:00.778243 140481689634688 learning.py:507] global step 1096: loss = 0.0773 (0.303 sec/step)\n",
            "I0719 09:25:01.098712 140481689634688 learning.py:507] global step 1097: loss = 0.2614 (0.319 sec/step)\n",
            "I0719 09:25:01.410590 140481689634688 learning.py:507] global step 1098: loss = 0.2543 (0.310 sec/step)\n",
            "I0719 09:25:01.753083 140481689634688 learning.py:507] global step 1099: loss = 0.1301 (0.341 sec/step)\n",
            "I0719 09:25:02.078081 140481689634688 learning.py:507] global step 1100: loss = 0.2939 (0.323 sec/step)\n",
            "I0719 09:25:02.437077 140481689634688 learning.py:507] global step 1101: loss = 0.0895 (0.357 sec/step)\n",
            "I0719 09:25:02.772826 140481689634688 learning.py:507] global step 1102: loss = 0.2018 (0.334 sec/step)\n",
            "I0719 09:25:03.124316 140481689634688 learning.py:507] global step 1103: loss = 0.1850 (0.350 sec/step)\n",
            "I0719 09:25:03.500116 140481689634688 learning.py:507] global step 1104: loss = 0.2807 (0.374 sec/step)\n",
            "I0719 09:25:03.792354 140481689634688 learning.py:507] global step 1105: loss = 0.1288 (0.290 sec/step)\n",
            "I0719 09:25:04.119798 140481689634688 learning.py:507] global step 1106: loss = 0.0652 (0.326 sec/step)\n",
            "I0719 09:25:04.487373 140481689634688 learning.py:507] global step 1107: loss = 0.4127 (0.366 sec/step)\n",
            "I0719 09:25:04.800963 140481689634688 learning.py:507] global step 1108: loss = 0.2145 (0.312 sec/step)\n",
            "I0719 09:25:05.113089 140481689634688 learning.py:507] global step 1109: loss = 0.1664 (0.310 sec/step)\n",
            "I0719 09:25:05.427469 140481689634688 learning.py:507] global step 1110: loss = 0.1396 (0.313 sec/step)\n",
            "I0719 09:25:05.752099 140481689634688 learning.py:507] global step 1111: loss = 0.1404 (0.323 sec/step)\n",
            "I0719 09:25:06.039070 140481689634688 learning.py:507] global step 1112: loss = 0.1713 (0.285 sec/step)\n",
            "I0719 09:25:06.365853 140481689634688 learning.py:507] global step 1113: loss = 0.0643 (0.325 sec/step)\n",
            "I0719 09:25:06.704067 140481689634688 learning.py:507] global step 1114: loss = 0.1044 (0.336 sec/step)\n",
            "I0719 09:25:07.046894 140481689634688 learning.py:507] global step 1115: loss = 0.1599 (0.341 sec/step)\n",
            "I0719 09:25:07.393773 140481689634688 learning.py:507] global step 1116: loss = 0.2732 (0.345 sec/step)\n",
            "I0719 09:25:07.758191 140481689634688 learning.py:507] global step 1117: loss = 0.3713 (0.363 sec/step)\n",
            "I0719 09:25:08.099091 140481689634688 learning.py:507] global step 1118: loss = 0.1698 (0.339 sec/step)\n",
            "I0719 09:25:08.423594 140481689634688 learning.py:507] global step 1119: loss = 0.1691 (0.323 sec/step)\n",
            "I0719 09:25:08.740235 140481689634688 learning.py:507] global step 1120: loss = 0.5579 (0.315 sec/step)\n",
            "I0719 09:25:09.073756 140481689634688 learning.py:507] global step 1121: loss = 0.1905 (0.331 sec/step)\n",
            "I0719 09:25:09.401810 140481689634688 learning.py:507] global step 1122: loss = 0.0740 (0.326 sec/step)\n",
            "I0719 09:25:09.758686 140481689634688 learning.py:507] global step 1123: loss = 0.2927 (0.355 sec/step)\n",
            "I0719 09:25:10.040429 140481689634688 learning.py:507] global step 1124: loss = 0.2048 (0.280 sec/step)\n",
            "I0719 09:25:10.343898 140481689634688 learning.py:507] global step 1125: loss = 0.3007 (0.302 sec/step)\n",
            "I0719 09:25:10.692703 140481689634688 learning.py:507] global step 1126: loss = 0.1741 (0.347 sec/step)\n",
            "I0719 09:25:10.971309 140481689634688 learning.py:507] global step 1127: loss = 0.1268 (0.277 sec/step)\n",
            "I0719 09:25:11.321192 140481689634688 learning.py:507] global step 1128: loss = 0.2536 (0.348 sec/step)\n",
            "I0719 09:25:11.680094 140481689634688 learning.py:507] global step 1129: loss = 0.2522 (0.357 sec/step)\n",
            "I0719 09:25:12.031549 140481689634688 learning.py:507] global step 1130: loss = 0.2020 (0.350 sec/step)\n",
            "I0719 09:25:12.357108 140481689634688 learning.py:507] global step 1131: loss = 0.0932 (0.324 sec/step)\n",
            "I0719 09:25:12.695776 140481689634688 learning.py:507] global step 1132: loss = 0.1302 (0.337 sec/step)\n",
            "I0719 09:25:13.026518 140481689634688 learning.py:507] global step 1133: loss = 0.0499 (0.329 sec/step)\n",
            "I0719 09:25:13.358632 140481689634688 learning.py:507] global step 1134: loss = 0.2399 (0.330 sec/step)\n",
            "I0719 09:25:13.654634 140481689634688 learning.py:507] global step 1135: loss = 0.0687 (0.294 sec/step)\n",
            "I0719 09:25:14.025612 140481689634688 learning.py:507] global step 1136: loss = 0.2802 (0.368 sec/step)\n",
            "I0719 09:25:14.388758 140481689634688 learning.py:507] global step 1137: loss = 0.4873 (0.361 sec/step)\n",
            "I0719 09:25:14.719724 140481689634688 learning.py:507] global step 1138: loss = 0.2366 (0.329 sec/step)\n",
            "I0719 09:25:15.031380 140481689634688 learning.py:507] global step 1139: loss = 0.3901 (0.310 sec/step)\n",
            "I0719 09:25:15.365720 140481689634688 learning.py:507] global step 1140: loss = 0.2375 (0.332 sec/step)\n",
            "I0719 09:25:15.698845 140481689634688 learning.py:507] global step 1141: loss = 0.1991 (0.331 sec/step)\n",
            "I0719 09:25:16.037066 140481689634688 learning.py:507] global step 1142: loss = 0.1654 (0.336 sec/step)\n",
            "I0719 09:25:16.378249 140481689634688 learning.py:507] global step 1143: loss = 0.1845 (0.339 sec/step)\n",
            "I0719 09:25:16.691224 140481689634688 learning.py:507] global step 1144: loss = 0.1508 (0.311 sec/step)\n",
            "I0719 09:25:17.043470 140481689634688 learning.py:507] global step 1145: loss = 0.3902 (0.350 sec/step)\n",
            "I0719 09:25:17.394243 140481689634688 learning.py:507] global step 1146: loss = 0.2686 (0.349 sec/step)\n",
            "I0719 09:25:17.732565 140481689634688 learning.py:507] global step 1147: loss = 0.4308 (0.337 sec/step)\n",
            "I0719 09:25:18.082231 140481689634688 learning.py:507] global step 1148: loss = 0.2283 (0.348 sec/step)\n",
            "I0719 09:25:18.373107 140481689634688 learning.py:507] global step 1149: loss = 0.3062 (0.289 sec/step)\n",
            "I0719 09:25:18.725706 140481689634688 learning.py:507] global step 1150: loss = 0.2978 (0.351 sec/step)\n",
            "I0719 09:25:19.056127 140481689634688 learning.py:507] global step 1151: loss = 0.1703 (0.329 sec/step)\n",
            "I0719 09:25:19.389339 140481689634688 learning.py:507] global step 1152: loss = 0.3060 (0.331 sec/step)\n",
            "I0719 09:25:19.740878 140481689634688 learning.py:507] global step 1153: loss = 0.1415 (0.350 sec/step)\n",
            "I0719 09:25:20.055552 140481689634688 learning.py:507] global step 1154: loss = 0.0983 (0.313 sec/step)\n",
            "I0719 09:25:20.378619 140481689634688 learning.py:507] global step 1155: loss = 0.0434 (0.321 sec/step)\n",
            "I0719 09:25:20.717992 140481689634688 learning.py:507] global step 1156: loss = 0.4345 (0.338 sec/step)\n",
            "I0719 09:25:21.076985 140481689634688 learning.py:507] global step 1157: loss = 0.4674 (0.357 sec/step)\n",
            "I0719 09:25:21.413362 140481689634688 learning.py:507] global step 1158: loss = 0.1591 (0.334 sec/step)\n",
            "I0719 09:25:21.736739 140481689634688 learning.py:507] global step 1159: loss = 0.1342 (0.321 sec/step)\n",
            "I0719 09:25:22.070490 140481689634688 learning.py:507] global step 1160: loss = 0.2180 (0.332 sec/step)\n",
            "I0719 09:25:22.404743 140481689634688 learning.py:507] global step 1161: loss = 0.2554 (0.332 sec/step)\n",
            "I0719 09:25:22.749569 140481689634688 learning.py:507] global step 1162: loss = 0.2544 (0.343 sec/step)\n",
            "I0719 09:25:23.050088 140481689634688 learning.py:507] global step 1163: loss = 0.0302 (0.298 sec/step)\n",
            "I0719 09:25:23.397701 140481689634688 learning.py:507] global step 1164: loss = 0.3743 (0.346 sec/step)\n",
            "I0719 09:25:23.707073 140481689634688 learning.py:507] global step 1165: loss = 0.0995 (0.307 sec/step)\n",
            "I0719 09:25:24.047005 140481689634688 learning.py:507] global step 1166: loss = 0.0429 (0.338 sec/step)\n",
            "I0719 09:25:24.393814 140481689634688 learning.py:507] global step 1167: loss = 0.2976 (0.345 sec/step)\n",
            "I0719 09:25:24.723103 140481689634688 learning.py:507] global step 1168: loss = 0.1987 (0.328 sec/step)\n",
            "I0719 09:25:25.064659 140481689634688 learning.py:507] global step 1169: loss = 0.0539 (0.340 sec/step)\n",
            "I0719 09:25:25.434965 140481689634688 learning.py:507] global step 1170: loss = 0.2217 (0.369 sec/step)\n",
            "I0719 09:25:25.775335 140481689634688 learning.py:507] global step 1171: loss = 0.1777 (0.339 sec/step)\n",
            "I0719 09:25:26.132151 140481689634688 learning.py:507] global step 1172: loss = 0.2485 (0.355 sec/step)\n",
            "I0719 09:25:26.479689 140481689634688 learning.py:507] global step 1173: loss = 0.0711 (0.346 sec/step)\n",
            "I0719 09:25:26.816252 140481689634688 learning.py:507] global step 1174: loss = 0.1047 (0.335 sec/step)\n",
            "I0719 09:25:27.170750 140481689634688 learning.py:507] global step 1175: loss = 0.0958 (0.353 sec/step)\n",
            "I0719 09:25:27.553126 140481689634688 learning.py:507] global step 1176: loss = 0.2110 (0.381 sec/step)\n",
            "I0719 09:25:27.902470 140481689634688 learning.py:507] global step 1177: loss = 0.1914 (0.348 sec/step)\n",
            "I0719 09:25:28.234722 140481689634688 learning.py:507] global step 1178: loss = 0.2339 (0.331 sec/step)\n",
            "I0719 09:25:28.571626 140481689634688 learning.py:507] global step 1179: loss = 0.0386 (0.335 sec/step)\n",
            "I0719 09:25:28.913813 140481689634688 learning.py:507] global step 1180: loss = 0.2311 (0.340 sec/step)\n",
            "I0719 09:25:29.272783 140481689634688 learning.py:507] global step 1181: loss = 0.1343 (0.357 sec/step)\n",
            "I0719 09:25:29.601873 140481689634688 learning.py:507] global step 1182: loss = 0.2103 (0.327 sec/step)\n",
            "I0719 09:25:29.967029 140481689634688 learning.py:507] global step 1183: loss = 0.2164 (0.363 sec/step)\n",
            "I0719 09:25:30.333544 140481689634688 learning.py:507] global step 1184: loss = 0.1091 (0.364 sec/step)\n",
            "I0719 09:25:30.662333 140481689634688 learning.py:507] global step 1185: loss = 0.2053 (0.327 sec/step)\n",
            "I0719 09:25:30.995802 140481689634688 learning.py:507] global step 1186: loss = 0.0585 (0.332 sec/step)\n",
            "I0719 09:25:31.335198 140481689634688 learning.py:507] global step 1187: loss = 0.2345 (0.338 sec/step)\n",
            "I0719 09:25:31.669799 140481689634688 learning.py:507] global step 1188: loss = 0.1769 (0.333 sec/step)\n",
            "I0719 09:25:32.005898 140481689634688 learning.py:507] global step 1189: loss = 0.2504 (0.334 sec/step)\n",
            "I0719 09:25:32.337375 140481689634688 learning.py:507] global step 1190: loss = 0.3099 (0.330 sec/step)\n",
            "I0719 09:25:32.677732 140481689634688 learning.py:507] global step 1191: loss = 0.1569 (0.339 sec/step)\n",
            "I0719 09:25:33.021971 140481689634688 learning.py:507] global step 1192: loss = 0.1774 (0.342 sec/step)\n",
            "I0719 09:25:33.349613 140481689634688 learning.py:507] global step 1193: loss = 0.3058 (0.325 sec/step)\n",
            "I0719 09:25:33.710083 140481689634688 learning.py:507] global step 1194: loss = 0.2382 (0.359 sec/step)\n",
            "I0719 09:25:34.065804 140481689634688 learning.py:507] global step 1195: loss = 0.0522 (0.354 sec/step)\n",
            "I0719 09:25:34.417818 140481689634688 learning.py:507] global step 1196: loss = 0.1812 (0.350 sec/step)\n",
            "I0719 09:25:34.721810 140481689634688 learning.py:507] global step 1197: loss = 0.1024 (0.302 sec/step)\n",
            "I0719 09:25:35.052320 140481689634688 learning.py:507] global step 1198: loss = 0.3096 (0.329 sec/step)\n",
            "I0719 09:25:35.393196 140481689634688 learning.py:507] global step 1199: loss = 0.1338 (0.339 sec/step)\n",
            "I0719 09:25:35.732553 140481689634688 learning.py:507] global step 1200: loss = 0.2046 (0.338 sec/step)\n",
            "I0719 09:25:36.061563 140481689634688 learning.py:507] global step 1201: loss = 0.1020 (0.327 sec/step)\n",
            "I0719 09:25:36.415339 140481689634688 learning.py:507] global step 1202: loss = 0.1043 (0.352 sec/step)\n",
            "I0719 09:25:36.720022 140481689634688 learning.py:507] global step 1203: loss = 0.1559 (0.303 sec/step)\n",
            "I0719 09:25:37.042768 140481689634688 learning.py:507] global step 1204: loss = 0.4238 (0.321 sec/step)\n",
            "I0719 09:25:37.380597 140481689634688 learning.py:507] global step 1205: loss = 0.3740 (0.336 sec/step)\n",
            "I0719 09:25:37.748023 140481689634688 learning.py:507] global step 1206: loss = 0.2252 (0.366 sec/step)\n",
            "I0719 09:25:38.047466 140481689634688 learning.py:507] global step 1207: loss = 0.0803 (0.298 sec/step)\n",
            "I0719 09:25:38.393745 140481689634688 learning.py:507] global step 1208: loss = 0.1788 (0.344 sec/step)\n",
            "I0719 09:25:38.721884 140481689634688 learning.py:507] global step 1209: loss = 0.2164 (0.326 sec/step)\n",
            "I0719 09:25:39.061383 140481689634688 learning.py:507] global step 1210: loss = 0.2187 (0.337 sec/step)\n",
            "I0719 09:25:39.404760 140481689634688 learning.py:507] global step 1211: loss = 0.0621 (0.342 sec/step)\n",
            "I0719 09:25:39.753322 140481689634688 learning.py:507] global step 1212: loss = 0.2092 (0.347 sec/step)\n",
            "I0719 09:25:40.079807 140481689634688 learning.py:507] global step 1213: loss = 0.1244 (0.325 sec/step)\n",
            "I0719 09:25:40.430119 140481689634688 learning.py:507] global step 1214: loss = 0.1101 (0.349 sec/step)\n",
            "I0719 09:25:40.709121 140481689634688 learning.py:507] global step 1215: loss = 0.1933 (0.277 sec/step)\n",
            "I0719 09:25:41.040090 140481689634688 learning.py:507] global step 1216: loss = 0.1763 (0.329 sec/step)\n",
            "I0719 09:25:41.361036 140481689634688 learning.py:507] global step 1217: loss = 0.1681 (0.319 sec/step)\n",
            "I0719 09:25:41.709255 140481689634688 learning.py:507] global step 1218: loss = 0.2418 (0.347 sec/step)\n",
            "I0719 09:25:42.033147 140481689634688 learning.py:507] global step 1219: loss = 0.3121 (0.322 sec/step)\n",
            "I0719 09:25:42.370383 140481689634688 learning.py:507] global step 1220: loss = 0.0382 (0.336 sec/step)\n",
            "I0719 09:25:42.709063 140481689634688 learning.py:507] global step 1221: loss = 0.1717 (0.337 sec/step)\n",
            "I0719 09:25:43.052491 140481689634688 learning.py:507] global step 1222: loss = 0.1920 (0.342 sec/step)\n",
            "I0719 09:25:43.364354 140481689634688 learning.py:507] global step 1223: loss = 0.4320 (0.310 sec/step)\n",
            "I0719 09:25:43.681596 140481689634688 learning.py:507] global step 1224: loss = 0.1363 (0.316 sec/step)\n",
            "I0719 09:25:44.005078 140481689634688 learning.py:507] global step 1225: loss = 0.2180 (0.322 sec/step)\n",
            "I0719 09:25:44.343581 140481689634688 learning.py:507] global step 1226: loss = 0.2423 (0.337 sec/step)\n",
            "I0719 09:25:44.684660 140481689634688 learning.py:507] global step 1227: loss = 0.0791 (0.340 sec/step)\n",
            "I0719 09:25:45.019184 140481689634688 learning.py:507] global step 1228: loss = 0.1272 (0.333 sec/step)\n",
            "I0719 09:25:45.349788 140481689634688 learning.py:507] global step 1229: loss = 0.0862 (0.329 sec/step)\n",
            "I0719 09:25:45.641423 140481689634688 learning.py:507] global step 1230: loss = 0.0446 (0.290 sec/step)\n",
            "I0719 09:25:45.989184 140481689634688 learning.py:507] global step 1231: loss = 0.1524 (0.346 sec/step)\n",
            "I0719 09:25:46.334915 140481689634688 learning.py:507] global step 1232: loss = 0.2894 (0.344 sec/step)\n",
            "I0719 09:25:46.623795 140481689634688 learning.py:507] global step 1233: loss = 0.1293 (0.287 sec/step)\n",
            "I0719 09:25:46.912021 140481689634688 learning.py:507] global step 1234: loss = 0.2893 (0.286 sec/step)\n",
            "I0719 09:25:47.255084 140481689634688 learning.py:507] global step 1235: loss = 0.1195 (0.341 sec/step)\n",
            "I0719 09:25:47.589756 140481689634688 learning.py:507] global step 1236: loss = 0.1203 (0.333 sec/step)\n",
            "I0719 09:25:47.938863 140481689634688 learning.py:507] global step 1237: loss = 0.2831 (0.347 sec/step)\n",
            "I0719 09:25:48.263679 140481689634688 learning.py:507] global step 1238: loss = 0.2583 (0.323 sec/step)\n",
            "I0719 09:25:48.588034 140481689634688 learning.py:507] global step 1239: loss = 0.1648 (0.323 sec/step)\n",
            "I0719 09:25:48.928772 140481689634688 learning.py:507] global step 1240: loss = 0.3335 (0.339 sec/step)\n",
            "I0719 09:25:49.242347 140481689634688 learning.py:507] global step 1241: loss = 0.0914 (0.312 sec/step)\n",
            "I0719 09:25:49.573720 140481689634688 learning.py:507] global step 1242: loss = 0.3026 (0.330 sec/step)\n",
            "I0719 09:25:49.905488 140481689634688 learning.py:507] global step 1243: loss = 0.1427 (0.330 sec/step)\n",
            "I0719 09:25:50.232178 140481689634688 learning.py:507] global step 1244: loss = 0.2162 (0.325 sec/step)\n",
            "I0719 09:25:50.570986 140481689634688 learning.py:507] global step 1245: loss = 0.2568 (0.337 sec/step)\n",
            "I0719 09:25:50.883662 140481689634688 learning.py:507] global step 1246: loss = 0.0553 (0.311 sec/step)\n",
            "I0719 09:25:51.189189 140481689634688 learning.py:507] global step 1247: loss = 0.1356 (0.304 sec/step)\n",
            "I0719 09:25:51.540253 140481689634688 learning.py:507] global step 1248: loss = 0.1066 (0.349 sec/step)\n",
            "I0719 09:25:51.825504 140481689634688 learning.py:507] global step 1249: loss = 0.0977 (0.283 sec/step)\n",
            "I0719 09:25:52.160890 140481689634688 learning.py:507] global step 1250: loss = 0.1994 (0.334 sec/step)\n",
            "I0719 09:25:52.492682 140481689634688 learning.py:507] global step 1251: loss = 0.1923 (0.330 sec/step)\n",
            "I0719 09:25:52.793091 140481689634688 learning.py:507] global step 1252: loss = 0.0803 (0.298 sec/step)\n",
            "I0719 09:25:53.106258 140481689634688 learning.py:507] global step 1253: loss = 0.0908 (0.311 sec/step)\n",
            "I0719 09:25:53.412459 140481689634688 learning.py:507] global step 1254: loss = 0.1686 (0.304 sec/step)\n",
            "I0719 09:25:53.722949 140481689634688 learning.py:507] global step 1255: loss = 0.1475 (0.309 sec/step)\n",
            "I0719 09:25:54.092497 140481689634688 learning.py:507] global step 1256: loss = 0.2256 (0.368 sec/step)\n",
            "I0719 09:25:54.424739 140481689634688 learning.py:507] global step 1257: loss = 0.1653 (0.331 sec/step)\n",
            "I0719 09:25:54.795535 140481689634688 learning.py:507] global step 1258: loss = 0.1909 (0.365 sec/step)\n",
            "I0719 09:25:55.138676 140481689634688 learning.py:507] global step 1259: loss = 0.1386 (0.341 sec/step)\n",
            "I0719 09:25:55.467653 140481689634688 learning.py:507] global step 1260: loss = 0.1748 (0.327 sec/step)\n",
            "I0719 09:25:55.809502 140481689634688 learning.py:507] global step 1261: loss = 0.1388 (0.340 sec/step)\n",
            "I0719 09:25:56.177774 140481689634688 learning.py:507] global step 1262: loss = 0.1987 (0.367 sec/step)\n",
            "I0719 09:25:56.510700 140481689634688 learning.py:507] global step 1263: loss = 0.2704 (0.331 sec/step)\n",
            "I0719 09:25:56.838639 140481689634688 learning.py:507] global step 1264: loss = 0.3018 (0.326 sec/step)\n",
            "I0719 09:25:57.159000 140481689634688 learning.py:507] global step 1265: loss = 0.1943 (0.318 sec/step)\n",
            "I0719 09:25:57.504255 140481689634688 learning.py:507] global step 1266: loss = 0.2022 (0.343 sec/step)\n",
            "I0719 09:25:57.817177 140481689634688 learning.py:507] global step 1267: loss = 0.1124 (0.311 sec/step)\n",
            "I0719 09:25:58.166946 140481689634688 learning.py:507] global step 1268: loss = 0.2775 (0.348 sec/step)\n",
            "I0719 09:25:58.448360 140481689634688 learning.py:507] global step 1269: loss = 0.1970 (0.279 sec/step)\n",
            "I0719 09:25:58.754818 140481689634688 learning.py:507] global step 1270: loss = 0.1304 (0.305 sec/step)\n",
            "I0719 09:25:59.101914 140481689634688 learning.py:507] global step 1271: loss = 0.3842 (0.344 sec/step)\n",
            "I0719 09:25:59.444449 140481689634688 learning.py:507] global step 1272: loss = 0.0869 (0.340 sec/step)\n",
            "I0719 09:25:59.795732 140481689634688 learning.py:507] global step 1273: loss = 0.2122 (0.349 sec/step)\n",
            "I0719 09:26:00.141238 140481689634688 learning.py:507] global step 1274: loss = 0.2472 (0.344 sec/step)\n",
            "I0719 09:26:00.505115 140481689634688 learning.py:507] global step 1275: loss = 0.1258 (0.362 sec/step)\n",
            "I0719 09:26:00.810994 140481689634688 learning.py:507] global step 1276: loss = 0.0876 (0.304 sec/step)\n",
            "I0719 09:26:01.109352 140481689634688 learning.py:507] global step 1277: loss = 0.1292 (0.296 sec/step)\n",
            "I0719 09:26:01.439771 140481689634688 learning.py:507] global step 1278: loss = 0.1396 (0.329 sec/step)\n",
            "I0719 09:26:01.770060 140481689634688 learning.py:507] global step 1279: loss = 0.2018 (0.328 sec/step)\n",
            "I0719 09:26:02.073558 140481689634688 learning.py:507] global step 1280: loss = 0.2075 (0.302 sec/step)\n",
            "I0719 09:26:02.428590 140481689634688 learning.py:507] global step 1281: loss = 0.1672 (0.353 sec/step)\n",
            "I0719 09:26:02.753658 140481689634688 learning.py:507] global step 1282: loss = 0.0525 (0.323 sec/step)\n",
            "I0719 09:26:03.046592 140481689634688 learning.py:507] global step 1283: loss = 0.3863 (0.291 sec/step)\n",
            "I0719 09:26:03.377655 140481689634688 learning.py:507] global step 1284: loss = 0.1005 (0.329 sec/step)\n",
            "I0719 09:26:03.725422 140481689634688 learning.py:507] global step 1285: loss = 0.2127 (0.346 sec/step)\n",
            "I0719 09:26:04.068216 140481689634688 learning.py:507] global step 1286: loss = 0.1764 (0.341 sec/step)\n",
            "I0719 09:26:04.395644 140481689634688 learning.py:507] global step 1287: loss = 0.0395 (0.326 sec/step)\n",
            "I0719 09:26:04.717617 140481689634688 learning.py:507] global step 1288: loss = 0.1771 (0.320 sec/step)\n",
            "I0719 09:26:05.051099 140481689634688 learning.py:507] global step 1289: loss = 0.2926 (0.332 sec/step)\n",
            "I0719 09:26:05.355633 140481689634688 learning.py:507] global step 1290: loss = 0.2220 (0.303 sec/step)\n",
            "I0719 09:26:05.690223 140481689634688 learning.py:507] global step 1291: loss = 0.6736 (0.333 sec/step)\n",
            "I0719 09:26:06.023692 140481689634688 learning.py:507] global step 1292: loss = 0.1891 (0.332 sec/step)\n",
            "I0719 09:26:06.360703 140481689634688 learning.py:507] global step 1293: loss = 0.1560 (0.335 sec/step)\n",
            "I0719 09:26:06.705953 140481689634688 learning.py:507] global step 1294: loss = 0.1821 (0.342 sec/step)\n",
            "I0719 09:26:07.030602 140481689634688 learning.py:507] global step 1295: loss = 0.1132 (0.322 sec/step)\n",
            "I0719 09:26:07.373802 140481689634688 learning.py:507] global step 1296: loss = 0.0977 (0.341 sec/step)\n",
            "I0719 09:26:07.696433 140481689634688 learning.py:507] global step 1297: loss = 0.1765 (0.321 sec/step)\n",
            "I0719 09:26:08.048108 140481689634688 learning.py:507] global step 1298: loss = 0.0898 (0.350 sec/step)\n",
            "I0719 09:26:08.393197 140481689634688 learning.py:507] global step 1299: loss = 0.2900 (0.343 sec/step)\n",
            "I0719 09:26:08.725219 140481689634688 learning.py:507] global step 1300: loss = 0.1974 (0.330 sec/step)\n",
            "I0719 09:26:09.024442 140481689634688 learning.py:507] global step 1301: loss = 0.1067 (0.297 sec/step)\n",
            "I0719 09:26:09.367619 140481689634688 learning.py:507] global step 1302: loss = 0.3562 (0.342 sec/step)\n",
            "I0719 09:26:09.699574 140481689634688 learning.py:507] global step 1303: loss = 0.1546 (0.330 sec/step)\n",
            "I0719 09:26:10.030659 140481689634688 learning.py:507] global step 1304: loss = 0.0663 (0.329 sec/step)\n",
            "I0719 09:26:10.372189 140481689634688 learning.py:507] global step 1305: loss = 0.1775 (0.340 sec/step)\n",
            "I0719 09:26:10.709637 140481689634688 learning.py:507] global step 1306: loss = 0.1134 (0.336 sec/step)\n",
            "I0719 09:26:11.067492 140481689634688 learning.py:507] global step 1307: loss = 0.2391 (0.356 sec/step)\n",
            "I0719 09:26:11.407516 140481689634688 learning.py:507] global step 1308: loss = 0.0314 (0.338 sec/step)\n",
            "I0719 09:26:11.749575 140481689634688 learning.py:507] global step 1309: loss = 0.1225 (0.340 sec/step)\n",
            "I0719 09:26:12.082358 140481689634688 learning.py:507] global step 1310: loss = 0.1874 (0.331 sec/step)\n",
            "I0719 09:26:12.399699 140481689634688 learning.py:507] global step 1311: loss = 0.5124 (0.316 sec/step)\n",
            "I0719 09:26:12.712508 140481689634688 learning.py:507] global step 1312: loss = 0.0974 (0.308 sec/step)\n",
            "I0719 09:26:12.999454 140481689634688 learning.py:507] global step 1313: loss = 0.1121 (0.285 sec/step)\n",
            "I0719 09:26:13.342851 140481689634688 learning.py:507] global step 1314: loss = 0.1793 (0.342 sec/step)\n",
            "I0719 09:26:13.624896 140481689634688 learning.py:507] global step 1315: loss = 0.1220 (0.280 sec/step)\n",
            "I0719 09:26:13.974707 140481689634688 learning.py:507] global step 1316: loss = 0.1219 (0.348 sec/step)\n",
            "I0719 09:26:14.318585 140481689634688 learning.py:507] global step 1317: loss = 0.3618 (0.341 sec/step)\n",
            "I0719 09:26:14.653069 140481689634688 learning.py:507] global step 1318: loss = 0.2284 (0.332 sec/step)\n",
            "I0719 09:26:14.995175 140481689634688 learning.py:507] global step 1319: loss = 0.2339 (0.340 sec/step)\n",
            "I0719 09:26:15.328920 140481689634688 learning.py:507] global step 1320: loss = 0.1524 (0.332 sec/step)\n",
            "I0719 09:26:15.639326 140481689634688 learning.py:507] global step 1321: loss = 0.1577 (0.309 sec/step)\n",
            "I0719 09:26:15.978563 140481689634688 learning.py:507] global step 1322: loss = 0.1151 (0.338 sec/step)\n",
            "I0719 09:26:16.323475 140481689634688 learning.py:507] global step 1323: loss = 0.2906 (0.343 sec/step)\n",
            "I0719 09:26:16.689840 140481689634688 learning.py:507] global step 1324: loss = 0.1181 (0.365 sec/step)\n",
            "I0719 09:26:17.069004 140481689634688 learning.py:507] global step 1325: loss = 0.2602 (0.377 sec/step)\n",
            "I0719 09:26:17.402414 140481689634688 learning.py:507] global step 1326: loss = 0.1315 (0.332 sec/step)\n",
            "I0719 09:26:17.734596 140481689634688 learning.py:507] global step 1327: loss = 0.2035 (0.330 sec/step)\n",
            "I0719 09:26:18.092628 140481689634688 learning.py:507] global step 1328: loss = 0.0400 (0.356 sec/step)\n",
            "I0719 09:26:18.430229 140481689634688 learning.py:507] global step 1329: loss = 0.2060 (0.336 sec/step)\n",
            "I0719 09:26:18.767479 140481689634688 learning.py:507] global step 1330: loss = 0.2407 (0.335 sec/step)\n",
            "I0719 09:26:19.076349 140481689634688 learning.py:507] global step 1331: loss = 0.3828 (0.307 sec/step)\n",
            "I0719 09:26:19.413821 140481689634688 learning.py:507] global step 1332: loss = 0.1326 (0.336 sec/step)\n",
            "I0719 09:26:19.762800 140481689634688 learning.py:507] global step 1333: loss = 0.0218 (0.347 sec/step)\n",
            "I0719 09:26:20.151253 140481689634688 learning.py:507] global step 1334: loss = 0.0878 (0.386 sec/step)\n",
            "I0719 09:26:20.518884 140481689634688 learning.py:507] global step 1335: loss = 0.3210 (0.365 sec/step)\n",
            "I0719 09:26:20.866070 140481689634688 learning.py:507] global step 1336: loss = 0.1046 (0.345 sec/step)\n",
            "I0719 09:26:21.169937 140481689634688 learning.py:507] global step 1337: loss = 0.0517 (0.302 sec/step)\n",
            "I0719 09:26:21.520793 140481689634688 learning.py:507] global step 1338: loss = 0.1579 (0.349 sec/step)\n",
            "I0719 09:26:21.861801 140481689634688 learning.py:507] global step 1339: loss = 0.0938 (0.339 sec/step)\n",
            "I0719 09:26:22.228244 140481689634688 learning.py:507] global step 1340: loss = 0.1641 (0.365 sec/step)\n",
            "I0719 09:26:22.542531 140481689634688 learning.py:507] global step 1341: loss = 0.2185 (0.312 sec/step)\n",
            "I0719 09:26:22.863495 140481689634688 learning.py:507] global step 1342: loss = 0.2196 (0.319 sec/step)\n",
            "I0719 09:26:23.201766 140481689634688 learning.py:507] global step 1343: loss = 0.4313 (0.337 sec/step)\n",
            "I0719 09:26:23.512985 140481689634688 learning.py:507] global step 1344: loss = 0.1808 (0.309 sec/step)\n",
            "I0719 09:26:23.862100 140481689634688 learning.py:507] global step 1345: loss = 0.1799 (0.347 sec/step)\n",
            "I0719 09:26:24.188694 140481689634688 learning.py:507] global step 1346: loss = 0.2403 (0.324 sec/step)\n",
            "I0719 09:26:24.511689 140481689634688 learning.py:507] global step 1347: loss = 0.1152 (0.321 sec/step)\n",
            "I0719 09:26:24.840393 140481689634688 learning.py:507] global step 1348: loss = 0.1274 (0.327 sec/step)\n",
            "I0719 09:26:25.128050 140481689634688 learning.py:507] global step 1349: loss = 0.1094 (0.286 sec/step)\n",
            "I0719 09:26:25.466169 140481689634688 learning.py:507] global step 1350: loss = 0.3195 (0.336 sec/step)\n",
            "I0719 09:26:25.751567 140481689634688 learning.py:507] global step 1351: loss = 0.1274 (0.284 sec/step)\n",
            "I0719 09:26:25.907090 140479034988288 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0719 09:26:26.262729 140481689634688 learning.py:507] global step 1352: loss = 0.1627 (0.405 sec/step)\n",
            "I0719 09:26:26.776288 140481689634688 learning.py:507] global step 1353: loss = 0.2421 (0.505 sec/step)\n",
            "I0719 09:26:27.242737 140481689634688 learning.py:507] global step 1354: loss = 0.2415 (0.437 sec/step)\n",
            "I0719 09:26:27.840865 140479018202880 supervisor.py:1050] Recording summary at step 1355.\n",
            "I0719 09:26:27.853555 140481689634688 learning.py:507] global step 1355: loss = 0.1352 (0.579 sec/step)\n",
            "I0719 09:26:27.876767 140479026595584 supervisor.py:1099] global_step/sec: 2.97809\n",
            "I0719 09:26:28.346119 140481689634688 learning.py:507] global step 1356: loss = 0.1798 (0.484 sec/step)\n",
            "I0719 09:26:28.747720 140481689634688 learning.py:507] global step 1357: loss = 0.1384 (0.399 sec/step)\n",
            "I0719 09:26:29.094189 140481689634688 learning.py:507] global step 1358: loss = 0.2446 (0.343 sec/step)\n",
            "I0719 09:26:29.427431 140481689634688 learning.py:507] global step 1359: loss = 0.2732 (0.331 sec/step)\n",
            "I0719 09:26:29.730770 140481689634688 learning.py:507] global step 1360: loss = 0.1401 (0.301 sec/step)\n",
            "I0719 09:26:30.103430 140481689634688 learning.py:507] global step 1361: loss = 0.2513 (0.371 sec/step)\n",
            "I0719 09:26:30.445696 140481689634688 learning.py:507] global step 1362: loss = 0.1548 (0.340 sec/step)\n",
            "I0719 09:26:30.730118 140481689634688 learning.py:507] global step 1363: loss = 0.1747 (0.282 sec/step)\n",
            "I0719 09:26:31.154919 140481689634688 learning.py:507] global step 1364: loss = 0.2589 (0.423 sec/step)\n",
            "I0719 09:26:31.463992 140481689634688 learning.py:507] global step 1365: loss = 0.1817 (0.307 sec/step)\n",
            "I0719 09:26:31.794766 140481689634688 learning.py:507] global step 1366: loss = 0.1214 (0.329 sec/step)\n",
            "I0719 09:26:32.134095 140481689634688 learning.py:507] global step 1367: loss = 0.1566 (0.337 sec/step)\n",
            "I0719 09:26:32.471233 140481689634688 learning.py:507] global step 1368: loss = 0.0368 (0.335 sec/step)\n",
            "I0719 09:26:32.808863 140481689634688 learning.py:507] global step 1369: loss = 0.0396 (0.336 sec/step)\n",
            "I0719 09:26:33.141822 140481689634688 learning.py:507] global step 1370: loss = 0.1516 (0.331 sec/step)\n",
            "I0719 09:26:33.489711 140481689634688 learning.py:507] global step 1371: loss = 0.2656 (0.346 sec/step)\n",
            "I0719 09:26:33.821439 140481689634688 learning.py:507] global step 1372: loss = 0.2287 (0.329 sec/step)\n",
            "I0719 09:26:34.132679 140481689634688 learning.py:507] global step 1373: loss = 0.0816 (0.309 sec/step)\n",
            "I0719 09:26:34.505760 140481689634688 learning.py:507] global step 1374: loss = 0.1031 (0.369 sec/step)\n",
            "I0719 09:26:34.848714 140481689634688 learning.py:507] global step 1375: loss = 0.1160 (0.341 sec/step)\n",
            "I0719 09:26:35.172608 140481689634688 learning.py:507] global step 1376: loss = 0.0513 (0.322 sec/step)\n",
            "I0719 09:26:35.523774 140481689634688 learning.py:507] global step 1377: loss = 0.1497 (0.349 sec/step)\n",
            "I0719 09:26:35.802128 140481689634688 learning.py:507] global step 1378: loss = 0.1951 (0.276 sec/step)\n",
            "I0719 09:26:36.169347 140481689634688 learning.py:507] global step 1379: loss = 0.1866 (0.365 sec/step)\n",
            "I0719 09:26:36.530577 140481689634688 learning.py:507] global step 1380: loss = 0.0715 (0.360 sec/step)\n",
            "I0719 09:26:36.875648 140481689634688 learning.py:507] global step 1381: loss = 0.3685 (0.343 sec/step)\n",
            "I0719 09:26:37.208554 140481689634688 learning.py:507] global step 1382: loss = 0.2375 (0.331 sec/step)\n",
            "I0719 09:26:37.542138 140481689634688 learning.py:507] global step 1383: loss = 0.0997 (0.332 sec/step)\n",
            "I0719 09:26:37.872048 140481689634688 learning.py:507] global step 1384: loss = 0.2773 (0.328 sec/step)\n",
            "I0719 09:26:38.205540 140481689634688 learning.py:507] global step 1385: loss = 0.3976 (0.332 sec/step)\n",
            "I0719 09:26:38.529394 140481689634688 learning.py:507] global step 1386: loss = 0.0454 (0.322 sec/step)\n",
            "I0719 09:26:38.884896 140481689634688 learning.py:507] global step 1387: loss = 0.0610 (0.354 sec/step)\n",
            "I0719 09:26:39.180427 140481689634688 learning.py:507] global step 1388: loss = 0.1000 (0.294 sec/step)\n",
            "I0719 09:26:39.495672 140481689634688 learning.py:507] global step 1389: loss = 0.1161 (0.313 sec/step)\n",
            "I0719 09:26:39.857114 140481689634688 learning.py:507] global step 1390: loss = 0.4366 (0.360 sec/step)\n",
            "I0719 09:26:40.192118 140481689634688 learning.py:507] global step 1391: loss = 0.1916 (0.333 sec/step)\n",
            "I0719 09:26:40.536366 140481689634688 learning.py:507] global step 1392: loss = 0.2014 (0.342 sec/step)\n",
            "I0719 09:26:40.848575 140481689634688 learning.py:507] global step 1393: loss = 0.0529 (0.310 sec/step)\n",
            "I0719 09:26:41.198063 140481689634688 learning.py:507] global step 1394: loss = 0.1511 (0.348 sec/step)\n",
            "I0719 09:26:41.532646 140481689634688 learning.py:507] global step 1395: loss = 0.2589 (0.328 sec/step)\n",
            "I0719 09:26:41.842425 140481689634688 learning.py:507] global step 1396: loss = 0.1125 (0.308 sec/step)\n",
            "I0719 09:26:42.221246 140481689634688 learning.py:507] global step 1397: loss = 0.2345 (0.377 sec/step)\n",
            "I0719 09:26:42.549411 140481689634688 learning.py:507] global step 1398: loss = 0.0362 (0.326 sec/step)\n",
            "I0719 09:26:42.860448 140481689634688 learning.py:507] global step 1399: loss = 0.0888 (0.309 sec/step)\n",
            "I0719 09:26:43.174650 140481689634688 learning.py:507] global step 1400: loss = 0.1598 (0.313 sec/step)\n",
            "I0719 09:26:43.523026 140481689634688 learning.py:507] global step 1401: loss = 0.2203 (0.346 sec/step)\n",
            "I0719 09:26:43.880624 140481689634688 learning.py:507] global step 1402: loss = 0.1279 (0.356 sec/step)\n",
            "I0719 09:26:44.223075 140481689634688 learning.py:507] global step 1403: loss = 0.2158 (0.341 sec/step)\n",
            "I0719 09:26:44.536325 140481689634688 learning.py:507] global step 1404: loss = 0.1026 (0.311 sec/step)\n",
            "I0719 09:26:44.861307 140481689634688 learning.py:507] global step 1405: loss = 0.1302 (0.323 sec/step)\n",
            "I0719 09:26:45.190550 140481689634688 learning.py:507] global step 1406: loss = 0.2044 (0.328 sec/step)\n",
            "I0719 09:26:45.543480 140481689634688 learning.py:507] global step 1407: loss = 0.1125 (0.351 sec/step)\n",
            "I0719 09:26:45.877117 140481689634688 learning.py:507] global step 1408: loss = 0.1990 (0.332 sec/step)\n",
            "I0719 09:26:46.206954 140481689634688 learning.py:507] global step 1409: loss = 0.1836 (0.328 sec/step)\n",
            "I0719 09:26:46.561548 140481689634688 learning.py:507] global step 1410: loss = 0.2399 (0.353 sec/step)\n",
            "I0719 09:26:46.899415 140481689634688 learning.py:507] global step 1411: loss = 0.1451 (0.336 sec/step)\n",
            "I0719 09:26:47.216823 140481689634688 learning.py:507] global step 1412: loss = 0.0767 (0.316 sec/step)\n",
            "I0719 09:26:47.540043 140481689634688 learning.py:507] global step 1413: loss = 0.1454 (0.321 sec/step)\n",
            "I0719 09:26:47.826534 140481689634688 learning.py:507] global step 1414: loss = 0.1184 (0.285 sec/step)\n",
            "I0719 09:26:48.180163 140481689634688 learning.py:507] global step 1415: loss = 0.1705 (0.352 sec/step)\n",
            "I0719 09:26:48.525181 140481689634688 learning.py:507] global step 1416: loss = 0.1709 (0.343 sec/step)\n",
            "I0719 09:26:48.853554 140481689634688 learning.py:507] global step 1417: loss = 0.1135 (0.327 sec/step)\n",
            "I0719 09:26:49.193065 140481689634688 learning.py:507] global step 1418: loss = 0.2654 (0.338 sec/step)\n",
            "I0719 09:26:49.551081 140481689634688 learning.py:507] global step 1419: loss = 0.0480 (0.356 sec/step)\n",
            "I0719 09:26:49.882321 140481689634688 learning.py:507] global step 1420: loss = 0.1311 (0.330 sec/step)\n",
            "I0719 09:26:50.180392 140481689634688 learning.py:507] global step 1421: loss = 0.1970 (0.296 sec/step)\n",
            "I0719 09:26:50.531897 140481689634688 learning.py:507] global step 1422: loss = 0.1998 (0.350 sec/step)\n",
            "I0719 09:26:50.876891 140481689634688 learning.py:507] global step 1423: loss = 0.0770 (0.343 sec/step)\n",
            "I0719 09:26:51.206506 140481689634688 learning.py:507] global step 1424: loss = 0.0511 (0.328 sec/step)\n",
            "I0719 09:26:51.543360 140481689634688 learning.py:507] global step 1425: loss = 0.2145 (0.335 sec/step)\n",
            "I0719 09:26:51.877932 140481689634688 learning.py:507] global step 1426: loss = 0.2320 (0.333 sec/step)\n",
            "I0719 09:26:52.234036 140481689634688 learning.py:507] global step 1427: loss = 0.0983 (0.354 sec/step)\n",
            "I0719 09:26:52.567213 140481689634688 learning.py:507] global step 1428: loss = 0.2129 (0.331 sec/step)\n",
            "I0719 09:26:52.902518 140481689634688 learning.py:507] global step 1429: loss = 0.1862 (0.334 sec/step)\n",
            "I0719 09:26:53.262732 140481689634688 learning.py:507] global step 1430: loss = 0.1618 (0.358 sec/step)\n",
            "I0719 09:26:53.554725 140481689634688 learning.py:507] global step 1431: loss = 0.3501 (0.290 sec/step)\n",
            "I0719 09:26:53.891663 140481689634688 learning.py:507] global step 1432: loss = 0.1204 (0.335 sec/step)\n",
            "I0719 09:26:54.242658 140481689634688 learning.py:507] global step 1433: loss = 0.1923 (0.349 sec/step)\n",
            "I0719 09:26:54.566599 140481689634688 learning.py:507] global step 1434: loss = 0.2404 (0.322 sec/step)\n",
            "I0719 09:26:54.913242 140481689634688 learning.py:507] global step 1435: loss = 0.1672 (0.345 sec/step)\n",
            "I0719 09:26:55.236072 140481689634688 learning.py:507] global step 1436: loss = 0.0575 (0.321 sec/step)\n",
            "I0719 09:26:55.548546 140481689634688 learning.py:507] global step 1437: loss = 0.4245 (0.311 sec/step)\n",
            "I0719 09:26:55.866384 140481689634688 learning.py:507] global step 1438: loss = 0.2024 (0.316 sec/step)\n",
            "I0719 09:26:56.225890 140481689634688 learning.py:507] global step 1439: loss = 0.3267 (0.358 sec/step)\n",
            "I0719 09:26:56.559517 140481689634688 learning.py:507] global step 1440: loss = 0.3029 (0.332 sec/step)\n",
            "I0719 09:26:56.887413 140481689634688 learning.py:507] global step 1441: loss = 0.3148 (0.326 sec/step)\n",
            "I0719 09:26:57.228196 140481689634688 learning.py:507] global step 1442: loss = 0.1376 (0.339 sec/step)\n",
            "I0719 09:26:57.565209 140481689634688 learning.py:507] global step 1443: loss = 0.1980 (0.335 sec/step)\n",
            "I0719 09:26:57.899530 140481689634688 learning.py:507] global step 1444: loss = 0.1130 (0.332 sec/step)\n",
            "I0719 09:26:58.245170 140481689634688 learning.py:507] global step 1445: loss = 0.2219 (0.343 sec/step)\n",
            "I0719 09:26:58.590611 140481689634688 learning.py:507] global step 1446: loss = 0.1175 (0.344 sec/step)\n",
            "I0719 09:26:58.945938 140481689634688 learning.py:507] global step 1447: loss = 0.2231 (0.353 sec/step)\n",
            "I0719 09:26:59.266132 140481689634688 learning.py:507] global step 1448: loss = 0.1292 (0.318 sec/step)\n",
            "I0719 09:26:59.610241 140481689634688 learning.py:507] global step 1449: loss = 0.1784 (0.342 sec/step)\n",
            "I0719 09:26:59.947741 140481689634688 learning.py:507] global step 1450: loss = 0.3274 (0.335 sec/step)\n",
            "I0719 09:27:00.255514 140481689634688 learning.py:507] global step 1451: loss = 0.0665 (0.306 sec/step)\n",
            "I0719 09:27:00.582195 140481689634688 learning.py:507] global step 1452: loss = 0.1395 (0.325 sec/step)\n",
            "I0719 09:27:00.915989 140481689634688 learning.py:507] global step 1453: loss = 0.0839 (0.332 sec/step)\n",
            "I0719 09:27:01.253075 140481689634688 learning.py:507] global step 1454: loss = 0.1266 (0.335 sec/step)\n",
            "I0719 09:27:01.590448 140481689634688 learning.py:507] global step 1455: loss = 0.0838 (0.336 sec/step)\n",
            "I0719 09:27:01.937228 140481689634688 learning.py:507] global step 1456: loss = 0.2085 (0.345 sec/step)\n",
            "I0719 09:27:02.283931 140481689634688 learning.py:507] global step 1457: loss = 0.1610 (0.345 sec/step)\n",
            "I0719 09:27:02.607744 140481689634688 learning.py:507] global step 1458: loss = 0.1120 (0.322 sec/step)\n",
            "I0719 09:27:02.956474 140481689634688 learning.py:507] global step 1459: loss = 0.1117 (0.347 sec/step)\n",
            "I0719 09:27:03.318340 140481689634688 learning.py:507] global step 1460: loss = 0.1272 (0.360 sec/step)\n",
            "I0719 09:27:03.654787 140481689634688 learning.py:507] global step 1461: loss = 0.2044 (0.335 sec/step)\n",
            "I0719 09:27:04.003401 140481689634688 learning.py:507] global step 1462: loss = 0.1732 (0.347 sec/step)\n",
            "I0719 09:27:04.340414 140481689634688 learning.py:507] global step 1463: loss = 0.1869 (0.335 sec/step)\n",
            "I0719 09:27:04.653161 140481689634688 learning.py:507] global step 1464: loss = 0.2126 (0.311 sec/step)\n",
            "I0719 09:27:05.004868 140481689634688 learning.py:507] global step 1465: loss = 0.2720 (0.350 sec/step)\n",
            "I0719 09:27:05.333865 140481689634688 learning.py:507] global step 1466: loss = 0.1807 (0.327 sec/step)\n",
            "I0719 09:27:05.658742 140481689634688 learning.py:507] global step 1467: loss = 0.1838 (0.323 sec/step)\n",
            "I0719 09:27:05.986954 140481689634688 learning.py:507] global step 1468: loss = 0.0705 (0.326 sec/step)\n",
            "I0719 09:27:06.278238 140481689634688 learning.py:507] global step 1469: loss = 0.0874 (0.290 sec/step)\n",
            "I0719 09:27:06.607933 140481689634688 learning.py:507] global step 1470: loss = 0.3003 (0.328 sec/step)\n",
            "I0719 09:27:06.933761 140481689634688 learning.py:507] global step 1471: loss = 0.2314 (0.324 sec/step)\n",
            "I0719 09:27:07.259489 140481689634688 learning.py:507] global step 1472: loss = 0.1543 (0.324 sec/step)\n",
            "I0719 09:27:07.583862 140481689634688 learning.py:507] global step 1473: loss = 0.2364 (0.323 sec/step)\n",
            "I0719 09:27:07.932908 140481689634688 learning.py:507] global step 1474: loss = 0.1503 (0.347 sec/step)\n",
            "I0719 09:27:08.275960 140481689634688 learning.py:507] global step 1475: loss = 0.1464 (0.341 sec/step)\n",
            "I0719 09:27:08.610325 140481689634688 learning.py:507] global step 1476: loss = 0.1356 (0.333 sec/step)\n",
            "I0719 09:27:08.905705 140481689634688 learning.py:507] global step 1477: loss = 0.1320 (0.293 sec/step)\n",
            "I0719 09:27:09.253153 140481689634688 learning.py:507] global step 1478: loss = 0.1792 (0.346 sec/step)\n",
            "I0719 09:27:09.594606 140481689634688 learning.py:507] global step 1479: loss = 0.1713 (0.339 sec/step)\n",
            "I0719 09:27:09.931503 140481689634688 learning.py:507] global step 1480: loss = 0.1563 (0.335 sec/step)\n",
            "I0719 09:27:10.217541 140481689634688 learning.py:507] global step 1481: loss = 0.2203 (0.284 sec/step)\n",
            "I0719 09:27:10.562446 140481689634688 learning.py:507] global step 1482: loss = 0.0854 (0.343 sec/step)\n",
            "I0719 09:27:10.905189 140481689634688 learning.py:507] global step 1483: loss = 0.1327 (0.341 sec/step)\n",
            "I0719 09:27:11.253239 140481689634688 learning.py:507] global step 1484: loss = 0.3382 (0.346 sec/step)\n",
            "I0719 09:27:11.565356 140481689634688 learning.py:507] global step 1485: loss = 0.1010 (0.310 sec/step)\n",
            "I0719 09:27:11.914248 140481689634688 learning.py:507] global step 1486: loss = 0.1550 (0.347 sec/step)\n",
            "I0719 09:27:12.275257 140481689634688 learning.py:507] global step 1487: loss = 0.0378 (0.359 sec/step)\n",
            "I0719 09:27:12.626448 140481689634688 learning.py:507] global step 1488: loss = 0.1359 (0.349 sec/step)\n",
            "I0719 09:27:12.977381 140481689634688 learning.py:507] global step 1489: loss = 0.2344 (0.349 sec/step)\n",
            "I0719 09:27:13.292198 140481689634688 learning.py:507] global step 1490: loss = 0.2432 (0.313 sec/step)\n",
            "I0719 09:27:13.658174 140481689634688 learning.py:507] global step 1491: loss = 0.2270 (0.364 sec/step)\n",
            "I0719 09:27:13.946617 140481689634688 learning.py:507] global step 1492: loss = 0.0939 (0.287 sec/step)\n",
            "I0719 09:27:14.258306 140481689634688 learning.py:507] global step 1493: loss = 0.1558 (0.309 sec/step)\n",
            "I0719 09:27:14.593034 140481689634688 learning.py:507] global step 1494: loss = 0.3190 (0.333 sec/step)\n",
            "I0719 09:27:14.898125 140481689634688 learning.py:507] global step 1495: loss = 0.0685 (0.301 sec/step)\n",
            "I0719 09:27:15.249433 140481689634688 learning.py:507] global step 1496: loss = 0.2432 (0.349 sec/step)\n",
            "I0719 09:27:15.595124 140481689634688 learning.py:507] global step 1497: loss = 0.1798 (0.343 sec/step)\n",
            "I0719 09:27:15.921364 140481689634688 learning.py:507] global step 1498: loss = 0.1652 (0.324 sec/step)\n",
            "I0719 09:27:16.235624 140481689634688 learning.py:507] global step 1499: loss = 0.1979 (0.312 sec/step)\n",
            "I0719 09:27:16.573694 140481689634688 learning.py:507] global step 1500: loss = 0.1534 (0.336 sec/step)\n",
            "I0719 09:27:16.901477 140481689634688 learning.py:507] global step 1501: loss = 0.2548 (0.326 sec/step)\n",
            "I0719 09:27:17.228041 140481689634688 learning.py:507] global step 1502: loss = 0.1170 (0.325 sec/step)\n",
            "I0719 09:27:17.561932 140481689634688 learning.py:507] global step 1503: loss = 0.2365 (0.332 sec/step)\n",
            "I0719 09:27:17.875106 140481689634688 learning.py:507] global step 1504: loss = 0.1331 (0.311 sec/step)\n",
            "I0719 09:27:18.194127 140481689634688 learning.py:507] global step 1505: loss = 0.1788 (0.317 sec/step)\n",
            "I0719 09:27:18.504256 140481689634688 learning.py:507] global step 1506: loss = 0.2089 (0.308 sec/step)\n",
            "I0719 09:27:18.795956 140481689634688 learning.py:507] global step 1507: loss = 0.1029 (0.290 sec/step)\n",
            "I0719 09:27:19.126215 140481689634688 learning.py:507] global step 1508: loss = 0.1364 (0.328 sec/step)\n",
            "I0719 09:27:19.445107 140481689634688 learning.py:507] global step 1509: loss = 0.2226 (0.317 sec/step)\n",
            "I0719 09:27:19.784817 140481689634688 learning.py:507] global step 1510: loss = 0.1376 (0.338 sec/step)\n",
            "I0719 09:27:20.118243 140481689634688 learning.py:507] global step 1511: loss = 0.2200 (0.332 sec/step)\n",
            "I0719 09:27:20.471673 140481689634688 learning.py:507] global step 1512: loss = 0.0492 (0.352 sec/step)\n",
            "I0719 09:27:20.795528 140481689634688 learning.py:507] global step 1513: loss = 0.1848 (0.322 sec/step)\n",
            "I0719 09:27:21.159616 140481689634688 learning.py:507] global step 1514: loss = 0.1152 (0.362 sec/step)\n",
            "I0719 09:27:21.492541 140481689634688 learning.py:507] global step 1515: loss = 0.1931 (0.331 sec/step)\n",
            "I0719 09:27:21.817688 140481689634688 learning.py:507] global step 1516: loss = 0.1117 (0.323 sec/step)\n",
            "I0719 09:27:22.144015 140481689634688 learning.py:507] global step 1517: loss = 0.1355 (0.324 sec/step)\n",
            "I0719 09:27:22.473152 140481689634688 learning.py:507] global step 1518: loss = 0.1083 (0.327 sec/step)\n",
            "I0719 09:27:22.815747 140481689634688 learning.py:507] global step 1519: loss = 0.1775 (0.341 sec/step)\n",
            "I0719 09:27:23.136700 140481689634688 learning.py:507] global step 1520: loss = 0.2006 (0.319 sec/step)\n",
            "I0719 09:27:23.471944 140481689634688 learning.py:507] global step 1521: loss = 0.0986 (0.333 sec/step)\n",
            "I0719 09:27:23.819335 140481689634688 learning.py:507] global step 1522: loss = 0.2027 (0.345 sec/step)\n",
            "I0719 09:27:24.165007 140481689634688 learning.py:507] global step 1523: loss = 0.0985 (0.344 sec/step)\n",
            "I0719 09:27:24.500591 140481689634688 learning.py:507] global step 1524: loss = 0.0911 (0.334 sec/step)\n",
            "I0719 09:27:24.820616 140481689634688 learning.py:507] global step 1525: loss = 0.2124 (0.318 sec/step)\n",
            "I0719 09:27:25.147778 140481689634688 learning.py:507] global step 1526: loss = 0.1376 (0.325 sec/step)\n",
            "I0719 09:27:25.463862 140481689634688 learning.py:507] global step 1527: loss = 0.1177 (0.314 sec/step)\n",
            "I0719 09:27:25.817468 140481689634688 learning.py:507] global step 1528: loss = 0.1632 (0.351 sec/step)\n",
            "I0719 09:27:26.165729 140481689634688 learning.py:507] global step 1529: loss = 0.2347 (0.346 sec/step)\n",
            "I0719 09:27:26.499180 140481689634688 learning.py:507] global step 1530: loss = 0.1362 (0.332 sec/step)\n",
            "I0719 09:27:26.843482 140481689634688 learning.py:507] global step 1531: loss = 0.1511 (0.343 sec/step)\n",
            "I0719 09:27:27.154787 140481689634688 learning.py:507] global step 1532: loss = 0.0340 (0.310 sec/step)\n",
            "I0719 09:27:27.444402 140481689634688 learning.py:507] global step 1533: loss = 0.3303 (0.288 sec/step)\n",
            "I0719 09:27:27.745372 140481689634688 learning.py:507] global step 1534: loss = 0.1393 (0.299 sec/step)\n",
            "I0719 09:27:28.031130 140481689634688 learning.py:507] global step 1535: loss = 0.1282 (0.284 sec/step)\n",
            "I0719 09:27:28.328913 140481689634688 learning.py:507] global step 1536: loss = 0.1005 (0.296 sec/step)\n",
            "I0719 09:27:28.659209 140481689634688 learning.py:507] global step 1537: loss = 0.2457 (0.328 sec/step)\n",
            "I0719 09:27:29.018964 140481689634688 learning.py:507] global step 1538: loss = 0.1020 (0.358 sec/step)\n",
            "I0719 09:27:29.368101 140481689634688 learning.py:507] global step 1539: loss = 0.1575 (0.347 sec/step)\n",
            "I0719 09:27:29.698298 140481689634688 learning.py:507] global step 1540: loss = 0.3187 (0.328 sec/step)\n",
            "I0719 09:27:30.023469 140481689634688 learning.py:507] global step 1541: loss = 0.0853 (0.324 sec/step)\n",
            "I0719 09:27:30.365252 140481689634688 learning.py:507] global step 1542: loss = 0.2227 (0.340 sec/step)\n",
            "I0719 09:27:30.692718 140481689634688 learning.py:507] global step 1543: loss = 0.0953 (0.326 sec/step)\n",
            "I0719 09:27:31.032934 140481689634688 learning.py:507] global step 1544: loss = 0.0600 (0.338 sec/step)\n",
            "I0719 09:27:31.368980 140481689634688 learning.py:507] global step 1545: loss = 0.0782 (0.334 sec/step)\n",
            "I0719 09:27:31.689823 140481689634688 learning.py:507] global step 1546: loss = 0.1653 (0.319 sec/step)\n",
            "I0719 09:27:32.068326 140481689634688 learning.py:507] global step 1547: loss = 0.1453 (0.377 sec/step)\n",
            "I0719 09:27:32.435639 140481689634688 learning.py:507] global step 1548: loss = 0.3310 (0.365 sec/step)\n",
            "I0719 09:27:32.749083 140481689634688 learning.py:507] global step 1549: loss = 0.1098 (0.311 sec/step)\n",
            "I0719 09:27:33.071780 140481689634688 learning.py:507] global step 1550: loss = 0.0355 (0.321 sec/step)\n",
            "I0719 09:27:33.396098 140481689634688 learning.py:507] global step 1551: loss = 0.0917 (0.323 sec/step)\n",
            "I0719 09:27:33.752151 140481689634688 learning.py:507] global step 1552: loss = 0.1936 (0.354 sec/step)\n",
            "I0719 09:27:34.109982 140481689634688 learning.py:507] global step 1553: loss = 0.1719 (0.356 sec/step)\n",
            "I0719 09:27:34.460227 140481689634688 learning.py:507] global step 1554: loss = 0.2100 (0.349 sec/step)\n",
            "I0719 09:27:34.838409 140481689634688 learning.py:507] global step 1555: loss = 0.0671 (0.376 sec/step)\n",
            "I0719 09:27:35.137147 140481689634688 learning.py:507] global step 1556: loss = 0.1648 (0.297 sec/step)\n",
            "I0719 09:27:35.484648 140481689634688 learning.py:507] global step 1557: loss = 0.1638 (0.346 sec/step)\n",
            "I0719 09:27:35.781857 140481689634688 learning.py:507] global step 1558: loss = 0.2322 (0.295 sec/step)\n",
            "I0719 09:27:36.125068 140481689634688 learning.py:507] global step 1559: loss = 0.1198 (0.342 sec/step)\n",
            "I0719 09:27:36.455754 140481689634688 learning.py:507] global step 1560: loss = 0.1557 (0.329 sec/step)\n",
            "I0719 09:27:36.821977 140481689634688 learning.py:507] global step 1561: loss = 0.2155 (0.364 sec/step)\n",
            "I0719 09:27:37.156207 140481689634688 learning.py:507] global step 1562: loss = 0.1150 (0.332 sec/step)\n",
            "I0719 09:27:37.520656 140481689634688 learning.py:507] global step 1563: loss = 0.1707 (0.363 sec/step)\n",
            "I0719 09:27:37.849230 140481689634688 learning.py:507] global step 1564: loss = 0.4614 (0.327 sec/step)\n",
            "I0719 09:27:38.166970 140481689634688 learning.py:507] global step 1565: loss = 0.2854 (0.316 sec/step)\n",
            "I0719 09:27:38.491701 140481689634688 learning.py:507] global step 1566: loss = 0.0814 (0.322 sec/step)\n",
            "I0719 09:27:38.838416 140481689634688 learning.py:507] global step 1567: loss = 0.2348 (0.345 sec/step)\n",
            "I0719 09:27:39.188148 140481689634688 learning.py:507] global step 1568: loss = 0.1982 (0.348 sec/step)\n",
            "I0719 09:27:39.546633 140481689634688 learning.py:507] global step 1569: loss = 0.1159 (0.357 sec/step)\n",
            "I0719 09:27:39.892176 140481689634688 learning.py:507] global step 1570: loss = 0.3358 (0.344 sec/step)\n",
            "I0719 09:27:40.201183 140481689634688 learning.py:507] global step 1571: loss = 0.3234 (0.307 sec/step)\n",
            "I0719 09:27:40.525786 140481689634688 learning.py:507] global step 1572: loss = 0.0720 (0.323 sec/step)\n",
            "I0719 09:27:40.861040 140481689634688 learning.py:507] global step 1573: loss = 0.1029 (0.334 sec/step)\n",
            "I0719 09:27:41.195654 140481689634688 learning.py:507] global step 1574: loss = 0.1264 (0.333 sec/step)\n",
            "I0719 09:27:41.512497 140481689634688 learning.py:507] global step 1575: loss = 0.0878 (0.315 sec/step)\n",
            "I0719 09:27:41.843608 140481689634688 learning.py:507] global step 1576: loss = 0.2495 (0.329 sec/step)\n",
            "I0719 09:27:42.199242 140481689634688 learning.py:507] global step 1577: loss = 0.1239 (0.354 sec/step)\n",
            "I0719 09:27:42.530050 140481689634688 learning.py:507] global step 1578: loss = 0.1902 (0.329 sec/step)\n",
            "I0719 09:27:42.860234 140481689634688 learning.py:507] global step 1579: loss = 0.1815 (0.329 sec/step)\n",
            "I0719 09:27:43.213819 140481689634688 learning.py:507] global step 1580: loss = 0.2387 (0.352 sec/step)\n",
            "I0719 09:27:43.562075 140481689634688 learning.py:507] global step 1581: loss = 0.1664 (0.346 sec/step)\n",
            "I0719 09:27:43.903608 140481689634688 learning.py:507] global step 1582: loss = 0.1980 (0.340 sec/step)\n",
            "I0719 09:27:44.244904 140481689634688 learning.py:507] global step 1583: loss = 0.0705 (0.339 sec/step)\n",
            "I0719 09:27:44.558408 140481689634688 learning.py:507] global step 1584: loss = 0.0824 (0.312 sec/step)\n",
            "I0719 09:27:44.896129 140481689634688 learning.py:507] global step 1585: loss = 0.1912 (0.336 sec/step)\n",
            "I0719 09:27:45.260447 140481689634688 learning.py:507] global step 1586: loss = 0.1896 (0.363 sec/step)\n",
            "I0719 09:27:45.604001 140481689634688 learning.py:507] global step 1587: loss = 0.3639 (0.342 sec/step)\n",
            "I0719 09:27:45.883599 140481689634688 learning.py:507] global step 1588: loss = 0.0653 (0.278 sec/step)\n",
            "I0719 09:27:46.225939 140481689634688 learning.py:507] global step 1589: loss = 0.1363 (0.341 sec/step)\n",
            "I0719 09:27:46.570602 140481689634688 learning.py:507] global step 1590: loss = 0.1827 (0.343 sec/step)\n",
            "I0719 09:27:46.898682 140481689634688 learning.py:507] global step 1591: loss = 0.0601 (0.326 sec/step)\n",
            "I0719 09:27:47.229713 140481689634688 learning.py:507] global step 1592: loss = 0.0716 (0.329 sec/step)\n",
            "I0719 09:27:47.558643 140481689634688 learning.py:507] global step 1593: loss = 0.1104 (0.327 sec/step)\n",
            "I0719 09:27:47.888720 140481689634688 learning.py:507] global step 1594: loss = 0.2017 (0.328 sec/step)\n",
            "I0719 09:27:48.215441 140481689634688 learning.py:507] global step 1595: loss = 0.1630 (0.325 sec/step)\n",
            "I0719 09:27:48.543366 140481689634688 learning.py:507] global step 1596: loss = 0.1579 (0.326 sec/step)\n",
            "I0719 09:27:48.889259 140481689634688 learning.py:507] global step 1597: loss = 0.1833 (0.344 sec/step)\n",
            "I0719 09:27:49.184216 140481689634688 learning.py:507] global step 1598: loss = 0.1577 (0.293 sec/step)\n",
            "I0719 09:27:49.530591 140481689634688 learning.py:507] global step 1599: loss = 0.0299 (0.344 sec/step)\n",
            "I0719 09:27:49.861536 140481689634688 learning.py:507] global step 1600: loss = 0.2140 (0.329 sec/step)\n",
            "I0719 09:27:50.192405 140481689634688 learning.py:507] global step 1601: loss = 0.1791 (0.329 sec/step)\n",
            "I0719 09:27:50.546255 140481689634688 learning.py:507] global step 1602: loss = 0.0959 (0.352 sec/step)\n",
            "I0719 09:27:50.891041 140481689634688 learning.py:507] global step 1603: loss = 0.3433 (0.343 sec/step)\n",
            "I0719 09:27:51.242795 140481689634688 learning.py:507] global step 1604: loss = 0.2287 (0.350 sec/step)\n",
            "I0719 09:27:51.592824 140481689634688 learning.py:507] global step 1605: loss = 0.1606 (0.348 sec/step)\n",
            "I0719 09:27:51.925802 140481689634688 learning.py:507] global step 1606: loss = 0.1342 (0.331 sec/step)\n",
            "I0719 09:27:52.282798 140481689634688 learning.py:507] global step 1607: loss = 0.1261 (0.355 sec/step)\n",
            "I0719 09:27:52.632668 140481689634688 learning.py:507] global step 1608: loss = 0.2655 (0.348 sec/step)\n",
            "I0719 09:27:52.958425 140481689634688 learning.py:507] global step 1609: loss = 0.1557 (0.324 sec/step)\n",
            "I0719 09:27:53.283005 140481689634688 learning.py:507] global step 1610: loss = 0.2523 (0.323 sec/step)\n",
            "I0719 09:27:53.607197 140481689634688 learning.py:507] global step 1611: loss = 0.1842 (0.322 sec/step)\n",
            "I0719 09:27:53.937171 140481689634688 learning.py:507] global step 1612: loss = 0.3373 (0.328 sec/step)\n",
            "I0719 09:27:54.267700 140481689634688 learning.py:507] global step 1613: loss = 0.3246 (0.329 sec/step)\n",
            "I0719 09:27:54.629710 140481689634688 learning.py:507] global step 1614: loss = 0.1480 (0.360 sec/step)\n",
            "I0719 09:27:54.975584 140481689634688 learning.py:507] global step 1615: loss = 0.2132 (0.344 sec/step)\n",
            "I0719 09:27:55.308636 140481689634688 learning.py:507] global step 1616: loss = 0.1628 (0.331 sec/step)\n",
            "I0719 09:27:55.642194 140481689634688 learning.py:507] global step 1617: loss = 0.1988 (0.332 sec/step)\n",
            "I0719 09:27:56.008253 140481689634688 learning.py:507] global step 1618: loss = 0.1343 (0.364 sec/step)\n",
            "I0719 09:27:56.358840 140481689634688 learning.py:507] global step 1619: loss = 0.1422 (0.348 sec/step)\n",
            "I0719 09:27:56.698368 140481689634688 learning.py:507] global step 1620: loss = 0.1728 (0.334 sec/step)\n",
            "I0719 09:27:57.016117 140481689634688 learning.py:507] global step 1621: loss = 0.1337 (0.316 sec/step)\n",
            "I0719 09:27:57.334450 140481689634688 learning.py:507] global step 1622: loss = 0.1163 (0.316 sec/step)\n",
            "I0719 09:27:57.626032 140481689634688 learning.py:507] global step 1623: loss = 0.3748 (0.290 sec/step)\n",
            "I0719 09:27:57.963363 140481689634688 learning.py:507] global step 1624: loss = 0.1557 (0.335 sec/step)\n",
            "I0719 09:27:58.293529 140481689634688 learning.py:507] global step 1625: loss = 0.1779 (0.328 sec/step)\n",
            "I0719 09:27:58.619157 140481689634688 learning.py:507] global step 1626: loss = 0.2391 (0.324 sec/step)\n",
            "I0719 09:27:58.966994 140481689634688 learning.py:507] global step 1627: loss = 0.1196 (0.346 sec/step)\n",
            "I0719 09:27:59.310185 140481689634688 learning.py:507] global step 1628: loss = 0.0903 (0.342 sec/step)\n",
            "I0719 09:27:59.664044 140481689634688 learning.py:507] global step 1629: loss = 0.2140 (0.352 sec/step)\n",
            "I0719 09:28:00.028388 140481689634688 learning.py:507] global step 1630: loss = 0.1705 (0.363 sec/step)\n",
            "I0719 09:28:00.366385 140481689634688 learning.py:507] global step 1631: loss = 0.0176 (0.336 sec/step)\n",
            "I0719 09:28:00.724189 140481689634688 learning.py:507] global step 1632: loss = 0.1286 (0.356 sec/step)\n",
            "I0719 09:28:01.061959 140481689634688 learning.py:507] global step 1633: loss = 0.1252 (0.336 sec/step)\n",
            "I0719 09:28:01.381028 140481689634688 learning.py:507] global step 1634: loss = 0.2814 (0.317 sec/step)\n",
            "I0719 09:28:01.691233 140481689634688 learning.py:507] global step 1635: loss = 0.2175 (0.308 sec/step)\n",
            "I0719 09:28:02.032801 140481689634688 learning.py:507] global step 1636: loss = 0.1717 (0.340 sec/step)\n",
            "I0719 09:28:02.385515 140481689634688 learning.py:507] global step 1637: loss = 0.2751 (0.351 sec/step)\n",
            "I0719 09:28:02.732325 140481689634688 learning.py:507] global step 1638: loss = 0.1956 (0.345 sec/step)\n",
            "I0719 09:28:03.097242 140481689634688 learning.py:507] global step 1639: loss = 0.1451 (0.363 sec/step)\n",
            "I0719 09:28:03.436634 140481689634688 learning.py:507] global step 1640: loss = 0.1275 (0.337 sec/step)\n",
            "I0719 09:28:03.789660 140481689634688 learning.py:507] global step 1641: loss = 0.1646 (0.351 sec/step)\n",
            "I0719 09:28:04.124484 140481689634688 learning.py:507] global step 1642: loss = 0.0860 (0.333 sec/step)\n",
            "I0719 09:28:04.451384 140481689634688 learning.py:507] global step 1643: loss = 0.0244 (0.325 sec/step)\n",
            "I0719 09:28:04.782505 140481689634688 learning.py:507] global step 1644: loss = 0.2379 (0.329 sec/step)\n",
            "I0719 09:28:05.172823 140481689634688 learning.py:507] global step 1645: loss = 0.1296 (0.389 sec/step)\n",
            "I0719 09:28:05.523259 140481689634688 learning.py:507] global step 1646: loss = 0.2516 (0.349 sec/step)\n",
            "I0719 09:28:05.850617 140481689634688 learning.py:507] global step 1647: loss = 0.1490 (0.326 sec/step)\n",
            "I0719 09:28:06.204370 140481689634688 learning.py:507] global step 1648: loss = 0.1354 (0.352 sec/step)\n",
            "I0719 09:28:06.533346 140481689634688 learning.py:507] global step 1649: loss = 0.1698 (0.327 sec/step)\n",
            "I0719 09:28:06.873757 140481689634688 learning.py:507] global step 1650: loss = 0.1402 (0.339 sec/step)\n",
            "I0719 09:28:07.250591 140481689634688 learning.py:507] global step 1651: loss = 0.1955 (0.375 sec/step)\n",
            "I0719 09:28:07.594752 140481689634688 learning.py:507] global step 1652: loss = 0.1003 (0.342 sec/step)\n",
            "I0719 09:28:07.939342 140481689634688 learning.py:507] global step 1653: loss = 0.1580 (0.342 sec/step)\n",
            "I0719 09:28:08.268627 140481689634688 learning.py:507] global step 1654: loss = 0.1895 (0.328 sec/step)\n",
            "I0719 09:28:08.620246 140481689634688 learning.py:507] global step 1655: loss = 0.1094 (0.350 sec/step)\n",
            "I0719 09:28:08.971530 140481689634688 learning.py:507] global step 1656: loss = 0.0773 (0.350 sec/step)\n",
            "I0719 09:28:09.293475 140481689634688 learning.py:507] global step 1657: loss = 0.2227 (0.320 sec/step)\n",
            "I0719 09:28:09.615804 140481689634688 learning.py:507] global step 1658: loss = 0.1618 (0.321 sec/step)\n",
            "I0719 09:28:09.960359 140481689634688 learning.py:507] global step 1659: loss = 0.2077 (0.343 sec/step)\n",
            "I0719 09:28:10.274554 140481689634688 learning.py:507] global step 1660: loss = 0.1501 (0.313 sec/step)\n",
            "I0719 09:28:10.616855 140481689634688 learning.py:507] global step 1661: loss = 0.0846 (0.340 sec/step)\n",
            "I0719 09:28:10.923103 140481689634688 learning.py:507] global step 1662: loss = 0.1110 (0.305 sec/step)\n",
            "I0719 09:28:11.273643 140481689634688 learning.py:507] global step 1663: loss = 0.1388 (0.349 sec/step)\n",
            "I0719 09:28:11.607681 140481689634688 learning.py:507] global step 1664: loss = 0.0781 (0.332 sec/step)\n",
            "I0719 09:28:11.951797 140481689634688 learning.py:507] global step 1665: loss = 0.2030 (0.342 sec/step)\n",
            "I0719 09:28:12.300885 140481689634688 learning.py:507] global step 1666: loss = 0.1456 (0.347 sec/step)\n",
            "I0719 09:28:12.632829 140481689634688 learning.py:507] global step 1667: loss = 0.1442 (0.330 sec/step)\n",
            "I0719 09:28:12.971102 140481689634688 learning.py:507] global step 1668: loss = 0.1612 (0.336 sec/step)\n",
            "I0719 09:28:13.295559 140481689634688 learning.py:507] global step 1669: loss = 0.0965 (0.323 sec/step)\n",
            "I0719 09:28:13.660080 140481689634688 learning.py:507] global step 1670: loss = 0.2089 (0.363 sec/step)\n",
            "I0719 09:28:14.006137 140481689634688 learning.py:507] global step 1671: loss = 0.1980 (0.344 sec/step)\n",
            "I0719 09:28:14.362779 140481689634688 learning.py:507] global step 1672: loss = 0.2428 (0.355 sec/step)\n",
            "I0719 09:28:14.664244 140481689634688 learning.py:507] global step 1673: loss = 0.2964 (0.300 sec/step)\n",
            "I0719 09:28:14.997871 140481689634688 learning.py:507] global step 1674: loss = 0.1421 (0.332 sec/step)\n",
            "I0719 09:28:15.301649 140481689634688 learning.py:507] global step 1675: loss = 0.3220 (0.302 sec/step)\n",
            "I0719 09:28:15.589628 140481689634688 learning.py:507] global step 1676: loss = 0.0677 (0.286 sec/step)\n",
            "I0719 09:28:15.935281 140481689634688 learning.py:507] global step 1677: loss = 0.0611 (0.344 sec/step)\n",
            "I0719 09:28:16.252007 140481689634688 learning.py:507] global step 1678: loss = 0.1843 (0.315 sec/step)\n",
            "I0719 09:28:16.588519 140481689634688 learning.py:507] global step 1679: loss = 0.0798 (0.335 sec/step)\n",
            "I0719 09:28:16.881206 140481689634688 learning.py:507] global step 1680: loss = 0.1277 (0.291 sec/step)\n",
            "I0719 09:28:17.177247 140481689634688 learning.py:507] global step 1681: loss = 0.1513 (0.294 sec/step)\n",
            "I0719 09:28:17.482664 140481689634688 learning.py:507] global step 1682: loss = 0.1768 (0.304 sec/step)\n",
            "I0719 09:28:17.806742 140481689634688 learning.py:507] global step 1683: loss = 0.1781 (0.322 sec/step)\n",
            "I0719 09:28:18.128494 140481689634688 learning.py:507] global step 1684: loss = 0.1382 (0.320 sec/step)\n",
            "I0719 09:28:18.492917 140481689634688 learning.py:507] global step 1685: loss = 0.1610 (0.363 sec/step)\n",
            "I0719 09:28:18.824125 140481689634688 learning.py:507] global step 1686: loss = 0.0281 (0.329 sec/step)\n",
            "I0719 09:28:19.174130 140481689634688 learning.py:507] global step 1687: loss = 0.3953 (0.348 sec/step)\n",
            "I0719 09:28:19.495196 140481689634688 learning.py:507] global step 1688: loss = 0.0327 (0.319 sec/step)\n",
            "I0719 09:28:19.845507 140481689634688 learning.py:507] global step 1689: loss = 0.1358 (0.348 sec/step)\n",
            "I0719 09:28:20.162472 140481689634688 learning.py:507] global step 1690: loss = 0.0713 (0.315 sec/step)\n",
            "I0719 09:28:20.507043 140481689634688 learning.py:507] global step 1691: loss = 0.0812 (0.343 sec/step)\n",
            "I0719 09:28:20.828023 140481689634688 learning.py:507] global step 1692: loss = 0.1028 (0.319 sec/step)\n",
            "I0719 09:28:21.180509 140481689634688 learning.py:507] global step 1693: loss = 0.1820 (0.351 sec/step)\n",
            "I0719 09:28:21.511289 140481689634688 learning.py:507] global step 1694: loss = 0.1219 (0.329 sec/step)\n",
            "I0719 09:28:21.848743 140481689634688 learning.py:507] global step 1695: loss = 0.1869 (0.336 sec/step)\n",
            "I0719 09:28:22.198391 140481689634688 learning.py:507] global step 1696: loss = 0.0470 (0.348 sec/step)\n",
            "I0719 09:28:22.546543 140481689634688 learning.py:507] global step 1697: loss = 0.2457 (0.346 sec/step)\n",
            "I0719 09:28:22.882668 140481689634688 learning.py:507] global step 1698: loss = 0.2013 (0.334 sec/step)\n",
            "I0719 09:28:23.220229 140481689634688 learning.py:507] global step 1699: loss = 0.1402 (0.336 sec/step)\n",
            "I0719 09:28:23.513914 140481689634688 learning.py:507] global step 1700: loss = 0.0709 (0.292 sec/step)\n",
            "I0719 09:28:23.829381 140481689634688 learning.py:507] global step 1701: loss = 0.0357 (0.314 sec/step)\n",
            "I0719 09:28:24.165112 140481689634688 learning.py:507] global step 1702: loss = 0.1745 (0.333 sec/step)\n",
            "I0719 09:28:24.502872 140481689634688 learning.py:507] global step 1703: loss = 0.2196 (0.336 sec/step)\n",
            "I0719 09:28:24.840708 140481689634688 learning.py:507] global step 1704: loss = 0.0983 (0.336 sec/step)\n",
            "I0719 09:28:25.142793 140481689634688 learning.py:507] global step 1705: loss = 0.0709 (0.300 sec/step)\n",
            "I0719 09:28:25.477823 140481689634688 learning.py:507] global step 1706: loss = 0.1320 (0.333 sec/step)\n",
            "I0719 09:28:25.817657 140481689634688 learning.py:507] global step 1707: loss = 0.1992 (0.338 sec/step)\n",
            "I0719 09:28:26.336747 140481689634688 learning.py:507] global step 1708: loss = 0.1208 (0.494 sec/step)\n",
            "I0719 09:28:26.854834 140481689634688 learning.py:507] global step 1709: loss = 0.1724 (0.509 sec/step)\n",
            "I0719 09:28:27.107424 140479018202880 supervisor.py:1050] Recording summary at step 1709.\n",
            "I0719 09:28:27.324749 140481689634688 learning.py:507] global step 1710: loss = 0.2220 (0.468 sec/step)\n",
            "I0719 09:28:27.662141 140479026595584 supervisor.py:1099] global_step/sec: 2.96364\n",
            "I0719 09:28:27.704445 140481689634688 learning.py:507] global step 1711: loss = 0.1644 (0.378 sec/step)\n",
            "I0719 09:28:28.060446 140481689634688 learning.py:507] global step 1712: loss = 0.0668 (0.354 sec/step)\n",
            "I0719 09:28:28.405971 140481689634688 learning.py:507] global step 1713: loss = 0.2781 (0.344 sec/step)\n",
            "I0719 09:28:28.732418 140481689634688 learning.py:507] global step 1714: loss = 0.1014 (0.325 sec/step)\n",
            "I0719 09:28:29.086945 140481689634688 learning.py:507] global step 1715: loss = 0.0697 (0.353 sec/step)\n",
            "I0719 09:28:29.454343 140481689634688 learning.py:507] global step 1716: loss = 0.2505 (0.366 sec/step)\n",
            "I0719 09:28:29.776604 140481689634688 learning.py:507] global step 1717: loss = 0.0332 (0.320 sec/step)\n",
            "I0719 09:28:30.125010 140481689634688 learning.py:507] global step 1718: loss = 0.1063 (0.347 sec/step)\n",
            "I0719 09:28:30.482946 140481689634688 learning.py:507] global step 1719: loss = 0.1343 (0.356 sec/step)\n",
            "I0719 09:28:30.822582 140481689634688 learning.py:507] global step 1720: loss = 0.0956 (0.337 sec/step)\n",
            "I0719 09:28:31.130080 140481689634688 learning.py:507] global step 1721: loss = 0.2518 (0.305 sec/step)\n",
            "I0719 09:28:31.455063 140481689634688 learning.py:507] global step 1722: loss = 0.1520 (0.323 sec/step)\n",
            "I0719 09:28:31.742052 140481689634688 learning.py:507] global step 1723: loss = 0.2706 (0.285 sec/step)\n",
            "I0719 09:28:32.037100 140481689634688 learning.py:507] global step 1724: loss = 0.1147 (0.293 sec/step)\n",
            "I0719 09:28:32.357441 140481689634688 learning.py:507] global step 1725: loss = 0.0642 (0.318 sec/step)\n",
            "I0719 09:28:32.695258 140481689634688 learning.py:507] global step 1726: loss = 0.1535 (0.336 sec/step)\n",
            "I0719 09:28:33.036938 140481689634688 learning.py:507] global step 1727: loss = 0.1268 (0.340 sec/step)\n",
            "I0719 09:28:33.362401 140481689634688 learning.py:507] global step 1728: loss = 0.1619 (0.324 sec/step)\n",
            "I0719 09:28:33.698586 140481689634688 learning.py:507] global step 1729: loss = 0.3169 (0.334 sec/step)\n",
            "I0719 09:28:34.028787 140481689634688 learning.py:507] global step 1730: loss = 0.1137 (0.328 sec/step)\n",
            "I0719 09:28:34.352545 140481689634688 learning.py:507] global step 1731: loss = 0.1250 (0.322 sec/step)\n",
            "I0719 09:28:34.693093 140481689634688 learning.py:507] global step 1732: loss = 0.1297 (0.339 sec/step)\n",
            "I0719 09:28:34.982848 140481689634688 learning.py:507] global step 1733: loss = 0.2451 (0.288 sec/step)\n",
            "I0719 09:28:35.317076 140481689634688 learning.py:507] global step 1734: loss = 0.0270 (0.333 sec/step)\n",
            "I0719 09:28:35.669850 140481689634688 learning.py:507] global step 1735: loss = 0.1571 (0.350 sec/step)\n",
            "I0719 09:28:36.005403 140481689634688 learning.py:507] global step 1736: loss = 0.1345 (0.334 sec/step)\n",
            "I0719 09:28:36.351489 140481689634688 learning.py:507] global step 1737: loss = 0.2086 (0.344 sec/step)\n",
            "I0719 09:28:36.697350 140481689634688 learning.py:507] global step 1738: loss = 0.2022 (0.344 sec/step)\n",
            "I0719 09:28:36.991145 140481689634688 learning.py:507] global step 1739: loss = 0.2706 (0.291 sec/step)\n",
            "I0719 09:28:37.328643 140481689634688 learning.py:507] global step 1740: loss = 0.2228 (0.336 sec/step)\n",
            "I0719 09:28:37.677392 140481689634688 learning.py:507] global step 1741: loss = 0.0408 (0.347 sec/step)\n",
            "I0719 09:28:37.987609 140481689634688 learning.py:507] global step 1742: loss = 0.1580 (0.308 sec/step)\n",
            "I0719 09:28:38.299779 140481689634688 learning.py:507] global step 1743: loss = 0.1080 (0.310 sec/step)\n",
            "I0719 09:28:38.627232 140481689634688 learning.py:507] global step 1744: loss = 0.0804 (0.326 sec/step)\n",
            "I0719 09:28:38.933957 140481689634688 learning.py:507] global step 1745: loss = 0.1383 (0.305 sec/step)\n",
            "I0719 09:28:39.261940 140481689634688 learning.py:507] global step 1746: loss = 0.0366 (0.326 sec/step)\n",
            "I0719 09:28:39.585014 140481689634688 learning.py:507] global step 1747: loss = 0.0279 (0.321 sec/step)\n",
            "I0719 09:28:39.888221 140481689634688 learning.py:507] global step 1748: loss = 0.1925 (0.301 sec/step)\n",
            "I0719 09:28:40.164329 140481689634688 learning.py:507] global step 1749: loss = 0.0736 (0.274 sec/step)\n",
            "I0719 09:28:40.534584 140481689634688 learning.py:507] global step 1750: loss = 0.1323 (0.368 sec/step)\n",
            "I0719 09:28:40.856910 140481689634688 learning.py:507] global step 1751: loss = 0.1015 (0.321 sec/step)\n",
            "I0719 09:28:41.187875 140481689634688 learning.py:507] global step 1752: loss = 0.0587 (0.329 sec/step)\n",
            "I0719 09:28:41.515962 140481689634688 learning.py:507] global step 1753: loss = 0.1736 (0.326 sec/step)\n",
            "I0719 09:28:41.858424 140481689634688 learning.py:507] global step 1754: loss = 0.2103 (0.341 sec/step)\n",
            "I0719 09:28:42.156519 140481689634688 learning.py:507] global step 1755: loss = 0.1678 (0.296 sec/step)\n",
            "I0719 09:28:42.508700 140481689634688 learning.py:507] global step 1756: loss = 0.1138 (0.350 sec/step)\n",
            "I0719 09:28:42.840849 140481689634688 learning.py:507] global step 1757: loss = 0.1956 (0.329 sec/step)\n",
            "I0719 09:28:43.167229 140481689634688 learning.py:507] global step 1758: loss = 0.2055 (0.325 sec/step)\n",
            "I0719 09:28:43.521651 140481689634688 learning.py:507] global step 1759: loss = 0.1049 (0.353 sec/step)\n",
            "I0719 09:28:43.869674 140481689634688 learning.py:507] global step 1760: loss = 0.0660 (0.346 sec/step)\n",
            "I0719 09:28:44.209123 140481689634688 learning.py:507] global step 1761: loss = 0.0860 (0.338 sec/step)\n",
            "I0719 09:28:44.539469 140481689634688 learning.py:507] global step 1762: loss = 0.1330 (0.329 sec/step)\n",
            "I0719 09:28:44.886135 140481689634688 learning.py:507] global step 1763: loss = 0.0533 (0.345 sec/step)\n",
            "I0719 09:28:45.255672 140481689634688 learning.py:507] global step 1764: loss = 0.1403 (0.368 sec/step)\n",
            "I0719 09:28:45.607387 140481689634688 learning.py:507] global step 1765: loss = 0.2946 (0.350 sec/step)\n",
            "I0719 09:28:45.934338 140481689634688 learning.py:507] global step 1766: loss = 0.2497 (0.325 sec/step)\n",
            "I0719 09:28:46.282997 140481689634688 learning.py:507] global step 1767: loss = 0.0739 (0.347 sec/step)\n",
            "I0719 09:28:46.616195 140481689634688 learning.py:507] global step 1768: loss = 0.2637 (0.332 sec/step)\n",
            "I0719 09:28:46.974661 140481689634688 learning.py:507] global step 1769: loss = 0.1128 (0.357 sec/step)\n",
            "I0719 09:28:47.325334 140481689634688 learning.py:507] global step 1770: loss = 0.2185 (0.349 sec/step)\n",
            "I0719 09:28:47.660361 140481689634688 learning.py:507] global step 1771: loss = 0.1146 (0.333 sec/step)\n",
            "I0719 09:28:47.979202 140481689634688 learning.py:507] global step 1772: loss = 0.0879 (0.317 sec/step)\n",
            "I0719 09:28:48.294630 140481689634688 learning.py:507] global step 1773: loss = 0.0638 (0.314 sec/step)\n",
            "I0719 09:28:48.625764 140481689634688 learning.py:507] global step 1774: loss = 0.1526 (0.329 sec/step)\n",
            "I0719 09:28:48.941419 140481689634688 learning.py:507] global step 1775: loss = 0.0437 (0.314 sec/step)\n",
            "I0719 09:28:49.289616 140481689634688 learning.py:507] global step 1776: loss = 0.0876 (0.346 sec/step)\n",
            "I0719 09:28:49.607610 140481689634688 learning.py:507] global step 1777: loss = 0.0704 (0.316 sec/step)\n",
            "I0719 09:28:49.922493 140481689634688 learning.py:507] global step 1778: loss = 0.0889 (0.313 sec/step)\n",
            "I0719 09:28:50.281219 140481689634688 learning.py:507] global step 1779: loss = 0.1978 (0.357 sec/step)\n",
            "I0719 09:28:50.692029 140481689634688 learning.py:507] global step 1780: loss = 0.1750 (0.409 sec/step)\n",
            "I0719 09:28:51.031469 140481689634688 learning.py:507] global step 1781: loss = 0.0839 (0.338 sec/step)\n",
            "I0719 09:28:51.345590 140481689634688 learning.py:507] global step 1782: loss = 0.2243 (0.312 sec/step)\n",
            "I0719 09:28:51.690979 140481689634688 learning.py:507] global step 1783: loss = 0.0927 (0.344 sec/step)\n",
            "I0719 09:28:52.032829 140481689634688 learning.py:507] global step 1784: loss = 0.1783 (0.340 sec/step)\n",
            "I0719 09:28:52.354315 140481689634688 learning.py:507] global step 1785: loss = 0.1200 (0.320 sec/step)\n",
            "I0719 09:28:52.699996 140481689634688 learning.py:507] global step 1786: loss = 0.1027 (0.344 sec/step)\n",
            "I0719 09:28:53.012512 140481689634688 learning.py:507] global step 1787: loss = 0.1111 (0.311 sec/step)\n",
            "I0719 09:28:53.331364 140481689634688 learning.py:507] global step 1788: loss = 0.1371 (0.317 sec/step)\n",
            "I0719 09:28:53.679247 140481689634688 learning.py:507] global step 1789: loss = 0.1447 (0.346 sec/step)\n",
            "I0719 09:28:54.022814 140481689634688 learning.py:507] global step 1790: loss = 0.2047 (0.342 sec/step)\n",
            "I0719 09:28:54.337820 140481689634688 learning.py:507] global step 1791: loss = 0.2458 (0.313 sec/step)\n",
            "I0719 09:28:54.689558 140481689634688 learning.py:507] global step 1792: loss = 0.0555 (0.350 sec/step)\n",
            "I0719 09:28:55.049555 140481689634688 learning.py:507] global step 1793: loss = 0.2031 (0.358 sec/step)\n",
            "I0719 09:28:55.376543 140481689634688 learning.py:507] global step 1794: loss = 0.1504 (0.325 sec/step)\n",
            "I0719 09:28:55.687658 140481689634688 learning.py:507] global step 1795: loss = 0.0737 (0.309 sec/step)\n",
            "I0719 09:28:56.029125 140481689634688 learning.py:507] global step 1796: loss = 0.3808 (0.339 sec/step)\n",
            "I0719 09:28:56.371429 140481689634688 learning.py:507] global step 1797: loss = 0.1215 (0.340 sec/step)\n",
            "I0719 09:28:56.699737 140481689634688 learning.py:507] global step 1798: loss = 0.0205 (0.326 sec/step)\n",
            "I0719 09:28:57.050659 140481689634688 learning.py:507] global step 1799: loss = 0.1532 (0.347 sec/step)\n",
            "I0719 09:28:57.392213 140481689634688 learning.py:507] global step 1800: loss = 0.0933 (0.340 sec/step)\n",
            "I0719 09:28:57.724123 140481689634688 learning.py:507] global step 1801: loss = 0.0706 (0.330 sec/step)\n",
            "I0719 09:28:58.061533 140481689634688 learning.py:507] global step 1802: loss = 0.2452 (0.336 sec/step)\n",
            "I0719 09:28:58.388482 140481689634688 learning.py:507] global step 1803: loss = 0.1836 (0.325 sec/step)\n",
            "I0719 09:28:58.730099 140481689634688 learning.py:507] global step 1804: loss = 0.1933 (0.340 sec/step)\n",
            "I0719 09:28:59.081247 140481689634688 learning.py:507] global step 1805: loss = 0.1159 (0.349 sec/step)\n",
            "I0719 09:28:59.422691 140481689634688 learning.py:507] global step 1806: loss = 0.2079 (0.340 sec/step)\n",
            "I0719 09:28:59.740606 140481689634688 learning.py:507] global step 1807: loss = 0.0482 (0.316 sec/step)\n",
            "I0719 09:29:00.047970 140481689634688 learning.py:507] global step 1808: loss = 0.0692 (0.305 sec/step)\n",
            "I0719 09:29:00.386328 140481689634688 learning.py:507] global step 1809: loss = 0.1393 (0.336 sec/step)\n",
            "I0719 09:29:00.726356 140481689634688 learning.py:507] global step 1810: loss = 0.1617 (0.338 sec/step)\n",
            "I0719 09:29:01.024388 140481689634688 learning.py:507] global step 1811: loss = 0.0858 (0.296 sec/step)\n",
            "I0719 09:29:01.355191 140481689634688 learning.py:507] global step 1812: loss = 0.1621 (0.329 sec/step)\n",
            "I0719 09:29:01.643473 140481689634688 learning.py:507] global step 1813: loss = 0.1803 (0.286 sec/step)\n",
            "I0719 09:29:01.996343 140481689634688 learning.py:507] global step 1814: loss = 0.1021 (0.350 sec/step)\n",
            "I0719 09:29:02.341506 140481689634688 learning.py:507] global step 1815: loss = 0.0670 (0.343 sec/step)\n",
            "I0719 09:29:02.696404 140481689634688 learning.py:507] global step 1816: loss = 0.2118 (0.353 sec/step)\n",
            "I0719 09:29:02.979656 140481689634688 learning.py:507] global step 1817: loss = 0.1550 (0.280 sec/step)\n",
            "I0719 09:29:03.310130 140481689634688 learning.py:507] global step 1818: loss = 0.1372 (0.328 sec/step)\n",
            "I0719 09:29:03.650419 140481689634688 learning.py:507] global step 1819: loss = 0.1377 (0.338 sec/step)\n",
            "I0719 09:29:03.985012 140481689634688 learning.py:507] global step 1820: loss = 0.0349 (0.333 sec/step)\n",
            "I0719 09:29:04.330917 140481689634688 learning.py:507] global step 1821: loss = 0.2336 (0.344 sec/step)\n",
            "I0719 09:29:04.669442 140481689634688 learning.py:507] global step 1822: loss = 0.0309 (0.337 sec/step)\n",
            "I0719 09:29:04.998143 140481689634688 learning.py:507] global step 1823: loss = 0.1151 (0.327 sec/step)\n",
            "I0719 09:29:05.347485 140481689634688 learning.py:507] global step 1824: loss = 0.2203 (0.347 sec/step)\n",
            "I0719 09:29:05.684482 140481689634688 learning.py:507] global step 1825: loss = 0.1680 (0.335 sec/step)\n",
            "I0719 09:29:06.023534 140481689634688 learning.py:507] global step 1826: loss = 0.2286 (0.337 sec/step)\n",
            "I0719 09:29:06.382811 140481689634688 learning.py:507] global step 1827: loss = 0.0807 (0.357 sec/step)\n",
            "I0719 09:29:06.715718 140481689634688 learning.py:507] global step 1828: loss = 0.2231 (0.331 sec/step)\n",
            "I0719 09:29:07.031876 140481689634688 learning.py:507] global step 1829: loss = 0.0785 (0.314 sec/step)\n",
            "I0719 09:29:07.366346 140481689634688 learning.py:507] global step 1830: loss = 0.1305 (0.333 sec/step)\n",
            "I0719 09:29:07.712318 140481689634688 learning.py:507] global step 1831: loss = 0.1691 (0.344 sec/step)\n",
            "I0719 09:29:08.046429 140481689634688 learning.py:507] global step 1832: loss = 0.1329 (0.333 sec/step)\n",
            "I0719 09:29:08.419655 140481689634688 learning.py:507] global step 1833: loss = 0.0551 (0.371 sec/step)\n",
            "I0719 09:29:08.727118 140481689634688 learning.py:507] global step 1834: loss = 0.1273 (0.306 sec/step)\n",
            "I0719 09:29:09.087982 140481689634688 learning.py:507] global step 1835: loss = 0.1880 (0.359 sec/step)\n",
            "I0719 09:29:09.432182 140481689634688 learning.py:507] global step 1836: loss = 0.1218 (0.343 sec/step)\n",
            "I0719 09:29:09.763971 140481689634688 learning.py:507] global step 1837: loss = 0.1462 (0.330 sec/step)\n",
            "I0719 09:29:10.050146 140481689634688 learning.py:507] global step 1838: loss = 0.1796 (0.284 sec/step)\n",
            "I0719 09:29:10.346431 140481689634688 learning.py:507] global step 1839: loss = 0.1260 (0.291 sec/step)\n",
            "I0719 09:29:10.660698 140481689634688 learning.py:507] global step 1840: loss = 0.0242 (0.313 sec/step)\n",
            "I0719 09:29:10.975893 140481689634688 learning.py:507] global step 1841: loss = 0.0671 (0.313 sec/step)\n",
            "I0719 09:29:11.310621 140481689634688 learning.py:507] global step 1842: loss = 0.1831 (0.333 sec/step)\n",
            "I0719 09:29:11.647496 140481689634688 learning.py:507] global step 1843: loss = 0.1593 (0.335 sec/step)\n",
            "I0719 09:29:11.998337 140481689634688 learning.py:507] global step 1844: loss = 0.1793 (0.349 sec/step)\n",
            "I0719 09:29:12.325728 140481689634688 learning.py:507] global step 1845: loss = 0.2550 (0.326 sec/step)\n",
            "I0719 09:29:12.672314 140481689634688 learning.py:507] global step 1846: loss = 0.0883 (0.345 sec/step)\n",
            "I0719 09:29:13.005946 140481689634688 learning.py:507] global step 1847: loss = 0.1619 (0.331 sec/step)\n",
            "I0719 09:29:13.340145 140481689634688 learning.py:507] global step 1848: loss = 0.0462 (0.332 sec/step)\n",
            "I0719 09:29:13.700897 140481689634688 learning.py:507] global step 1849: loss = 0.0491 (0.359 sec/step)\n",
            "I0719 09:29:13.972188 140481689634688 learning.py:507] global step 1850: loss = 0.2341 (0.270 sec/step)\n",
            "I0719 09:29:14.322178 140481689634688 learning.py:507] global step 1851: loss = 0.1197 (0.348 sec/step)\n",
            "I0719 09:29:14.660370 140481689634688 learning.py:507] global step 1852: loss = 0.1139 (0.334 sec/step)\n",
            "I0719 09:29:15.009250 140481689634688 learning.py:507] global step 1853: loss = 0.0705 (0.347 sec/step)\n",
            "I0719 09:29:15.348636 140481689634688 learning.py:507] global step 1854: loss = 0.0545 (0.337 sec/step)\n",
            "I0719 09:29:15.683373 140481689634688 learning.py:507] global step 1855: loss = 0.0473 (0.332 sec/step)\n",
            "I0719 09:29:16.033654 140481689634688 learning.py:507] global step 1856: loss = 0.1192 (0.347 sec/step)\n",
            "I0719 09:29:16.386595 140481689634688 learning.py:507] global step 1857: loss = 0.1347 (0.351 sec/step)\n",
            "I0719 09:29:16.710843 140481689634688 learning.py:507] global step 1858: loss = 0.1319 (0.323 sec/step)\n",
            "I0719 09:29:17.044432 140481689634688 learning.py:507] global step 1859: loss = 0.2431 (0.332 sec/step)\n",
            "I0719 09:29:17.381903 140481689634688 learning.py:507] global step 1860: loss = 0.0447 (0.336 sec/step)\n",
            "I0719 09:29:17.710620 140481689634688 learning.py:507] global step 1861: loss = 0.1404 (0.327 sec/step)\n",
            "I0719 09:29:18.025353 140481689634688 learning.py:507] global step 1862: loss = 0.1934 (0.311 sec/step)\n",
            "I0719 09:29:18.367814 140481689634688 learning.py:507] global step 1863: loss = 0.1461 (0.341 sec/step)\n",
            "I0719 09:29:18.722666 140481689634688 learning.py:507] global step 1864: loss = 0.0702 (0.353 sec/step)\n",
            "I0719 09:29:19.054626 140481689634688 learning.py:507] global step 1865: loss = 0.1853 (0.330 sec/step)\n",
            "I0719 09:29:19.393116 140481689634688 learning.py:507] global step 1866: loss = 0.2611 (0.337 sec/step)\n",
            "I0719 09:29:19.726227 140481689634688 learning.py:507] global step 1867: loss = 0.0920 (0.331 sec/step)\n",
            "I0719 09:29:20.096182 140481689634688 learning.py:507] global step 1868: loss = 0.1987 (0.368 sec/step)\n",
            "I0719 09:29:20.462660 140481689634688 learning.py:507] global step 1869: loss = 0.1892 (0.365 sec/step)\n",
            "I0719 09:29:20.797958 140481689634688 learning.py:507] global step 1870: loss = 0.1367 (0.333 sec/step)\n",
            "I0719 09:29:21.113112 140481689634688 learning.py:507] global step 1871: loss = 0.1392 (0.313 sec/step)\n",
            "I0719 09:29:21.463332 140481689634688 learning.py:507] global step 1872: loss = 0.1714 (0.348 sec/step)\n",
            "I0719 09:29:21.828101 140481689634688 learning.py:507] global step 1873: loss = 0.1056 (0.363 sec/step)\n",
            "I0719 09:29:22.184048 140481689634688 learning.py:507] global step 1874: loss = 0.1767 (0.354 sec/step)\n",
            "I0719 09:29:22.543609 140481689634688 learning.py:507] global step 1875: loss = 0.1672 (0.358 sec/step)\n",
            "I0719 09:29:22.871297 140481689634688 learning.py:507] global step 1876: loss = 0.0356 (0.324 sec/step)\n",
            "I0719 09:29:23.206704 140481689634688 learning.py:507] global step 1877: loss = 0.0843 (0.332 sec/step)\n",
            "I0719 09:29:23.517634 140481689634688 learning.py:507] global step 1878: loss = 0.0946 (0.309 sec/step)\n",
            "I0719 09:29:23.857182 140481689634688 learning.py:507] global step 1879: loss = 0.1282 (0.338 sec/step)\n",
            "I0719 09:29:24.199254 140481689634688 learning.py:507] global step 1880: loss = 0.1538 (0.340 sec/step)\n",
            "I0719 09:29:24.519638 140481689634688 learning.py:507] global step 1881: loss = 0.1019 (0.319 sec/step)\n",
            "I0719 09:29:24.859591 140481689634688 learning.py:507] global step 1882: loss = 0.0220 (0.338 sec/step)\n",
            "I0719 09:29:25.198127 140481689634688 learning.py:507] global step 1883: loss = 0.1549 (0.337 sec/step)\n",
            "I0719 09:29:25.525798 140481689634688 learning.py:507] global step 1884: loss = 0.1205 (0.326 sec/step)\n",
            "I0719 09:29:25.844460 140481689634688 learning.py:507] global step 1885: loss = 0.1216 (0.317 sec/step)\n",
            "I0719 09:29:26.192008 140481689634688 learning.py:507] global step 1886: loss = 0.0202 (0.346 sec/step)\n",
            "I0719 09:29:26.544407 140481689634688 learning.py:507] global step 1887: loss = 0.0973 (0.351 sec/step)\n",
            "I0719 09:29:26.924825 140481689634688 learning.py:507] global step 1888: loss = 0.0871 (0.379 sec/step)\n",
            "I0719 09:29:27.316576 140481689634688 learning.py:507] global step 1889: loss = 0.1738 (0.390 sec/step)\n",
            "I0719 09:29:27.604296 140481689634688 learning.py:507] global step 1890: loss = 0.0853 (0.286 sec/step)\n",
            "I0719 09:29:27.927981 140481689634688 learning.py:507] global step 1891: loss = 0.3498 (0.322 sec/step)\n",
            "I0719 09:29:28.275502 140481689634688 learning.py:507] global step 1892: loss = 0.1702 (0.346 sec/step)\n",
            "I0719 09:29:28.603696 140481689634688 learning.py:507] global step 1893: loss = 0.2552 (0.326 sec/step)\n",
            "I0719 09:29:28.936511 140481689634688 learning.py:507] global step 1894: loss = 0.1247 (0.331 sec/step)\n",
            "I0719 09:29:29.279932 140481689634688 learning.py:507] global step 1895: loss = 0.1581 (0.342 sec/step)\n",
            "I0719 09:29:29.633939 140481689634688 learning.py:507] global step 1896: loss = 0.0429 (0.352 sec/step)\n",
            "I0719 09:29:29.910238 140481689634688 learning.py:507] global step 1897: loss = 0.1050 (0.274 sec/step)\n",
            "I0719 09:29:30.266286 140481689634688 learning.py:507] global step 1898: loss = 0.0845 (0.354 sec/step)\n",
            "I0719 09:29:30.595020 140481689634688 learning.py:507] global step 1899: loss = 0.1580 (0.327 sec/step)\n",
            "I0719 09:29:30.934592 140481689634688 learning.py:507] global step 1900: loss = 0.2517 (0.338 sec/step)\n",
            "I0719 09:29:31.262896 140481689634688 learning.py:507] global step 1901: loss = 0.1653 (0.326 sec/step)\n",
            "I0719 09:29:31.604958 140481689634688 learning.py:507] global step 1902: loss = 0.1635 (0.340 sec/step)\n",
            "I0719 09:29:31.919957 140481689634688 learning.py:507] global step 1903: loss = 0.0993 (0.313 sec/step)\n",
            "I0719 09:29:32.216810 140481689634688 learning.py:507] global step 1904: loss = 0.1722 (0.295 sec/step)\n",
            "I0719 09:29:32.549458 140481689634688 learning.py:507] global step 1905: loss = 0.1492 (0.331 sec/step)\n",
            "I0719 09:29:32.891885 140481689634688 learning.py:507] global step 1906: loss = 0.1526 (0.341 sec/step)\n",
            "I0719 09:29:33.248996 140481689634688 learning.py:507] global step 1907: loss = 0.1246 (0.355 sec/step)\n",
            "I0719 09:29:33.589498 140481689634688 learning.py:507] global step 1908: loss = 0.1247 (0.338 sec/step)\n",
            "I0719 09:29:33.929833 140481689634688 learning.py:507] global step 1909: loss = 0.1415 (0.338 sec/step)\n",
            "I0719 09:29:34.257920 140481689634688 learning.py:507] global step 1910: loss = 0.1034 (0.326 sec/step)\n",
            "I0719 09:29:34.588216 140481689634688 learning.py:507] global step 1911: loss = 0.1034 (0.329 sec/step)\n",
            "I0719 09:29:34.954119 140481689634688 learning.py:507] global step 1912: loss = 0.0161 (0.364 sec/step)\n",
            "I0719 09:29:35.237031 140481689634688 learning.py:507] global step 1913: loss = 0.1281 (0.281 sec/step)\n",
            "I0719 09:29:35.578117 140481689634688 learning.py:507] global step 1914: loss = 0.1851 (0.339 sec/step)\n",
            "I0719 09:29:35.878787 140481689634688 learning.py:507] global step 1915: loss = 0.2524 (0.299 sec/step)\n",
            "I0719 09:29:36.219653 140481689634688 learning.py:507] global step 1916: loss = 0.3630 (0.339 sec/step)\n",
            "I0719 09:29:36.564133 140481689634688 learning.py:507] global step 1917: loss = 0.1543 (0.343 sec/step)\n",
            "I0719 09:29:36.913696 140481689634688 learning.py:507] global step 1918: loss = 0.1460 (0.348 sec/step)\n",
            "I0719 09:29:37.267486 140481689634688 learning.py:507] global step 1919: loss = 0.0923 (0.352 sec/step)\n",
            "I0719 09:29:37.605757 140481689634688 learning.py:507] global step 1920: loss = 0.0184 (0.335 sec/step)\n",
            "I0719 09:29:37.944299 140481689634688 learning.py:507] global step 1921: loss = 0.1775 (0.337 sec/step)\n",
            "I0719 09:29:38.312201 140481689634688 learning.py:507] global step 1922: loss = 0.1873 (0.366 sec/step)\n",
            "I0719 09:29:38.615154 140481689634688 learning.py:507] global step 1923: loss = 0.1233 (0.301 sec/step)\n",
            "I0719 09:29:38.940128 140481689634688 learning.py:507] global step 1924: loss = 0.0452 (0.323 sec/step)\n",
            "I0719 09:29:39.264776 140481689634688 learning.py:507] global step 1925: loss = 0.1416 (0.323 sec/step)\n",
            "I0719 09:29:39.585769 140481689634688 learning.py:507] global step 1926: loss = 0.0941 (0.319 sec/step)\n",
            "I0719 09:29:39.904444 140481689634688 learning.py:507] global step 1927: loss = 0.1567 (0.317 sec/step)\n",
            "I0719 09:29:40.233050 140481689634688 learning.py:507] global step 1928: loss = 0.0421 (0.327 sec/step)\n",
            "I0719 09:29:40.569060 140481689634688 learning.py:507] global step 1929: loss = 0.1290 (0.334 sec/step)\n",
            "I0719 09:29:40.909008 140481689634688 learning.py:507] global step 1930: loss = 0.0921 (0.338 sec/step)\n",
            "I0719 09:29:41.251718 140481689634688 learning.py:507] global step 1931: loss = 0.3356 (0.341 sec/step)\n",
            "I0719 09:29:41.582003 140481689634688 learning.py:507] global step 1932: loss = 0.0874 (0.329 sec/step)\n",
            "I0719 09:29:41.920452 140481689634688 learning.py:507] global step 1933: loss = 0.0672 (0.337 sec/step)\n",
            "I0719 09:29:42.271088 140481689634688 learning.py:507] global step 1934: loss = 0.1495 (0.349 sec/step)\n",
            "I0719 09:29:42.614788 140481689634688 learning.py:507] global step 1935: loss = 0.1393 (0.342 sec/step)\n",
            "I0719 09:29:42.962363 140481689634688 learning.py:507] global step 1936: loss = 0.1928 (0.346 sec/step)\n",
            "I0719 09:29:43.290179 140481689634688 learning.py:507] global step 1937: loss = 0.0328 (0.326 sec/step)\n",
            "I0719 09:29:43.651156 140481689634688 learning.py:507] global step 1938: loss = 0.3182 (0.359 sec/step)\n",
            "I0719 09:29:44.002051 140481689634688 learning.py:507] global step 1939: loss = 0.1673 (0.349 sec/step)\n",
            "I0719 09:29:44.311896 140481689634688 learning.py:507] global step 1940: loss = 0.1281 (0.308 sec/step)\n",
            "I0719 09:29:44.618172 140481689634688 learning.py:507] global step 1941: loss = 0.0678 (0.305 sec/step)\n",
            "I0719 09:29:44.941139 140481689634688 learning.py:507] global step 1942: loss = 0.2433 (0.321 sec/step)\n",
            "I0719 09:29:45.273380 140481689634688 learning.py:507] global step 1943: loss = 0.0974 (0.331 sec/step)\n",
            "I0719 09:29:45.617899 140481689634688 learning.py:507] global step 1944: loss = 0.0874 (0.343 sec/step)\n",
            "I0719 09:29:45.918528 140481689634688 learning.py:507] global step 1945: loss = 0.1702 (0.299 sec/step)\n",
            "I0719 09:29:46.246981 140481689634688 learning.py:507] global step 1946: loss = 0.2709 (0.327 sec/step)\n",
            "I0719 09:29:46.588681 140481689634688 learning.py:507] global step 1947: loss = 0.1620 (0.340 sec/step)\n",
            "I0719 09:29:46.900911 140481689634688 learning.py:507] global step 1948: loss = 0.1735 (0.310 sec/step)\n",
            "I0719 09:29:47.229498 140481689634688 learning.py:507] global step 1949: loss = 0.1426 (0.326 sec/step)\n",
            "I0719 09:29:47.514900 140481689634688 learning.py:507] global step 1950: loss = 0.1510 (0.284 sec/step)\n",
            "I0719 09:29:47.840562 140481689634688 learning.py:507] global step 1951: loss = 0.2077 (0.324 sec/step)\n",
            "I0719 09:29:48.174547 140481689634688 learning.py:507] global step 1952: loss = 0.0545 (0.332 sec/step)\n",
            "I0719 09:29:48.502429 140481689634688 learning.py:507] global step 1953: loss = 0.1582 (0.326 sec/step)\n",
            "I0719 09:29:48.848594 140481689634688 learning.py:507] global step 1954: loss = 0.4110 (0.343 sec/step)\n",
            "I0719 09:29:49.201660 140481689634688 learning.py:507] global step 1955: loss = 0.1776 (0.351 sec/step)\n",
            "I0719 09:29:49.530671 140481689634688 learning.py:507] global step 1956: loss = 0.1095 (0.327 sec/step)\n",
            "I0719 09:29:49.875682 140481689634688 learning.py:507] global step 1957: loss = 0.0273 (0.343 sec/step)\n",
            "I0719 09:29:50.218600 140481689634688 learning.py:507] global step 1958: loss = 0.1040 (0.341 sec/step)\n",
            "I0719 09:29:50.563681 140481689634688 learning.py:507] global step 1959: loss = 0.0921 (0.343 sec/step)\n",
            "I0719 09:29:50.848018 140481689634688 learning.py:507] global step 1960: loss = 0.1230 (0.282 sec/step)\n",
            "I0719 09:29:51.188148 140481689634688 learning.py:507] global step 1961: loss = 0.2171 (0.338 sec/step)\n",
            "I0719 09:29:51.526571 140481689634688 learning.py:507] global step 1962: loss = 0.1690 (0.337 sec/step)\n",
            "I0719 09:29:51.848870 140481689634688 learning.py:507] global step 1963: loss = 0.2706 (0.320 sec/step)\n",
            "I0719 09:29:52.163195 140481689634688 learning.py:507] global step 1964: loss = 0.1597 (0.312 sec/step)\n",
            "I0719 09:29:52.531183 140481689634688 learning.py:507] global step 1965: loss = 0.1057 (0.366 sec/step)\n",
            "I0719 09:29:52.882616 140481689634688 learning.py:507] global step 1966: loss = 0.0781 (0.350 sec/step)\n",
            "I0719 09:29:53.191322 140481689634688 learning.py:507] global step 1967: loss = 0.1433 (0.307 sec/step)\n",
            "I0719 09:29:53.494855 140481689634688 learning.py:507] global step 1968: loss = 0.1194 (0.302 sec/step)\n",
            "I0719 09:29:53.799474 140481689634688 learning.py:507] global step 1969: loss = 0.1422 (0.303 sec/step)\n",
            "I0719 09:29:54.155925 140481689634688 learning.py:507] global step 1970: loss = 0.0927 (0.355 sec/step)\n",
            "I0719 09:29:54.428325 140481689634688 learning.py:507] global step 1971: loss = 0.1820 (0.271 sec/step)\n",
            "I0719 09:29:54.787914 140481689634688 learning.py:507] global step 1972: loss = 0.2277 (0.358 sec/step)\n",
            "I0719 09:29:55.127383 140481689634688 learning.py:507] global step 1973: loss = 0.1817 (0.338 sec/step)\n",
            "I0719 09:29:55.487424 140481689634688 learning.py:507] global step 1974: loss = 0.1006 (0.358 sec/step)\n",
            "I0719 09:29:55.780552 140481689634688 learning.py:507] global step 1975: loss = 0.1444 (0.291 sec/step)\n",
            "I0719 09:29:56.130436 140481689634688 learning.py:507] global step 1976: loss = 0.1692 (0.348 sec/step)\n",
            "I0719 09:29:56.438453 140481689634688 learning.py:507] global step 1977: loss = 0.0639 (0.306 sec/step)\n",
            "I0719 09:29:56.816676 140481689634688 learning.py:507] global step 1978: loss = 0.0733 (0.376 sec/step)\n",
            "I0719 09:29:57.154600 140481689634688 learning.py:507] global step 1979: loss = 0.0312 (0.336 sec/step)\n",
            "I0719 09:29:57.480538 140481689634688 learning.py:507] global step 1980: loss = 0.1434 (0.324 sec/step)\n",
            "I0719 09:29:57.803003 140481689634688 learning.py:507] global step 1981: loss = 0.0890 (0.320 sec/step)\n",
            "I0719 09:29:58.095548 140481689634688 learning.py:507] global step 1982: loss = 0.2483 (0.291 sec/step)\n",
            "I0719 09:29:58.405131 140481689634688 learning.py:507] global step 1983: loss = 0.1378 (0.308 sec/step)\n",
            "I0719 09:29:58.709481 140481689634688 learning.py:507] global step 1984: loss = 0.3664 (0.302 sec/step)\n",
            "I0719 09:29:59.047216 140481689634688 learning.py:507] global step 1985: loss = 0.1539 (0.336 sec/step)\n",
            "I0719 09:29:59.367641 140481689634688 learning.py:507] global step 1986: loss = 0.2226 (0.319 sec/step)\n",
            "I0719 09:29:59.709482 140481689634688 learning.py:507] global step 1987: loss = 0.0478 (0.340 sec/step)\n",
            "I0719 09:30:00.058354 140481689634688 learning.py:507] global step 1988: loss = 0.1426 (0.347 sec/step)\n",
            "I0719 09:30:00.398128 140481689634688 learning.py:507] global step 1989: loss = 0.1085 (0.338 sec/step)\n",
            "I0719 09:30:00.688196 140481689634688 learning.py:507] global step 1990: loss = 0.1239 (0.288 sec/step)\n",
            "I0719 09:30:01.063730 140481689634688 learning.py:507] global step 1991: loss = 0.0545 (0.374 sec/step)\n",
            "I0719 09:30:01.378524 140481689634688 learning.py:507] global step 1992: loss = 0.0627 (0.313 sec/step)\n",
            "I0719 09:30:01.720883 140481689634688 learning.py:507] global step 1993: loss = 0.2638 (0.340 sec/step)\n",
            "I0719 09:30:02.040464 140481689634688 learning.py:507] global step 1994: loss = 0.0893 (0.318 sec/step)\n",
            "I0719 09:30:02.369488 140481689634688 learning.py:507] global step 1995: loss = 0.0480 (0.327 sec/step)\n",
            "I0719 09:30:02.685994 140481689634688 learning.py:507] global step 1996: loss = 0.1374 (0.311 sec/step)\n",
            "I0719 09:30:03.051345 140481689634688 learning.py:507] global step 1997: loss = 0.2030 (0.360 sec/step)\n",
            "I0719 09:30:03.394477 140481689634688 learning.py:507] global step 1998: loss = 0.1727 (0.341 sec/step)\n",
            "I0719 09:30:03.724616 140481689634688 learning.py:507] global step 1999: loss = 0.0413 (0.328 sec/step)\n",
            "I0719 09:30:04.076853 140481689634688 learning.py:507] global step 2000: loss = 0.1363 (0.351 sec/step)\n",
            "I0719 09:30:04.403086 140481689634688 learning.py:507] global step 2001: loss = 0.1740 (0.324 sec/step)\n",
            "I0719 09:30:04.738419 140481689634688 learning.py:507] global step 2002: loss = 0.1461 (0.333 sec/step)\n",
            "I0719 09:30:05.072260 140481689634688 learning.py:507] global step 2003: loss = 0.0189 (0.332 sec/step)\n",
            "I0719 09:30:05.416287 140481689634688 learning.py:507] global step 2004: loss = 0.0923 (0.342 sec/step)\n",
            "I0719 09:30:05.754890 140481689634688 learning.py:507] global step 2005: loss = 0.1552 (0.337 sec/step)\n",
            "I0719 09:30:06.085576 140481689634688 learning.py:507] global step 2006: loss = 0.2056 (0.329 sec/step)\n",
            "I0719 09:30:06.453782 140481689634688 learning.py:507] global step 2007: loss = 0.0253 (0.366 sec/step)\n",
            "I0719 09:30:06.786824 140481689634688 learning.py:507] global step 2008: loss = 0.0203 (0.331 sec/step)\n",
            "I0719 09:30:07.125135 140481689634688 learning.py:507] global step 2009: loss = 0.1026 (0.336 sec/step)\n",
            "I0719 09:30:07.472968 140481689634688 learning.py:507] global step 2010: loss = 0.1571 (0.346 sec/step)\n",
            "I0719 09:30:07.819627 140481689634688 learning.py:507] global step 2011: loss = 0.2069 (0.345 sec/step)\n",
            "I0719 09:30:08.114877 140481689634688 learning.py:507] global step 2012: loss = 0.1913 (0.293 sec/step)\n",
            "I0719 09:30:08.442204 140481689634688 learning.py:507] global step 2013: loss = 0.1145 (0.326 sec/step)\n",
            "I0719 09:30:08.772253 140481689634688 learning.py:507] global step 2014: loss = 0.0655 (0.328 sec/step)\n",
            "I0719 09:30:09.117354 140481689634688 learning.py:507] global step 2015: loss = 0.1114 (0.343 sec/step)\n",
            "I0719 09:30:09.465229 140481689634688 learning.py:507] global step 2016: loss = 0.0964 (0.346 sec/step)\n",
            "I0719 09:30:09.827604 140481689634688 learning.py:507] global step 2017: loss = 0.2502 (0.360 sec/step)\n",
            "I0719 09:30:10.129412 140481689634688 learning.py:507] global step 2018: loss = 0.1927 (0.300 sec/step)\n",
            "I0719 09:30:10.478397 140481689634688 learning.py:507] global step 2019: loss = 0.1660 (0.347 sec/step)\n",
            "I0719 09:30:10.817231 140481689634688 learning.py:507] global step 2020: loss = 0.0520 (0.337 sec/step)\n",
            "I0719 09:30:11.177433 140481689634688 learning.py:507] global step 2021: loss = 0.1437 (0.358 sec/step)\n",
            "I0719 09:30:11.529687 140481689634688 learning.py:507] global step 2022: loss = 0.1219 (0.350 sec/step)\n",
            "I0719 09:30:11.851804 140481689634688 learning.py:507] global step 2023: loss = 0.2113 (0.320 sec/step)\n",
            "I0719 09:30:12.207649 140481689634688 learning.py:507] global step 2024: loss = 0.1615 (0.353 sec/step)\n",
            "I0719 09:30:12.563681 140481689634688 learning.py:507] global step 2025: loss = 0.1035 (0.354 sec/step)\n",
            "I0719 09:30:12.854305 140481689634688 learning.py:507] global step 2026: loss = 0.1506 (0.289 sec/step)\n",
            "I0719 09:30:13.223648 140481689634688 learning.py:507] global step 2027: loss = 0.2375 (0.368 sec/step)\n",
            "I0719 09:30:13.576127 140481689634688 learning.py:507] global step 2028: loss = 0.0418 (0.351 sec/step)\n",
            "I0719 09:30:13.908817 140481689634688 learning.py:507] global step 2029: loss = 0.1059 (0.331 sec/step)\n",
            "I0719 09:30:14.214675 140481689634688 learning.py:507] global step 2030: loss = 0.1071 (0.304 sec/step)\n",
            "I0719 09:30:14.558896 140481689634688 learning.py:507] global step 2031: loss = 0.0992 (0.342 sec/step)\n",
            "I0719 09:30:14.949210 140481689634688 learning.py:507] global step 2032: loss = 0.1833 (0.388 sec/step)\n",
            "I0719 09:30:15.269758 140481689634688 learning.py:507] global step 2033: loss = 0.1089 (0.319 sec/step)\n",
            "I0719 09:30:15.615390 140481689634688 learning.py:507] global step 2034: loss = 0.0284 (0.344 sec/step)\n",
            "I0719 09:30:15.965637 140481689634688 learning.py:507] global step 2035: loss = 0.0371 (0.349 sec/step)\n",
            "I0719 09:30:16.330880 140481689634688 learning.py:507] global step 2036: loss = 0.0314 (0.363 sec/step)\n",
            "I0719 09:30:16.677258 140481689634688 learning.py:507] global step 2037: loss = 0.0822 (0.344 sec/step)\n",
            "I0719 09:30:17.008399 140481689634688 learning.py:507] global step 2038: loss = 0.1436 (0.329 sec/step)\n",
            "I0719 09:30:17.309959 140481689634688 learning.py:507] global step 2039: loss = 0.1822 (0.300 sec/step)\n",
            "I0719 09:30:17.671214 140481689634688 learning.py:507] global step 2040: loss = 0.2143 (0.359 sec/step)\n",
            "I0719 09:30:18.033637 140481689634688 learning.py:507] global step 2041: loss = 0.1161 (0.361 sec/step)\n",
            "I0719 09:30:18.359934 140481689634688 learning.py:507] global step 2042: loss = 0.0908 (0.325 sec/step)\n",
            "I0719 09:30:18.709306 140481689634688 learning.py:507] global step 2043: loss = 0.0932 (0.347 sec/step)\n",
            "I0719 09:30:19.045950 140481689634688 learning.py:507] global step 2044: loss = 0.1728 (0.335 sec/step)\n",
            "I0719 09:30:19.371259 140481689634688 learning.py:507] global step 2045: loss = 0.0970 (0.324 sec/step)\n",
            "I0719 09:30:19.717990 140481689634688 learning.py:507] global step 2046: loss = 0.0328 (0.344 sec/step)\n",
            "I0719 09:30:20.063436 140481689634688 learning.py:507] global step 2047: loss = 0.1936 (0.343 sec/step)\n",
            "I0719 09:30:20.407670 140481689634688 learning.py:507] global step 2048: loss = 0.0894 (0.342 sec/step)\n",
            "I0719 09:30:20.741801 140481689634688 learning.py:507] global step 2049: loss = 0.1759 (0.332 sec/step)\n",
            "I0719 09:30:21.132699 140481689634688 learning.py:507] global step 2050: loss = 0.1124 (0.389 sec/step)\n",
            "I0719 09:30:21.471753 140481689634688 learning.py:507] global step 2051: loss = 0.0592 (0.337 sec/step)\n",
            "I0719 09:30:21.829972 140481689634688 learning.py:507] global step 2052: loss = 0.1604 (0.356 sec/step)\n",
            "I0719 09:30:22.160679 140481689634688 learning.py:507] global step 2053: loss = 0.1286 (0.329 sec/step)\n",
            "I0719 09:30:22.507390 140481689634688 learning.py:507] global step 2054: loss = 0.0291 (0.345 sec/step)\n",
            "I0719 09:30:22.857037 140481689634688 learning.py:507] global step 2055: loss = 0.2202 (0.348 sec/step)\n",
            "I0719 09:30:23.218575 140481689634688 learning.py:507] global step 2056: loss = 0.0615 (0.360 sec/step)\n",
            "I0719 09:30:23.567212 140481689634688 learning.py:507] global step 2057: loss = 0.0914 (0.344 sec/step)\n",
            "I0719 09:30:23.927015 140481689634688 learning.py:507] global step 2058: loss = 0.2322 (0.358 sec/step)\n",
            "I0719 09:30:24.210235 140481689634688 learning.py:507] global step 2059: loss = 0.1233 (0.281 sec/step)\n",
            "I0719 09:30:24.531359 140481689634688 learning.py:507] global step 2060: loss = 0.0440 (0.319 sec/step)\n",
            "I0719 09:30:24.837859 140481689634688 learning.py:507] global step 2061: loss = 0.2576 (0.305 sec/step)\n",
            "I0719 09:30:25.189912 140481689634688 learning.py:507] global step 2062: loss = 0.1261 (0.350 sec/step)\n",
            "I0719 09:30:25.514505 140481689634688 learning.py:507] global step 2063: loss = 0.1655 (0.323 sec/step)\n",
            "I0719 09:30:25.859235 140481689634688 learning.py:507] global step 2064: loss = 0.0379 (0.343 sec/step)\n",
            "I0719 09:30:26.354358 140481689634688 learning.py:507] global step 2065: loss = 0.1353 (0.450 sec/step)\n",
            "I0719 09:30:26.819925 140481689634688 learning.py:507] global step 2066: loss = 0.0894 (0.460 sec/step)\n",
            "I0719 09:30:27.259020 140481689634688 learning.py:507] global step 2067: loss = 0.2880 (0.437 sec/step)\n",
            "I0719 09:30:27.404585 140479018202880 supervisor.py:1050] Recording summary at step 2067.\n",
            "I0719 09:30:27.598124 140481689634688 learning.py:507] global step 2068: loss = 0.2520 (0.337 sec/step)\n",
            "I0719 09:30:27.661852 140479026595584 supervisor.py:1099] global_step/sec: 2.98334\n",
            "I0719 09:30:27.942674 140481689634688 learning.py:507] global step 2069: loss = 0.1475 (0.342 sec/step)\n",
            "I0719 09:30:28.258404 140481689634688 learning.py:507] global step 2070: loss = 0.0856 (0.314 sec/step)\n",
            "I0719 09:30:28.609077 140481689634688 learning.py:507] global step 2071: loss = 0.2293 (0.349 sec/step)\n",
            "I0719 09:30:28.964627 140481689634688 learning.py:507] global step 2072: loss = 0.0938 (0.354 sec/step)\n",
            "I0719 09:30:29.297094 140481689634688 learning.py:507] global step 2073: loss = 0.0331 (0.331 sec/step)\n",
            "I0719 09:30:29.628704 140481689634688 learning.py:507] global step 2074: loss = 0.1331 (0.330 sec/step)\n",
            "I0719 09:30:29.929818 140481689634688 learning.py:507] global step 2075: loss = 0.1539 (0.299 sec/step)\n",
            "I0719 09:30:30.278769 140481689634688 learning.py:507] global step 2076: loss = 0.1526 (0.347 sec/step)\n",
            "I0719 09:30:30.618718 140481689634688 learning.py:507] global step 2077: loss = 0.2120 (0.338 sec/step)\n",
            "I0719 09:30:30.965342 140481689634688 learning.py:507] global step 2078: loss = 0.0722 (0.345 sec/step)\n",
            "I0719 09:30:31.250458 140481689634688 learning.py:507] global step 2079: loss = 0.1239 (0.283 sec/step)\n",
            "I0719 09:30:31.528554 140481689634688 learning.py:507] global step 2080: loss = 0.0827 (0.276 sec/step)\n",
            "I0719 09:30:31.919491 140481689634688 learning.py:507] global step 2081: loss = 0.1277 (0.389 sec/step)\n",
            "I0719 09:30:32.240395 140481689634688 learning.py:507] global step 2082: loss = 0.1214 (0.319 sec/step)\n",
            "I0719 09:30:32.553660 140481689634688 learning.py:507] global step 2083: loss = 0.1750 (0.312 sec/step)\n",
            "I0719 09:30:32.870419 140481689634688 learning.py:507] global step 2084: loss = 0.0813 (0.315 sec/step)\n",
            "I0719 09:30:33.202934 140481689634688 learning.py:507] global step 2085: loss = 0.0837 (0.331 sec/step)\n",
            "I0719 09:30:33.541891 140481689634688 learning.py:507] global step 2086: loss = 0.2372 (0.335 sec/step)\n",
            "I0719 09:30:33.896437 140481689634688 learning.py:507] global step 2087: loss = 0.0755 (0.353 sec/step)\n",
            "I0719 09:30:34.227690 140481689634688 learning.py:507] global step 2088: loss = 0.1687 (0.329 sec/step)\n",
            "I0719 09:30:34.567522 140481689634688 learning.py:507] global step 2089: loss = 0.1293 (0.338 sec/step)\n",
            "I0719 09:30:34.895059 140481689634688 learning.py:507] global step 2090: loss = 0.1313 (0.326 sec/step)\n",
            "I0719 09:30:35.172856 140481689634688 learning.py:507] global step 2091: loss = 0.0601 (0.276 sec/step)\n",
            "I0719 09:30:35.510058 140481689634688 learning.py:507] global step 2092: loss = 0.0773 (0.335 sec/step)\n",
            "I0719 09:30:35.854537 140481689634688 learning.py:507] global step 2093: loss = 0.0502 (0.343 sec/step)\n",
            "I0719 09:30:36.224611 140481689634688 learning.py:507] global step 2094: loss = 0.1717 (0.368 sec/step)\n",
            "I0719 09:30:36.542015 140481689634688 learning.py:507] global step 2095: loss = 0.0647 (0.316 sec/step)\n",
            "I0719 09:30:36.890296 140481689634688 learning.py:507] global step 2096: loss = 0.1454 (0.347 sec/step)\n",
            "I0719 09:30:37.234222 140481689634688 learning.py:507] global step 2097: loss = 0.1297 (0.342 sec/step)\n",
            "I0719 09:30:37.565416 140481689634688 learning.py:507] global step 2098: loss = 0.1911 (0.329 sec/step)\n",
            "I0719 09:30:37.906427 140481689634688 learning.py:507] global step 2099: loss = 0.1318 (0.340 sec/step)\n",
            "I0719 09:30:38.252424 140481689634688 learning.py:507] global step 2100: loss = 0.3819 (0.344 sec/step)\n",
            "I0719 09:30:38.599610 140481689634688 learning.py:507] global step 2101: loss = 0.1148 (0.345 sec/step)\n",
            "I0719 09:30:38.935580 140481689634688 learning.py:507] global step 2102: loss = 0.1225 (0.334 sec/step)\n",
            "I0719 09:30:39.228715 140481689634688 learning.py:507] global step 2103: loss = 0.1368 (0.288 sec/step)\n",
            "I0719 09:30:39.558788 140481689634688 learning.py:507] global step 2104: loss = 0.1293 (0.327 sec/step)\n",
            "I0719 09:30:39.907148 140481689634688 learning.py:507] global step 2105: loss = 0.0482 (0.346 sec/step)\n",
            "I0719 09:30:40.279789 140481689634688 learning.py:507] global step 2106: loss = 0.0878 (0.371 sec/step)\n",
            "I0719 09:30:40.611565 140481689634688 learning.py:507] global step 2107: loss = 0.0192 (0.330 sec/step)\n",
            "I0719 09:30:40.958740 140481689634688 learning.py:507] global step 2108: loss = 0.0711 (0.345 sec/step)\n",
            "I0719 09:30:41.294992 140481689634688 learning.py:507] global step 2109: loss = 0.1870 (0.335 sec/step)\n",
            "I0719 09:30:41.632037 140481689634688 learning.py:507] global step 2110: loss = 0.1279 (0.335 sec/step)\n",
            "I0719 09:30:42.021734 140481689634688 learning.py:507] global step 2111: loss = 0.1176 (0.388 sec/step)\n",
            "I0719 09:30:42.355417 140481689634688 learning.py:507] global step 2112: loss = 0.1655 (0.332 sec/step)\n",
            "I0719 09:30:42.715411 140481689634688 learning.py:507] global step 2113: loss = 0.1232 (0.358 sec/step)\n",
            "I0719 09:30:43.016828 140481689634688 learning.py:507] global step 2114: loss = 0.0593 (0.300 sec/step)\n",
            "I0719 09:30:43.353049 140481689634688 learning.py:507] global step 2115: loss = 0.2070 (0.334 sec/step)\n",
            "I0719 09:30:43.684089 140481689634688 learning.py:507] global step 2116: loss = 0.1591 (0.329 sec/step)\n",
            "I0719 09:30:44.033150 140481689634688 learning.py:507] global step 2117: loss = 0.1458 (0.347 sec/step)\n",
            "I0719 09:30:44.382388 140481689634688 learning.py:507] global step 2118: loss = 0.0911 (0.347 sec/step)\n",
            "I0719 09:30:44.752670 140481689634688 learning.py:507] global step 2119: loss = 0.1448 (0.368 sec/step)\n",
            "I0719 09:30:45.094029 140481689634688 learning.py:507] global step 2120: loss = 0.1399 (0.340 sec/step)\n",
            "I0719 09:30:45.417284 140481689634688 learning.py:507] global step 2121: loss = 0.2438 (0.321 sec/step)\n",
            "I0719 09:30:45.762623 140481689634688 learning.py:507] global step 2122: loss = 0.2323 (0.343 sec/step)\n",
            "I0719 09:30:46.089314 140481689634688 learning.py:507] global step 2123: loss = 0.1747 (0.325 sec/step)\n",
            "I0719 09:30:46.447192 140481689634688 learning.py:507] global step 2124: loss = 0.0413 (0.356 sec/step)\n",
            "I0719 09:30:46.765811 140481689634688 learning.py:507] global step 2125: loss = 0.1849 (0.317 sec/step)\n",
            "I0719 09:30:47.124553 140481689634688 learning.py:507] global step 2126: loss = 0.0734 (0.357 sec/step)\n",
            "I0719 09:30:47.458352 140481689634688 learning.py:507] global step 2127: loss = 0.0597 (0.332 sec/step)\n",
            "I0719 09:30:47.793472 140481689634688 learning.py:507] global step 2128: loss = 0.1505 (0.333 sec/step)\n",
            "I0719 09:30:48.129526 140481689634688 learning.py:507] global step 2129: loss = 0.1284 (0.334 sec/step)\n",
            "I0719 09:30:48.469527 140481689634688 learning.py:507] global step 2130: loss = 0.0283 (0.338 sec/step)\n",
            "I0719 09:30:48.815655 140481689634688 learning.py:507] global step 2131: loss = 0.1400 (0.344 sec/step)\n",
            "I0719 09:30:49.142799 140481689634688 learning.py:507] global step 2132: loss = 0.0214 (0.325 sec/step)\n",
            "I0719 09:30:49.426327 140481689634688 learning.py:507] global step 2133: loss = 0.2393 (0.282 sec/step)\n",
            "I0719 09:30:49.775890 140481689634688 learning.py:507] global step 2134: loss = 0.1064 (0.348 sec/step)\n",
            "I0719 09:30:50.113369 140481689634688 learning.py:507] global step 2135: loss = 0.0306 (0.335 sec/step)\n",
            "I0719 09:30:50.435790 140481689634688 learning.py:507] global step 2136: loss = 0.1560 (0.321 sec/step)\n",
            "I0719 09:30:50.762356 140481689634688 learning.py:507] global step 2137: loss = 0.1410 (0.325 sec/step)\n",
            "I0719 09:30:51.114704 140481689634688 learning.py:507] global step 2138: loss = 0.1343 (0.350 sec/step)\n",
            "I0719 09:30:51.452809 140481689634688 learning.py:507] global step 2139: loss = 0.1313 (0.336 sec/step)\n",
            "I0719 09:30:51.785664 140481689634688 learning.py:507] global step 2140: loss = 0.1287 (0.331 sec/step)\n",
            "I0719 09:30:52.131113 140481689634688 learning.py:507] global step 2141: loss = 0.2232 (0.344 sec/step)\n",
            "I0719 09:30:52.476649 140481689634688 learning.py:507] global step 2142: loss = 0.1229 (0.344 sec/step)\n",
            "I0719 09:30:52.802145 140481689634688 learning.py:507] global step 2143: loss = 0.1381 (0.324 sec/step)\n",
            "I0719 09:30:53.155212 140481689634688 learning.py:507] global step 2144: loss = 0.1248 (0.351 sec/step)\n",
            "I0719 09:30:53.492663 140481689634688 learning.py:507] global step 2145: loss = 0.1568 (0.336 sec/step)\n",
            "I0719 09:30:53.817892 140481689634688 learning.py:507] global step 2146: loss = 0.1334 (0.324 sec/step)\n",
            "I0719 09:30:54.162883 140481689634688 learning.py:507] global step 2147: loss = 0.0777 (0.343 sec/step)\n",
            "I0719 09:30:54.515498 140481689634688 learning.py:507] global step 2148: loss = 0.1093 (0.351 sec/step)\n",
            "I0719 09:30:54.802515 140481689634688 learning.py:507] global step 2149: loss = 0.2118 (0.285 sec/step)\n",
            "I0719 09:30:55.136902 140481689634688 learning.py:507] global step 2150: loss = 0.1390 (0.332 sec/step)\n",
            "I0719 09:30:55.426542 140481689634688 learning.py:507] global step 2151: loss = 0.1327 (0.287 sec/step)\n",
            "I0719 09:30:55.775230 140481689634688 learning.py:507] global step 2152: loss = 0.1160 (0.347 sec/step)\n",
            "I0719 09:30:56.135718 140481689634688 learning.py:507] global step 2153: loss = 0.2307 (0.358 sec/step)\n",
            "I0719 09:30:56.454179 140481689634688 learning.py:507] global step 2154: loss = 0.1165 (0.316 sec/step)\n",
            "I0719 09:30:56.765121 140481689634688 learning.py:507] global step 2155: loss = 0.1077 (0.309 sec/step)\n",
            "I0719 09:30:57.053184 140481689634688 learning.py:507] global step 2156: loss = 0.0936 (0.286 sec/step)\n",
            "I0719 09:30:57.375057 140481689634688 learning.py:507] global step 2157: loss = 0.0976 (0.320 sec/step)\n",
            "I0719 09:30:57.683467 140481689634688 learning.py:507] global step 2158: loss = 0.1419 (0.307 sec/step)\n",
            "I0719 09:30:58.003616 140481689634688 learning.py:507] global step 2159: loss = 0.0922 (0.318 sec/step)\n",
            "I0719 09:30:58.347951 140481689634688 learning.py:507] global step 2160: loss = 0.1818 (0.342 sec/step)\n",
            "I0719 09:30:58.672882 140481689634688 learning.py:507] global step 2161: loss = 0.1692 (0.323 sec/step)\n",
            "I0719 09:30:58.957553 140481689634688 learning.py:507] global step 2162: loss = 0.0669 (0.282 sec/step)\n",
            "I0719 09:30:59.295143 140481689634688 learning.py:507] global step 2163: loss = 0.0428 (0.336 sec/step)\n",
            "I0719 09:30:59.633742 140481689634688 learning.py:507] global step 2164: loss = 0.1225 (0.337 sec/step)\n",
            "I0719 09:30:59.909112 140481689634688 learning.py:507] global step 2165: loss = 0.0519 (0.274 sec/step)\n",
            "I0719 09:31:00.244236 140481689634688 learning.py:507] global step 2166: loss = 0.1874 (0.333 sec/step)\n",
            "I0719 09:31:00.594388 140481689634688 learning.py:507] global step 2167: loss = 0.0405 (0.348 sec/step)\n",
            "I0719 09:31:00.936993 140481689634688 learning.py:507] global step 2168: loss = 0.1423 (0.341 sec/step)\n",
            "I0719 09:31:01.267347 140481689634688 learning.py:507] global step 2169: loss = 0.1437 (0.328 sec/step)\n",
            "I0719 09:31:01.634756 140481689634688 learning.py:507] global step 2170: loss = 0.0233 (0.366 sec/step)\n",
            "I0719 09:31:01.946115 140481689634688 learning.py:507] global step 2171: loss = 0.1141 (0.310 sec/step)\n",
            "I0719 09:31:02.257597 140481689634688 learning.py:507] global step 2172: loss = 0.2639 (0.310 sec/step)\n",
            "I0719 09:31:02.546901 140481689634688 learning.py:507] global step 2173: loss = 0.0996 (0.287 sec/step)\n",
            "I0719 09:31:02.880966 140481689634688 learning.py:507] global step 2174: loss = 0.2797 (0.332 sec/step)\n",
            "I0719 09:31:03.207311 140481689634688 learning.py:507] global step 2175: loss = 0.0295 (0.324 sec/step)\n",
            "I0719 09:31:03.554080 140481689634688 learning.py:507] global step 2176: loss = 0.3527 (0.345 sec/step)\n",
            "I0719 09:31:03.883146 140481689634688 learning.py:507] global step 2177: loss = 0.2401 (0.327 sec/step)\n",
            "I0719 09:31:04.215129 140481689634688 learning.py:507] global step 2178: loss = 0.1051 (0.330 sec/step)\n",
            "I0719 09:31:04.553142 140481689634688 learning.py:507] global step 2179: loss = 0.1143 (0.336 sec/step)\n",
            "I0719 09:31:04.882975 140481689634688 learning.py:507] global step 2180: loss = 0.1125 (0.328 sec/step)\n",
            "I0719 09:31:05.208728 140481689634688 learning.py:507] global step 2181: loss = 0.0472 (0.323 sec/step)\n",
            "I0719 09:31:05.543371 140481689634688 learning.py:507] global step 2182: loss = 0.3024 (0.333 sec/step)\n",
            "I0719 09:31:05.880369 140481689634688 learning.py:507] global step 2183: loss = 0.0558 (0.335 sec/step)\n",
            "I0719 09:31:06.250503 140481689634688 learning.py:507] global step 2184: loss = 0.1396 (0.368 sec/step)\n",
            "I0719 09:31:06.576232 140481689634688 learning.py:507] global step 2185: loss = 0.0717 (0.324 sec/step)\n",
            "I0719 09:31:06.922410 140481689634688 learning.py:507] global step 2186: loss = 0.1173 (0.344 sec/step)\n",
            "I0719 09:31:07.248342 140481689634688 learning.py:507] global step 2187: loss = 0.0469 (0.324 sec/step)\n",
            "I0719 09:31:07.572126 140481689634688 learning.py:507] global step 2188: loss = 0.1424 (0.322 sec/step)\n",
            "I0719 09:31:07.956243 140481689634688 learning.py:507] global step 2189: loss = 0.1434 (0.382 sec/step)\n",
            "I0719 09:31:08.303686 140481689634688 learning.py:507] global step 2190: loss = 0.1151 (0.344 sec/step)\n",
            "I0719 09:31:08.634829 140481689634688 learning.py:507] global step 2191: loss = 0.1805 (0.329 sec/step)\n",
            "I0719 09:31:08.971645 140481689634688 learning.py:507] global step 2192: loss = 0.0760 (0.335 sec/step)\n",
            "I0719 09:31:09.340802 140481689634688 learning.py:507] global step 2193: loss = 0.1008 (0.367 sec/step)\n",
            "I0719 09:31:09.692248 140481689634688 learning.py:507] global step 2194: loss = 0.0835 (0.350 sec/step)\n",
            "I0719 09:31:10.073263 140481689634688 learning.py:507] global step 2195: loss = 0.1211 (0.379 sec/step)\n",
            "I0719 09:31:10.385664 140481689634688 learning.py:507] global step 2196: loss = 0.0904 (0.311 sec/step)\n",
            "I0719 09:31:10.750212 140481689634688 learning.py:507] global step 2197: loss = 0.1190 (0.363 sec/step)\n",
            "I0719 09:31:11.081056 140481689634688 learning.py:507] global step 2198: loss = 0.1750 (0.329 sec/step)\n",
            "I0719 09:31:11.416526 140481689634688 learning.py:507] global step 2199: loss = 0.0188 (0.334 sec/step)\n",
            "I0719 09:31:11.720309 140481689634688 learning.py:507] global step 2200: loss = 0.0362 (0.302 sec/step)\n",
            "I0719 09:31:12.066569 140481689634688 learning.py:507] global step 2201: loss = 0.1331 (0.345 sec/step)\n",
            "I0719 09:31:12.401210 140481689634688 learning.py:507] global step 2202: loss = 0.2041 (0.333 sec/step)\n",
            "I0719 09:31:12.749358 140481689634688 learning.py:507] global step 2203: loss = 0.1085 (0.346 sec/step)\n",
            "I0719 09:31:13.066666 140481689634688 learning.py:507] global step 2204: loss = 0.1843 (0.315 sec/step)\n",
            "I0719 09:31:13.372824 140481689634688 learning.py:507] global step 2205: loss = 0.2824 (0.304 sec/step)\n",
            "I0719 09:31:13.715745 140481689634688 learning.py:507] global step 2206: loss = 0.4068 (0.341 sec/step)\n",
            "I0719 09:31:14.077430 140481689634688 learning.py:507] global step 2207: loss = 0.0967 (0.360 sec/step)\n",
            "I0719 09:31:14.421470 140481689634688 learning.py:507] global step 2208: loss = 0.1794 (0.342 sec/step)\n",
            "I0719 09:31:14.735781 140481689634688 learning.py:507] global step 2209: loss = 0.1429 (0.312 sec/step)\n",
            "I0719 09:31:15.097238 140481689634688 learning.py:507] global step 2210: loss = 0.1064 (0.360 sec/step)\n",
            "I0719 09:31:15.418815 140481689634688 learning.py:507] global step 2211: loss = 0.0976 (0.319 sec/step)\n",
            "I0719 09:31:15.774630 140481689634688 learning.py:507] global step 2212: loss = 0.0433 (0.354 sec/step)\n",
            "I0719 09:31:16.124393 140481689634688 learning.py:507] global step 2213: loss = 0.0938 (0.348 sec/step)\n",
            "I0719 09:31:16.476373 140481689634688 learning.py:507] global step 2214: loss = 0.1852 (0.350 sec/step)\n",
            "I0719 09:31:16.809646 140481689634688 learning.py:507] global step 2215: loss = 0.0249 (0.332 sec/step)\n",
            "I0719 09:31:17.148052 140481689634688 learning.py:507] global step 2216: loss = 0.1076 (0.337 sec/step)\n",
            "I0719 09:31:17.506923 140481689634688 learning.py:507] global step 2217: loss = 0.1400 (0.357 sec/step)\n",
            "I0719 09:31:17.848953 140481689634688 learning.py:507] global step 2218: loss = 0.1171 (0.340 sec/step)\n",
            "I0719 09:31:18.223180 140481689634688 learning.py:507] global step 2219: loss = 0.1122 (0.372 sec/step)\n",
            "I0719 09:31:18.524489 140481689634688 learning.py:507] global step 2220: loss = 0.1808 (0.300 sec/step)\n",
            "I0719 09:31:18.797553 140481689634688 learning.py:507] global step 2221: loss = 0.1223 (0.271 sec/step)\n",
            "I0719 09:31:19.131235 140481689634688 learning.py:507] global step 2222: loss = 0.0403 (0.331 sec/step)\n",
            "I0719 09:31:19.463665 140481689634688 learning.py:507] global step 2223: loss = 0.1423 (0.330 sec/step)\n",
            "I0719 09:31:19.815718 140481689634688 learning.py:507] global step 2224: loss = 0.1393 (0.350 sec/step)\n",
            "I0719 09:31:20.161165 140481689634688 learning.py:507] global step 2225: loss = 0.1480 (0.344 sec/step)\n",
            "I0719 09:31:20.476392 140481689634688 learning.py:507] global step 2226: loss = 0.1638 (0.312 sec/step)\n",
            "I0719 09:31:20.792883 140481689634688 learning.py:507] global step 2227: loss = 0.0459 (0.315 sec/step)\n",
            "I0719 09:31:21.152611 140481689634688 learning.py:507] global step 2228: loss = 0.1335 (0.358 sec/step)\n",
            "I0719 09:31:21.530156 140481689634688 learning.py:507] global step 2229: loss = 0.1971 (0.376 sec/step)\n",
            "I0719 09:31:21.862756 140481689634688 learning.py:507] global step 2230: loss = 0.1045 (0.331 sec/step)\n",
            "I0719 09:31:22.198583 140481689634688 learning.py:507] global step 2231: loss = 0.1184 (0.333 sec/step)\n",
            "I0719 09:31:22.549386 140481689634688 learning.py:507] global step 2232: loss = 0.1384 (0.347 sec/step)\n",
            "I0719 09:31:22.837874 140481689634688 learning.py:507] global step 2233: loss = 0.1043 (0.287 sec/step)\n",
            "I0719 09:31:23.169536 140481689634688 learning.py:507] global step 2234: loss = 0.0840 (0.330 sec/step)\n",
            "I0719 09:31:23.531160 140481689634688 learning.py:507] global step 2235: loss = 0.0954 (0.360 sec/step)\n",
            "I0719 09:31:23.851971 140481689634688 learning.py:507] global step 2236: loss = 0.0855 (0.319 sec/step)\n",
            "I0719 09:31:24.189849 140481689634688 learning.py:507] global step 2237: loss = 0.2197 (0.336 sec/step)\n",
            "I0719 09:31:24.547830 140481689634688 learning.py:507] global step 2238: loss = 0.0659 (0.355 sec/step)\n",
            "I0719 09:31:24.891337 140481689634688 learning.py:507] global step 2239: loss = 0.1019 (0.342 sec/step)\n",
            "I0719 09:31:25.260312 140481689634688 learning.py:507] global step 2240: loss = 0.0506 (0.367 sec/step)\n",
            "I0719 09:31:25.626145 140481689634688 learning.py:507] global step 2241: loss = 0.1601 (0.363 sec/step)\n",
            "I0719 09:31:25.952778 140481689634688 learning.py:507] global step 2242: loss = 0.3091 (0.325 sec/step)\n",
            "I0719 09:31:26.288570 140481689634688 learning.py:507] global step 2243: loss = 0.1445 (0.334 sec/step)\n",
            "I0719 09:31:26.610396 140481689634688 learning.py:507] global step 2244: loss = 0.1120 (0.320 sec/step)\n",
            "I0719 09:31:26.943504 140481689634688 learning.py:507] global step 2245: loss = 0.2812 (0.331 sec/step)\n",
            "I0719 09:31:27.315226 140481689634688 learning.py:507] global step 2246: loss = 0.1029 (0.370 sec/step)\n",
            "I0719 09:31:27.634683 140481689634688 learning.py:507] global step 2247: loss = 0.0791 (0.318 sec/step)\n",
            "I0719 09:31:27.985696 140481689634688 learning.py:507] global step 2248: loss = 0.1478 (0.349 sec/step)\n",
            "I0719 09:31:28.325910 140481689634688 learning.py:507] global step 2249: loss = 0.0490 (0.338 sec/step)\n",
            "I0719 09:31:28.681861 140481689634688 learning.py:507] global step 2250: loss = 0.1263 (0.354 sec/step)\n",
            "I0719 09:31:29.019240 140481689634688 learning.py:507] global step 2251: loss = 0.2225 (0.336 sec/step)\n",
            "I0719 09:31:29.326329 140481689634688 learning.py:507] global step 2252: loss = 0.2224 (0.305 sec/step)\n",
            "I0719 09:31:29.673623 140481689634688 learning.py:507] global step 2253: loss = 0.1776 (0.345 sec/step)\n",
            "I0719 09:31:29.996692 140481689634688 learning.py:507] global step 2254: loss = 0.1481 (0.321 sec/step)\n",
            "I0719 09:31:30.348593 140481689634688 learning.py:507] global step 2255: loss = 0.0897 (0.350 sec/step)\n",
            "I0719 09:31:30.703504 140481689634688 learning.py:507] global step 2256: loss = 0.0956 (0.353 sec/step)\n",
            "I0719 09:31:31.037435 140481689634688 learning.py:507] global step 2257: loss = 0.1377 (0.332 sec/step)\n",
            "I0719 09:31:31.389316 140481689634688 learning.py:507] global step 2258: loss = 0.1431 (0.350 sec/step)\n",
            "I0719 09:31:31.738305 140481689634688 learning.py:507] global step 2259: loss = 0.0281 (0.347 sec/step)\n",
            "I0719 09:31:32.072302 140481689634688 learning.py:507] global step 2260: loss = 0.1106 (0.332 sec/step)\n",
            "I0719 09:31:32.422844 140481689634688 learning.py:507] global step 2261: loss = 0.1102 (0.349 sec/step)\n",
            "I0719 09:31:32.774767 140481689634688 learning.py:507] global step 2262: loss = 0.1483 (0.350 sec/step)\n",
            "I0719 09:31:33.076703 140481689634688 learning.py:507] global step 2263: loss = 0.0669 (0.300 sec/step)\n",
            "I0719 09:31:33.440206 140481689634688 learning.py:507] global step 2264: loss = 0.1248 (0.362 sec/step)\n",
            "I0719 09:31:33.766356 140481689634688 learning.py:507] global step 2265: loss = 0.0629 (0.324 sec/step)\n",
            "I0719 09:31:34.122556 140481689634688 learning.py:507] global step 2266: loss = 0.1028 (0.354 sec/step)\n",
            "I0719 09:31:34.425648 140481689634688 learning.py:507] global step 2267: loss = 0.0908 (0.302 sec/step)\n",
            "I0719 09:31:34.771793 140481689634688 learning.py:507] global step 2268: loss = 0.1263 (0.344 sec/step)\n",
            "I0719 09:31:35.113293 140481689634688 learning.py:507] global step 2269: loss = 0.2166 (0.340 sec/step)\n",
            "I0719 09:31:35.444852 140481689634688 learning.py:507] global step 2270: loss = 0.0830 (0.330 sec/step)\n",
            "I0719 09:31:35.791327 140481689634688 learning.py:507] global step 2271: loss = 0.0377 (0.344 sec/step)\n",
            "I0719 09:31:36.135372 140481689634688 learning.py:507] global step 2272: loss = 0.1091 (0.342 sec/step)\n",
            "I0719 09:31:36.457921 140481689634688 learning.py:507] global step 2273: loss = 0.0613 (0.321 sec/step)\n",
            "I0719 09:31:36.822078 140481689634688 learning.py:507] global step 2274: loss = 0.1423 (0.362 sec/step)\n",
            "I0719 09:31:37.138016 140481689634688 learning.py:507] global step 2275: loss = 0.1026 (0.314 sec/step)\n",
            "I0719 09:31:37.470936 140481689634688 learning.py:507] global step 2276: loss = 0.1604 (0.331 sec/step)\n",
            "I0719 09:31:37.788776 140481689634688 learning.py:507] global step 2277: loss = 0.0697 (0.316 sec/step)\n",
            "I0719 09:31:38.080161 140481689634688 learning.py:507] global step 2278: loss = 0.0865 (0.289 sec/step)\n",
            "I0719 09:31:38.407513 140481689634688 learning.py:507] global step 2279: loss = 0.1138 (0.326 sec/step)\n",
            "I0719 09:31:38.804371 140481689634688 learning.py:507] global step 2280: loss = 0.1074 (0.395 sec/step)\n",
            "I0719 09:31:39.090392 140481689634688 learning.py:507] global step 2281: loss = 0.1880 (0.284 sec/step)\n",
            "I0719 09:31:39.427946 140481689634688 learning.py:507] global step 2282: loss = 0.1404 (0.336 sec/step)\n",
            "I0719 09:31:39.739404 140481689634688 learning.py:507] global step 2283: loss = 0.1468 (0.309 sec/step)\n",
            "I0719 09:31:40.029980 140481689634688 learning.py:507] global step 2284: loss = 0.1180 (0.289 sec/step)\n",
            "I0719 09:31:40.327534 140481689634688 learning.py:507] global step 2285: loss = 0.1032 (0.296 sec/step)\n",
            "I0719 09:31:40.657546 140481689634688 learning.py:507] global step 2286: loss = 0.0911 (0.328 sec/step)\n",
            "I0719 09:31:40.984036 140481689634688 learning.py:507] global step 2287: loss = 0.0196 (0.325 sec/step)\n",
            "I0719 09:31:41.329431 140481689634688 learning.py:507] global step 2288: loss = 0.0570 (0.344 sec/step)\n",
            "I0719 09:31:41.670136 140481689634688 learning.py:507] global step 2289: loss = 0.1681 (0.339 sec/step)\n",
            "I0719 09:31:42.023262 140481689634688 learning.py:507] global step 2290: loss = 0.2339 (0.351 sec/step)\n",
            "I0719 09:31:42.348137 140481689634688 learning.py:507] global step 2291: loss = 0.0849 (0.323 sec/step)\n",
            "I0719 09:31:42.705692 140481689634688 learning.py:507] global step 2292: loss = 0.0593 (0.356 sec/step)\n",
            "I0719 09:31:43.014459 140481689634688 learning.py:507] global step 2293: loss = 0.1202 (0.307 sec/step)\n",
            "I0719 09:31:43.363288 140481689634688 learning.py:507] global step 2294: loss = 0.0381 (0.347 sec/step)\n",
            "I0719 09:31:43.720096 140481689634688 learning.py:507] global step 2295: loss = 0.2009 (0.355 sec/step)\n",
            "I0719 09:31:44.056455 140481689634688 learning.py:507] global step 2296: loss = 0.0181 (0.334 sec/step)\n",
            "I0719 09:31:44.391947 140481689634688 learning.py:507] global step 2297: loss = 0.1862 (0.334 sec/step)\n",
            "I0719 09:31:44.718705 140481689634688 learning.py:507] global step 2298: loss = 0.0208 (0.325 sec/step)\n",
            "I0719 09:31:45.064769 140481689634688 learning.py:507] global step 2299: loss = 0.1274 (0.344 sec/step)\n",
            "I0719 09:31:45.425806 140481689634688 learning.py:507] global step 2300: loss = 0.1381 (0.359 sec/step)\n",
            "I0719 09:31:45.752339 140481689634688 learning.py:507] global step 2301: loss = 0.1531 (0.325 sec/step)\n",
            "I0719 09:31:46.111906 140481689634688 learning.py:507] global step 2302: loss = 0.0609 (0.358 sec/step)\n",
            "I0719 09:31:46.447831 140481689634688 learning.py:507] global step 2303: loss = 0.2387 (0.334 sec/step)\n",
            "I0719 09:31:46.721706 140481689634688 learning.py:507] global step 2304: loss = 0.1271 (0.272 sec/step)\n",
            "I0719 09:31:47.095186 140481689634688 learning.py:507] global step 2305: loss = 0.1057 (0.372 sec/step)\n",
            "I0719 09:31:47.437453 140481689634688 learning.py:507] global step 2306: loss = 0.1076 (0.340 sec/step)\n",
            "I0719 09:31:47.737771 140481689634688 learning.py:507] global step 2307: loss = 0.2275 (0.299 sec/step)\n",
            "I0719 09:31:48.091375 140481689634688 learning.py:507] global step 2308: loss = 0.0426 (0.352 sec/step)\n",
            "I0719 09:31:48.408199 140481689634688 learning.py:507] global step 2309: loss = 0.2357 (0.315 sec/step)\n",
            "I0719 09:31:48.731544 140481689634688 learning.py:507] global step 2310: loss = 0.2164 (0.322 sec/step)\n",
            "I0719 09:31:49.117459 140481689634688 learning.py:507] global step 2311: loss = 0.2034 (0.384 sec/step)\n",
            "I0719 09:31:49.411034 140481689634688 learning.py:507] global step 2312: loss = 0.0766 (0.292 sec/step)\n",
            "I0719 09:31:49.764683 140481689634688 learning.py:507] global step 2313: loss = 0.0685 (0.352 sec/step)\n",
            "I0719 09:31:50.091018 140481689634688 learning.py:507] global step 2314: loss = 0.1067 (0.325 sec/step)\n",
            "I0719 09:31:50.437416 140481689634688 learning.py:507] global step 2315: loss = 0.1657 (0.345 sec/step)\n",
            "I0719 09:31:50.784353 140481689634688 learning.py:507] global step 2316: loss = 0.1514 (0.345 sec/step)\n",
            "I0719 09:31:51.142634 140481689634688 learning.py:507] global step 2317: loss = 0.1905 (0.356 sec/step)\n",
            "I0719 09:31:51.472447 140481689634688 learning.py:507] global step 2318: loss = 0.1313 (0.328 sec/step)\n",
            "I0719 09:31:51.823143 140481689634688 learning.py:507] global step 2319: loss = 0.1937 (0.349 sec/step)\n",
            "I0719 09:31:52.164661 140481689634688 learning.py:507] global step 2320: loss = 0.2121 (0.339 sec/step)\n",
            "I0719 09:31:52.499363 140481689634688 learning.py:507] global step 2321: loss = 0.0574 (0.333 sec/step)\n",
            "I0719 09:31:52.835385 140481689634688 learning.py:507] global step 2322: loss = 0.3528 (0.334 sec/step)\n",
            "I0719 09:31:53.151632 140481689634688 learning.py:507] global step 2323: loss = 0.0704 (0.314 sec/step)\n",
            "I0719 09:31:53.477710 140481689634688 learning.py:507] global step 2324: loss = 0.1285 (0.324 sec/step)\n",
            "I0719 09:31:53.777874 140481689634688 learning.py:507] global step 2325: loss = 0.0603 (0.299 sec/step)\n",
            "I0719 09:31:54.110533 140481689634688 learning.py:507] global step 2326: loss = 0.1411 (0.330 sec/step)\n",
            "I0719 09:31:54.431835 140481689634688 learning.py:507] global step 2327: loss = 0.0850 (0.319 sec/step)\n",
            "I0719 09:31:54.758674 140481689634688 learning.py:507] global step 2328: loss = 0.0888 (0.325 sec/step)\n",
            "I0719 09:31:55.098052 140481689634688 learning.py:507] global step 2329: loss = 0.1572 (0.338 sec/step)\n",
            "I0719 09:31:55.406852 140481689634688 learning.py:507] global step 2330: loss = 0.0525 (0.307 sec/step)\n",
            "I0719 09:31:55.740108 140481689634688 learning.py:507] global step 2331: loss = 0.1029 (0.331 sec/step)\n",
            "I0719 09:31:56.056350 140481689634688 learning.py:507] global step 2332: loss = 0.0959 (0.314 sec/step)\n",
            "I0719 09:31:56.370853 140481689634688 learning.py:507] global step 2333: loss = 0.1013 (0.313 sec/step)\n",
            "I0719 09:31:56.660174 140481689634688 learning.py:507] global step 2334: loss = 0.1316 (0.288 sec/step)\n",
            "I0719 09:31:56.989665 140481689634688 learning.py:507] global step 2335: loss = 0.0390 (0.328 sec/step)\n",
            "I0719 09:31:57.308253 140481689634688 learning.py:507] global step 2336: loss = 0.0212 (0.317 sec/step)\n",
            "I0719 09:31:57.650825 140481689634688 learning.py:507] global step 2337: loss = 0.0917 (0.341 sec/step)\n",
            "I0719 09:31:57.990482 140481689634688 learning.py:507] global step 2338: loss = 0.1865 (0.338 sec/step)\n",
            "I0719 09:31:58.336162 140481689634688 learning.py:507] global step 2339: loss = 0.1994 (0.344 sec/step)\n",
            "I0719 09:31:58.670806 140481689634688 learning.py:507] global step 2340: loss = 0.2207 (0.333 sec/step)\n",
            "I0719 09:31:59.009560 140481689634688 learning.py:507] global step 2341: loss = 0.1624 (0.337 sec/step)\n",
            "I0719 09:31:59.320769 140481689634688 learning.py:507] global step 2342: loss = 0.1883 (0.309 sec/step)\n",
            "I0719 09:31:59.693485 140481689634688 learning.py:507] global step 2343: loss = 0.0381 (0.371 sec/step)\n",
            "I0719 09:32:00.008574 140481689634688 learning.py:507] global step 2344: loss = 0.0340 (0.313 sec/step)\n",
            "I0719 09:32:00.342473 140481689634688 learning.py:507] global step 2345: loss = 0.0778 (0.332 sec/step)\n",
            "I0719 09:32:00.695970 140481689634688 learning.py:507] global step 2346: loss = 0.0887 (0.352 sec/step)\n",
            "I0719 09:32:00.983064 140481689634688 learning.py:507] global step 2347: loss = 0.1859 (0.285 sec/step)\n",
            "I0719 09:32:01.326855 140481689634688 learning.py:507] global step 2348: loss = 0.1908 (0.342 sec/step)\n",
            "I0719 09:32:01.680391 140481689634688 learning.py:507] global step 2349: loss = 0.1026 (0.351 sec/step)\n",
            "I0719 09:32:02.022874 140481689634688 learning.py:507] global step 2350: loss = 0.1021 (0.341 sec/step)\n",
            "I0719 09:32:02.320244 140481689634688 learning.py:507] global step 2351: loss = 0.0861 (0.296 sec/step)\n",
            "I0719 09:32:02.668052 140481689634688 learning.py:507] global step 2352: loss = 0.0726 (0.346 sec/step)\n",
            "I0719 09:32:02.988158 140481689634688 learning.py:507] global step 2353: loss = 0.0836 (0.318 sec/step)\n",
            "I0719 09:32:03.304802 140481689634688 learning.py:507] global step 2354: loss = 0.3530 (0.315 sec/step)\n",
            "I0719 09:32:03.646044 140481689634688 learning.py:507] global step 2355: loss = 0.0646 (0.340 sec/step)\n",
            "I0719 09:32:03.983664 140481689634688 learning.py:507] global step 2356: loss = 0.1322 (0.336 sec/step)\n",
            "I0719 09:32:04.324871 140481689634688 learning.py:507] global step 2357: loss = 0.0749 (0.339 sec/step)\n",
            "I0719 09:32:04.619145 140481689634688 learning.py:507] global step 2358: loss = 0.0515 (0.292 sec/step)\n",
            "I0719 09:32:04.986758 140481689634688 learning.py:507] global step 2359: loss = 0.0909 (0.366 sec/step)\n",
            "I0719 09:32:05.304290 140481689634688 learning.py:507] global step 2360: loss = 0.1006 (0.316 sec/step)\n",
            "I0719 09:32:05.655598 140481689634688 learning.py:507] global step 2361: loss = 0.1349 (0.349 sec/step)\n",
            "I0719 09:32:05.983376 140481689634688 learning.py:507] global step 2362: loss = 0.0926 (0.326 sec/step)\n",
            "I0719 09:32:06.319697 140481689634688 learning.py:507] global step 2363: loss = 0.1163 (0.334 sec/step)\n",
            "I0719 09:32:06.667509 140481689634688 learning.py:507] global step 2364: loss = 0.2622 (0.346 sec/step)\n",
            "I0719 09:32:07.008158 140481689634688 learning.py:507] global step 2365: loss = 0.0898 (0.339 sec/step)\n",
            "I0719 09:32:07.376748 140481689634688 learning.py:507] global step 2366: loss = 0.1843 (0.367 sec/step)\n",
            "I0719 09:32:07.725188 140481689634688 learning.py:507] global step 2367: loss = 0.1430 (0.346 sec/step)\n",
            "I0719 09:32:08.083034 140481689634688 learning.py:507] global step 2368: loss = 0.0572 (0.356 sec/step)\n",
            "I0719 09:32:08.423987 140481689634688 learning.py:507] global step 2369: loss = 0.0916 (0.339 sec/step)\n",
            "I0719 09:32:08.773465 140481689634688 learning.py:507] global step 2370: loss = 0.1782 (0.348 sec/step)\n",
            "I0719 09:32:09.157108 140481689634688 learning.py:507] global step 2371: loss = 0.1559 (0.382 sec/step)\n",
            "I0719 09:32:09.501172 140481689634688 learning.py:507] global step 2372: loss = 0.2117 (0.343 sec/step)\n",
            "I0719 09:32:09.811962 140481689634688 learning.py:507] global step 2373: loss = 0.2806 (0.309 sec/step)\n",
            "I0719 09:32:10.116032 140481689634688 learning.py:507] global step 2374: loss = 0.0669 (0.302 sec/step)\n",
            "I0719 09:32:10.481399 140481689634688 learning.py:507] global step 2375: loss = 0.1411 (0.364 sec/step)\n",
            "I0719 09:32:10.848680 140481689634688 learning.py:507] global step 2376: loss = 0.1471 (0.365 sec/step)\n",
            "I0719 09:32:11.182902 140481689634688 learning.py:507] global step 2377: loss = 0.1332 (0.332 sec/step)\n",
            "I0719 09:32:11.520224 140481689634688 learning.py:507] global step 2378: loss = 0.2157 (0.335 sec/step)\n",
            "I0719 09:32:11.855413 140481689634688 learning.py:507] global step 2379: loss = 0.1276 (0.333 sec/step)\n",
            "I0719 09:32:12.169047 140481689634688 learning.py:507] global step 2380: loss = 0.1886 (0.312 sec/step)\n",
            "I0719 09:32:12.519719 140481689634688 learning.py:507] global step 2381: loss = 0.0580 (0.349 sec/step)\n",
            "I0719 09:32:12.846363 140481689634688 learning.py:507] global step 2382: loss = 0.0473 (0.325 sec/step)\n",
            "I0719 09:32:13.191365 140481689634688 learning.py:507] global step 2383: loss = 0.0857 (0.343 sec/step)\n",
            "I0719 09:32:13.534604 140481689634688 learning.py:507] global step 2384: loss = 0.1382 (0.341 sec/step)\n",
            "I0719 09:32:13.867172 140481689634688 learning.py:507] global step 2385: loss = 0.0326 (0.331 sec/step)\n",
            "I0719 09:32:14.154098 140481689634688 learning.py:507] global step 2386: loss = 0.0572 (0.285 sec/step)\n",
            "I0719 09:32:14.509504 140481689634688 learning.py:507] global step 2387: loss = 0.2135 (0.354 sec/step)\n",
            "I0719 09:32:14.858202 140481689634688 learning.py:507] global step 2388: loss = 0.1638 (0.347 sec/step)\n",
            "I0719 09:32:15.190093 140481689634688 learning.py:507] global step 2389: loss = 0.1254 (0.330 sec/step)\n",
            "I0719 09:32:15.548892 140481689634688 learning.py:507] global step 2390: loss = 0.0503 (0.357 sec/step)\n",
            "I0719 09:32:15.875710 140481689634688 learning.py:507] global step 2391: loss = 0.1419 (0.325 sec/step)\n",
            "I0719 09:32:16.227705 140481689634688 learning.py:507] global step 2392: loss = 0.1877 (0.350 sec/step)\n",
            "I0719 09:32:16.549286 140481689634688 learning.py:507] global step 2393: loss = 0.0905 (0.320 sec/step)\n",
            "I0719 09:32:16.886309 140481689634688 learning.py:507] global step 2394: loss = 0.0219 (0.334 sec/step)\n",
            "I0719 09:32:17.201222 140481689634688 learning.py:507] global step 2395: loss = 0.0695 (0.313 sec/step)\n",
            "I0719 09:32:17.545171 140481689634688 learning.py:507] global step 2396: loss = 0.1051 (0.342 sec/step)\n",
            "I0719 09:32:17.872723 140481689634688 learning.py:507] global step 2397: loss = 0.0943 (0.326 sec/step)\n",
            "I0719 09:32:18.176773 140481689634688 learning.py:507] global step 2398: loss = 0.0966 (0.302 sec/step)\n",
            "I0719 09:32:18.503098 140481689634688 learning.py:507] global step 2399: loss = 0.1533 (0.324 sec/step)\n",
            "I0719 09:32:18.830146 140481689634688 learning.py:507] global step 2400: loss = 0.1053 (0.323 sec/step)\n",
            "I0719 09:32:19.174112 140481689634688 learning.py:507] global step 2401: loss = 0.1343 (0.342 sec/step)\n",
            "I0719 09:32:19.525682 140481689634688 learning.py:507] global step 2402: loss = 0.1007 (0.350 sec/step)\n",
            "I0719 09:32:19.877813 140481689634688 learning.py:507] global step 2403: loss = 0.1219 (0.350 sec/step)\n",
            "I0719 09:32:20.225142 140481689634688 learning.py:507] global step 2404: loss = 0.1093 (0.345 sec/step)\n",
            "I0719 09:32:20.562064 140481689634688 learning.py:507] global step 2405: loss = 0.2577 (0.335 sec/step)\n",
            "I0719 09:32:20.888111 140481689634688 learning.py:507] global step 2406: loss = 0.1206 (0.324 sec/step)\n",
            "I0719 09:32:21.224298 140481689634688 learning.py:507] global step 2407: loss = 0.1118 (0.334 sec/step)\n",
            "I0719 09:32:21.540104 140481689634688 learning.py:507] global step 2408: loss = 0.0642 (0.314 sec/step)\n",
            "I0719 09:32:21.892483 140481689634688 learning.py:507] global step 2409: loss = 0.1030 (0.350 sec/step)\n",
            "I0719 09:32:22.225357 140481689634688 learning.py:507] global step 2410: loss = 0.1128 (0.330 sec/step)\n",
            "I0719 09:32:22.575680 140481689634688 learning.py:507] global step 2411: loss = 0.1698 (0.348 sec/step)\n",
            "I0719 09:32:22.921808 140481689634688 learning.py:507] global step 2412: loss = 0.1772 (0.344 sec/step)\n",
            "I0719 09:32:23.231443 140481689634688 learning.py:507] global step 2413: loss = 0.0695 (0.308 sec/step)\n",
            "I0719 09:32:23.562925 140481689634688 learning.py:507] global step 2414: loss = 0.1407 (0.329 sec/step)\n",
            "I0719 09:32:23.878115 140481689634688 learning.py:507] global step 2415: loss = 0.0743 (0.313 sec/step)\n",
            "I0719 09:32:24.219366 140481689634688 learning.py:507] global step 2416: loss = 0.2469 (0.339 sec/step)\n",
            "I0719 09:32:24.575313 140481689634688 learning.py:507] global step 2417: loss = 0.1000 (0.354 sec/step)\n",
            "I0719 09:32:24.894977 140481689634688 learning.py:507] global step 2418: loss = 0.0463 (0.318 sec/step)\n",
            "I0719 09:32:25.219502 140481689634688 learning.py:507] global step 2419: loss = 0.2161 (0.323 sec/step)\n",
            "I0719 09:32:25.556854 140481689634688 learning.py:507] global step 2420: loss = 0.2278 (0.335 sec/step)\n",
            "I0719 09:32:25.876358 140481689634688 learning.py:507] global step 2421: loss = 0.1980 (0.317 sec/step)\n",
            "I0719 09:32:26.398562 140481689634688 learning.py:507] global step 2422: loss = 0.1178 (0.477 sec/step)\n",
            "I0719 09:32:26.840026 140481689634688 learning.py:507] global step 2423: loss = 0.0284 (0.407 sec/step)\n",
            "I0719 09:32:27.236793 140481689634688 learning.py:507] global step 2424: loss = 0.1027 (0.393 sec/step)\n",
            "I0719 09:32:27.361656 140479018202880 supervisor.py:1050] Recording summary at step 2424.\n",
            "I0719 09:32:27.577660 140481689634688 learning.py:507] global step 2425: loss = 0.0535 (0.336 sec/step)\n",
            "I0719 09:32:27.661794 140479026595584 supervisor.py:1099] global_step/sec: 2.975\n",
            "I0719 09:32:27.923841 140481689634688 learning.py:507] global step 2426: loss = 0.1964 (0.344 sec/step)\n",
            "I0719 09:32:28.274555 140481689634688 learning.py:507] global step 2427: loss = 0.1307 (0.349 sec/step)\n",
            "I0719 09:32:28.603999 140481689634688 learning.py:507] global step 2428: loss = 0.1543 (0.328 sec/step)\n",
            "I0719 09:32:28.902054 140481689634688 learning.py:507] global step 2429: loss = 0.0654 (0.296 sec/step)\n",
            "I0719 09:32:29.247092 140481689634688 learning.py:507] global step 2430: loss = 0.0360 (0.343 sec/step)\n",
            "I0719 09:32:29.599530 140481689634688 learning.py:507] global step 2431: loss = 0.1267 (0.351 sec/step)\n",
            "I0719 09:32:29.884928 140481689634688 learning.py:507] global step 2432: loss = 0.1897 (0.279 sec/step)\n",
            "I0719 09:32:30.219955 140481689634688 learning.py:507] global step 2433: loss = 0.1315 (0.332 sec/step)\n",
            "I0719 09:32:30.566467 140481689634688 learning.py:507] global step 2434: loss = 0.1395 (0.345 sec/step)\n",
            "I0719 09:32:30.919188 140481689634688 learning.py:507] global step 2435: loss = 0.0777 (0.351 sec/step)\n",
            "I0719 09:32:31.271103 140481689634688 learning.py:507] global step 2436: loss = 0.0388 (0.350 sec/step)\n",
            "I0719 09:32:31.634659 140481689634688 learning.py:507] global step 2437: loss = 0.1138 (0.362 sec/step)\n",
            "I0719 09:32:31.979885 140481689634688 learning.py:507] global step 2438: loss = 0.0812 (0.343 sec/step)\n",
            "I0719 09:32:32.325505 140481689634688 learning.py:507] global step 2439: loss = 0.0596 (0.344 sec/step)\n",
            "I0719 09:32:32.674124 140481689634688 learning.py:507] global step 2440: loss = 0.1175 (0.347 sec/step)\n",
            "I0719 09:32:32.977337 140481689634688 learning.py:507] global step 2441: loss = 0.0518 (0.301 sec/step)\n",
            "I0719 09:32:33.302349 140481689634688 learning.py:507] global step 2442: loss = 0.0802 (0.323 sec/step)\n",
            "I0719 09:32:33.649996 140481689634688 learning.py:507] global step 2443: loss = 0.0770 (0.346 sec/step)\n",
            "I0719 09:32:34.009236 140481689634688 learning.py:507] global step 2444: loss = 0.1274 (0.357 sec/step)\n",
            "I0719 09:32:34.375126 140481689634688 learning.py:507] global step 2445: loss = 0.1202 (0.364 sec/step)\n",
            "I0719 09:32:34.715567 140481689634688 learning.py:507] global step 2446: loss = 0.0877 (0.339 sec/step)\n",
            "I0719 09:32:35.074479 140481689634688 learning.py:507] global step 2447: loss = 0.0958 (0.357 sec/step)\n",
            "I0719 09:32:35.415807 140481689634688 learning.py:507] global step 2448: loss = 0.1403 (0.340 sec/step)\n",
            "I0719 09:32:35.756411 140481689634688 learning.py:507] global step 2449: loss = 0.0950 (0.339 sec/step)\n",
            "I0719 09:32:36.094296 140481689634688 learning.py:507] global step 2450: loss = 0.1191 (0.336 sec/step)\n",
            "I0719 09:32:36.439571 140481689634688 learning.py:507] global step 2451: loss = 0.0643 (0.343 sec/step)\n",
            "I0719 09:32:36.772701 140481689634688 learning.py:507] global step 2452: loss = 0.0761 (0.331 sec/step)\n",
            "I0719 09:32:37.087913 140481689634688 learning.py:507] global step 2453: loss = 0.1801 (0.313 sec/step)\n",
            "I0719 09:32:37.375174 140481689634688 learning.py:507] global step 2454: loss = 0.1475 (0.286 sec/step)\n",
            "I0719 09:32:37.703516 140481689634688 learning.py:507] global step 2455: loss = 0.1293 (0.327 sec/step)\n",
            "I0719 09:32:38.039865 140481689634688 learning.py:507] global step 2456: loss = 0.1465 (0.333 sec/step)\n",
            "I0719 09:32:38.420677 140481689634688 learning.py:507] global step 2457: loss = 0.0691 (0.379 sec/step)\n",
            "I0719 09:32:38.806492 140481689634688 learning.py:507] global step 2458: loss = 0.0203 (0.384 sec/step)\n",
            "I0719 09:32:39.142425 140481689634688 learning.py:507] global step 2459: loss = 0.2196 (0.334 sec/step)\n",
            "I0719 09:32:39.464452 140481689634688 learning.py:507] global step 2460: loss = 0.0684 (0.320 sec/step)\n",
            "I0719 09:32:39.792481 140481689634688 learning.py:507] global step 2461: loss = 0.1839 (0.326 sec/step)\n",
            "I0719 09:32:40.125955 140481689634688 learning.py:507] global step 2462: loss = 0.0290 (0.332 sec/step)\n",
            "I0719 09:32:40.472422 140481689634688 learning.py:507] global step 2463: loss = 0.0238 (0.345 sec/step)\n",
            "I0719 09:32:40.804601 140481689634688 learning.py:507] global step 2464: loss = 0.0278 (0.330 sec/step)\n",
            "I0719 09:32:41.153109 140481689634688 learning.py:507] global step 2465: loss = 0.0693 (0.347 sec/step)\n",
            "I0719 09:32:41.501242 140481689634688 learning.py:507] global step 2466: loss = 0.1137 (0.346 sec/step)\n",
            "I0719 09:32:41.840095 140481689634688 learning.py:507] global step 2467: loss = 0.0199 (0.337 sec/step)\n",
            "I0719 09:32:42.179413 140481689634688 learning.py:507] global step 2468: loss = 0.1558 (0.338 sec/step)\n",
            "I0719 09:32:42.514964 140481689634688 learning.py:507] global step 2469: loss = 0.2759 (0.334 sec/step)\n",
            "I0719 09:32:42.858986 140481689634688 learning.py:507] global step 2470: loss = 0.0958 (0.342 sec/step)\n",
            "I0719 09:32:43.198297 140481689634688 learning.py:507] global step 2471: loss = 0.1601 (0.337 sec/step)\n",
            "I0719 09:32:43.492467 140481689634688 learning.py:507] global step 2472: loss = 0.2140 (0.292 sec/step)\n",
            "I0719 09:32:43.846455 140481689634688 learning.py:507] global step 2473: loss = 0.0649 (0.352 sec/step)\n",
            "I0719 09:32:44.197350 140481689634688 learning.py:507] global step 2474: loss = 0.1067 (0.349 sec/step)\n",
            "I0719 09:32:44.503053 140481689634688 learning.py:507] global step 2475: loss = 0.1063 (0.304 sec/step)\n",
            "I0719 09:32:44.853045 140481689634688 learning.py:507] global step 2476: loss = 0.2048 (0.348 sec/step)\n",
            "I0719 09:32:45.159616 140481689634688 learning.py:507] global step 2477: loss = 0.3682 (0.305 sec/step)\n",
            "I0719 09:32:45.514901 140481689634688 learning.py:507] global step 2478: loss = 0.2580 (0.353 sec/step)\n",
            "I0719 09:32:45.849039 140481689634688 learning.py:507] global step 2479: loss = 0.1153 (0.332 sec/step)\n",
            "I0719 09:32:46.133671 140481689634688 learning.py:507] global step 2480: loss = 0.1874 (0.283 sec/step)\n",
            "I0719 09:32:46.475214 140481689634688 learning.py:507] global step 2481: loss = 0.1724 (0.340 sec/step)\n",
            "I0719 09:32:46.770068 140481689634688 learning.py:507] global step 2482: loss = 0.1687 (0.293 sec/step)\n",
            "I0719 09:32:47.133630 140481689634688 learning.py:507] global step 2483: loss = 0.0936 (0.361 sec/step)\n",
            "I0719 09:32:47.477206 140481689634688 learning.py:507] global step 2484: loss = 0.1338 (0.342 sec/step)\n",
            "I0719 09:32:47.830104 140481689634688 learning.py:507] global step 2485: loss = 0.1531 (0.351 sec/step)\n",
            "I0719 09:32:48.161313 140481689634688 learning.py:507] global step 2486: loss = 0.0317 (0.329 sec/step)\n",
            "I0719 09:32:48.508072 140481689634688 learning.py:507] global step 2487: loss = 0.1205 (0.345 sec/step)\n",
            "I0719 09:32:48.859889 140481689634688 learning.py:507] global step 2488: loss = 0.0482 (0.350 sec/step)\n",
            "I0719 09:32:49.199900 140481689634688 learning.py:507] global step 2489: loss = 0.2728 (0.338 sec/step)\n",
            "I0719 09:32:49.547634 140481689634688 learning.py:507] global step 2490: loss = 0.2112 (0.345 sec/step)\n",
            "I0719 09:32:49.843358 140481689634688 learning.py:507] global step 2491: loss = 0.2039 (0.294 sec/step)\n",
            "I0719 09:32:50.131537 140481689634688 learning.py:507] global step 2492: loss = 0.1007 (0.287 sec/step)\n",
            "I0719 09:32:50.478155 140481689634688 learning.py:507] global step 2493: loss = 0.1516 (0.345 sec/step)\n",
            "I0719 09:32:50.818300 140481689634688 learning.py:507] global step 2494: loss = 0.1721 (0.338 sec/step)\n",
            "I0719 09:32:51.138058 140481689634688 learning.py:507] global step 2495: loss = 0.1584 (0.318 sec/step)\n",
            "I0719 09:32:51.489395 140481689634688 learning.py:507] global step 2496: loss = 0.1013 (0.349 sec/step)\n",
            "I0719 09:32:51.834836 140481689634688 learning.py:507] global step 2497: loss = 0.1484 (0.343 sec/step)\n",
            "I0719 09:32:52.165784 140481689634688 learning.py:507] global step 2498: loss = 0.0810 (0.329 sec/step)\n",
            "I0719 09:32:52.501423 140481689634688 learning.py:507] global step 2499: loss = 0.0970 (0.334 sec/step)\n",
            "I0719 09:32:52.869028 140481689634688 learning.py:507] global step 2500: loss = 0.1293 (0.366 sec/step)\n",
            "I0719 09:32:53.203873 140481689634688 learning.py:507] global step 2501: loss = 0.1955 (0.333 sec/step)\n",
            "I0719 09:32:53.537838 140481689634688 learning.py:507] global step 2502: loss = 0.3090 (0.332 sec/step)\n",
            "I0719 09:32:53.910043 140481689634688 learning.py:507] global step 2503: loss = 0.1036 (0.370 sec/step)\n",
            "I0719 09:32:54.232663 140481689634688 learning.py:507] global step 2504: loss = 0.1614 (0.321 sec/step)\n",
            "I0719 09:32:54.579998 140481689634688 learning.py:507] global step 2505: loss = 0.0786 (0.346 sec/step)\n",
            "I0719 09:32:54.922006 140481689634688 learning.py:507] global step 2506: loss = 0.0399 (0.340 sec/step)\n",
            "I0719 09:32:55.259938 140481689634688 learning.py:507] global step 2507: loss = 0.1344 (0.336 sec/step)\n",
            "I0719 09:32:55.613055 140481689634688 learning.py:507] global step 2508: loss = 0.0606 (0.351 sec/step)\n",
            "I0719 09:32:55.958648 140481689634688 learning.py:507] global step 2509: loss = 0.0930 (0.344 sec/step)\n",
            "I0719 09:32:56.299835 140481689634688 learning.py:507] global step 2510: loss = 0.1631 (0.339 sec/step)\n",
            "I0719 09:32:56.662131 140481689634688 learning.py:507] global step 2511: loss = 0.0606 (0.360 sec/step)\n",
            "I0719 09:32:56.991583 140481689634688 learning.py:507] global step 2512: loss = 0.1086 (0.327 sec/step)\n",
            "I0719 09:32:57.347533 140481689634688 learning.py:507] global step 2513: loss = 0.0788 (0.354 sec/step)\n",
            "I0719 09:32:57.685972 140481689634688 learning.py:507] global step 2514: loss = 0.1096 (0.337 sec/step)\n",
            "I0719 09:32:58.014698 140481689634688 learning.py:507] global step 2515: loss = 0.1020 (0.327 sec/step)\n",
            "I0719 09:32:58.309281 140481689634688 learning.py:507] global step 2516: loss = 0.1726 (0.293 sec/step)\n",
            "I0719 09:32:58.616387 140481689634688 learning.py:507] global step 2517: loss = 0.2521 (0.305 sec/step)\n",
            "I0719 09:32:58.953903 140481689634688 learning.py:507] global step 2518: loss = 0.1062 (0.336 sec/step)\n",
            "I0719 09:32:59.295724 140481689634688 learning.py:507] global step 2519: loss = 0.0197 (0.340 sec/step)\n",
            "I0719 09:32:59.651460 140481689634688 learning.py:507] global step 2520: loss = 0.1232 (0.354 sec/step)\n",
            "I0719 09:32:59.983321 140481689634688 learning.py:507] global step 2521: loss = 0.1582 (0.329 sec/step)\n",
            "I0719 09:33:00.339720 140481689634688 learning.py:507] global step 2522: loss = 0.1148 (0.354 sec/step)\n",
            "I0719 09:33:00.680432 140481689634688 learning.py:507] global step 2523: loss = 0.0997 (0.339 sec/step)\n",
            "I0719 09:33:00.975225 140481689634688 learning.py:507] global step 2524: loss = 0.1777 (0.293 sec/step)\n",
            "I0719 09:33:01.323024 140481689634688 learning.py:507] global step 2525: loss = 0.1447 (0.346 sec/step)\n",
            "I0719 09:33:01.659001 140481689634688 learning.py:507] global step 2526: loss = 0.0303 (0.334 sec/step)\n",
            "I0719 09:33:01.988355 140481689634688 learning.py:507] global step 2527: loss = 0.0852 (0.328 sec/step)\n",
            "I0719 09:33:02.340136 140481689634688 learning.py:507] global step 2528: loss = 0.0851 (0.350 sec/step)\n",
            "I0719 09:33:02.692832 140481689634688 learning.py:507] global step 2529: loss = 0.0709 (0.351 sec/step)\n",
            "I0719 09:33:03.011814 140481689634688 learning.py:507] global step 2530: loss = 0.1383 (0.317 sec/step)\n",
            "I0719 09:33:03.346462 140481689634688 learning.py:507] global step 2531: loss = 0.1677 (0.332 sec/step)\n",
            "I0719 09:33:03.681030 140481689634688 learning.py:507] global step 2532: loss = 0.0669 (0.333 sec/step)\n",
            "I0719 09:33:04.004505 140481689634688 learning.py:507] global step 2533: loss = 0.1415 (0.322 sec/step)\n",
            "I0719 09:33:04.367022 140481689634688 learning.py:507] global step 2534: loss = 0.0518 (0.361 sec/step)\n",
            "I0719 09:33:04.703053 140481689634688 learning.py:507] global step 2535: loss = 0.1602 (0.334 sec/step)\n",
            "I0719 09:33:05.031612 140481689634688 learning.py:507] global step 2536: loss = 0.1014 (0.327 sec/step)\n",
            "I0719 09:33:05.362157 140481689634688 learning.py:507] global step 2537: loss = 0.0860 (0.329 sec/step)\n",
            "I0719 09:33:05.687425 140481689634688 learning.py:507] global step 2538: loss = 0.1355 (0.324 sec/step)\n",
            "I0719 09:33:06.020195 140481689634688 learning.py:507] global step 2539: loss = 0.0895 (0.331 sec/step)\n",
            "I0719 09:33:06.330686 140481689634688 learning.py:507] global step 2540: loss = 0.0721 (0.309 sec/step)\n",
            "I0719 09:33:06.664223 140481689634688 learning.py:507] global step 2541: loss = 0.0848 (0.332 sec/step)\n",
            "I0719 09:33:07.002352 140481689634688 learning.py:507] global step 2542: loss = 0.1425 (0.336 sec/step)\n",
            "I0719 09:33:07.328072 140481689634688 learning.py:507] global step 2543: loss = 0.1152 (0.324 sec/step)\n",
            "I0719 09:33:07.651634 140481689634688 learning.py:507] global step 2544: loss = 0.2619 (0.322 sec/step)\n",
            "I0719 09:33:07.970342 140481689634688 learning.py:507] global step 2545: loss = 0.1519 (0.317 sec/step)\n",
            "I0719 09:33:08.292571 140481689634688 learning.py:507] global step 2546: loss = 0.0530 (0.320 sec/step)\n",
            "I0719 09:33:08.602361 140481689634688 learning.py:507] global step 2547: loss = 0.0358 (0.308 sec/step)\n",
            "I0719 09:33:08.958199 140481689634688 learning.py:507] global step 2548: loss = 0.1131 (0.354 sec/step)\n",
            "I0719 09:33:09.282238 140481689634688 learning.py:507] global step 2549: loss = 0.0981 (0.322 sec/step)\n",
            "I0719 09:33:09.625355 140481689634688 learning.py:507] global step 2550: loss = 0.1067 (0.341 sec/step)\n",
            "I0719 09:33:09.968604 140481689634688 learning.py:507] global step 2551: loss = 0.1478 (0.341 sec/step)\n",
            "I0719 09:33:10.306041 140481689634688 learning.py:507] global step 2552: loss = 0.1716 (0.336 sec/step)\n",
            "I0719 09:33:10.615451 140481689634688 learning.py:507] global step 2553: loss = 0.1159 (0.308 sec/step)\n",
            "I0719 09:33:10.898413 140481689634688 learning.py:507] global step 2554: loss = 0.0931 (0.281 sec/step)\n",
            "I0719 09:33:11.193913 140481689634688 learning.py:507] global step 2555: loss = 0.1009 (0.294 sec/step)\n",
            "I0719 09:33:11.526321 140481689634688 learning.py:507] global step 2556: loss = 0.1426 (0.331 sec/step)\n",
            "I0719 09:33:11.850240 140481689634688 learning.py:507] global step 2557: loss = 0.1033 (0.322 sec/step)\n",
            "I0719 09:33:12.173655 140481689634688 learning.py:507] global step 2558: loss = 0.1824 (0.322 sec/step)\n",
            "I0719 09:33:12.531818 140481689634688 learning.py:507] global step 2559: loss = 0.1301 (0.356 sec/step)\n",
            "I0719 09:33:12.839836 140481689634688 learning.py:507] global step 2560: loss = 0.0902 (0.306 sec/step)\n",
            "I0719 09:33:13.163393 140481689634688 learning.py:507] global step 2561: loss = 0.1000 (0.322 sec/step)\n",
            "I0719 09:33:13.507718 140481689634688 learning.py:507] global step 2562: loss = 0.0649 (0.343 sec/step)\n",
            "I0719 09:33:13.823742 140481689634688 learning.py:507] global step 2563: loss = 0.0263 (0.314 sec/step)\n",
            "I0719 09:33:14.154580 140481689634688 learning.py:507] global step 2564: loss = 0.1540 (0.329 sec/step)\n",
            "I0719 09:33:14.474999 140481689634688 learning.py:507] global step 2565: loss = 0.2009 (0.319 sec/step)\n",
            "I0719 09:33:14.751953 140481689634688 learning.py:507] global step 2566: loss = 0.1515 (0.275 sec/step)\n",
            "I0719 09:33:15.100440 140481689634688 learning.py:507] global step 2567: loss = 0.0995 (0.347 sec/step)\n",
            "I0719 09:33:15.423573 140481689634688 learning.py:507] global step 2568: loss = 0.1087 (0.321 sec/step)\n",
            "I0719 09:33:15.765632 140481689634688 learning.py:507] global step 2569: loss = 0.1084 (0.340 sec/step)\n",
            "I0719 09:33:16.113576 140481689634688 learning.py:507] global step 2570: loss = 0.1066 (0.346 sec/step)\n",
            "I0719 09:33:16.445726 140481689634688 learning.py:507] global step 2571: loss = 0.1217 (0.330 sec/step)\n",
            "I0719 09:33:16.784587 140481689634688 learning.py:507] global step 2572: loss = 0.1178 (0.337 sec/step)\n",
            "I0719 09:33:17.121393 140481689634688 learning.py:507] global step 2573: loss = 0.1752 (0.335 sec/step)\n",
            "I0719 09:33:17.407183 140481689634688 learning.py:507] global step 2574: loss = 0.1250 (0.284 sec/step)\n",
            "I0719 09:33:17.758468 140481689634688 learning.py:507] global step 2575: loss = 0.1417 (0.350 sec/step)\n",
            "I0719 09:33:18.086632 140481689634688 learning.py:507] global step 2576: loss = 0.0998 (0.327 sec/step)\n",
            "I0719 09:33:18.377323 140481689634688 learning.py:507] global step 2577: loss = 0.1138 (0.289 sec/step)\n",
            "I0719 09:33:18.722572 140481689634688 learning.py:507] global step 2578: loss = 0.1163 (0.343 sec/step)\n",
            "I0719 09:33:19.067473 140481689634688 learning.py:507] global step 2579: loss = 0.1416 (0.343 sec/step)\n",
            "I0719 09:33:19.402589 140481689634688 learning.py:507] global step 2580: loss = 0.1156 (0.333 sec/step)\n",
            "I0719 09:33:19.748716 140481689634688 learning.py:507] global step 2581: loss = 0.0885 (0.344 sec/step)\n",
            "I0719 09:33:20.104463 140481689634688 learning.py:507] global step 2582: loss = 0.0492 (0.354 sec/step)\n",
            "I0719 09:33:20.418614 140481689634688 learning.py:507] global step 2583: loss = 0.2251 (0.312 sec/step)\n",
            "I0719 09:33:20.774693 140481689634688 learning.py:507] global step 2584: loss = 0.0779 (0.355 sec/step)\n",
            "I0719 09:33:21.049954 140481689634688 learning.py:507] global step 2585: loss = 0.1297 (0.273 sec/step)\n",
            "I0719 09:33:21.403296 140481689634688 learning.py:507] global step 2586: loss = 0.1316 (0.351 sec/step)\n",
            "I0719 09:33:21.733164 140481689634688 learning.py:507] global step 2587: loss = 0.1222 (0.328 sec/step)\n",
            "I0719 09:33:22.053552 140481689634688 learning.py:507] global step 2588: loss = 0.0702 (0.319 sec/step)\n",
            "I0719 09:33:22.410987 140481689634688 learning.py:507] global step 2589: loss = 0.0628 (0.355 sec/step)\n",
            "I0719 09:33:22.775464 140481689634688 learning.py:507] global step 2590: loss = 0.1561 (0.362 sec/step)\n",
            "I0719 09:33:23.119243 140481689634688 learning.py:507] global step 2591: loss = 0.0344 (0.342 sec/step)\n",
            "I0719 09:33:23.413138 140481689634688 learning.py:507] global step 2592: loss = 0.1072 (0.292 sec/step)\n",
            "I0719 09:33:23.738698 140481689634688 learning.py:507] global step 2593: loss = 0.0382 (0.324 sec/step)\n",
            "I0719 09:33:24.080083 140481689634688 learning.py:507] global step 2594: loss = 0.0863 (0.340 sec/step)\n",
            "I0719 09:33:24.427699 140481689634688 learning.py:507] global step 2595: loss = 0.2678 (0.346 sec/step)\n",
            "I0719 09:33:24.777399 140481689634688 learning.py:507] global step 2596: loss = 0.0878 (0.347 sec/step)\n",
            "I0719 09:33:25.099999 140481689634688 learning.py:507] global step 2597: loss = 0.1865 (0.321 sec/step)\n",
            "I0719 09:33:25.411534 140481689634688 learning.py:507] global step 2598: loss = 0.1340 (0.310 sec/step)\n",
            "I0719 09:33:25.757376 140481689634688 learning.py:507] global step 2599: loss = 0.0231 (0.339 sec/step)\n",
            "I0719 09:33:26.106738 140481689634688 learning.py:507] global step 2600: loss = 0.0475 (0.347 sec/step)\n",
            "I0719 09:33:26.400144 140481689634688 learning.py:507] global step 2601: loss = 0.0267 (0.290 sec/step)\n",
            "I0719 09:33:26.749053 140481689634688 learning.py:507] global step 2602: loss = 0.1369 (0.344 sec/step)\n",
            "I0719 09:33:27.103283 140481689634688 learning.py:507] global step 2603: loss = 0.2830 (0.352 sec/step)\n",
            "I0719 09:33:27.430257 140481689634688 learning.py:507] global step 2604: loss = 0.1014 (0.325 sec/step)\n",
            "I0719 09:33:27.765743 140481689634688 learning.py:507] global step 2605: loss = 0.1525 (0.334 sec/step)\n",
            "I0719 09:33:28.077017 140481689634688 learning.py:507] global step 2606: loss = 0.1967 (0.309 sec/step)\n",
            "I0719 09:33:28.417675 140481689634688 learning.py:507] global step 2607: loss = 0.1275 (0.339 sec/step)\n",
            "I0719 09:33:28.744949 140481689634688 learning.py:507] global step 2608: loss = 0.0787 (0.326 sec/step)\n",
            "I0719 09:33:29.073028 140481689634688 learning.py:507] global step 2609: loss = 0.1010 (0.326 sec/step)\n",
            "I0719 09:33:29.408300 140481689634688 learning.py:507] global step 2610: loss = 0.0760 (0.333 sec/step)\n",
            "I0719 09:33:29.750536 140481689634688 learning.py:507] global step 2611: loss = 0.0966 (0.340 sec/step)\n",
            "I0719 09:33:30.107675 140481689634688 learning.py:507] global step 2612: loss = 0.0892 (0.355 sec/step)\n",
            "I0719 09:33:30.445956 140481689634688 learning.py:507] global step 2613: loss = 0.1402 (0.337 sec/step)\n",
            "I0719 09:33:30.787429 140481689634688 learning.py:507] global step 2614: loss = 0.0715 (0.340 sec/step)\n",
            "I0719 09:33:31.139549 140481689634688 learning.py:507] global step 2615: loss = 0.0286 (0.350 sec/step)\n",
            "I0719 09:33:31.525965 140481689634688 learning.py:507] global step 2616: loss = 0.0918 (0.385 sec/step)\n",
            "I0719 09:33:31.824401 140481689634688 learning.py:507] global step 2617: loss = 0.0793 (0.297 sec/step)\n",
            "I0719 09:33:32.147410 140481689634688 learning.py:507] global step 2618: loss = 0.1549 (0.321 sec/step)\n",
            "I0719 09:33:32.444391 140481689634688 learning.py:507] global step 2619: loss = 0.1543 (0.294 sec/step)\n",
            "I0719 09:33:32.790218 140481689634688 learning.py:507] global step 2620: loss = 0.1362 (0.344 sec/step)\n",
            "I0719 09:33:33.145190 140481689634688 learning.py:507] global step 2621: loss = 0.1994 (0.353 sec/step)\n",
            "I0719 09:33:33.492193 140481689634688 learning.py:507] global step 2622: loss = 0.1387 (0.345 sec/step)\n",
            "I0719 09:33:33.822907 140481689634688 learning.py:507] global step 2623: loss = 0.0937 (0.329 sec/step)\n",
            "I0719 09:33:34.138710 140481689634688 learning.py:507] global step 2624: loss = 0.0581 (0.314 sec/step)\n",
            "I0719 09:33:34.428594 140481689634688 learning.py:507] global step 2625: loss = 0.0361 (0.288 sec/step)\n",
            "I0719 09:33:34.731424 140481689634688 learning.py:507] global step 2626: loss = 0.3438 (0.301 sec/step)\n",
            "I0719 09:33:35.063153 140481689634688 learning.py:507] global step 2627: loss = 0.1039 (0.330 sec/step)\n",
            "I0719 09:33:35.365430 140481689634688 learning.py:507] global step 2628: loss = 0.1396 (0.301 sec/step)\n",
            "I0719 09:33:35.695935 140481689634688 learning.py:507] global step 2629: loss = 0.0390 (0.329 sec/step)\n",
            "I0719 09:33:35.985106 140481689634688 learning.py:507] global step 2630: loss = 0.0966 (0.287 sec/step)\n",
            "I0719 09:33:36.332088 140481689634688 learning.py:507] global step 2631: loss = 0.0676 (0.345 sec/step)\n",
            "I0719 09:33:36.670386 140481689634688 learning.py:507] global step 2632: loss = 0.1372 (0.337 sec/step)\n",
            "I0719 09:33:36.998306 140481689634688 learning.py:507] global step 2633: loss = 0.1542 (0.326 sec/step)\n",
            "I0719 09:33:37.333768 140481689634688 learning.py:507] global step 2634: loss = 0.1534 (0.334 sec/step)\n",
            "I0719 09:33:37.656034 140481689634688 learning.py:507] global step 2635: loss = 0.1087 (0.321 sec/step)\n",
            "I0719 09:33:37.992612 140481689634688 learning.py:507] global step 2636: loss = 0.1644 (0.335 sec/step)\n",
            "I0719 09:33:38.329875 140481689634688 learning.py:507] global step 2637: loss = 0.0413 (0.335 sec/step)\n",
            "I0719 09:33:38.651811 140481689634688 learning.py:507] global step 2638: loss = 0.0251 (0.320 sec/step)\n",
            "I0719 09:33:38.981869 140481689634688 learning.py:507] global step 2639: loss = 0.0268 (0.328 sec/step)\n",
            "I0719 09:33:39.325084 140481689634688 learning.py:507] global step 2640: loss = 0.1218 (0.342 sec/step)\n",
            "I0719 09:33:39.670160 140481689634688 learning.py:507] global step 2641: loss = 0.0419 (0.343 sec/step)\n",
            "I0719 09:33:40.016642 140481689634688 learning.py:507] global step 2642: loss = 0.0910 (0.345 sec/step)\n",
            "I0719 09:33:40.383878 140481689634688 learning.py:507] global step 2643: loss = 0.1675 (0.365 sec/step)\n",
            "I0719 09:33:40.672564 140481689634688 learning.py:507] global step 2644: loss = 0.0941 (0.287 sec/step)\n",
            "I0719 09:33:41.001472 140481689634688 learning.py:507] global step 2645: loss = 0.1556 (0.326 sec/step)\n",
            "I0719 09:33:41.306149 140481689634688 learning.py:507] global step 2646: loss = 0.0796 (0.303 sec/step)\n",
            "I0719 09:33:41.655251 140481689634688 learning.py:507] global step 2647: loss = 0.1321 (0.347 sec/step)\n",
            "I0719 09:33:42.001707 140481689634688 learning.py:507] global step 2648: loss = 0.1146 (0.345 sec/step)\n",
            "I0719 09:33:42.325812 140481689634688 learning.py:507] global step 2649: loss = 0.0827 (0.323 sec/step)\n",
            "I0719 09:33:42.639295 140481689634688 learning.py:507] global step 2650: loss = 0.2166 (0.312 sec/step)\n",
            "I0719 09:33:42.969520 140481689634688 learning.py:507] global step 2651: loss = 0.0720 (0.329 sec/step)\n",
            "I0719 09:33:43.326326 140481689634688 learning.py:507] global step 2652: loss = 0.1402 (0.355 sec/step)\n",
            "I0719 09:33:43.661368 140481689634688 learning.py:507] global step 2653: loss = 0.0456 (0.333 sec/step)\n",
            "I0719 09:33:44.018216 140481689634688 learning.py:507] global step 2654: loss = 0.1377 (0.355 sec/step)\n",
            "I0719 09:33:44.356652 140481689634688 learning.py:507] global step 2655: loss = 0.0348 (0.337 sec/step)\n",
            "I0719 09:33:44.700489 140481689634688 learning.py:507] global step 2656: loss = 0.1663 (0.342 sec/step)\n",
            "I0719 09:33:45.027933 140481689634688 learning.py:507] global step 2657: loss = 0.1947 (0.326 sec/step)\n",
            "I0719 09:33:45.364775 140481689634688 learning.py:507] global step 2658: loss = 0.0545 (0.335 sec/step)\n",
            "I0719 09:33:45.715388 140481689634688 learning.py:507] global step 2659: loss = 0.2245 (0.349 sec/step)\n",
            "I0719 09:33:46.045409 140481689634688 learning.py:507] global step 2660: loss = 0.2026 (0.328 sec/step)\n",
            "I0719 09:33:46.395335 140481689634688 learning.py:507] global step 2661: loss = 0.1076 (0.348 sec/step)\n",
            "I0719 09:33:46.719921 140481689634688 learning.py:507] global step 2662: loss = 0.1196 (0.323 sec/step)\n",
            "I0719 09:33:47.076238 140481689634688 learning.py:507] global step 2663: loss = 0.0551 (0.354 sec/step)\n",
            "I0719 09:33:47.446569 140481689634688 learning.py:507] global step 2664: loss = 0.0344 (0.369 sec/step)\n",
            "I0719 09:33:47.792336 140481689634688 learning.py:507] global step 2665: loss = 0.0860 (0.344 sec/step)\n",
            "I0719 09:33:48.120834 140481689634688 learning.py:507] global step 2666: loss = 0.2055 (0.327 sec/step)\n",
            "I0719 09:33:48.449835 140481689634688 learning.py:507] global step 2667: loss = 0.2066 (0.327 sec/step)\n",
            "I0719 09:33:48.813960 140481689634688 learning.py:507] global step 2668: loss = 0.1016 (0.362 sec/step)\n",
            "I0719 09:33:49.162946 140481689634688 learning.py:507] global step 2669: loss = 0.0380 (0.347 sec/step)\n",
            "I0719 09:33:49.499309 140481689634688 learning.py:507] global step 2670: loss = 0.1139 (0.334 sec/step)\n",
            "I0719 09:33:49.847088 140481689634688 learning.py:507] global step 2671: loss = 0.1450 (0.346 sec/step)\n",
            "I0719 09:33:50.195256 140481689634688 learning.py:507] global step 2672: loss = 0.0885 (0.346 sec/step)\n",
            "I0719 09:33:50.533429 140481689634688 learning.py:507] global step 2673: loss = 0.1310 (0.336 sec/step)\n",
            "I0719 09:33:50.856429 140481689634688 learning.py:507] global step 2674: loss = 0.0860 (0.321 sec/step)\n",
            "I0719 09:33:51.199988 140481689634688 learning.py:507] global step 2675: loss = 0.0980 (0.342 sec/step)\n",
            "I0719 09:33:51.519666 140481689634688 learning.py:507] global step 2676: loss = 0.1266 (0.318 sec/step)\n",
            "I0719 09:33:51.850836 140481689634688 learning.py:507] global step 2677: loss = 0.1311 (0.329 sec/step)\n",
            "I0719 09:33:52.185659 140481689634688 learning.py:507] global step 2678: loss = 0.2212 (0.333 sec/step)\n",
            "I0719 09:33:52.519526 140481689634688 learning.py:507] global step 2679: loss = 0.1158 (0.332 sec/step)\n",
            "I0719 09:33:52.854590 140481689634688 learning.py:507] global step 2680: loss = 0.2091 (0.333 sec/step)\n",
            "I0719 09:33:53.187510 140481689634688 learning.py:507] global step 2681: loss = 0.0478 (0.331 sec/step)\n",
            "I0719 09:33:53.543820 140481689634688 learning.py:507] global step 2682: loss = 0.0926 (0.354 sec/step)\n",
            "I0719 09:33:53.884072 140481689634688 learning.py:507] global step 2683: loss = 0.2604 (0.338 sec/step)\n",
            "I0719 09:33:54.218069 140481689634688 learning.py:507] global step 2684: loss = 0.1439 (0.332 sec/step)\n",
            "I0719 09:33:54.567316 140481689634688 learning.py:507] global step 2685: loss = 0.1079 (0.346 sec/step)\n",
            "I0719 09:33:54.904438 140481689634688 learning.py:507] global step 2686: loss = 0.0953 (0.331 sec/step)\n",
            "I0719 09:33:55.243123 140481689634688 learning.py:507] global step 2687: loss = 0.0242 (0.337 sec/step)\n",
            "I0719 09:33:55.588396 140481689634688 learning.py:507] global step 2688: loss = 0.0802 (0.344 sec/step)\n",
            "I0719 09:33:55.927914 140481689634688 learning.py:507] global step 2689: loss = 0.0931 (0.337 sec/step)\n",
            "I0719 09:33:56.278148 140481689634688 learning.py:507] global step 2690: loss = 0.1100 (0.348 sec/step)\n",
            "I0719 09:33:56.633553 140481689634688 learning.py:507] global step 2691: loss = 0.1695 (0.352 sec/step)\n",
            "I0719 09:33:56.976449 140481689634688 learning.py:507] global step 2692: loss = 0.1248 (0.341 sec/step)\n",
            "I0719 09:33:57.269645 140481689634688 learning.py:507] global step 2693: loss = 0.0843 (0.292 sec/step)\n",
            "I0719 09:33:57.557920 140481689634688 learning.py:507] global step 2694: loss = 0.0825 (0.287 sec/step)\n",
            "I0719 09:33:57.890151 140481689634688 learning.py:507] global step 2695: loss = 0.0660 (0.330 sec/step)\n",
            "I0719 09:33:58.229349 140481689634688 learning.py:507] global step 2696: loss = 0.1289 (0.338 sec/step)\n",
            "I0719 09:33:58.589749 140481689634688 learning.py:507] global step 2697: loss = 0.1697 (0.359 sec/step)\n",
            "I0719 09:33:58.900376 140481689634688 learning.py:507] global step 2698: loss = 0.0437 (0.309 sec/step)\n",
            "I0719 09:33:59.238045 140481689634688 learning.py:507] global step 2699: loss = 0.0848 (0.336 sec/step)\n",
            "I0719 09:33:59.557201 140481689634688 learning.py:507] global step 2700: loss = 0.1216 (0.317 sec/step)\n",
            "I0719 09:33:59.847630 140481689634688 learning.py:507] global step 2701: loss = 0.2225 (0.289 sec/step)\n",
            "I0719 09:34:00.188923 140481689634688 learning.py:507] global step 2702: loss = 0.0750 (0.339 sec/step)\n",
            "I0719 09:34:00.463814 140481689634688 learning.py:507] global step 2703: loss = 0.1713 (0.273 sec/step)\n",
            "I0719 09:34:00.806227 140481689634688 learning.py:507] global step 2704: loss = 0.1118 (0.341 sec/step)\n",
            "I0719 09:34:01.155933 140481689634688 learning.py:507] global step 2705: loss = 0.0523 (0.348 sec/step)\n",
            "I0719 09:34:01.464749 140481689634688 learning.py:507] global step 2706: loss = 0.0846 (0.307 sec/step)\n",
            "I0719 09:34:01.799982 140481689634688 learning.py:507] global step 2707: loss = 0.0917 (0.334 sec/step)\n",
            "I0719 09:34:02.153982 140481689634688 learning.py:507] global step 2708: loss = 0.0804 (0.352 sec/step)\n",
            "I0719 09:34:02.514079 140481689634688 learning.py:507] global step 2709: loss = 0.1428 (0.358 sec/step)\n",
            "I0719 09:34:02.848818 140481689634688 learning.py:507] global step 2710: loss = 0.0701 (0.333 sec/step)\n",
            "I0719 09:34:03.179300 140481689634688 learning.py:507] global step 2711: loss = 0.1207 (0.329 sec/step)\n",
            "I0719 09:34:03.525679 140481689634688 learning.py:507] global step 2712: loss = 0.1232 (0.344 sec/step)\n",
            "I0719 09:34:03.872665 140481689634688 learning.py:507] global step 2713: loss = 0.1178 (0.345 sec/step)\n",
            "I0719 09:34:04.219076 140481689634688 learning.py:507] global step 2714: loss = 0.0834 (0.345 sec/step)\n",
            "I0719 09:34:04.542652 140481689634688 learning.py:507] global step 2715: loss = 0.1325 (0.322 sec/step)\n",
            "I0719 09:34:04.889104 140481689634688 learning.py:507] global step 2716: loss = 0.3069 (0.345 sec/step)\n",
            "I0719 09:34:05.245836 140481689634688 learning.py:507] global step 2717: loss = 0.1662 (0.355 sec/step)\n",
            "I0719 09:34:05.610419 140481689634688 learning.py:507] global step 2718: loss = 0.1263 (0.363 sec/step)\n",
            "I0719 09:34:05.952011 140481689634688 learning.py:507] global step 2719: loss = 0.1372 (0.340 sec/step)\n",
            "I0719 09:34:06.301191 140481689634688 learning.py:507] global step 2720: loss = 0.1347 (0.348 sec/step)\n",
            "I0719 09:34:06.616832 140481689634688 learning.py:507] global step 2721: loss = 0.0680 (0.314 sec/step)\n",
            "I0719 09:34:06.962916 140481689634688 learning.py:507] global step 2722: loss = 0.0808 (0.344 sec/step)\n",
            "I0719 09:34:07.284568 140481689634688 learning.py:507] global step 2723: loss = 0.1440 (0.320 sec/step)\n",
            "I0719 09:34:07.561570 140481689634688 learning.py:507] global step 2724: loss = 0.1041 (0.275 sec/step)\n",
            "I0719 09:34:07.848819 140481689634688 learning.py:507] global step 2725: loss = 0.0867 (0.285 sec/step)\n",
            "I0719 09:34:08.215215 140481689634688 learning.py:507] global step 2726: loss = 0.0796 (0.363 sec/step)\n",
            "I0719 09:34:08.540802 140481689634688 learning.py:507] global step 2727: loss = 0.1138 (0.324 sec/step)\n",
            "I0719 09:34:08.881112 140481689634688 learning.py:507] global step 2728: loss = 0.2158 (0.338 sec/step)\n",
            "I0719 09:34:09.186475 140481689634688 learning.py:507] global step 2729: loss = 0.2319 (0.303 sec/step)\n",
            "I0719 09:34:09.495257 140481689634688 learning.py:507] global step 2730: loss = 0.0915 (0.307 sec/step)\n",
            "I0719 09:34:09.783332 140481689634688 learning.py:507] global step 2731: loss = 0.1554 (0.286 sec/step)\n",
            "I0719 09:34:10.116315 140481689634688 learning.py:507] global step 2732: loss = 0.0572 (0.331 sec/step)\n",
            "I0719 09:34:10.441423 140481689634688 learning.py:507] global step 2733: loss = 0.1222 (0.323 sec/step)\n",
            "I0719 09:34:10.720507 140481689634688 learning.py:507] global step 2734: loss = 0.0763 (0.277 sec/step)\n",
            "I0719 09:34:11.002947 140481689634688 learning.py:507] global step 2735: loss = 0.1380 (0.281 sec/step)\n",
            "I0719 09:34:11.348526 140481689634688 learning.py:507] global step 2736: loss = 0.1771 (0.344 sec/step)\n",
            "I0719 09:34:11.700244 140481689634688 learning.py:507] global step 2737: loss = 0.1011 (0.350 sec/step)\n",
            "I0719 09:34:12.045816 140481689634688 learning.py:507] global step 2738: loss = 0.0598 (0.344 sec/step)\n",
            "I0719 09:34:12.424036 140481689634688 learning.py:507] global step 2739: loss = 0.2042 (0.377 sec/step)\n",
            "I0719 09:34:12.750707 140481689634688 learning.py:507] global step 2740: loss = 0.1085 (0.325 sec/step)\n",
            "I0719 09:34:13.085790 140481689634688 learning.py:507] global step 2741: loss = 0.2296 (0.333 sec/step)\n",
            "I0719 09:34:13.427527 140481689634688 learning.py:507] global step 2742: loss = 0.2098 (0.340 sec/step)\n",
            "I0719 09:34:13.749923 140481689634688 learning.py:507] global step 2743: loss = 0.1806 (0.321 sec/step)\n",
            "I0719 09:34:14.110042 140481689634688 learning.py:507] global step 2744: loss = 0.0383 (0.358 sec/step)\n",
            "I0719 09:34:14.456531 140481689634688 learning.py:507] global step 2745: loss = 0.0777 (0.345 sec/step)\n",
            "I0719 09:34:14.777336 140481689634688 learning.py:507] global step 2746: loss = 0.0923 (0.319 sec/step)\n",
            "I0719 09:34:15.110112 140481689634688 learning.py:507] global step 2747: loss = 0.1149 (0.331 sec/step)\n",
            "I0719 09:34:15.467488 140481689634688 learning.py:507] global step 2748: loss = 0.0668 (0.356 sec/step)\n",
            "I0719 09:34:15.810101 140481689634688 learning.py:507] global step 2749: loss = 0.0782 (0.341 sec/step)\n",
            "I0719 09:34:16.164172 140481689634688 learning.py:507] global step 2750: loss = 0.0918 (0.352 sec/step)\n",
            "I0719 09:34:16.506704 140481689634688 learning.py:507] global step 2751: loss = 0.1109 (0.341 sec/step)\n",
            "I0719 09:34:16.819502 140481689634688 learning.py:507] global step 2752: loss = 0.1081 (0.311 sec/step)\n",
            "I0719 09:34:17.151744 140481689634688 learning.py:507] global step 2753: loss = 0.0496 (0.330 sec/step)\n",
            "I0719 09:34:17.451246 140481689634688 learning.py:507] global step 2754: loss = 0.1906 (0.298 sec/step)\n",
            "I0719 09:34:17.794078 140481689634688 learning.py:507] global step 2755: loss = 0.0530 (0.336 sec/step)\n",
            "I0719 09:34:18.140162 140481689634688 learning.py:507] global step 2756: loss = 0.0548 (0.344 sec/step)\n",
            "I0719 09:34:18.429906 140481689634688 learning.py:507] global step 2757: loss = 0.0351 (0.288 sec/step)\n",
            "I0719 09:34:18.760608 140481689634688 learning.py:507] global step 2758: loss = 0.1086 (0.329 sec/step)\n",
            "I0719 09:34:19.086597 140481689634688 learning.py:507] global step 2759: loss = 0.0408 (0.324 sec/step)\n",
            "I0719 09:34:19.415738 140481689634688 learning.py:507] global step 2760: loss = 0.0230 (0.327 sec/step)\n",
            "I0719 09:34:19.767382 140481689634688 learning.py:507] global step 2761: loss = 0.1398 (0.350 sec/step)\n",
            "I0719 09:34:20.100886 140481689634688 learning.py:507] global step 2762: loss = 0.1664 (0.331 sec/step)\n",
            "I0719 09:34:20.459254 140481689634688 learning.py:507] global step 2763: loss = 0.2154 (0.356 sec/step)\n",
            "I0719 09:34:20.794982 140481689634688 learning.py:507] global step 2764: loss = 0.1310 (0.334 sec/step)\n",
            "I0719 09:34:21.118613 140481689634688 learning.py:507] global step 2765: loss = 0.1115 (0.322 sec/step)\n",
            "I0719 09:34:21.462433 140481689634688 learning.py:507] global step 2766: loss = 0.0466 (0.342 sec/step)\n",
            "I0719 09:34:21.815548 140481689634688 learning.py:507] global step 2767: loss = 0.0750 (0.351 sec/step)\n",
            "I0719 09:34:22.110424 140481689634688 learning.py:507] global step 2768: loss = 0.0399 (0.293 sec/step)\n",
            "I0719 09:34:22.447427 140481689634688 learning.py:507] global step 2769: loss = 0.0494 (0.335 sec/step)\n",
            "I0719 09:34:22.760964 140481689634688 learning.py:507] global step 2770: loss = 0.0213 (0.311 sec/step)\n",
            "I0719 09:34:23.081132 140481689634688 learning.py:507] global step 2771: loss = 0.0161 (0.318 sec/step)\n",
            "I0719 09:34:23.418724 140481689634688 learning.py:507] global step 2772: loss = 0.2477 (0.336 sec/step)\n",
            "I0719 09:34:23.771966 140481689634688 learning.py:507] global step 2773: loss = 0.1741 (0.352 sec/step)\n",
            "I0719 09:34:24.080473 140481689634688 learning.py:507] global step 2774: loss = 0.1481 (0.307 sec/step)\n",
            "I0719 09:34:24.423188 140481689634688 learning.py:507] global step 2775: loss = 0.1063 (0.340 sec/step)\n",
            "I0719 09:34:24.776187 140481689634688 learning.py:507] global step 2776: loss = 0.0274 (0.350 sec/step)\n",
            "I0719 09:34:25.066985 140481689634688 learning.py:507] global step 2777: loss = 0.0651 (0.289 sec/step)\n",
            "I0719 09:34:25.422531 140481689634688 learning.py:507] global step 2778: loss = 0.0595 (0.354 sec/step)\n",
            "I0719 09:34:25.757604 140481689634688 learning.py:507] global step 2779: loss = 0.0717 (0.333 sec/step)\n",
            "I0719 09:34:26.187625 140481689634688 learning.py:507] global step 2780: loss = 0.1045 (0.377 sec/step)\n",
            "I0719 09:34:26.767636 140481689634688 learning.py:507] global step 2781: loss = 0.0251 (0.547 sec/step)\n",
            "I0719 09:34:27.150010 140481689634688 learning.py:507] global step 2782: loss = 0.1675 (0.358 sec/step)\n",
            "I0719 09:34:27.328168 140479018202880 supervisor.py:1050] Recording summary at step 2782.\n",
            "I0719 09:34:27.528458 140481689634688 learning.py:507] global step 2783: loss = 0.1023 (0.377 sec/step)\n",
            "I0719 09:34:27.664530 140479026595584 supervisor.py:1099] global_step/sec: 2.98327\n",
            "I0719 09:34:27.898475 140481689634688 learning.py:507] global step 2784: loss = 0.1486 (0.368 sec/step)\n",
            "I0719 09:34:28.224427 140481689634688 learning.py:507] global step 2785: loss = 0.0260 (0.324 sec/step)\n",
            "I0719 09:34:28.562642 140481689634688 learning.py:507] global step 2786: loss = 0.1174 (0.337 sec/step)\n",
            "I0719 09:34:28.904611 140481689634688 learning.py:507] global step 2787: loss = 0.0888 (0.340 sec/step)\n",
            "I0719 09:34:29.242146 140481689634688 learning.py:507] global step 2788: loss = 0.1566 (0.335 sec/step)\n",
            "I0719 09:34:29.582739 140481689634688 learning.py:507] global step 2789: loss = 0.1199 (0.339 sec/step)\n",
            "I0719 09:34:29.932460 140481689634688 learning.py:507] global step 2790: loss = 0.0333 (0.348 sec/step)\n",
            "I0719 09:34:30.264100 140481689634688 learning.py:507] global step 2791: loss = 0.0301 (0.330 sec/step)\n",
            "I0719 09:34:30.594802 140481689634688 learning.py:507] global step 2792: loss = 0.1901 (0.328 sec/step)\n",
            "I0719 09:34:30.930863 140481689634688 learning.py:507] global step 2793: loss = 0.0271 (0.333 sec/step)\n",
            "I0719 09:34:31.257728 140481689634688 learning.py:507] global step 2794: loss = 0.1213 (0.325 sec/step)\n",
            "I0719 09:34:31.563937 140481689634688 learning.py:507] global step 2795: loss = 0.2422 (0.304 sec/step)\n",
            "I0719 09:34:31.872559 140481689634688 learning.py:507] global step 2796: loss = 0.0610 (0.307 sec/step)\n",
            "I0719 09:34:32.216094 140481689634688 learning.py:507] global step 2797: loss = 0.0889 (0.342 sec/step)\n",
            "I0719 09:34:32.550819 140481689634688 learning.py:507] global step 2798: loss = 0.1109 (0.333 sec/step)\n",
            "I0719 09:34:32.885326 140481689634688 learning.py:507] global step 2799: loss = 0.1758 (0.333 sec/step)\n",
            "I0719 09:34:33.182188 140481689634688 learning.py:507] global step 2800: loss = 0.0774 (0.295 sec/step)\n",
            "I0719 09:34:33.522320 140481689634688 learning.py:507] global step 2801: loss = 0.2507 (0.338 sec/step)\n",
            "I0719 09:34:33.852150 140481689634688 learning.py:507] global step 2802: loss = 0.0877 (0.328 sec/step)\n",
            "I0719 09:34:34.187366 140481689634688 learning.py:507] global step 2803: loss = 0.0187 (0.333 sec/step)\n",
            "I0719 09:34:34.542248 140481689634688 learning.py:507] global step 2804: loss = 0.0617 (0.353 sec/step)\n",
            "I0719 09:34:34.883743 140481689634688 learning.py:507] global step 2805: loss = 0.1029 (0.340 sec/step)\n",
            "I0719 09:34:35.219062 140481689634688 learning.py:507] global step 2806: loss = 0.0985 (0.333 sec/step)\n",
            "I0719 09:34:35.559942 140481689634688 learning.py:507] global step 2807: loss = 0.1166 (0.339 sec/step)\n",
            "I0719 09:34:35.908107 140481689634688 learning.py:507] global step 2808: loss = 0.0615 (0.346 sec/step)\n",
            "I0719 09:34:36.259223 140481689634688 learning.py:507] global step 2809: loss = 0.0350 (0.349 sec/step)\n",
            "I0719 09:34:36.634158 140481689634688 learning.py:507] global step 2810: loss = 0.0919 (0.372 sec/step)\n",
            "I0719 09:34:36.968609 140481689634688 learning.py:507] global step 2811: loss = 0.0626 (0.333 sec/step)\n",
            "I0719 09:34:37.294928 140481689634688 learning.py:507] global step 2812: loss = 0.1261 (0.324 sec/step)\n",
            "I0719 09:34:37.647893 140481689634688 learning.py:507] global step 2813: loss = 0.0631 (0.351 sec/step)\n",
            "I0719 09:34:37.971938 140481689634688 learning.py:507] global step 2814: loss = 0.1259 (0.322 sec/step)\n",
            "I0719 09:34:38.276090 140481689634688 learning.py:507] global step 2815: loss = 0.1828 (0.302 sec/step)\n",
            "I0719 09:34:38.638339 140481689634688 learning.py:507] global step 2816: loss = 0.0717 (0.360 sec/step)\n",
            "I0719 09:34:38.981880 140481689634688 learning.py:507] global step 2817: loss = 0.1756 (0.342 sec/step)\n",
            "I0719 09:34:39.309260 140481689634688 learning.py:507] global step 2818: loss = 0.0567 (0.325 sec/step)\n",
            "I0719 09:34:39.651223 140481689634688 learning.py:507] global step 2819: loss = 0.1026 (0.340 sec/step)\n",
            "I0719 09:34:39.989557 140481689634688 learning.py:507] global step 2820: loss = 0.1156 (0.337 sec/step)\n",
            "I0719 09:34:40.310482 140481689634688 learning.py:507] global step 2821: loss = 0.1144 (0.319 sec/step)\n",
            "I0719 09:34:40.661595 140481689634688 learning.py:507] global step 2822: loss = 0.1289 (0.349 sec/step)\n",
            "I0719 09:34:41.024583 140481689634688 learning.py:507] global step 2823: loss = 0.0487 (0.361 sec/step)\n",
            "I0719 09:34:41.357141 140481689634688 learning.py:507] global step 2824: loss = 0.1222 (0.331 sec/step)\n",
            "I0719 09:34:41.690422 140481689634688 learning.py:507] global step 2825: loss = 0.1429 (0.332 sec/step)\n",
            "I0719 09:34:42.052671 140481689634688 learning.py:507] global step 2826: loss = 0.0512 (0.360 sec/step)\n",
            "I0719 09:34:42.376191 140481689634688 learning.py:507] global step 2827: loss = 0.2124 (0.322 sec/step)\n",
            "I0719 09:34:42.721333 140481689634688 learning.py:507] global step 2828: loss = 0.0359 (0.343 sec/step)\n",
            "I0719 09:34:43.050586 140481689634688 learning.py:507] global step 2829: loss = 0.1091 (0.328 sec/step)\n",
            "I0719 09:34:43.380250 140481689634688 learning.py:507] global step 2830: loss = 0.3406 (0.328 sec/step)\n",
            "I0719 09:34:43.705442 140481689634688 learning.py:507] global step 2831: loss = 0.2624 (0.323 sec/step)\n",
            "I0719 09:34:44.050358 140481689634688 learning.py:507] global step 2832: loss = 0.1112 (0.343 sec/step)\n",
            "I0719 09:34:44.384533 140481689634688 learning.py:507] global step 2833: loss = 0.1279 (0.332 sec/step)\n",
            "I0719 09:34:44.685137 140481689634688 learning.py:507] global step 2834: loss = 0.0504 (0.299 sec/step)\n",
            "I0719 09:34:45.030998 140481689634688 learning.py:507] global step 2835: loss = 0.1169 (0.344 sec/step)\n",
            "I0719 09:34:45.372657 140481689634688 learning.py:507] global step 2836: loss = 0.1647 (0.340 sec/step)\n",
            "I0719 09:34:45.741199 140481689634688 learning.py:507] global step 2837: loss = 0.0774 (0.367 sec/step)\n",
            "I0719 09:34:46.148445 140481689634688 learning.py:507] global step 2838: loss = 0.0241 (0.406 sec/step)\n",
            "I0719 09:34:46.483713 140481689634688 learning.py:507] global step 2839: loss = 0.1353 (0.334 sec/step)\n",
            "I0719 09:34:46.833412 140481689634688 learning.py:507] global step 2840: loss = 0.1676 (0.348 sec/step)\n",
            "I0719 09:34:47.137660 140481689634688 learning.py:507] global step 2841: loss = 0.1582 (0.303 sec/step)\n",
            "I0719 09:34:47.473413 140481689634688 learning.py:507] global step 2842: loss = 0.1424 (0.331 sec/step)\n",
            "I0719 09:34:47.768525 140481689634688 learning.py:507] global step 2843: loss = 0.0895 (0.292 sec/step)\n",
            "I0719 09:34:48.098710 140481689634688 learning.py:507] global step 2844: loss = 0.2108 (0.328 sec/step)\n",
            "I0719 09:34:48.459647 140481689634688 learning.py:507] global step 2845: loss = 0.1018 (0.359 sec/step)\n",
            "I0719 09:34:48.809232 140481689634688 learning.py:507] global step 2846: loss = 0.1221 (0.348 sec/step)\n",
            "I0719 09:34:49.153981 140481689634688 learning.py:507] global step 2847: loss = 0.2490 (0.343 sec/step)\n",
            "I0719 09:34:49.438219 140481689634688 learning.py:507] global step 2848: loss = 0.1239 (0.282 sec/step)\n",
            "I0719 09:34:49.770378 140481689634688 learning.py:507] global step 2849: loss = 0.1353 (0.330 sec/step)\n",
            "I0719 09:34:50.099710 140481689634688 learning.py:507] global step 2850: loss = 0.1991 (0.327 sec/step)\n",
            "I0719 09:34:50.433341 140481689634688 learning.py:507] global step 2851: loss = 0.1133 (0.331 sec/step)\n",
            "I0719 09:34:50.777669 140481689634688 learning.py:507] global step 2852: loss = 0.1622 (0.343 sec/step)\n",
            "I0719 09:34:51.100661 140481689634688 learning.py:507] global step 2853: loss = 0.0336 (0.321 sec/step)\n",
            "I0719 09:34:51.453427 140481689634688 learning.py:507] global step 2854: loss = 0.1475 (0.351 sec/step)\n",
            "I0719 09:34:51.778770 140481689634688 learning.py:507] global step 2855: loss = 0.0790 (0.323 sec/step)\n",
            "I0719 09:34:52.075927 140481689634688 learning.py:507] global step 2856: loss = 0.0399 (0.295 sec/step)\n",
            "I0719 09:34:52.403845 140481689634688 learning.py:507] global step 2857: loss = 0.1515 (0.326 sec/step)\n",
            "I0719 09:34:52.726177 140481689634688 learning.py:507] global step 2858: loss = 0.0617 (0.321 sec/step)\n",
            "I0719 09:34:53.056067 140481689634688 learning.py:507] global step 2859: loss = 0.0685 (0.328 sec/step)\n",
            "I0719 09:34:53.400382 140481689634688 learning.py:507] global step 2860: loss = 0.2000 (0.342 sec/step)\n",
            "I0719 09:34:53.733537 140481689634688 learning.py:507] global step 2861: loss = 0.0774 (0.331 sec/step)\n",
            "I0719 09:34:54.047503 140481689634688 learning.py:507] global step 2862: loss = 0.0724 (0.312 sec/step)\n",
            "I0719 09:34:54.378793 140481689634688 learning.py:507] global step 2863: loss = 0.1175 (0.330 sec/step)\n",
            "I0719 09:34:54.722090 140481689634688 learning.py:507] global step 2864: loss = 0.1603 (0.342 sec/step)\n",
            "I0719 09:34:55.041687 140481689634688 learning.py:507] global step 2865: loss = 0.0946 (0.318 sec/step)\n",
            "I0719 09:34:55.382958 140481689634688 learning.py:507] global step 2866: loss = 0.0680 (0.339 sec/step)\n",
            "I0719 09:34:55.730064 140481689634688 learning.py:507] global step 2867: loss = 0.0804 (0.345 sec/step)\n",
            "I0719 09:34:56.072384 140481689634688 learning.py:507] global step 2868: loss = 0.0340 (0.341 sec/step)\n",
            "I0719 09:34:56.438634 140481689634688 learning.py:507] global step 2869: loss = 0.1250 (0.365 sec/step)\n",
            "I0719 09:34:56.774182 140481689634688 learning.py:507] global step 2870: loss = 0.1243 (0.334 sec/step)\n",
            "I0719 09:34:57.140656 140481689634688 learning.py:507] global step 2871: loss = 0.1156 (0.365 sec/step)\n",
            "I0719 09:34:57.481779 140481689634688 learning.py:507] global step 2872: loss = 0.1489 (0.339 sec/step)\n",
            "I0719 09:34:57.828809 140481689634688 learning.py:507] global step 2873: loss = 0.1715 (0.345 sec/step)\n",
            "I0719 09:34:58.156678 140481689634688 learning.py:507] global step 2874: loss = 0.0444 (0.326 sec/step)\n",
            "I0719 09:34:58.524454 140481689634688 learning.py:507] global step 2875: loss = 0.0798 (0.365 sec/step)\n",
            "I0719 09:34:58.857330 140481689634688 learning.py:507] global step 2876: loss = 0.1463 (0.331 sec/step)\n",
            "I0719 09:34:59.216186 140481689634688 learning.py:507] global step 2877: loss = 0.0837 (0.357 sec/step)\n",
            "I0719 09:34:59.552127 140481689634688 learning.py:507] global step 2878: loss = 0.1328 (0.334 sec/step)\n",
            "I0719 09:34:59.902479 140481689634688 learning.py:507] global step 2879: loss = 0.0464 (0.348 sec/step)\n",
            "I0719 09:35:00.213695 140481689634688 learning.py:507] global step 2880: loss = 0.0870 (0.309 sec/step)\n",
            "I0719 09:35:00.562017 140481689634688 learning.py:507] global step 2881: loss = 0.0241 (0.347 sec/step)\n",
            "I0719 09:35:00.860022 140481689634688 learning.py:507] global step 2882: loss = 0.0738 (0.296 sec/step)\n",
            "I0719 09:35:01.191083 140481689634688 learning.py:507] global step 2883: loss = 0.2343 (0.329 sec/step)\n",
            "I0719 09:35:01.522364 140481689634688 learning.py:507] global step 2884: loss = 0.0977 (0.330 sec/step)\n",
            "I0719 09:35:01.851638 140481689634688 learning.py:507] global step 2885: loss = 0.0258 (0.327 sec/step)\n",
            "I0719 09:35:02.179357 140481689634688 learning.py:507] global step 2886: loss = 0.1258 (0.326 sec/step)\n",
            "I0719 09:35:02.504407 140481689634688 learning.py:507] global step 2887: loss = 0.1072 (0.322 sec/step)\n",
            "I0719 09:35:02.853725 140481689634688 learning.py:507] global step 2888: loss = 0.1725 (0.347 sec/step)\n",
            "I0719 09:35:03.187626 140481689634688 learning.py:507] global step 2889: loss = 0.1226 (0.332 sec/step)\n",
            "I0719 09:35:03.552513 140481689634688 learning.py:507] global step 2890: loss = 0.1360 (0.363 sec/step)\n",
            "I0719 09:35:03.878586 140481689634688 learning.py:507] global step 2891: loss = 0.1612 (0.324 sec/step)\n",
            "I0719 09:35:04.164890 140481689634688 learning.py:507] global step 2892: loss = 0.0467 (0.284 sec/step)\n",
            "I0719 09:35:04.524096 140481689634688 learning.py:507] global step 2893: loss = 0.0761 (0.357 sec/step)\n",
            "I0719 09:35:04.887127 140481689634688 learning.py:507] global step 2894: loss = 0.1204 (0.361 sec/step)\n",
            "I0719 09:35:05.187635 140481689634688 learning.py:507] global step 2895: loss = 0.0802 (0.299 sec/step)\n",
            "I0719 09:35:05.511889 140481689634688 learning.py:507] global step 2896: loss = 0.2701 (0.322 sec/step)\n",
            "I0719 09:35:05.864169 140481689634688 learning.py:507] global step 2897: loss = 0.1255 (0.350 sec/step)\n",
            "I0719 09:35:06.189605 140481689634688 learning.py:507] global step 2898: loss = 0.1004 (0.324 sec/step)\n",
            "I0719 09:35:06.540605 140481689634688 learning.py:507] global step 2899: loss = 0.1224 (0.349 sec/step)\n",
            "I0719 09:35:06.871155 140481689634688 learning.py:507] global step 2900: loss = 0.0867 (0.329 sec/step)\n",
            "I0719 09:35:07.229774 140481689634688 learning.py:507] global step 2901: loss = 0.1875 (0.357 sec/step)\n",
            "I0719 09:35:07.579372 140481689634688 learning.py:507] global step 2902: loss = 0.0605 (0.348 sec/step)\n",
            "I0719 09:35:07.905568 140481689634688 learning.py:507] global step 2903: loss = 0.0950 (0.324 sec/step)\n",
            "I0719 09:35:08.254045 140481689634688 learning.py:507] global step 2904: loss = 0.1234 (0.347 sec/step)\n",
            "I0719 09:35:08.576466 140481689634688 learning.py:507] global step 2905: loss = 0.2471 (0.321 sec/step)\n",
            "I0719 09:35:08.925241 140481689634688 learning.py:507] global step 2906: loss = 0.0725 (0.347 sec/step)\n",
            "I0719 09:35:09.282031 140481689634688 learning.py:507] global step 2907: loss = 0.1015 (0.353 sec/step)\n",
            "I0719 09:35:09.614334 140481689634688 learning.py:507] global step 2908: loss = 0.1474 (0.330 sec/step)\n",
            "I0719 09:35:09.910892 140481689634688 learning.py:507] global step 2909: loss = 0.1154 (0.295 sec/step)\n",
            "I0719 09:35:10.256217 140481689634688 learning.py:507] global step 2910: loss = 0.0255 (0.344 sec/step)\n",
            "I0719 09:35:10.589463 140481689634688 learning.py:507] global step 2911: loss = 0.0344 (0.332 sec/step)\n",
            "I0719 09:35:10.900559 140481689634688 learning.py:507] global step 2912: loss = 0.1062 (0.309 sec/step)\n",
            "I0719 09:35:11.232944 140481689634688 learning.py:507] global step 2913: loss = 0.1648 (0.331 sec/step)\n",
            "I0719 09:35:11.558217 140481689634688 learning.py:507] global step 2914: loss = 0.1494 (0.323 sec/step)\n",
            "I0719 09:35:11.899354 140481689634688 learning.py:507] global step 2915: loss = 0.1170 (0.339 sec/step)\n",
            "I0719 09:35:12.217722 140481689634688 learning.py:507] global step 2916: loss = 0.1459 (0.317 sec/step)\n",
            "I0719 09:35:12.561465 140481689634688 learning.py:507] global step 2917: loss = 0.2492 (0.342 sec/step)\n",
            "I0719 09:35:12.910677 140481689634688 learning.py:507] global step 2918: loss = 0.1318 (0.347 sec/step)\n",
            "I0719 09:35:13.248054 140481689634688 learning.py:507] global step 2919: loss = 0.0535 (0.336 sec/step)\n",
            "I0719 09:35:13.600810 140481689634688 learning.py:507] global step 2920: loss = 0.1052 (0.351 sec/step)\n",
            "I0719 09:35:13.898593 140481689634688 learning.py:507] global step 2921: loss = 0.0484 (0.296 sec/step)\n",
            "I0719 09:35:14.221975 140481689634688 learning.py:507] global step 2922: loss = 0.1102 (0.322 sec/step)\n",
            "I0719 09:35:14.523428 140481689634688 learning.py:507] global step 2923: loss = 0.0736 (0.300 sec/step)\n",
            "I0719 09:35:14.867738 140481689634688 learning.py:507] global step 2924: loss = 0.0452 (0.342 sec/step)\n",
            "I0719 09:35:15.172679 140481689634688 learning.py:507] global step 2925: loss = 0.0679 (0.303 sec/step)\n",
            "I0719 09:35:15.467357 140481689634688 learning.py:507] global step 2926: loss = 0.1238 (0.293 sec/step)\n",
            "I0719 09:35:15.810652 140481689634688 learning.py:507] global step 2927: loss = 0.1099 (0.342 sec/step)\n",
            "I0719 09:35:16.112848 140481689634688 learning.py:507] global step 2928: loss = 0.2541 (0.300 sec/step)\n",
            "I0719 09:35:16.442827 140481689634688 learning.py:507] global step 2929: loss = 0.1105 (0.328 sec/step)\n",
            "I0719 09:35:16.773147 140481689634688 learning.py:507] global step 2930: loss = 0.0281 (0.329 sec/step)\n",
            "I0719 09:35:17.134370 140481689634688 learning.py:507] global step 2931: loss = 0.0324 (0.360 sec/step)\n",
            "I0719 09:35:17.479603 140481689634688 learning.py:507] global step 2932: loss = 0.0738 (0.343 sec/step)\n",
            "I0719 09:35:17.819471 140481689634688 learning.py:507] global step 2933: loss = 0.1218 (0.338 sec/step)\n",
            "I0719 09:35:18.166479 140481689634688 learning.py:507] global step 2934: loss = 0.1468 (0.345 sec/step)\n",
            "I0719 09:35:18.514623 140481689634688 learning.py:507] global step 2935: loss = 0.0779 (0.346 sec/step)\n",
            "I0719 09:35:18.850444 140481689634688 learning.py:507] global step 2936: loss = 0.0938 (0.334 sec/step)\n",
            "I0719 09:35:19.185336 140481689634688 learning.py:507] global step 2937: loss = 0.0210 (0.333 sec/step)\n",
            "I0719 09:35:19.502032 140481689634688 learning.py:507] global step 2938: loss = 0.0896 (0.315 sec/step)\n",
            "I0719 09:35:19.862416 140481689634688 learning.py:507] global step 2939: loss = 0.0881 (0.359 sec/step)\n",
            "I0719 09:35:20.200834 140481689634688 learning.py:507] global step 2940: loss = 0.0310 (0.337 sec/step)\n",
            "I0719 09:35:20.550929 140481689634688 learning.py:507] global step 2941: loss = 0.0563 (0.348 sec/step)\n",
            "I0719 09:35:20.884964 140481689634688 learning.py:507] global step 2942: loss = 0.1846 (0.332 sec/step)\n",
            "I0719 09:35:21.210200 140481689634688 learning.py:507] global step 2943: loss = 0.1734 (0.324 sec/step)\n",
            "I0719 09:35:21.540299 140481689634688 learning.py:507] global step 2944: loss = 0.0914 (0.328 sec/step)\n",
            "I0719 09:35:21.886968 140481689634688 learning.py:507] global step 2945: loss = 0.0536 (0.345 sec/step)\n",
            "I0719 09:35:22.230381 140481689634688 learning.py:507] global step 2946: loss = 0.1102 (0.341 sec/step)\n",
            "I0719 09:35:22.573236 140481689634688 learning.py:507] global step 2947: loss = 0.1526 (0.341 sec/step)\n",
            "I0719 09:35:22.925359 140481689634688 learning.py:507] global step 2948: loss = 0.1565 (0.350 sec/step)\n",
            "I0719 09:35:23.230577 140481689634688 learning.py:507] global step 2949: loss = 0.0388 (0.303 sec/step)\n",
            "I0719 09:35:23.560550 140481689634688 learning.py:507] global step 2950: loss = 0.0685 (0.328 sec/step)\n",
            "I0719 09:35:23.885366 140481689634688 learning.py:507] global step 2951: loss = 0.0964 (0.323 sec/step)\n",
            "I0719 09:35:24.224950 140481689634688 learning.py:507] global step 2952: loss = 0.0498 (0.338 sec/step)\n",
            "I0719 09:35:24.562348 140481689634688 learning.py:507] global step 2953: loss = 0.0584 (0.336 sec/step)\n",
            "I0719 09:35:24.879293 140481689634688 learning.py:507] global step 2954: loss = 0.0195 (0.315 sec/step)\n",
            "I0719 09:35:25.209327 140481689634688 learning.py:507] global step 2955: loss = 0.0855 (0.328 sec/step)\n",
            "I0719 09:35:25.543427 140481689634688 learning.py:507] global step 2956: loss = 0.1239 (0.332 sec/step)\n",
            "I0719 09:35:25.880685 140481689634688 learning.py:507] global step 2957: loss = 0.0478 (0.335 sec/step)\n",
            "I0719 09:35:26.221975 140481689634688 learning.py:507] global step 2958: loss = 0.1372 (0.340 sec/step)\n",
            "I0719 09:35:26.525101 140481689634688 learning.py:507] global step 2959: loss = 0.1061 (0.301 sec/step)\n",
            "I0719 09:35:26.903364 140481689634688 learning.py:507] global step 2960: loss = 0.1517 (0.376 sec/step)\n",
            "I0719 09:35:27.234748 140481689634688 learning.py:507] global step 2961: loss = 0.0260 (0.330 sec/step)\n",
            "I0719 09:35:27.531531 140481689634688 learning.py:507] global step 2962: loss = 0.1573 (0.295 sec/step)\n",
            "I0719 09:35:27.865839 140481689634688 learning.py:507] global step 2963: loss = 0.1363 (0.333 sec/step)\n",
            "I0719 09:35:28.208326 140481689634688 learning.py:507] global step 2964: loss = 0.0218 (0.341 sec/step)\n",
            "I0719 09:35:28.541099 140481689634688 learning.py:507] global step 2965: loss = 0.2431 (0.331 sec/step)\n",
            "I0719 09:35:28.902624 140481689634688 learning.py:507] global step 2966: loss = 0.0482 (0.360 sec/step)\n",
            "I0719 09:35:29.228662 140481689634688 learning.py:507] global step 2967: loss = 0.0804 (0.324 sec/step)\n",
            "I0719 09:35:29.557439 140481689634688 learning.py:507] global step 2968: loss = 0.0855 (0.327 sec/step)\n",
            "I0719 09:35:29.923907 140481689634688 learning.py:507] global step 2969: loss = 0.0845 (0.365 sec/step)\n",
            "I0719 09:35:30.266233 140481689634688 learning.py:507] global step 2970: loss = 0.1263 (0.341 sec/step)\n",
            "I0719 09:35:30.614463 140481689634688 learning.py:507] global step 2971: loss = 0.1354 (0.347 sec/step)\n",
            "I0719 09:35:30.957511 140481689634688 learning.py:507] global step 2972: loss = 0.0992 (0.341 sec/step)\n",
            "I0719 09:35:31.319467 140481689634688 learning.py:507] global step 2973: loss = 0.0647 (0.360 sec/step)\n",
            "I0719 09:35:31.672256 140481689634688 learning.py:507] global step 2974: loss = 0.0464 (0.349 sec/step)\n",
            "I0719 09:35:31.998492 140481689634688 learning.py:507] global step 2975: loss = 0.0770 (0.324 sec/step)\n",
            "I0719 09:35:32.281878 140481689634688 learning.py:507] global step 2976: loss = 0.0782 (0.282 sec/step)\n",
            "I0719 09:35:32.579758 140481689634688 learning.py:507] global step 2977: loss = 0.0603 (0.295 sec/step)\n",
            "I0719 09:35:32.884232 140481689634688 learning.py:507] global step 2978: loss = 0.0494 (0.302 sec/step)\n",
            "I0719 09:35:33.236907 140481689634688 learning.py:507] global step 2979: loss = 0.1719 (0.350 sec/step)\n",
            "I0719 09:35:33.577540 140481689634688 learning.py:507] global step 2980: loss = 0.1064 (0.339 sec/step)\n",
            "I0719 09:35:33.901781 140481689634688 learning.py:507] global step 2981: loss = 0.1386 (0.323 sec/step)\n",
            "I0719 09:35:34.237939 140481689634688 learning.py:507] global step 2982: loss = 0.0787 (0.334 sec/step)\n",
            "I0719 09:35:34.551636 140481689634688 learning.py:507] global step 2983: loss = 0.0866 (0.312 sec/step)\n",
            "I0719 09:35:34.889949 140481689634688 learning.py:507] global step 2984: loss = 0.1507 (0.336 sec/step)\n",
            "I0719 09:35:35.226610 140481689634688 learning.py:507] global step 2985: loss = 0.0758 (0.335 sec/step)\n",
            "I0719 09:35:35.577875 140481689634688 learning.py:507] global step 2986: loss = 0.1232 (0.350 sec/step)\n",
            "I0719 09:35:35.900923 140481689634688 learning.py:507] global step 2987: loss = 0.0399 (0.321 sec/step)\n",
            "I0719 09:35:36.223865 140481689634688 learning.py:507] global step 2988: loss = 0.0570 (0.321 sec/step)\n",
            "I0719 09:35:36.544919 140481689634688 learning.py:507] global step 2989: loss = 0.1047 (0.319 sec/step)\n",
            "I0719 09:35:36.869644 140481689634688 learning.py:507] global step 2990: loss = 0.2422 (0.323 sec/step)\n",
            "I0719 09:35:37.202433 140481689634688 learning.py:507] global step 2991: loss = 0.1018 (0.331 sec/step)\n",
            "I0719 09:35:37.554656 140481689634688 learning.py:507] global step 2992: loss = 0.1107 (0.349 sec/step)\n",
            "I0719 09:35:37.873308 140481689634688 learning.py:507] global step 2993: loss = 0.0481 (0.317 sec/step)\n",
            "I0719 09:35:38.177018 140481689634688 learning.py:507] global step 2994: loss = 0.0770 (0.302 sec/step)\n",
            "I0719 09:35:38.496330 140481689634688 learning.py:507] global step 2995: loss = 0.0828 (0.318 sec/step)\n",
            "I0719 09:35:38.807645 140481689634688 learning.py:507] global step 2996: loss = 0.0620 (0.309 sec/step)\n",
            "I0719 09:35:39.143252 140481689634688 learning.py:507] global step 2997: loss = 0.1076 (0.333 sec/step)\n",
            "I0719 09:35:39.483550 140481689634688 learning.py:507] global step 2998: loss = 0.1916 (0.338 sec/step)\n",
            "I0719 09:35:39.834133 140481689634688 learning.py:507] global step 2999: loss = 0.2392 (0.349 sec/step)\n",
            "I0719 09:35:40.191976 140481689634688 learning.py:507] global step 3000: loss = 0.0147 (0.355 sec/step)\n",
            "I0719 09:35:40.560372 140481689634688 learning.py:507] global step 3001: loss = 0.0877 (0.366 sec/step)\n",
            "I0719 09:35:40.916645 140481689634688 learning.py:507] global step 3002: loss = 0.0551 (0.354 sec/step)\n",
            "I0719 09:35:41.255196 140481689634688 learning.py:507] global step 3003: loss = 0.2065 (0.337 sec/step)\n",
            "I0719 09:35:41.623746 140481689634688 learning.py:507] global step 3004: loss = 0.1462 (0.365 sec/step)\n",
            "I0719 09:35:41.947809 140481689634688 learning.py:507] global step 3005: loss = 0.0775 (0.322 sec/step)\n",
            "I0719 09:35:42.226826 140481689634688 learning.py:507] global step 3006: loss = 0.0982 (0.277 sec/step)\n",
            "I0719 09:35:42.538681 140481689634688 learning.py:507] global step 3007: loss = 0.2040 (0.310 sec/step)\n",
            "I0719 09:35:42.893917 140481689634688 learning.py:507] global step 3008: loss = 0.0997 (0.353 sec/step)\n",
            "I0719 09:35:43.218155 140481689634688 learning.py:507] global step 3009: loss = 0.0567 (0.322 sec/step)\n",
            "I0719 09:35:43.555888 140481689634688 learning.py:507] global step 3010: loss = 0.0622 (0.336 sec/step)\n",
            "I0719 09:35:43.913577 140481689634688 learning.py:507] global step 3011: loss = 0.0752 (0.356 sec/step)\n",
            "I0719 09:35:44.246300 140481689634688 learning.py:507] global step 3012: loss = 0.2038 (0.331 sec/step)\n",
            "I0719 09:35:44.571643 140481689634688 learning.py:507] global step 3013: loss = 0.0695 (0.324 sec/step)\n",
            "I0719 09:35:44.886604 140481689634688 learning.py:507] global step 3014: loss = 0.0615 (0.313 sec/step)\n",
            "I0719 09:35:45.217930 140481689634688 learning.py:507] global step 3015: loss = 0.0223 (0.330 sec/step)\n",
            "I0719 09:35:45.531654 140481689634688 learning.py:507] global step 3016: loss = 0.0830 (0.312 sec/step)\n",
            "I0719 09:35:45.886448 140481689634688 learning.py:507] global step 3017: loss = 0.0260 (0.353 sec/step)\n",
            "I0719 09:35:46.247249 140481689634688 learning.py:507] global step 3018: loss = 0.1235 (0.359 sec/step)\n",
            "I0719 09:35:46.526042 140481689634688 learning.py:507] global step 3019: loss = 0.0986 (0.277 sec/step)\n",
            "I0719 09:35:46.875895 140481689634688 learning.py:507] global step 3020: loss = 0.0485 (0.348 sec/step)\n",
            "I0719 09:35:47.214481 140481689634688 learning.py:507] global step 3021: loss = 0.1765 (0.337 sec/step)\n",
            "I0719 09:35:47.495450 140481689634688 learning.py:507] global step 3022: loss = 0.1223 (0.279 sec/step)\n",
            "I0719 09:35:47.788750 140481689634688 learning.py:507] global step 3023: loss = 0.1431 (0.292 sec/step)\n",
            "I0719 09:35:48.133935 140481689634688 learning.py:507] global step 3024: loss = 0.0312 (0.343 sec/step)\n",
            "I0719 09:35:48.441144 140481689634688 learning.py:507] global step 3025: loss = 0.0708 (0.305 sec/step)\n",
            "I0719 09:35:48.790372 140481689634688 learning.py:507] global step 3026: loss = 0.1076 (0.347 sec/step)\n",
            "I0719 09:35:49.138774 140481689634688 learning.py:507] global step 3027: loss = 0.1323 (0.347 sec/step)\n",
            "I0719 09:35:49.470679 140481689634688 learning.py:507] global step 3028: loss = 0.0604 (0.330 sec/step)\n",
            "I0719 09:35:49.750342 140481689634688 learning.py:507] global step 3029: loss = 0.0377 (0.278 sec/step)\n",
            "I0719 09:35:50.124325 140481689634688 learning.py:507] global step 3030: loss = 0.1146 (0.372 sec/step)\n",
            "I0719 09:35:50.458972 140481689634688 learning.py:507] global step 3031: loss = 0.0993 (0.333 sec/step)\n",
            "I0719 09:35:50.786579 140481689634688 learning.py:507] global step 3032: loss = 0.1486 (0.326 sec/step)\n",
            "I0719 09:35:51.145429 140481689634688 learning.py:507] global step 3033: loss = 0.0855 (0.357 sec/step)\n",
            "I0719 09:35:51.504148 140481689634688 learning.py:507] global step 3034: loss = 0.2438 (0.357 sec/step)\n",
            "I0719 09:35:51.865038 140481689634688 learning.py:507] global step 3035: loss = 0.0145 (0.359 sec/step)\n",
            "I0719 09:35:52.200641 140481689634688 learning.py:507] global step 3036: loss = 0.0994 (0.334 sec/step)\n",
            "I0719 09:35:52.534460 140481689634688 learning.py:507] global step 3037: loss = 0.1398 (0.332 sec/step)\n",
            "I0719 09:35:52.842408 140481689634688 learning.py:507] global step 3038: loss = 0.0746 (0.306 sec/step)\n",
            "I0719 09:35:53.195008 140481689634688 learning.py:507] global step 3039: loss = 0.1215 (0.351 sec/step)\n",
            "I0719 09:35:53.539554 140481689634688 learning.py:507] global step 3040: loss = 0.2578 (0.343 sec/step)\n",
            "I0719 09:35:53.866500 140481689634688 learning.py:507] global step 3041: loss = 0.0921 (0.325 sec/step)\n",
            "I0719 09:35:54.207359 140481689634688 learning.py:507] global step 3042: loss = 0.0937 (0.339 sec/step)\n",
            "I0719 09:35:54.545472 140481689634688 learning.py:507] global step 3043: loss = 0.0976 (0.336 sec/step)\n",
            "I0719 09:35:54.894449 140481689634688 learning.py:507] global step 3044: loss = 0.1087 (0.347 sec/step)\n",
            "I0719 09:35:55.261850 140481689634688 learning.py:507] global step 3045: loss = 0.0952 (0.366 sec/step)\n",
            "I0719 09:35:55.591249 140481689634688 learning.py:507] global step 3046: loss = 0.0457 (0.328 sec/step)\n",
            "I0719 09:35:55.914826 140481689634688 learning.py:507] global step 3047: loss = 0.0842 (0.322 sec/step)\n",
            "I0719 09:35:56.255145 140481689634688 learning.py:507] global step 3048: loss = 0.0656 (0.339 sec/step)\n",
            "I0719 09:35:56.598511 140481689634688 learning.py:507] global step 3049: loss = 0.0344 (0.341 sec/step)\n",
            "I0719 09:35:56.942857 140481689634688 learning.py:507] global step 3050: loss = 0.1204 (0.343 sec/step)\n",
            "I0719 09:35:57.314575 140481689634688 learning.py:507] global step 3051: loss = 0.1670 (0.370 sec/step)\n",
            "I0719 09:35:57.637702 140481689634688 learning.py:507] global step 3052: loss = 0.0528 (0.321 sec/step)\n",
            "I0719 09:35:57.987649 140481689634688 learning.py:507] global step 3053: loss = 0.1202 (0.348 sec/step)\n",
            "I0719 09:35:58.325622 140481689634688 learning.py:507] global step 3054: loss = 0.0237 (0.336 sec/step)\n",
            "I0719 09:35:58.633630 140481689634688 learning.py:507] global step 3055: loss = 0.1185 (0.306 sec/step)\n",
            "I0719 09:35:58.979679 140481689634688 learning.py:507] global step 3056: loss = 0.0857 (0.344 sec/step)\n",
            "I0719 09:35:59.332361 140481689634688 learning.py:507] global step 3057: loss = 0.1406 (0.351 sec/step)\n",
            "I0719 09:35:59.623483 140481689634688 learning.py:507] global step 3058: loss = 0.0254 (0.289 sec/step)\n",
            "I0719 09:35:59.950823 140481689634688 learning.py:507] global step 3059: loss = 0.0779 (0.326 sec/step)\n",
            "I0719 09:36:00.295837 140481689634688 learning.py:507] global step 3060: loss = 0.0701 (0.343 sec/step)\n",
            "I0719 09:36:00.626389 140481689634688 learning.py:507] global step 3061: loss = 0.1552 (0.329 sec/step)\n",
            "I0719 09:36:00.967013 140481689634688 learning.py:507] global step 3062: loss = 0.1167 (0.339 sec/step)\n",
            "I0719 09:36:01.312172 140481689634688 learning.py:507] global step 3063: loss = 0.0256 (0.344 sec/step)\n",
            "I0719 09:36:01.613853 140481689634688 learning.py:507] global step 3064: loss = 0.0876 (0.300 sec/step)\n",
            "I0719 09:36:01.949609 140481689634688 learning.py:507] global step 3065: loss = 0.1053 (0.332 sec/step)\n",
            "I0719 09:36:02.283106 140481689634688 learning.py:507] global step 3066: loss = 0.1226 (0.332 sec/step)\n",
            "I0719 09:36:02.583939 140481689634688 learning.py:507] global step 3067: loss = 0.0663 (0.299 sec/step)\n",
            "I0719 09:36:02.917676 140481689634688 learning.py:507] global step 3068: loss = 0.0714 (0.332 sec/step)\n",
            "I0719 09:36:03.219367 140481689634688 learning.py:507] global step 3069: loss = 0.1251 (0.299 sec/step)\n",
            "I0719 09:36:03.568305 140481689634688 learning.py:507] global step 3070: loss = 0.0577 (0.343 sec/step)\n",
            "I0719 09:36:03.900291 140481689634688 learning.py:507] global step 3071: loss = 0.0837 (0.330 sec/step)\n",
            "I0719 09:36:04.235593 140481689634688 learning.py:507] global step 3072: loss = 0.1862 (0.333 sec/step)\n",
            "I0719 09:36:04.589579 140481689634688 learning.py:507] global step 3073: loss = 0.0790 (0.350 sec/step)\n",
            "I0719 09:36:04.925194 140481689634688 learning.py:507] global step 3074: loss = 0.0634 (0.334 sec/step)\n",
            "I0719 09:36:05.260900 140481689634688 learning.py:507] global step 3075: loss = 0.1020 (0.334 sec/step)\n",
            "I0719 09:36:05.613363 140481689634688 learning.py:507] global step 3076: loss = 0.0918 (0.350 sec/step)\n",
            "I0719 09:36:05.976785 140481689634688 learning.py:507] global step 3077: loss = 0.0900 (0.362 sec/step)\n",
            "I0719 09:36:06.295977 140481689634688 learning.py:507] global step 3078: loss = 0.0606 (0.317 sec/step)\n",
            "I0719 09:36:06.621463 140481689634688 learning.py:507] global step 3079: loss = 0.0504 (0.324 sec/step)\n",
            "I0719 09:36:06.907653 140481689634688 learning.py:507] global step 3080: loss = 0.1019 (0.285 sec/step)\n",
            "I0719 09:36:07.227485 140481689634688 learning.py:507] global step 3081: loss = 0.0856 (0.318 sec/step)\n",
            "I0719 09:36:07.565577 140481689634688 learning.py:507] global step 3082: loss = 0.0681 (0.336 sec/step)\n",
            "I0719 09:36:07.919963 140481689634688 learning.py:507] global step 3083: loss = 0.0809 (0.353 sec/step)\n",
            "I0719 09:36:08.268654 140481689634688 learning.py:507] global step 3084: loss = 0.1035 (0.347 sec/step)\n",
            "I0719 09:36:08.602059 140481689634688 learning.py:507] global step 3085: loss = 0.0234 (0.331 sec/step)\n",
            "I0719 09:36:08.933377 140481689634688 learning.py:507] global step 3086: loss = 0.0594 (0.330 sec/step)\n",
            "I0719 09:36:09.276212 140481689634688 learning.py:507] global step 3087: loss = 0.1165 (0.341 sec/step)\n",
            "I0719 09:36:09.618647 140481689634688 learning.py:507] global step 3088: loss = 0.0727 (0.341 sec/step)\n",
            "I0719 09:36:09.928622 140481689634688 learning.py:507] global step 3089: loss = 0.0612 (0.308 sec/step)\n",
            "I0719 09:36:10.253432 140481689634688 learning.py:507] global step 3090: loss = 0.1144 (0.323 sec/step)\n",
            "I0719 09:36:10.602504 140481689634688 learning.py:507] global step 3091: loss = 0.1592 (0.347 sec/step)\n",
            "I0719 09:36:10.946040 140481689634688 learning.py:507] global step 3092: loss = 0.1440 (0.342 sec/step)\n",
            "I0719 09:36:11.287297 140481689634688 learning.py:507] global step 3093: loss = 0.0822 (0.339 sec/step)\n",
            "I0719 09:36:11.606101 140481689634688 learning.py:507] global step 3094: loss = 0.0591 (0.317 sec/step)\n",
            "I0719 09:36:11.953058 140481689634688 learning.py:507] global step 3095: loss = 0.0577 (0.345 sec/step)\n",
            "I0719 09:36:12.335381 140481689634688 learning.py:507] global step 3096: loss = 0.0612 (0.381 sec/step)\n",
            "I0719 09:36:12.691739 140481689634688 learning.py:507] global step 3097: loss = 0.0726 (0.355 sec/step)\n",
            "I0719 09:36:13.023373 140481689634688 learning.py:507] global step 3098: loss = 0.0291 (0.330 sec/step)\n",
            "I0719 09:36:13.364646 140481689634688 learning.py:507] global step 3099: loss = 0.1043 (0.339 sec/step)\n",
            "I0719 09:36:13.711359 140481689634688 learning.py:507] global step 3100: loss = 0.0821 (0.345 sec/step)\n",
            "I0719 09:36:14.015843 140481689634688 learning.py:507] global step 3101: loss = 0.2525 (0.303 sec/step)\n",
            "I0719 09:36:14.358692 140481689634688 learning.py:507] global step 3102: loss = 0.1195 (0.341 sec/step)\n",
            "I0719 09:36:14.660510 140481689634688 learning.py:507] global step 3103: loss = 0.0608 (0.300 sec/step)\n",
            "I0719 09:36:15.007030 140481689634688 learning.py:507] global step 3104: loss = 0.0178 (0.345 sec/step)\n",
            "I0719 09:36:15.334866 140481689634688 learning.py:507] global step 3105: loss = 0.1866 (0.326 sec/step)\n",
            "I0719 09:36:15.667031 140481689634688 learning.py:507] global step 3106: loss = 0.1205 (0.330 sec/step)\n",
            "I0719 09:36:16.015797 140481689634688 learning.py:507] global step 3107: loss = 0.0694 (0.347 sec/step)\n",
            "I0719 09:36:16.356870 140481689634688 learning.py:507] global step 3108: loss = 0.1323 (0.339 sec/step)\n",
            "I0719 09:36:16.700951 140481689634688 learning.py:507] global step 3109: loss = 0.0232 (0.342 sec/step)\n",
            "I0719 09:36:17.056364 140481689634688 learning.py:507] global step 3110: loss = 0.0891 (0.353 sec/step)\n",
            "I0719 09:36:17.399789 140481689634688 learning.py:507] global step 3111: loss = 0.2023 (0.342 sec/step)\n",
            "I0719 09:36:17.757865 140481689634688 learning.py:507] global step 3112: loss = 0.1087 (0.356 sec/step)\n",
            "I0719 09:36:18.092073 140481689634688 learning.py:507] global step 3113: loss = 0.1090 (0.332 sec/step)\n",
            "I0719 09:36:18.398947 140481689634688 learning.py:507] global step 3114: loss = 0.0435 (0.305 sec/step)\n",
            "I0719 09:36:18.763575 140481689634688 learning.py:507] global step 3115: loss = 0.0686 (0.363 sec/step)\n",
            "I0719 09:36:19.108603 140481689634688 learning.py:507] global step 3116: loss = 0.1358 (0.343 sec/step)\n",
            "I0719 09:36:19.431446 140481689634688 learning.py:507] global step 3117: loss = 0.0944 (0.321 sec/step)\n",
            "I0719 09:36:19.809696 140481689634688 learning.py:507] global step 3118: loss = 0.1452 (0.377 sec/step)\n",
            "I0719 09:36:20.160022 140481689634688 learning.py:507] global step 3119: loss = 0.1249 (0.348 sec/step)\n",
            "I0719 09:36:20.500662 140481689634688 learning.py:507] global step 3120: loss = 0.0915 (0.339 sec/step)\n",
            "I0719 09:36:20.839204 140481689634688 learning.py:507] global step 3121: loss = 0.2059 (0.337 sec/step)\n",
            "I0719 09:36:21.153111 140481689634688 learning.py:507] global step 3122: loss = 0.0154 (0.312 sec/step)\n",
            "I0719 09:36:21.488152 140481689634688 learning.py:507] global step 3123: loss = 0.0295 (0.333 sec/step)\n",
            "I0719 09:36:21.852688 140481689634688 learning.py:507] global step 3124: loss = 0.0874 (0.363 sec/step)\n",
            "I0719 09:36:22.179137 140481689634688 learning.py:507] global step 3125: loss = 0.2978 (0.325 sec/step)\n",
            "I0719 09:36:22.532598 140481689634688 learning.py:507] global step 3126: loss = 0.0428 (0.352 sec/step)\n",
            "I0719 09:36:22.897644 140481689634688 learning.py:507] global step 3127: loss = 0.0512 (0.363 sec/step)\n",
            "I0719 09:36:23.207632 140481689634688 learning.py:507] global step 3128: loss = 0.1054 (0.308 sec/step)\n",
            "I0719 09:36:23.535549 140481689634688 learning.py:507] global step 3129: loss = 0.2649 (0.325 sec/step)\n",
            "I0719 09:36:23.877699 140481689634688 learning.py:507] global step 3130: loss = 0.1111 (0.340 sec/step)\n",
            "I0719 09:36:24.226958 140481689634688 learning.py:507] global step 3131: loss = 0.1226 (0.347 sec/step)\n",
            "I0719 09:36:24.590561 140481689634688 learning.py:507] global step 3132: loss = 0.0725 (0.362 sec/step)\n",
            "I0719 09:36:24.904488 140481689634688 learning.py:507] global step 3133: loss = 0.0994 (0.312 sec/step)\n",
            "I0719 09:36:25.231765 140481689634688 learning.py:507] global step 3134: loss = 0.1172 (0.325 sec/step)\n",
            "I0719 09:36:25.577455 140481689634688 learning.py:507] global step 3135: loss = 0.1267 (0.344 sec/step)\n",
            "I0719 09:36:25.907174 140479034988288 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0719 09:36:26.145176 140481689634688 learning.py:507] global step 3136: loss = 0.0767 (0.559 sec/step)\n",
            "I0719 09:36:26.840508 140481689634688 learning.py:507] global step 3137: loss = 0.1342 (0.535 sec/step)\n",
            "I0719 09:36:27.375871 140481689634688 learning.py:507] global step 3138: loss = 0.1499 (0.505 sec/step)\n",
            "I0719 09:36:27.812808 140479018202880 supervisor.py:1050] Recording summary at step 3139.\n",
            "I0719 09:36:27.824323 140481689634688 learning.py:507] global step 3139: loss = 0.0951 (0.423 sec/step)\n",
            "I0719 09:36:28.318020 140481689634688 learning.py:507] global step 3140: loss = 0.0497 (0.486 sec/step)\n",
            "I0719 09:36:28.719036 140481689634688 learning.py:507] global step 3141: loss = 0.0426 (0.399 sec/step)\n",
            "I0719 09:36:29.050041 140481689634688 learning.py:507] global step 3142: loss = 0.1897 (0.329 sec/step)\n",
            "I0719 09:36:29.383949 140481689634688 learning.py:507] global step 3143: loss = 0.0603 (0.332 sec/step)\n",
            "I0719 09:36:29.699167 140481689634688 learning.py:507] global step 3144: loss = 0.0905 (0.313 sec/step)\n",
            "I0719 09:36:30.046628 140481689634688 learning.py:507] global step 3145: loss = 0.2002 (0.346 sec/step)\n",
            "I0719 09:36:30.374008 140481689634688 learning.py:507] global step 3146: loss = 0.1806 (0.326 sec/step)\n",
            "I0719 09:36:30.703875 140481689634688 learning.py:507] global step 3147: loss = 0.1087 (0.328 sec/step)\n",
            "I0719 09:36:31.019824 140481689634688 learning.py:507] global step 3148: loss = 0.1536 (0.313 sec/step)\n",
            "I0719 09:36:31.358524 140481689634688 learning.py:507] global step 3149: loss = 0.0632 (0.337 sec/step)\n",
            "I0719 09:36:31.736282 140481689634688 learning.py:507] global step 3150: loss = 0.1056 (0.376 sec/step)\n",
            "I0719 09:36:32.067982 140481689634688 learning.py:507] global step 3151: loss = 0.0916 (0.330 sec/step)\n",
            "I0719 09:36:32.405800 140481689634688 learning.py:507] global step 3152: loss = 0.0650 (0.336 sec/step)\n",
            "I0719 09:36:32.709673 140481689634688 learning.py:507] global step 3153: loss = 0.0754 (0.302 sec/step)\n",
            "I0719 09:36:33.018624 140481689634688 learning.py:507] global step 3154: loss = 0.1619 (0.307 sec/step)\n",
            "I0719 09:36:33.366339 140481689634688 learning.py:507] global step 3155: loss = 0.0989 (0.346 sec/step)\n",
            "I0719 09:36:33.659495 140481689634688 learning.py:507] global step 3156: loss = 0.1540 (0.292 sec/step)\n",
            "I0719 09:36:34.023304 140481689634688 learning.py:507] global step 3157: loss = 0.1655 (0.362 sec/step)\n",
            "I0719 09:36:34.336340 140481689634688 learning.py:507] global step 3158: loss = 0.2467 (0.311 sec/step)\n",
            "I0719 09:36:34.672821 140481689634688 learning.py:507] global step 3159: loss = 0.0912 (0.334 sec/step)\n",
            "I0719 09:36:34.995524 140481689634688 learning.py:507] global step 3160: loss = 0.0854 (0.321 sec/step)\n",
            "I0719 09:36:35.345413 140481689634688 learning.py:507] global step 3161: loss = 0.0453 (0.348 sec/step)\n",
            "I0719 09:36:35.658417 140481689634688 learning.py:507] global step 3162: loss = 0.1710 (0.311 sec/step)\n",
            "I0719 09:36:36.008155 140481689634688 learning.py:507] global step 3163: loss = 0.1326 (0.348 sec/step)\n",
            "I0719 09:36:36.355666 140481689634688 learning.py:507] global step 3164: loss = 0.1641 (0.346 sec/step)\n",
            "I0719 09:36:36.669800 140481689634688 learning.py:507] global step 3165: loss = 0.0215 (0.313 sec/step)\n",
            "I0719 09:36:37.006456 140481689634688 learning.py:507] global step 3166: loss = 0.0938 (0.335 sec/step)\n",
            "I0719 09:36:37.346431 140481689634688 learning.py:507] global step 3167: loss = 0.1172 (0.338 sec/step)\n",
            "I0719 09:36:37.675040 140481689634688 learning.py:507] global step 3168: loss = 0.1025 (0.327 sec/step)\n",
            "I0719 09:36:38.014197 140481689634688 learning.py:507] global step 3169: loss = 0.1620 (0.337 sec/step)\n",
            "I0719 09:36:38.360472 140481689634688 learning.py:507] global step 3170: loss = 0.2079 (0.345 sec/step)\n",
            "I0719 09:36:38.691488 140481689634688 learning.py:507] global step 3171: loss = 0.0836 (0.329 sec/step)\n",
            "I0719 09:36:39.018898 140481689634688 learning.py:507] global step 3172: loss = 0.1070 (0.325 sec/step)\n",
            "I0719 09:36:39.344225 140481689634688 learning.py:507] global step 3173: loss = 0.0947 (0.324 sec/step)\n",
            "I0719 09:36:39.715708 140481689634688 learning.py:507] global step 3174: loss = 0.1121 (0.370 sec/step)\n",
            "I0719 09:36:40.050897 140481689634688 learning.py:507] global step 3175: loss = 0.0870 (0.333 sec/step)\n",
            "I0719 09:36:40.367427 140481689634688 learning.py:507] global step 3176: loss = 0.0869 (0.315 sec/step)\n",
            "I0719 09:36:40.736974 140481689634688 learning.py:507] global step 3177: loss = 0.0363 (0.368 sec/step)\n",
            "I0719 09:36:41.079971 140481689634688 learning.py:507] global step 3178: loss = 0.1152 (0.341 sec/step)\n",
            "I0719 09:36:41.425661 140481689634688 learning.py:507] global step 3179: loss = 0.0966 (0.344 sec/step)\n",
            "I0719 09:36:41.772386 140481689634688 learning.py:507] global step 3180: loss = 0.0919 (0.345 sec/step)\n",
            "I0719 09:36:42.078896 140481689634688 learning.py:507] global step 3181: loss = 0.0649 (0.305 sec/step)\n",
            "I0719 09:36:42.429041 140481689634688 learning.py:507] global step 3182: loss = 0.0719 (0.348 sec/step)\n",
            "I0719 09:36:42.747447 140481689634688 learning.py:507] global step 3183: loss = 0.0146 (0.317 sec/step)\n",
            "I0719 09:36:43.086941 140481689634688 learning.py:507] global step 3184: loss = 0.0968 (0.338 sec/step)\n",
            "I0719 09:36:43.415401 140481689634688 learning.py:507] global step 3185: loss = 0.0641 (0.326 sec/step)\n",
            "I0719 09:36:43.766451 140481689634688 learning.py:507] global step 3186: loss = 0.1149 (0.349 sec/step)\n",
            "I0719 09:36:44.110638 140481689634688 learning.py:507] global step 3187: loss = 0.0876 (0.343 sec/step)\n",
            "I0719 09:36:44.433456 140481689634688 learning.py:507] global step 3188: loss = 0.0553 (0.321 sec/step)\n",
            "I0719 09:36:44.775187 140481689634688 learning.py:507] global step 3189: loss = 0.0544 (0.340 sec/step)\n",
            "I0719 09:36:45.124624 140481689634688 learning.py:507] global step 3190: loss = 0.0493 (0.348 sec/step)\n",
            "I0719 09:36:45.462826 140481689634688 learning.py:507] global step 3191: loss = 0.0584 (0.336 sec/step)\n",
            "I0719 09:36:45.831564 140481689634688 learning.py:507] global step 3192: loss = 0.1608 (0.367 sec/step)\n",
            "I0719 09:36:46.196202 140481689634688 learning.py:507] global step 3193: loss = 0.0576 (0.363 sec/step)\n",
            "I0719 09:36:46.538952 140481689634688 learning.py:507] global step 3194: loss = 0.1039 (0.341 sec/step)\n",
            "I0719 09:36:46.884296 140481689634688 learning.py:507] global step 3195: loss = 0.1127 (0.343 sec/step)\n",
            "I0719 09:36:47.199446 140481689634688 learning.py:507] global step 3196: loss = 0.1081 (0.313 sec/step)\n",
            "I0719 09:36:47.538957 140481689634688 learning.py:507] global step 3197: loss = 0.0697 (0.337 sec/step)\n",
            "I0719 09:36:47.886108 140481689634688 learning.py:507] global step 3198: loss = 0.0474 (0.345 sec/step)\n",
            "I0719 09:36:48.236572 140481689634688 learning.py:507] global step 3199: loss = 0.1138 (0.349 sec/step)\n",
            "I0719 09:36:48.564189 140481689634688 learning.py:507] global step 3200: loss = 0.1042 (0.324 sec/step)\n",
            "I0719 09:36:48.908881 140481689634688 learning.py:507] global step 3201: loss = 0.0895 (0.343 sec/step)\n",
            "I0719 09:36:49.244627 140481689634688 learning.py:507] global step 3202: loss = 0.0610 (0.334 sec/step)\n",
            "I0719 09:36:49.598413 140481689634688 learning.py:507] global step 3203: loss = 0.1707 (0.352 sec/step)\n",
            "I0719 09:36:49.909654 140481689634688 learning.py:507] global step 3204: loss = 0.0944 (0.308 sec/step)\n",
            "I0719 09:36:50.220671 140481689634688 learning.py:507] global step 3205: loss = 0.0736 (0.309 sec/step)\n",
            "I0719 09:36:50.536894 140481689634688 learning.py:507] global step 3206: loss = 0.0689 (0.314 sec/step)\n",
            "I0719 09:36:50.833936 140481689634688 learning.py:507] global step 3207: loss = 0.0789 (0.295 sec/step)\n",
            "I0719 09:36:51.119398 140481689634688 learning.py:507] global step 3208: loss = 0.1220 (0.284 sec/step)\n",
            "I0719 09:36:51.445654 140481689634688 learning.py:507] global step 3209: loss = 0.0778 (0.325 sec/step)\n",
            "I0719 09:36:51.798253 140481689634688 learning.py:507] global step 3210: loss = 0.0360 (0.351 sec/step)\n",
            "I0719 09:36:52.073971 140481689634688 learning.py:507] global step 3211: loss = 0.1851 (0.274 sec/step)\n",
            "I0719 09:36:52.416646 140481689634688 learning.py:507] global step 3212: loss = 0.0759 (0.341 sec/step)\n",
            "I0719 09:36:52.764445 140481689634688 learning.py:507] global step 3213: loss = 0.1212 (0.346 sec/step)\n",
            "I0719 09:36:53.105454 140481689634688 learning.py:507] global step 3214: loss = 0.1654 (0.339 sec/step)\n",
            "I0719 09:36:53.439495 140481689634688 learning.py:507] global step 3215: loss = 0.0986 (0.332 sec/step)\n",
            "I0719 09:36:53.749934 140481689634688 learning.py:507] global step 3216: loss = 0.0494 (0.309 sec/step)\n",
            "I0719 09:36:54.080377 140481689634688 learning.py:507] global step 3217: loss = 0.1020 (0.328 sec/step)\n",
            "I0719 09:36:54.408625 140481689634688 learning.py:507] global step 3218: loss = 0.0190 (0.327 sec/step)\n",
            "I0719 09:36:54.734554 140481689634688 learning.py:507] global step 3219: loss = 0.0815 (0.324 sec/step)\n",
            "I0719 09:36:55.080963 140481689634688 learning.py:507] global step 3220: loss = 0.0547 (0.345 sec/step)\n",
            "I0719 09:36:55.413172 140481689634688 learning.py:507] global step 3221: loss = 0.0994 (0.330 sec/step)\n",
            "I0719 09:36:55.762895 140481689634688 learning.py:507] global step 3222: loss = 0.0678 (0.347 sec/step)\n",
            "I0719 09:36:56.130750 140481689634688 learning.py:507] global step 3223: loss = 0.0361 (0.363 sec/step)\n",
            "I0719 09:36:56.464601 140481689634688 learning.py:507] global step 3224: loss = 0.1179 (0.332 sec/step)\n",
            "I0719 09:36:56.815811 140481689634688 learning.py:507] global step 3225: loss = 0.0485 (0.348 sec/step)\n",
            "I0719 09:36:57.155541 140481689634688 learning.py:507] global step 3226: loss = 0.1388 (0.337 sec/step)\n",
            "I0719 09:36:57.456420 140481689634688 learning.py:507] global step 3227: loss = 0.0472 (0.299 sec/step)\n",
            "I0719 09:36:57.760082 140481689634688 learning.py:507] global step 3228: loss = 0.0607 (0.302 sec/step)\n",
            "I0719 09:36:58.094906 140481689634688 learning.py:507] global step 3229: loss = 0.1009 (0.333 sec/step)\n",
            "I0719 09:36:58.457573 140481689634688 learning.py:507] global step 3230: loss = 0.2459 (0.361 sec/step)\n",
            "I0719 09:36:58.796333 140481689634688 learning.py:507] global step 3231: loss = 0.0557 (0.337 sec/step)\n",
            "I0719 09:36:59.127968 140481689634688 learning.py:507] global step 3232: loss = 0.1367 (0.330 sec/step)\n",
            "I0719 09:36:59.427356 140481689634688 learning.py:507] global step 3233: loss = 0.0448 (0.298 sec/step)\n",
            "I0719 09:36:59.751430 140481689634688 learning.py:507] global step 3234: loss = 0.0495 (0.322 sec/step)\n",
            "I0719 09:37:00.062335 140481689634688 learning.py:507] global step 3235: loss = 0.0349 (0.309 sec/step)\n",
            "I0719 09:37:00.426945 140481689634688 learning.py:507] global step 3236: loss = 0.1138 (0.363 sec/step)\n",
            "I0719 09:37:00.753685 140481689634688 learning.py:507] global step 3237: loss = 0.1348 (0.325 sec/step)\n",
            "I0719 09:37:01.107159 140481689634688 learning.py:507] global step 3238: loss = 0.0828 (0.352 sec/step)\n",
            "I0719 09:37:01.455571 140481689634688 learning.py:507] global step 3239: loss = 0.1166 (0.347 sec/step)\n",
            "I0719 09:37:01.812556 140481689634688 learning.py:507] global step 3240: loss = 0.1146 (0.355 sec/step)\n",
            "I0719 09:37:02.170714 140481689634688 learning.py:507] global step 3241: loss = 0.1491 (0.357 sec/step)\n",
            "I0719 09:37:02.473604 140481689634688 learning.py:507] global step 3242: loss = 0.0463 (0.301 sec/step)\n",
            "I0719 09:37:02.842309 140481689634688 learning.py:507] global step 3243: loss = 0.1158 (0.367 sec/step)\n",
            "I0719 09:37:03.192391 140481689634688 learning.py:507] global step 3244: loss = 0.1009 (0.348 sec/step)\n",
            "I0719 09:37:03.495729 140481689634688 learning.py:507] global step 3245: loss = 0.0893 (0.301 sec/step)\n",
            "I0719 09:37:03.853093 140481689634688 learning.py:507] global step 3246: loss = 0.1169 (0.356 sec/step)\n",
            "I0719 09:37:04.199886 140481689634688 learning.py:507] global step 3247: loss = 0.0975 (0.345 sec/step)\n",
            "I0719 09:37:04.528966 140481689634688 learning.py:507] global step 3248: loss = 0.1481 (0.327 sec/step)\n",
            "I0719 09:37:04.860641 140481689634688 learning.py:507] global step 3249: loss = 0.0242 (0.330 sec/step)\n",
            "I0719 09:37:05.210901 140481689634688 learning.py:507] global step 3250: loss = 0.0365 (0.348 sec/step)\n",
            "I0719 09:37:05.526900 140481689634688 learning.py:507] global step 3251: loss = 0.1187 (0.314 sec/step)\n",
            "I0719 09:37:05.855526 140481689634688 learning.py:507] global step 3252: loss = 0.0651 (0.327 sec/step)\n",
            "I0719 09:37:06.222197 140481689634688 learning.py:507] global step 3253: loss = 0.0731 (0.365 sec/step)\n",
            "I0719 09:37:06.566296 140481689634688 learning.py:507] global step 3254: loss = 0.1735 (0.342 sec/step)\n",
            "I0719 09:37:06.929638 140481689634688 learning.py:507] global step 3255: loss = 0.0895 (0.361 sec/step)\n",
            "I0719 09:37:07.272078 140481689634688 learning.py:507] global step 3256: loss = 0.1019 (0.341 sec/step)\n",
            "I0719 09:37:07.599712 140481689634688 learning.py:507] global step 3257: loss = 0.0495 (0.326 sec/step)\n",
            "I0719 09:37:07.935651 140481689634688 learning.py:507] global step 3258: loss = 0.0961 (0.334 sec/step)\n",
            "I0719 09:37:08.310205 140481689634688 learning.py:507] global step 3259: loss = 0.0729 (0.373 sec/step)\n",
            "I0719 09:37:08.652229 140481689634688 learning.py:507] global step 3260: loss = 0.1522 (0.340 sec/step)\n",
            "I0719 09:37:08.999397 140481689634688 learning.py:507] global step 3261: loss = 0.1504 (0.345 sec/step)\n",
            "I0719 09:37:09.332651 140481689634688 learning.py:507] global step 3262: loss = 0.0685 (0.331 sec/step)\n",
            "I0719 09:37:09.662762 140481689634688 learning.py:507] global step 3263: loss = 0.0125 (0.328 sec/step)\n",
            "I0719 09:37:09.982586 140481689634688 learning.py:507] global step 3264: loss = 0.1242 (0.318 sec/step)\n",
            "I0719 09:37:10.285491 140481689634688 learning.py:507] global step 3265: loss = 0.1452 (0.301 sec/step)\n",
            "I0719 09:37:10.636079 140481689634688 learning.py:507] global step 3266: loss = 0.2124 (0.349 sec/step)\n",
            "I0719 09:37:10.959667 140481689634688 learning.py:507] global step 3267: loss = 0.0122 (0.321 sec/step)\n",
            "I0719 09:37:11.300451 140481689634688 learning.py:507] global step 3268: loss = 0.0832 (0.339 sec/step)\n",
            "I0719 09:37:11.641852 140481689634688 learning.py:507] global step 3269: loss = 0.1148 (0.340 sec/step)\n",
            "I0719 09:37:11.932108 140481689634688 learning.py:507] global step 3270: loss = 0.0930 (0.289 sec/step)\n",
            "I0719 09:37:12.271727 140481689634688 learning.py:507] global step 3271: loss = 0.0943 (0.337 sec/step)\n",
            "I0719 09:37:12.613889 140481689634688 learning.py:507] global step 3272: loss = 0.1224 (0.339 sec/step)\n",
            "I0719 09:37:12.880595 140481689634688 learning.py:507] global step 3273: loss = 0.0910 (0.265 sec/step)\n",
            "I0719 09:37:13.257942 140481689634688 learning.py:507] global step 3274: loss = 0.0801 (0.375 sec/step)\n",
            "I0719 09:37:13.607129 140481689634688 learning.py:507] global step 3275: loss = 0.0885 (0.348 sec/step)\n",
            "I0719 09:37:13.931659 140481689634688 learning.py:507] global step 3276: loss = 0.1016 (0.323 sec/step)\n",
            "I0719 09:37:14.264981 140481689634688 learning.py:507] global step 3277: loss = 0.1512 (0.332 sec/step)\n",
            "I0719 09:37:14.604490 140481689634688 learning.py:507] global step 3278: loss = 0.1211 (0.338 sec/step)\n",
            "I0719 09:37:14.958398 140481689634688 learning.py:507] global step 3279: loss = 0.0950 (0.352 sec/step)\n",
            "I0719 09:37:15.302789 140481689634688 learning.py:507] global step 3280: loss = 0.0666 (0.343 sec/step)\n",
            "I0719 09:37:15.628430 140481689634688 learning.py:507] global step 3281: loss = 0.0747 (0.324 sec/step)\n",
            "I0719 09:37:15.978369 140481689634688 learning.py:507] global step 3282: loss = 0.0664 (0.348 sec/step)\n",
            "I0719 09:37:16.312957 140481689634688 learning.py:507] global step 3283: loss = 0.0980 (0.333 sec/step)\n",
            "I0719 09:37:16.660168 140481689634688 learning.py:507] global step 3284: loss = 0.0930 (0.344 sec/step)\n",
            "I0719 09:37:17.009738 140481689634688 learning.py:507] global step 3285: loss = 0.0951 (0.348 sec/step)\n",
            "I0719 09:37:17.354073 140481689634688 learning.py:507] global step 3286: loss = 0.0349 (0.343 sec/step)\n",
            "I0719 09:37:17.693077 140481689634688 learning.py:507] global step 3287: loss = 0.0947 (0.337 sec/step)\n",
            "I0719 09:37:18.041831 140481689634688 learning.py:507] global step 3288: loss = 0.0728 (0.347 sec/step)\n",
            "I0719 09:37:18.402467 140481689634688 learning.py:507] global step 3289: loss = 0.0576 (0.359 sec/step)\n",
            "I0719 09:37:18.756966 140481689634688 learning.py:507] global step 3290: loss = 0.0847 (0.351 sec/step)\n",
            "I0719 09:37:19.110301 140481689634688 learning.py:507] global step 3291: loss = 0.0414 (0.352 sec/step)\n",
            "I0719 09:37:19.461516 140481689634688 learning.py:507] global step 3292: loss = 0.1493 (0.349 sec/step)\n",
            "I0719 09:37:19.791541 140481689634688 learning.py:507] global step 3293: loss = 0.1387 (0.328 sec/step)\n",
            "I0719 09:37:20.067293 140481689634688 learning.py:507] global step 3294: loss = 0.1033 (0.274 sec/step)\n",
            "I0719 09:37:20.396433 140481689634688 learning.py:507] global step 3295: loss = 0.0336 (0.327 sec/step)\n",
            "I0719 09:37:20.688691 140481689634688 learning.py:507] global step 3296: loss = 0.1582 (0.291 sec/step)\n",
            "I0719 09:37:21.039361 140481689634688 learning.py:507] global step 3297: loss = 0.1169 (0.349 sec/step)\n",
            "I0719 09:37:21.374432 140481689634688 learning.py:507] global step 3298: loss = 0.0754 (0.333 sec/step)\n",
            "I0719 09:37:21.712207 140481689634688 learning.py:507] global step 3299: loss = 0.0651 (0.336 sec/step)\n",
            "I0719 09:37:21.992520 140481689634688 learning.py:507] global step 3300: loss = 0.0366 (0.279 sec/step)\n",
            "I0719 09:37:22.336370 140481689634688 learning.py:507] global step 3301: loss = 0.1442 (0.342 sec/step)\n",
            "I0719 09:37:22.681645 140481689634688 learning.py:507] global step 3302: loss = 0.0973 (0.344 sec/step)\n",
            "I0719 09:37:23.012837 140481689634688 learning.py:507] global step 3303: loss = 0.2376 (0.329 sec/step)\n",
            "I0719 09:37:23.325166 140481689634688 learning.py:507] global step 3304: loss = 0.2781 (0.310 sec/step)\n",
            "I0719 09:37:23.674034 140481689634688 learning.py:507] global step 3305: loss = 0.0591 (0.345 sec/step)\n",
            "I0719 09:37:24.009437 140481689634688 learning.py:507] global step 3306: loss = 0.1039 (0.333 sec/step)\n",
            "I0719 09:37:24.331854 140481689634688 learning.py:507] global step 3307: loss = 0.1503 (0.321 sec/step)\n",
            "I0719 09:37:24.675630 140481689634688 learning.py:507] global step 3308: loss = 0.0796 (0.342 sec/step)\n",
            "I0719 09:37:24.967010 140481689634688 learning.py:507] global step 3309: loss = 0.0448 (0.290 sec/step)\n",
            "I0719 09:37:25.253556 140481689634688 learning.py:507] global step 3310: loss = 0.1074 (0.285 sec/step)\n",
            "I0719 09:37:25.597578 140481689634688 learning.py:507] global step 3311: loss = 0.0238 (0.341 sec/step)\n",
            "I0719 09:37:25.970252 140481689634688 learning.py:507] global step 3312: loss = 0.0654 (0.371 sec/step)\n",
            "I0719 09:37:26.302039 140481689634688 learning.py:507] global step 3313: loss = 0.1296 (0.330 sec/step)\n",
            "I0719 09:37:26.640376 140481689634688 learning.py:507] global step 3314: loss = 0.0549 (0.336 sec/step)\n",
            "I0719 09:37:26.970240 140481689634688 learning.py:507] global step 3315: loss = 0.1124 (0.328 sec/step)\n",
            "I0719 09:37:27.298458 140481689634688 learning.py:507] global step 3316: loss = 0.1311 (0.326 sec/step)\n",
            "I0719 09:37:27.629191 140481689634688 learning.py:507] global step 3317: loss = 0.0741 (0.329 sec/step)\n",
            "I0719 09:37:28.006600 140481689634688 learning.py:507] global step 3318: loss = 0.1143 (0.376 sec/step)\n",
            "I0719 09:37:28.338594 140481689634688 learning.py:507] global step 3319: loss = 0.1121 (0.330 sec/step)\n",
            "I0719 09:37:28.671199 140481689634688 learning.py:507] global step 3320: loss = 0.0429 (0.331 sec/step)\n",
            "I0719 09:37:29.072967 140481689634688 learning.py:507] global step 3321: loss = 0.1340 (0.400 sec/step)\n",
            "I0719 09:37:29.405575 140481689634688 learning.py:507] global step 3322: loss = 0.1137 (0.329 sec/step)\n",
            "I0719 09:37:29.772855 140481689634688 learning.py:507] global step 3323: loss = 0.0717 (0.365 sec/step)\n",
            "I0719 09:37:30.111785 140481689634688 learning.py:507] global step 3324: loss = 0.0935 (0.336 sec/step)\n",
            "I0719 09:37:30.436642 140481689634688 learning.py:507] global step 3325: loss = 0.0541 (0.323 sec/step)\n",
            "I0719 09:37:30.748669 140481689634688 learning.py:507] global step 3326: loss = 0.1048 (0.310 sec/step)\n",
            "I0719 09:37:31.096118 140481689634688 learning.py:507] global step 3327: loss = 0.0366 (0.345 sec/step)\n",
            "I0719 09:37:31.430183 140481689634688 learning.py:507] global step 3328: loss = 0.0417 (0.332 sec/step)\n",
            "I0719 09:37:31.756096 140481689634688 learning.py:507] global step 3329: loss = 0.2288 (0.324 sec/step)\n",
            "I0719 09:37:32.085338 140481689634688 learning.py:507] global step 3330: loss = 0.0527 (0.327 sec/step)\n",
            "I0719 09:37:32.459897 140481689634688 learning.py:507] global step 3331: loss = 0.0963 (0.373 sec/step)\n",
            "I0719 09:37:32.794349 140481689634688 learning.py:507] global step 3332: loss = 0.0821 (0.332 sec/step)\n",
            "I0719 09:37:33.091624 140481689634688 learning.py:507] global step 3333: loss = 0.0597 (0.295 sec/step)\n",
            "I0719 09:37:33.425896 140481689634688 learning.py:507] global step 3334: loss = 0.0253 (0.332 sec/step)\n",
            "I0719 09:37:33.763386 140481689634688 learning.py:507] global step 3335: loss = 0.1666 (0.335 sec/step)\n",
            "I0719 09:37:34.136572 140481689634688 learning.py:507] global step 3336: loss = 0.1066 (0.371 sec/step)\n",
            "I0719 09:37:34.508573 140481689634688 learning.py:507] global step 3337: loss = 0.0637 (0.370 sec/step)\n",
            "I0719 09:37:34.860437 140481689634688 learning.py:507] global step 3338: loss = 0.0839 (0.350 sec/step)\n",
            "I0719 09:37:35.219120 140481689634688 learning.py:507] global step 3339: loss = 0.1344 (0.356 sec/step)\n",
            "I0719 09:37:35.540072 140481689634688 learning.py:507] global step 3340: loss = 0.0601 (0.319 sec/step)\n",
            "I0719 09:37:35.857368 140481689634688 learning.py:507] global step 3341: loss = 0.0821 (0.315 sec/step)\n",
            "I0719 09:37:36.184583 140481689634688 learning.py:507] global step 3342: loss = 0.1011 (0.325 sec/step)\n",
            "I0719 09:37:36.521661 140481689634688 learning.py:507] global step 3343: loss = 0.1038 (0.335 sec/step)\n",
            "I0719 09:37:36.865817 140481689634688 learning.py:507] global step 3344: loss = 0.1229 (0.342 sec/step)\n",
            "I0719 09:37:37.217914 140481689634688 learning.py:507] global step 3345: loss = 0.0880 (0.350 sec/step)\n",
            "I0719 09:37:37.551642 140481689634688 learning.py:507] global step 3346: loss = 0.1683 (0.332 sec/step)\n",
            "I0719 09:37:37.882626 140481689634688 learning.py:507] global step 3347: loss = 0.2011 (0.328 sec/step)\n",
            "I0719 09:37:38.235474 140481689634688 learning.py:507] global step 3348: loss = 0.1072 (0.351 sec/step)\n",
            "I0719 09:37:38.524549 140481689634688 learning.py:507] global step 3349: loss = 0.0681 (0.287 sec/step)\n",
            "I0719 09:37:38.831138 140481689634688 learning.py:507] global step 3350: loss = 0.1864 (0.305 sec/step)\n",
            "I0719 09:37:39.163789 140481689634688 learning.py:507] global step 3351: loss = 0.0467 (0.331 sec/step)\n",
            "I0719 09:37:39.481943 140481689634688 learning.py:507] global step 3352: loss = 0.2029 (0.317 sec/step)\n",
            "I0719 09:37:39.824568 140481689634688 learning.py:507] global step 3353: loss = 0.0763 (0.341 sec/step)\n",
            "I0719 09:37:40.164127 140481689634688 learning.py:507] global step 3354: loss = 0.0598 (0.338 sec/step)\n",
            "I0719 09:37:40.500464 140481689634688 learning.py:507] global step 3355: loss = 0.0977 (0.334 sec/step)\n",
            "I0719 09:37:40.862780 140481689634688 learning.py:507] global step 3356: loss = 0.0967 (0.361 sec/step)\n",
            "I0719 09:37:41.186197 140481689634688 learning.py:507] global step 3357: loss = 0.0740 (0.322 sec/step)\n",
            "I0719 09:37:41.516595 140481689634688 learning.py:507] global step 3358: loss = 0.1115 (0.328 sec/step)\n",
            "I0719 09:37:41.830708 140481689634688 learning.py:507] global step 3359: loss = 0.0193 (0.312 sec/step)\n",
            "I0719 09:37:42.147921 140481689634688 learning.py:507] global step 3360: loss = 0.1978 (0.315 sec/step)\n",
            "I0719 09:37:42.419311 140481689634688 learning.py:507] global step 3361: loss = 0.0751 (0.270 sec/step)\n",
            "I0719 09:37:42.772217 140481689634688 learning.py:507] global step 3362: loss = 0.0783 (0.351 sec/step)\n",
            "I0719 09:37:43.117682 140481689634688 learning.py:507] global step 3363: loss = 0.1340 (0.344 sec/step)\n",
            "I0719 09:37:43.445691 140481689634688 learning.py:507] global step 3364: loss = 0.1120 (0.326 sec/step)\n",
            "I0719 09:37:43.769633 140481689634688 learning.py:507] global step 3365: loss = 0.0655 (0.322 sec/step)\n",
            "I0719 09:37:44.087611 140481689634688 learning.py:507] global step 3366: loss = 0.1453 (0.316 sec/step)\n",
            "I0719 09:37:44.385032 140481689634688 learning.py:507] global step 3367: loss = 0.0477 (0.296 sec/step)\n",
            "I0719 09:37:44.732060 140481689634688 learning.py:507] global step 3368: loss = 0.1123 (0.345 sec/step)\n",
            "I0719 09:37:45.022848 140481689634688 learning.py:507] global step 3369: loss = 0.0841 (0.289 sec/step)\n",
            "I0719 09:37:45.365720 140481689634688 learning.py:507] global step 3370: loss = 0.1923 (0.341 sec/step)\n",
            "I0719 09:37:45.697195 140481689634688 learning.py:507] global step 3371: loss = 0.0850 (0.330 sec/step)\n",
            "I0719 09:37:46.025571 140481689634688 learning.py:507] global step 3372: loss = 0.0888 (0.327 sec/step)\n",
            "I0719 09:37:46.348187 140481689634688 learning.py:507] global step 3373: loss = 0.1644 (0.321 sec/step)\n",
            "I0719 09:37:46.684128 140481689634688 learning.py:507] global step 3374: loss = 0.0388 (0.333 sec/step)\n",
            "I0719 09:37:46.979669 140481689634688 learning.py:507] global step 3375: loss = 0.0583 (0.293 sec/step)\n",
            "I0719 09:37:47.318459 140481689634688 learning.py:507] global step 3376: loss = 0.0731 (0.336 sec/step)\n",
            "I0719 09:37:47.664119 140481689634688 learning.py:507] global step 3377: loss = 0.2072 (0.344 sec/step)\n",
            "I0719 09:37:48.012331 140481689634688 learning.py:507] global step 3378: loss = 0.0796 (0.346 sec/step)\n",
            "I0719 09:37:48.323410 140481689634688 learning.py:507] global step 3379: loss = 0.0570 (0.309 sec/step)\n",
            "I0719 09:37:48.627154 140481689634688 learning.py:507] global step 3380: loss = 0.0512 (0.302 sec/step)\n",
            "I0719 09:37:48.946894 140481689634688 learning.py:507] global step 3381: loss = 0.0396 (0.318 sec/step)\n",
            "I0719 09:37:49.288883 140481689634688 learning.py:507] global step 3382: loss = 0.0882 (0.340 sec/step)\n",
            "I0719 09:37:49.630371 140481689634688 learning.py:507] global step 3383: loss = 0.0545 (0.340 sec/step)\n",
            "I0719 09:37:49.958687 140481689634688 learning.py:507] global step 3384: loss = 0.0487 (0.327 sec/step)\n",
            "I0719 09:37:50.294803 140481689634688 learning.py:507] global step 3385: loss = 0.0777 (0.334 sec/step)\n",
            "I0719 09:37:50.624621 140481689634688 learning.py:507] global step 3386: loss = 0.0440 (0.328 sec/step)\n",
            "I0719 09:37:50.963605 140481689634688 learning.py:507] global step 3387: loss = 0.0204 (0.337 sec/step)\n",
            "I0719 09:37:51.278431 140481689634688 learning.py:507] global step 3388: loss = 0.1795 (0.313 sec/step)\n",
            "I0719 09:37:51.625139 140481689634688 learning.py:507] global step 3389: loss = 0.1294 (0.345 sec/step)\n",
            "I0719 09:37:51.969151 140481689634688 learning.py:507] global step 3390: loss = 0.0469 (0.342 sec/step)\n",
            "I0719 09:37:52.314150 140481689634688 learning.py:507] global step 3391: loss = 0.0743 (0.343 sec/step)\n",
            "I0719 09:37:52.596540 140481689634688 learning.py:507] global step 3392: loss = 0.0269 (0.281 sec/step)\n",
            "I0719 09:37:52.862592 140481689634688 learning.py:507] global step 3393: loss = 0.1110 (0.264 sec/step)\n",
            "I0719 09:37:53.209563 140481689634688 learning.py:507] global step 3394: loss = 0.1245 (0.345 sec/step)\n",
            "I0719 09:37:53.528376 140481689634688 learning.py:507] global step 3395: loss = 0.1084 (0.317 sec/step)\n",
            "I0719 09:37:53.798892 140481689634688 learning.py:507] global step 3396: loss = 0.1308 (0.269 sec/step)\n",
            "I0719 09:37:54.124331 140481689634688 learning.py:507] global step 3397: loss = 0.0428 (0.324 sec/step)\n",
            "I0719 09:37:54.456495 140481689634688 learning.py:507] global step 3398: loss = 0.0860 (0.331 sec/step)\n",
            "I0719 09:37:54.817563 140481689634688 learning.py:507] global step 3399: loss = 0.0540 (0.358 sec/step)\n",
            "I0719 09:37:55.148455 140481689634688 learning.py:507] global step 3400: loss = 0.0888 (0.329 sec/step)\n",
            "I0719 09:37:55.493481 140481689634688 learning.py:507] global step 3401: loss = 0.1101 (0.339 sec/step)\n",
            "I0719 09:37:55.845055 140481689634688 learning.py:507] global step 3402: loss = 0.0643 (0.347 sec/step)\n",
            "I0719 09:37:56.180878 140481689634688 learning.py:507] global step 3403: loss = 0.1713 (0.334 sec/step)\n",
            "I0719 09:37:56.505853 140481689634688 learning.py:507] global step 3404: loss = 0.0344 (0.323 sec/step)\n",
            "I0719 09:37:56.861613 140481689634688 learning.py:507] global step 3405: loss = 0.0908 (0.354 sec/step)\n",
            "I0719 09:37:57.177263 140481689634688 learning.py:507] global step 3406: loss = 0.1064 (0.314 sec/step)\n",
            "I0719 09:37:57.542890 140481689634688 learning.py:507] global step 3407: loss = 0.0795 (0.364 sec/step)\n",
            "I0719 09:37:57.876728 140481689634688 learning.py:507] global step 3408: loss = 0.1454 (0.332 sec/step)\n",
            "I0719 09:37:58.211061 140481689634688 learning.py:507] global step 3409: loss = 0.1326 (0.333 sec/step)\n",
            "I0719 09:37:58.539483 140481689634688 learning.py:507] global step 3410: loss = 0.1096 (0.326 sec/step)\n",
            "I0719 09:37:58.872916 140481689634688 learning.py:507] global step 3411: loss = 0.1992 (0.332 sec/step)\n",
            "I0719 09:37:59.208656 140481689634688 learning.py:507] global step 3412: loss = 0.0804 (0.334 sec/step)\n",
            "I0719 09:37:59.545259 140481689634688 learning.py:507] global step 3413: loss = 0.0143 (0.335 sec/step)\n",
            "I0719 09:37:59.883814 140481689634688 learning.py:507] global step 3414: loss = 0.0558 (0.337 sec/step)\n",
            "I0719 09:38:00.241723 140481689634688 learning.py:507] global step 3415: loss = 0.2349 (0.356 sec/step)\n",
            "I0719 09:38:00.552181 140481689634688 learning.py:507] global step 3416: loss = 0.0696 (0.308 sec/step)\n",
            "I0719 09:38:00.897779 140481689634688 learning.py:507] global step 3417: loss = 0.0982 (0.344 sec/step)\n",
            "I0719 09:38:01.218072 140481689634688 learning.py:507] global step 3418: loss = 0.1338 (0.319 sec/step)\n",
            "I0719 09:38:01.573247 140481689634688 learning.py:507] global step 3419: loss = 0.0603 (0.353 sec/step)\n",
            "I0719 09:38:01.911839 140481689634688 learning.py:507] global step 3420: loss = 0.1914 (0.337 sec/step)\n",
            "I0719 09:38:02.186562 140481689634688 learning.py:507] global step 3421: loss = 0.0795 (0.273 sec/step)\n",
            "I0719 09:38:02.532152 140481689634688 learning.py:507] global step 3422: loss = 0.1082 (0.344 sec/step)\n",
            "I0719 09:38:02.861242 140481689634688 learning.py:507] global step 3423: loss = 0.1581 (0.327 sec/step)\n",
            "I0719 09:38:03.148835 140481689634688 learning.py:507] global step 3424: loss = 0.1018 (0.286 sec/step)\n",
            "I0719 09:38:03.476723 140481689634688 learning.py:507] global step 3425: loss = 0.0874 (0.326 sec/step)\n",
            "I0719 09:38:03.823151 140481689634688 learning.py:507] global step 3426: loss = 0.1073 (0.345 sec/step)\n",
            "I0719 09:38:04.144235 140481689634688 learning.py:507] global step 3427: loss = 0.1237 (0.318 sec/step)\n",
            "I0719 09:38:04.452152 140481689634688 learning.py:507] global step 3428: loss = 0.0137 (0.306 sec/step)\n",
            "I0719 09:38:04.770713 140481689634688 learning.py:507] global step 3429: loss = 0.0991 (0.317 sec/step)\n",
            "I0719 09:38:05.121992 140481689634688 learning.py:507] global step 3430: loss = 0.0728 (0.350 sec/step)\n",
            "I0719 09:38:05.429638 140481689634688 learning.py:507] global step 3431: loss = 0.0911 (0.306 sec/step)\n",
            "I0719 09:38:05.762464 140481689634688 learning.py:507] global step 3432: loss = 0.0449 (0.331 sec/step)\n",
            "I0719 09:38:06.062095 140481689634688 learning.py:507] global step 3433: loss = 0.1214 (0.297 sec/step)\n",
            "I0719 09:38:06.395651 140481689634688 learning.py:507] global step 3434: loss = 0.1274 (0.332 sec/step)\n",
            "I0719 09:38:06.724352 140481689634688 learning.py:507] global step 3435: loss = 0.0800 (0.327 sec/step)\n",
            "I0719 09:38:07.050511 140481689634688 learning.py:507] global step 3436: loss = 0.0211 (0.324 sec/step)\n",
            "I0719 09:38:07.374835 140481689634688 learning.py:507] global step 3437: loss = 0.0896 (0.323 sec/step)\n",
            "I0719 09:38:07.726490 140481689634688 learning.py:507] global step 3438: loss = 0.1697 (0.350 sec/step)\n",
            "I0719 09:38:08.065828 140481689634688 learning.py:507] global step 3439: loss = 0.0420 (0.337 sec/step)\n",
            "I0719 09:38:08.413818 140481689634688 learning.py:507] global step 3440: loss = 0.0587 (0.344 sec/step)\n",
            "I0719 09:38:08.763972 140481689634688 learning.py:507] global step 3441: loss = 0.1853 (0.348 sec/step)\n",
            "I0719 09:38:09.119468 140481689634688 learning.py:507] global step 3442: loss = 0.0746 (0.354 sec/step)\n",
            "I0719 09:38:09.463250 140481689634688 learning.py:507] global step 3443: loss = 0.0716 (0.342 sec/step)\n",
            "I0719 09:38:09.793092 140481689634688 learning.py:507] global step 3444: loss = 0.1243 (0.328 sec/step)\n",
            "I0719 09:38:10.136001 140481689634688 learning.py:507] global step 3445: loss = 0.1159 (0.341 sec/step)\n",
            "I0719 09:38:10.475514 140481689634688 learning.py:507] global step 3446: loss = 0.0349 (0.338 sec/step)\n",
            "I0719 09:38:10.791887 140481689634688 learning.py:507] global step 3447: loss = 0.0821 (0.315 sec/step)\n",
            "I0719 09:38:11.129605 140481689634688 learning.py:507] global step 3448: loss = 0.1255 (0.336 sec/step)\n",
            "I0719 09:38:11.455731 140481689634688 learning.py:507] global step 3449: loss = 0.1292 (0.324 sec/step)\n",
            "I0719 09:38:11.785501 140481689634688 learning.py:507] global step 3450: loss = 0.0704 (0.328 sec/step)\n",
            "I0719 09:38:12.110370 140481689634688 learning.py:507] global step 3451: loss = 0.0183 (0.323 sec/step)\n",
            "I0719 09:38:12.465894 140481689634688 learning.py:507] global step 3452: loss = 0.0975 (0.354 sec/step)\n",
            "I0719 09:38:12.807906 140481689634688 learning.py:507] global step 3453: loss = 0.1115 (0.340 sec/step)\n",
            "I0719 09:38:13.115036 140481689634688 learning.py:507] global step 3454: loss = 0.1646 (0.305 sec/step)\n",
            "I0719 09:38:13.451041 140481689634688 learning.py:507] global step 3455: loss = 0.1815 (0.334 sec/step)\n",
            "I0719 09:38:13.771990 140481689634688 learning.py:507] global step 3456: loss = 0.0616 (0.319 sec/step)\n",
            "I0719 09:38:14.093394 140481689634688 learning.py:507] global step 3457: loss = 0.1662 (0.319 sec/step)\n",
            "I0719 09:38:14.404148 140481689634688 learning.py:507] global step 3458: loss = 0.0563 (0.309 sec/step)\n",
            "I0719 09:38:14.694722 140481689634688 learning.py:507] global step 3459: loss = 0.2187 (0.289 sec/step)\n",
            "I0719 09:38:15.033566 140481689634688 learning.py:507] global step 3460: loss = 0.0634 (0.337 sec/step)\n",
            "I0719 09:38:15.364638 140481689634688 learning.py:507] global step 3461: loss = 0.1467 (0.329 sec/step)\n",
            "I0719 09:38:15.676176 140481689634688 learning.py:507] global step 3462: loss = 0.0663 (0.310 sec/step)\n",
            "I0719 09:38:16.012038 140481689634688 learning.py:507] global step 3463: loss = 0.0603 (0.334 sec/step)\n",
            "I0719 09:38:16.290576 140481689634688 learning.py:507] global step 3464: loss = 0.0935 (0.277 sec/step)\n",
            "I0719 09:38:16.610384 140481689634688 learning.py:507] global step 3465: loss = 0.1322 (0.318 sec/step)\n",
            "I0719 09:38:16.957171 140481689634688 learning.py:507] global step 3466: loss = 0.1452 (0.345 sec/step)\n",
            "I0719 09:38:17.296493 140481689634688 learning.py:507] global step 3467: loss = 0.0793 (0.338 sec/step)\n",
            "I0719 09:38:17.604844 140481689634688 learning.py:507] global step 3468: loss = 0.1177 (0.307 sec/step)\n",
            "I0719 09:38:17.949611 140481689634688 learning.py:507] global step 3469: loss = 0.0766 (0.343 sec/step)\n",
            "I0719 09:38:18.296322 140481689634688 learning.py:507] global step 3470: loss = 0.0880 (0.345 sec/step)\n",
            "I0719 09:38:18.630978 140481689634688 learning.py:507] global step 3471: loss = 0.0468 (0.333 sec/step)\n",
            "I0719 09:38:18.952812 140481689634688 learning.py:507] global step 3472: loss = 0.0704 (0.320 sec/step)\n",
            "I0719 09:38:19.304631 140481689634688 learning.py:507] global step 3473: loss = 0.0385 (0.350 sec/step)\n",
            "I0719 09:38:19.622619 140481689634688 learning.py:507] global step 3474: loss = 0.1435 (0.316 sec/step)\n",
            "I0719 09:38:19.954738 140481689634688 learning.py:507] global step 3475: loss = 0.1386 (0.330 sec/step)\n",
            "I0719 09:38:20.289792 140481689634688 learning.py:507] global step 3476: loss = 0.0973 (0.333 sec/step)\n",
            "I0719 09:38:20.631705 140481689634688 learning.py:507] global step 3477: loss = 0.0307 (0.340 sec/step)\n",
            "I0719 09:38:20.924119 140481689634688 learning.py:507] global step 3478: loss = 0.0657 (0.287 sec/step)\n",
            "I0719 09:38:21.287455 140481689634688 learning.py:507] global step 3479: loss = 0.0488 (0.361 sec/step)\n",
            "I0719 09:38:21.569857 140481689634688 learning.py:507] global step 3480: loss = 0.0807 (0.281 sec/step)\n",
            "I0719 09:38:21.884097 140481689634688 learning.py:507] global step 3481: loss = 0.0955 (0.312 sec/step)\n",
            "I0719 09:38:22.208077 140481689634688 learning.py:507] global step 3482: loss = 0.0144 (0.322 sec/step)\n",
            "I0719 09:38:22.560019 140481689634688 learning.py:507] global step 3483: loss = 0.2003 (0.350 sec/step)\n",
            "I0719 09:38:22.871334 140481689634688 learning.py:507] global step 3484: loss = 0.1600 (0.309 sec/step)\n",
            "I0719 09:38:23.241003 140481689634688 learning.py:507] global step 3485: loss = 0.0626 (0.368 sec/step)\n",
            "I0719 09:38:23.596141 140481689634688 learning.py:507] global step 3486: loss = 0.0860 (0.353 sec/step)\n",
            "I0719 09:38:23.940062 140481689634688 learning.py:507] global step 3487: loss = 0.0581 (0.342 sec/step)\n",
            "I0719 09:38:24.269226 140481689634688 learning.py:507] global step 3488: loss = 0.0339 (0.327 sec/step)\n",
            "I0719 09:38:24.593709 140481689634688 learning.py:507] global step 3489: loss = 0.0972 (0.323 sec/step)\n",
            "I0719 09:38:24.936123 140481689634688 learning.py:507] global step 3490: loss = 0.0732 (0.341 sec/step)\n",
            "I0719 09:38:25.268723 140481689634688 learning.py:507] global step 3491: loss = 0.0538 (0.331 sec/step)\n",
            "I0719 09:38:25.576796 140481689634688 learning.py:507] global step 3492: loss = 0.0753 (0.306 sec/step)\n",
            "I0719 09:38:25.929095 140481689634688 learning.py:507] global step 3493: loss = 0.1347 (0.351 sec/step)\n",
            "I0719 09:38:26.469437 140481689634688 learning.py:507] global step 3494: loss = 0.0834 (0.474 sec/step)\n",
            "I0719 09:38:26.922471 140481689634688 learning.py:507] global step 3495: loss = 0.1070 (0.421 sec/step)\n",
            "I0719 09:38:27.371861 140481689634688 learning.py:507] global step 3496: loss = 0.1223 (0.445 sec/step)\n",
            "I0719 09:38:27.476856 140479018202880 supervisor.py:1050] Recording summary at step 3496.\n",
            "I0719 09:38:27.722081 140481689634688 learning.py:507] global step 3497: loss = 0.1283 (0.349 sec/step)\n",
            "I0719 09:38:28.050887 140481689634688 learning.py:507] global step 3498: loss = 0.0772 (0.327 sec/step)\n",
            "I0719 09:38:28.342342 140481689634688 learning.py:507] global step 3499: loss = 0.0573 (0.290 sec/step)\n",
            "I0719 09:38:28.678644 140481689634688 learning.py:507] global step 3500: loss = 0.0849 (0.335 sec/step)\n",
            "I0719 09:38:29.017554 140481689634688 learning.py:507] global step 3501: loss = 0.1017 (0.337 sec/step)\n",
            "I0719 09:38:29.362932 140481689634688 learning.py:507] global step 3502: loss = 0.2534 (0.343 sec/step)\n",
            "I0719 09:38:29.702153 140481689634688 learning.py:507] global step 3503: loss = 0.1095 (0.337 sec/step)\n",
            "I0719 09:38:30.043977 140481689634688 learning.py:507] global step 3504: loss = 0.1198 (0.340 sec/step)\n",
            "I0719 09:38:30.336116 140481689634688 learning.py:507] global step 3505: loss = 0.1177 (0.290 sec/step)\n",
            "I0719 09:38:30.682767 140481689634688 learning.py:507] global step 3506: loss = 0.1531 (0.345 sec/step)\n",
            "I0719 09:38:31.009314 140481689634688 learning.py:507] global step 3507: loss = 0.1790 (0.325 sec/step)\n",
            "I0719 09:38:31.351521 140481689634688 learning.py:507] global step 3508: loss = 0.0852 (0.340 sec/step)\n",
            "I0719 09:38:31.703113 140481689634688 learning.py:507] global step 3509: loss = 0.1722 (0.350 sec/step)\n",
            "I0719 09:38:32.032903 140481689634688 learning.py:507] global step 3510: loss = 0.1813 (0.328 sec/step)\n",
            "I0719 09:38:32.372481 140481689634688 learning.py:507] global step 3511: loss = 0.1356 (0.338 sec/step)\n",
            "I0719 09:38:32.719697 140481689634688 learning.py:507] global step 3512: loss = 0.0339 (0.346 sec/step)\n",
            "I0719 09:38:33.074258 140481689634688 learning.py:507] global step 3513: loss = 0.0506 (0.353 sec/step)\n",
            "I0719 09:38:33.430634 140481689634688 learning.py:507] global step 3514: loss = 0.0598 (0.354 sec/step)\n",
            "I0719 09:38:33.756566 140481689634688 learning.py:507] global step 3515: loss = 0.0804 (0.324 sec/step)\n",
            "I0719 09:38:34.078138 140481689634688 learning.py:507] global step 3516: loss = 0.0979 (0.320 sec/step)\n",
            "I0719 09:38:34.393987 140481689634688 learning.py:507] global step 3517: loss = 0.1102 (0.314 sec/step)\n",
            "I0719 09:38:34.757131 140481689634688 learning.py:507] global step 3518: loss = 0.0876 (0.361 sec/step)\n",
            "I0719 09:38:35.096151 140481689634688 learning.py:507] global step 3519: loss = 0.0573 (0.337 sec/step)\n",
            "I0719 09:38:35.435097 140481689634688 learning.py:507] global step 3520: loss = 0.0398 (0.337 sec/step)\n",
            "I0719 09:38:35.758732 140481689634688 learning.py:507] global step 3521: loss = 0.0930 (0.322 sec/step)\n",
            "I0719 09:38:36.106389 140481689634688 learning.py:507] global step 3522: loss = 0.0908 (0.346 sec/step)\n",
            "I0719 09:38:36.448309 140481689634688 learning.py:507] global step 3523: loss = 0.1413 (0.340 sec/step)\n",
            "I0719 09:38:36.788888 140481689634688 learning.py:507] global step 3524: loss = 0.0254 (0.339 sec/step)\n",
            "I0719 09:38:37.119309 140481689634688 learning.py:507] global step 3525: loss = 0.2801 (0.329 sec/step)\n",
            "I0719 09:38:37.444961 140481689634688 learning.py:507] global step 3526: loss = 0.0562 (0.324 sec/step)\n",
            "I0719 09:38:37.772197 140481689634688 learning.py:507] global step 3527: loss = 0.2989 (0.325 sec/step)\n",
            "I0719 09:38:38.119158 140481689634688 learning.py:507] global step 3528: loss = 0.0861 (0.345 sec/step)\n",
            "I0719 09:38:38.471222 140481689634688 learning.py:507] global step 3529: loss = 0.0220 (0.350 sec/step)\n",
            "I0719 09:38:38.793115 140481689634688 learning.py:507] global step 3530: loss = 0.0875 (0.320 sec/step)\n",
            "I0719 09:38:39.124820 140481689634688 learning.py:507] global step 3531: loss = 0.0945 (0.330 sec/step)\n",
            "I0719 09:38:39.412505 140481689634688 learning.py:507] global step 3532: loss = 0.1111 (0.286 sec/step)\n",
            "I0719 09:38:39.783166 140481689634688 learning.py:507] global step 3533: loss = 0.1300 (0.369 sec/step)\n",
            "I0719 09:38:40.055709 140481689634688 learning.py:507] global step 3534: loss = 0.1631 (0.271 sec/step)\n",
            "I0719 09:38:40.393898 140481689634688 learning.py:507] global step 3535: loss = 0.1155 (0.336 sec/step)\n",
            "I0719 09:38:40.715299 140481689634688 learning.py:507] global step 3536: loss = 0.1097 (0.319 sec/step)\n",
            "I0719 09:38:41.059484 140481689634688 learning.py:507] global step 3537: loss = 0.0761 (0.343 sec/step)\n",
            "I0719 09:38:41.412139 140481689634688 learning.py:507] global step 3538: loss = 0.0607 (0.351 sec/step)\n",
            "I0719 09:38:41.764860 140481689634688 learning.py:507] global step 3539: loss = 0.0637 (0.351 sec/step)\n",
            "I0719 09:38:42.068095 140481689634688 learning.py:507] global step 3540: loss = 0.1743 (0.301 sec/step)\n",
            "I0719 09:38:42.387249 140481689634688 learning.py:507] global step 3541: loss = 0.1781 (0.317 sec/step)\n",
            "I0719 09:38:42.696547 140481689634688 learning.py:507] global step 3542: loss = 0.0821 (0.308 sec/step)\n",
            "I0719 09:38:43.070675 140481689634688 learning.py:507] global step 3543: loss = 0.0957 (0.372 sec/step)\n",
            "I0719 09:38:43.408400 140481689634688 learning.py:507] global step 3544: loss = 0.1044 (0.336 sec/step)\n",
            "I0719 09:38:43.733442 140481689634688 learning.py:507] global step 3545: loss = 0.1001 (0.323 sec/step)\n",
            "I0719 09:38:44.042301 140481689634688 learning.py:507] global step 3546: loss = 0.2649 (0.307 sec/step)\n",
            "I0719 09:38:44.402008 140481689634688 learning.py:507] global step 3547: loss = 0.0833 (0.358 sec/step)\n",
            "I0719 09:38:44.753625 140481689634688 learning.py:507] global step 3548: loss = 0.1081 (0.350 sec/step)\n",
            "I0719 09:38:45.149794 140481689634688 learning.py:507] global step 3549: loss = 0.0525 (0.394 sec/step)\n",
            "I0719 09:38:45.500330 140481689634688 learning.py:507] global step 3550: loss = 0.1228 (0.349 sec/step)\n",
            "I0719 09:38:45.829215 140481689634688 learning.py:507] global step 3551: loss = 0.0795 (0.327 sec/step)\n",
            "I0719 09:38:46.168891 140481689634688 learning.py:507] global step 3552: loss = 0.0472 (0.338 sec/step)\n",
            "I0719 09:38:46.502839 140481689634688 learning.py:507] global step 3553: loss = 0.0973 (0.332 sec/step)\n",
            "I0719 09:38:46.838845 140481689634688 learning.py:507] global step 3554: loss = 0.1557 (0.334 sec/step)\n",
            "I0719 09:38:47.181497 140481689634688 learning.py:507] global step 3555: loss = 0.0862 (0.341 sec/step)\n",
            "I0719 09:38:47.515151 140481689634688 learning.py:507] global step 3556: loss = 0.2481 (0.332 sec/step)\n",
            "I0719 09:38:47.840062 140481689634688 learning.py:507] global step 3557: loss = 0.0924 (0.323 sec/step)\n",
            "I0719 09:38:48.159666 140481689634688 learning.py:507] global step 3558: loss = 0.1706 (0.318 sec/step)\n",
            "I0719 09:38:48.508078 140481689634688 learning.py:507] global step 3559: loss = 0.0785 (0.347 sec/step)\n",
            "I0719 09:38:48.854210 140481689634688 learning.py:507] global step 3560: loss = 0.0807 (0.345 sec/step)\n",
            "I0719 09:38:49.189580 140481689634688 learning.py:507] global step 3561: loss = 0.1754 (0.334 sec/step)\n",
            "I0719 09:38:49.476317 140481689634688 learning.py:507] global step 3562: loss = 0.0713 (0.285 sec/step)\n",
            "I0719 09:38:49.820445 140481689634688 learning.py:507] global step 3563: loss = 0.0621 (0.342 sec/step)\n",
            "I0719 09:38:50.158814 140481689634688 learning.py:507] global step 3564: loss = 0.1301 (0.337 sec/step)\n",
            "I0719 09:38:50.494551 140481689634688 learning.py:507] global step 3565: loss = 0.0771 (0.334 sec/step)\n",
            "I0719 09:38:50.837395 140481689634688 learning.py:507] global step 3566: loss = 0.1748 (0.341 sec/step)\n",
            "I0719 09:38:51.182064 140481689634688 learning.py:507] global step 3567: loss = 0.0320 (0.343 sec/step)\n",
            "I0719 09:38:51.506611 140481689634688 learning.py:507] global step 3568: loss = 0.1134 (0.323 sec/step)\n",
            "I0719 09:38:51.841408 140481689634688 learning.py:507] global step 3569: loss = 0.0782 (0.333 sec/step)\n",
            "I0719 09:38:52.199060 140481689634688 learning.py:507] global step 3570: loss = 0.0833 (0.356 sec/step)\n",
            "I0719 09:38:52.529350 140481689634688 learning.py:507] global step 3571: loss = 0.0643 (0.329 sec/step)\n",
            "I0719 09:38:52.836922 140481689634688 learning.py:507] global step 3572: loss = 0.0365 (0.306 sec/step)\n",
            "I0719 09:38:53.139500 140481689634688 learning.py:507] global step 3573: loss = 0.0481 (0.301 sec/step)\n",
            "I0719 09:38:53.486404 140481689634688 learning.py:507] global step 3574: loss = 0.0531 (0.345 sec/step)\n",
            "I0719 09:38:53.847191 140481689634688 learning.py:507] global step 3575: loss = 0.1485 (0.359 sec/step)\n",
            "I0719 09:38:54.179678 140481689634688 learning.py:507] global step 3576: loss = 0.0873 (0.331 sec/step)\n",
            "I0719 09:38:54.515367 140481689634688 learning.py:507] global step 3577: loss = 0.0263 (0.332 sec/step)\n",
            "I0719 09:38:54.857978 140481689634688 learning.py:507] global step 3578: loss = 0.1073 (0.341 sec/step)\n",
            "I0719 09:38:55.132412 140481689634688 learning.py:507] global step 3579: loss = 0.1190 (0.273 sec/step)\n",
            "I0719 09:38:55.478299 140481689634688 learning.py:507] global step 3580: loss = 0.1038 (0.344 sec/step)\n",
            "I0719 09:38:55.824295 140481689634688 learning.py:507] global step 3581: loss = 0.0550 (0.344 sec/step)\n",
            "I0719 09:38:56.157799 140481689634688 learning.py:507] global step 3582: loss = 0.1653 (0.332 sec/step)\n",
            "I0719 09:38:56.489589 140481689634688 learning.py:507] global step 3583: loss = 0.1022 (0.330 sec/step)\n",
            "I0719 09:38:56.832992 140481689634688 learning.py:507] global step 3584: loss = 0.0713 (0.341 sec/step)\n",
            "I0719 09:38:57.164802 140481689634688 learning.py:507] global step 3585: loss = 0.0868 (0.330 sec/step)\n",
            "I0719 09:38:57.515650 140481689634688 learning.py:507] global step 3586: loss = 0.0525 (0.349 sec/step)\n",
            "I0719 09:38:57.817807 140481689634688 learning.py:507] global step 3587: loss = 0.0718 (0.300 sec/step)\n",
            "I0719 09:38:58.176330 140481689634688 learning.py:507] global step 3588: loss = 0.0952 (0.357 sec/step)\n",
            "I0719 09:38:58.492527 140481689634688 learning.py:507] global step 3589: loss = 0.0616 (0.315 sec/step)\n",
            "I0719 09:38:58.844220 140481689634688 learning.py:507] global step 3590: loss = 0.0896 (0.350 sec/step)\n",
            "I0719 09:38:59.134174 140481689634688 learning.py:507] global step 3591: loss = 0.0536 (0.288 sec/step)\n",
            "I0719 09:38:59.481652 140481689634688 learning.py:507] global step 3592: loss = 0.2160 (0.346 sec/step)\n",
            "I0719 09:38:59.801634 140481689634688 learning.py:507] global step 3593: loss = 0.1231 (0.318 sec/step)\n",
            "I0719 09:39:00.126651 140481689634688 learning.py:507] global step 3594: loss = 0.1705 (0.323 sec/step)\n",
            "I0719 09:39:00.463290 140481689634688 learning.py:507] global step 3595: loss = 0.0637 (0.335 sec/step)\n",
            "I0719 09:39:00.807130 140481689634688 learning.py:507] global step 3596: loss = 0.0509 (0.342 sec/step)\n",
            "I0719 09:39:01.117613 140481689634688 learning.py:507] global step 3597: loss = 0.1037 (0.309 sec/step)\n",
            "I0719 09:39:01.458935 140481689634688 learning.py:507] global step 3598: loss = 0.0504 (0.340 sec/step)\n",
            "I0719 09:39:01.799537 140481689634688 learning.py:507] global step 3599: loss = 0.0417 (0.339 sec/step)\n",
            "I0719 09:39:02.146049 140481689634688 learning.py:507] global step 3600: loss = 0.1629 (0.345 sec/step)\n",
            "I0719 09:39:02.501697 140481689634688 learning.py:507] global step 3601: loss = 0.0607 (0.354 sec/step)\n",
            "I0719 09:39:02.826711 140481689634688 learning.py:507] global step 3602: loss = 0.0222 (0.323 sec/step)\n",
            "I0719 09:39:03.156179 140481689634688 learning.py:507] global step 3603: loss = 0.0809 (0.327 sec/step)\n",
            "I0719 09:39:03.502807 140481689634688 learning.py:507] global step 3604: loss = 0.0527 (0.345 sec/step)\n",
            "I0719 09:39:03.834558 140481689634688 learning.py:507] global step 3605: loss = 0.0782 (0.330 sec/step)\n",
            "I0719 09:39:04.181158 140481689634688 learning.py:507] global step 3606: loss = 0.0658 (0.345 sec/step)\n",
            "I0719 09:39:04.537911 140481689634688 learning.py:507] global step 3607: loss = 0.1890 (0.355 sec/step)\n",
            "I0719 09:39:04.832475 140481689634688 learning.py:507] global step 3608: loss = 0.0830 (0.293 sec/step)\n",
            "I0719 09:39:05.158522 140481689634688 learning.py:507] global step 3609: loss = 0.0200 (0.324 sec/step)\n",
            "I0719 09:39:05.497079 140481689634688 learning.py:507] global step 3610: loss = 0.1059 (0.337 sec/step)\n",
            "I0719 09:39:05.830862 140481689634688 learning.py:507] global step 3611: loss = 0.1103 (0.330 sec/step)\n",
            "I0719 09:39:06.125430 140481689634688 learning.py:507] global step 3612: loss = 0.1668 (0.293 sec/step)\n",
            "I0719 09:39:06.401567 140481689634688 learning.py:507] global step 3613: loss = 0.1168 (0.274 sec/step)\n",
            "I0719 09:39:06.700693 140481689634688 learning.py:507] global step 3614: loss = 0.1320 (0.297 sec/step)\n",
            "I0719 09:39:07.016074 140481689634688 learning.py:507] global step 3615: loss = 0.0643 (0.314 sec/step)\n",
            "I0719 09:39:07.351356 140481689634688 learning.py:507] global step 3616: loss = 0.0296 (0.334 sec/step)\n",
            "I0719 09:39:07.689465 140481689634688 learning.py:507] global step 3617: loss = 0.1216 (0.336 sec/step)\n",
            "I0719 09:39:08.006476 140481689634688 learning.py:507] global step 3618: loss = 0.0796 (0.315 sec/step)\n",
            "I0719 09:39:08.342184 140481689634688 learning.py:507] global step 3619: loss = 0.0480 (0.334 sec/step)\n",
            "I0719 09:39:08.698621 140481689634688 learning.py:507] global step 3620: loss = 0.1238 (0.354 sec/step)\n",
            "I0719 09:39:09.042821 140481689634688 learning.py:507] global step 3621: loss = 0.0506 (0.342 sec/step)\n",
            "I0719 09:39:09.389528 140481689634688 learning.py:507] global step 3622: loss = 0.1373 (0.345 sec/step)\n",
            "I0719 09:39:09.727959 140481689634688 learning.py:507] global step 3623: loss = 0.0544 (0.337 sec/step)\n",
            "I0719 09:39:10.031844 140481689634688 learning.py:507] global step 3624: loss = 0.1327 (0.302 sec/step)\n",
            "I0719 09:39:10.342580 140481689634688 learning.py:507] global step 3625: loss = 0.0887 (0.309 sec/step)\n",
            "I0719 09:39:10.684135 140481689634688 learning.py:507] global step 3626: loss = 0.0946 (0.340 sec/step)\n",
            "I0719 09:39:11.025995 140481689634688 learning.py:507] global step 3627: loss = 0.0854 (0.340 sec/step)\n",
            "I0719 09:39:11.359162 140481689634688 learning.py:507] global step 3628: loss = 0.0928 (0.331 sec/step)\n",
            "I0719 09:39:11.703042 140481689634688 learning.py:507] global step 3629: loss = 0.0285 (0.342 sec/step)\n",
            "I0719 09:39:11.986739 140481689634688 learning.py:507] global step 3630: loss = 0.0509 (0.282 sec/step)\n",
            "I0719 09:39:12.330801 140481689634688 learning.py:507] global step 3631: loss = 0.0732 (0.342 sec/step)\n",
            "I0719 09:39:12.654156 140481689634688 learning.py:507] global step 3632: loss = 0.0821 (0.321 sec/step)\n",
            "I0719 09:39:12.999389 140481689634688 learning.py:507] global step 3633: loss = 0.0655 (0.343 sec/step)\n",
            "I0719 09:39:13.327982 140481689634688 learning.py:507] global step 3634: loss = 0.0310 (0.327 sec/step)\n",
            "I0719 09:39:13.653096 140481689634688 learning.py:507] global step 3635: loss = 0.0654 (0.323 sec/step)\n",
            "I0719 09:39:14.013545 140481689634688 learning.py:507] global step 3636: loss = 0.0556 (0.359 sec/step)\n",
            "I0719 09:39:14.344804 140481689634688 learning.py:507] global step 3637: loss = 0.1277 (0.329 sec/step)\n",
            "I0719 09:39:14.687325 140481689634688 learning.py:507] global step 3638: loss = 0.0360 (0.341 sec/step)\n",
            "I0719 09:39:15.034712 140481689634688 learning.py:507] global step 3639: loss = 0.1947 (0.346 sec/step)\n",
            "I0719 09:39:15.379194 140481689634688 learning.py:507] global step 3640: loss = 0.0669 (0.343 sec/step)\n",
            "I0719 09:39:15.711588 140481689634688 learning.py:507] global step 3641: loss = 0.1398 (0.331 sec/step)\n",
            "I0719 09:39:16.089519 140481689634688 learning.py:507] global step 3642: loss = 0.1096 (0.376 sec/step)\n",
            "I0719 09:39:16.382332 140481689634688 learning.py:507] global step 3643: loss = 0.2172 (0.291 sec/step)\n",
            "I0719 09:39:16.699128 140481689634688 learning.py:507] global step 3644: loss = 0.0340 (0.315 sec/step)\n",
            "I0719 09:39:17.021808 140481689634688 learning.py:507] global step 3645: loss = 0.0965 (0.321 sec/step)\n",
            "I0719 09:39:17.358643 140481689634688 learning.py:507] global step 3646: loss = 0.1253 (0.335 sec/step)\n",
            "I0719 09:39:17.709655 140481689634688 learning.py:507] global step 3647: loss = 0.0645 (0.349 sec/step)\n",
            "I0719 09:39:18.048704 140481689634688 learning.py:507] global step 3648: loss = 0.0403 (0.337 sec/step)\n",
            "I0719 09:39:18.405925 140481689634688 learning.py:507] global step 3649: loss = 0.0889 (0.355 sec/step)\n",
            "I0719 09:39:18.762338 140481689634688 learning.py:507] global step 3650: loss = 0.1010 (0.354 sec/step)\n",
            "I0719 09:39:19.107858 140481689634688 learning.py:507] global step 3651: loss = 0.1232 (0.343 sec/step)\n",
            "I0719 09:39:19.460229 140481689634688 learning.py:507] global step 3652: loss = 0.0244 (0.350 sec/step)\n",
            "I0719 09:39:19.834222 140481689634688 learning.py:507] global step 3653: loss = 0.1161 (0.372 sec/step)\n",
            "I0719 09:39:20.166823 140481689634688 learning.py:507] global step 3654: loss = 0.0911 (0.331 sec/step)\n",
            "I0719 09:39:20.478075 140481689634688 learning.py:507] global step 3655: loss = 0.0916 (0.309 sec/step)\n",
            "I0719 09:39:20.829870 140481689634688 learning.py:507] global step 3656: loss = 0.0948 (0.350 sec/step)\n",
            "I0719 09:39:21.142131 140481689634688 learning.py:507] global step 3657: loss = 0.0855 (0.310 sec/step)\n",
            "I0719 09:39:21.441307 140481689634688 learning.py:507] global step 3658: loss = 0.0607 (0.297 sec/step)\n",
            "I0719 09:39:21.786697 140481689634688 learning.py:507] global step 3659: loss = 0.1243 (0.344 sec/step)\n",
            "I0719 09:39:22.126176 140481689634688 learning.py:507] global step 3660: loss = 0.0708 (0.338 sec/step)\n",
            "I0719 09:39:22.455958 140481689634688 learning.py:507] global step 3661: loss = 0.0797 (0.328 sec/step)\n",
            "I0719 09:39:22.827541 140481689634688 learning.py:507] global step 3662: loss = 0.0490 (0.370 sec/step)\n",
            "I0719 09:39:23.169953 140481689634688 learning.py:507] global step 3663: loss = 0.1110 (0.341 sec/step)\n",
            "I0719 09:39:23.507098 140481689634688 learning.py:507] global step 3664: loss = 0.1299 (0.335 sec/step)\n",
            "I0719 09:39:23.776935 140481689634688 learning.py:507] global step 3665: loss = 0.0545 (0.268 sec/step)\n",
            "I0719 09:39:24.112450 140481689634688 learning.py:507] global step 3666: loss = 0.1432 (0.334 sec/step)\n",
            "I0719 09:39:24.442559 140481689634688 learning.py:507] global step 3667: loss = 0.0645 (0.328 sec/step)\n",
            "I0719 09:39:24.778871 140481689634688 learning.py:507] global step 3668: loss = 0.2230 (0.334 sec/step)\n",
            "I0719 09:39:25.141502 140481689634688 learning.py:507] global step 3669: loss = 0.0754 (0.361 sec/step)\n",
            "I0719 09:39:25.485545 140481689634688 learning.py:507] global step 3670: loss = 0.1275 (0.342 sec/step)\n",
            "I0719 09:39:25.828668 140481689634688 learning.py:507] global step 3671: loss = 0.0980 (0.341 sec/step)\n",
            "I0719 09:39:26.180026 140481689634688 learning.py:507] global step 3672: loss = 0.1149 (0.350 sec/step)\n",
            "I0719 09:39:26.514910 140481689634688 learning.py:507] global step 3673: loss = 0.1242 (0.333 sec/step)\n",
            "I0719 09:39:26.847409 140481689634688 learning.py:507] global step 3674: loss = 0.0537 (0.331 sec/step)\n",
            "I0719 09:39:27.181403 140481689634688 learning.py:507] global step 3675: loss = 0.1501 (0.332 sec/step)\n",
            "I0719 09:39:27.497750 140481689634688 learning.py:507] global step 3676: loss = 0.0691 (0.314 sec/step)\n",
            "I0719 09:39:27.799080 140481689634688 learning.py:507] global step 3677: loss = 0.0755 (0.299 sec/step)\n",
            "I0719 09:39:28.141814 140481689634688 learning.py:507] global step 3678: loss = 0.1058 (0.340 sec/step)\n",
            "I0719 09:39:28.496178 140481689634688 learning.py:507] global step 3679: loss = 0.0656 (0.353 sec/step)\n",
            "I0719 09:39:28.818729 140481689634688 learning.py:507] global step 3680: loss = 0.1277 (0.321 sec/step)\n",
            "I0719 09:39:29.116352 140481689634688 learning.py:507] global step 3681: loss = 0.0863 (0.296 sec/step)\n",
            "I0719 09:39:29.446233 140481689634688 learning.py:507] global step 3682: loss = 0.0358 (0.328 sec/step)\n",
            "I0719 09:39:29.790614 140481689634688 learning.py:507] global step 3683: loss = 0.1113 (0.343 sec/step)\n",
            "I0719 09:39:30.107480 140481689634688 learning.py:507] global step 3684: loss = 0.0475 (0.315 sec/step)\n",
            "I0719 09:39:30.443651 140481689634688 learning.py:507] global step 3685: loss = 0.0110 (0.334 sec/step)\n",
            "I0719 09:39:30.794102 140481689634688 learning.py:507] global step 3686: loss = 0.0868 (0.349 sec/step)\n",
            "I0719 09:39:31.098786 140481689634688 learning.py:507] global step 3687: loss = 0.0569 (0.303 sec/step)\n",
            "I0719 09:39:31.460930 140481689634688 learning.py:507] global step 3688: loss = 0.0632 (0.360 sec/step)\n",
            "I0719 09:39:31.761759 140481689634688 learning.py:507] global step 3689: loss = 0.0523 (0.299 sec/step)\n",
            "I0719 09:39:32.105946 140481689634688 learning.py:507] global step 3690: loss = 0.1083 (0.343 sec/step)\n",
            "I0719 09:39:32.445745 140481689634688 learning.py:507] global step 3691: loss = 0.0232 (0.338 sec/step)\n",
            "I0719 09:39:32.784681 140481689634688 learning.py:507] global step 3692: loss = 0.1034 (0.337 sec/step)\n",
            "I0719 09:39:33.119226 140481689634688 learning.py:507] global step 3693: loss = 0.1142 (0.333 sec/step)\n",
            "I0719 09:39:33.473327 140481689634688 learning.py:507] global step 3694: loss = 0.0770 (0.352 sec/step)\n",
            "I0719 09:39:33.804582 140481689634688 learning.py:507] global step 3695: loss = 0.0970 (0.329 sec/step)\n",
            "I0719 09:39:34.084177 140481689634688 learning.py:507] global step 3696: loss = 0.1437 (0.278 sec/step)\n",
            "I0719 09:39:34.438435 140481689634688 learning.py:507] global step 3697: loss = 0.0315 (0.352 sec/step)\n",
            "I0719 09:39:34.747656 140481689634688 learning.py:507] global step 3698: loss = 0.1493 (0.307 sec/step)\n",
            "I0719 09:39:35.034101 140481689634688 learning.py:507] global step 3699: loss = 0.0940 (0.285 sec/step)\n",
            "I0719 09:39:35.354256 140481689634688 learning.py:507] global step 3700: loss = 0.0884 (0.318 sec/step)\n",
            "I0719 09:39:35.699874 140481689634688 learning.py:507] global step 3701: loss = 0.0987 (0.344 sec/step)\n",
            "I0719 09:39:36.033620 140481689634688 learning.py:507] global step 3702: loss = 0.0184 (0.332 sec/step)\n",
            "I0719 09:39:36.349779 140481689634688 learning.py:507] global step 3703: loss = 0.0603 (0.314 sec/step)\n",
            "I0719 09:39:36.654201 140481689634688 learning.py:507] global step 3704: loss = 0.0593 (0.303 sec/step)\n",
            "I0719 09:39:36.999702 140481689634688 learning.py:507] global step 3705: loss = 0.1414 (0.344 sec/step)\n",
            "I0719 09:39:37.350039 140481689634688 learning.py:507] global step 3706: loss = 0.1085 (0.349 sec/step)\n",
            "I0719 09:39:37.677055 140481689634688 learning.py:507] global step 3707: loss = 0.1289 (0.325 sec/step)\n",
            "I0719 09:39:38.009574 140481689634688 learning.py:507] global step 3708: loss = 0.1480 (0.331 sec/step)\n",
            "I0719 09:39:38.348658 140481689634688 learning.py:507] global step 3709: loss = 0.1183 (0.337 sec/step)\n",
            "I0719 09:39:38.696229 140481689634688 learning.py:507] global step 3710: loss = 0.1488 (0.346 sec/step)\n",
            "I0719 09:39:39.041583 140481689634688 learning.py:507] global step 3711: loss = 0.1947 (0.343 sec/step)\n",
            "I0719 09:39:39.372648 140481689634688 learning.py:507] global step 3712: loss = 0.0223 (0.329 sec/step)\n",
            "I0719 09:39:39.724934 140481689634688 learning.py:507] global step 3713: loss = 0.0608 (0.350 sec/step)\n",
            "I0719 09:39:40.057529 140481689634688 learning.py:507] global step 3714: loss = 0.1310 (0.331 sec/step)\n",
            "I0719 09:39:40.360166 140481689634688 learning.py:507] global step 3715: loss = 0.0976 (0.301 sec/step)\n",
            "I0719 09:39:40.707525 140481689634688 learning.py:507] global step 3716: loss = 0.1335 (0.345 sec/step)\n",
            "I0719 09:39:40.994369 140481689634688 learning.py:507] global step 3717: loss = 0.0500 (0.285 sec/step)\n",
            "I0719 09:39:41.330160 140481689634688 learning.py:507] global step 3718: loss = 0.0908 (0.334 sec/step)\n",
            "I0719 09:39:41.617950 140481689634688 learning.py:507] global step 3719: loss = 0.0548 (0.286 sec/step)\n",
            "I0719 09:39:41.923735 140481689634688 learning.py:507] global step 3720: loss = 0.0767 (0.304 sec/step)\n",
            "I0719 09:39:42.261869 140481689634688 learning.py:507] global step 3721: loss = 0.0508 (0.336 sec/step)\n",
            "I0719 09:39:42.615211 140481689634688 learning.py:507] global step 3722: loss = 0.0763 (0.352 sec/step)\n",
            "I0719 09:39:42.962989 140481689634688 learning.py:507] global step 3723: loss = 0.0866 (0.346 sec/step)\n",
            "I0719 09:39:43.290979 140481689634688 learning.py:507] global step 3724: loss = 0.1746 (0.326 sec/step)\n",
            "I0719 09:39:43.637707 140481689634688 learning.py:507] global step 3725: loss = 0.1412 (0.345 sec/step)\n",
            "I0719 09:39:43.970056 140481689634688 learning.py:507] global step 3726: loss = 0.1077 (0.330 sec/step)\n",
            "I0719 09:39:44.312510 140481689634688 learning.py:507] global step 3727: loss = 0.0487 (0.341 sec/step)\n",
            "I0719 09:39:44.605442 140481689634688 learning.py:507] global step 3728: loss = 0.0455 (0.291 sec/step)\n",
            "I0719 09:39:44.910394 140481689634688 learning.py:507] global step 3729: loss = 0.0840 (0.303 sec/step)\n",
            "I0719 09:39:45.198081 140481689634688 learning.py:507] global step 3730: loss = 0.1604 (0.286 sec/step)\n",
            "I0719 09:39:45.517292 140481689634688 learning.py:507] global step 3731: loss = 0.0734 (0.317 sec/step)\n",
            "I0719 09:39:45.845958 140481689634688 learning.py:507] global step 3732: loss = 0.0703 (0.327 sec/step)\n",
            "I0719 09:39:46.166629 140481689634688 learning.py:507] global step 3733: loss = 0.0737 (0.319 sec/step)\n",
            "I0719 09:39:46.476539 140481689634688 learning.py:507] global step 3734: loss = 0.0973 (0.308 sec/step)\n",
            "I0719 09:39:46.807225 140481689634688 learning.py:507] global step 3735: loss = 0.1087 (0.329 sec/step)\n",
            "I0719 09:39:47.147563 140481689634688 learning.py:507] global step 3736: loss = 0.1615 (0.339 sec/step)\n",
            "I0719 09:39:47.437436 140481689634688 learning.py:507] global step 3737: loss = 0.1417 (0.288 sec/step)\n",
            "I0719 09:39:47.782390 140481689634688 learning.py:507] global step 3738: loss = 0.1035 (0.343 sec/step)\n",
            "I0719 09:39:48.104538 140481689634688 learning.py:507] global step 3739: loss = 0.1588 (0.320 sec/step)\n",
            "I0719 09:39:48.453820 140481689634688 learning.py:507] global step 3740: loss = 0.1146 (0.347 sec/step)\n",
            "I0719 09:39:48.790861 140481689634688 learning.py:507] global step 3741: loss = 0.0996 (0.335 sec/step)\n",
            "I0719 09:39:49.138357 140481689634688 learning.py:507] global step 3742: loss = 0.0230 (0.346 sec/step)\n",
            "I0719 09:39:49.468332 140481689634688 learning.py:507] global step 3743: loss = 0.0450 (0.328 sec/step)\n",
            "I0719 09:39:49.794670 140481689634688 learning.py:507] global step 3744: loss = 0.2782 (0.324 sec/step)\n",
            "I0719 09:39:50.110566 140481689634688 learning.py:507] global step 3745: loss = 0.0175 (0.314 sec/step)\n",
            "I0719 09:39:50.441770 140481689634688 learning.py:507] global step 3746: loss = 0.0736 (0.330 sec/step)\n",
            "I0719 09:39:50.766823 140481689634688 learning.py:507] global step 3747: loss = 0.0336 (0.320 sec/step)\n",
            "I0719 09:39:51.136612 140481689634688 learning.py:507] global step 3748: loss = 0.0628 (0.367 sec/step)\n",
            "I0719 09:39:51.464537 140481689634688 learning.py:507] global step 3749: loss = 0.1298 (0.326 sec/step)\n",
            "I0719 09:39:51.804945 140481689634688 learning.py:507] global step 3750: loss = 0.0748 (0.338 sec/step)\n",
            "I0719 09:39:52.121237 140481689634688 learning.py:507] global step 3751: loss = 0.0317 (0.313 sec/step)\n",
            "I0719 09:39:52.450797 140481689634688 learning.py:507] global step 3752: loss = 0.1068 (0.328 sec/step)\n",
            "I0719 09:39:52.778854 140481689634688 learning.py:507] global step 3753: loss = 0.0293 (0.326 sec/step)\n",
            "I0719 09:39:53.150047 140481689634688 learning.py:507] global step 3754: loss = 0.0544 (0.369 sec/step)\n",
            "I0719 09:39:53.473307 140481689634688 learning.py:507] global step 3755: loss = 0.0247 (0.321 sec/step)\n",
            "I0719 09:39:53.800847 140481689634688 learning.py:507] global step 3756: loss = 0.0210 (0.326 sec/step)\n",
            "I0719 09:39:54.129176 140481689634688 learning.py:507] global step 3757: loss = 0.1246 (0.326 sec/step)\n",
            "I0719 09:39:54.474838 140481689634688 learning.py:507] global step 3758: loss = 0.0741 (0.344 sec/step)\n",
            "I0719 09:39:54.805718 140481689634688 learning.py:507] global step 3759: loss = 0.0877 (0.329 sec/step)\n",
            "I0719 09:39:55.135283 140481689634688 learning.py:507] global step 3760: loss = 0.0984 (0.328 sec/step)\n",
            "I0719 09:39:55.492968 140481689634688 learning.py:507] global step 3761: loss = 0.0137 (0.356 sec/step)\n",
            "I0719 09:39:55.824766 140481689634688 learning.py:507] global step 3762: loss = 0.0343 (0.330 sec/step)\n",
            "I0719 09:39:56.166003 140481689634688 learning.py:507] global step 3763: loss = 0.0218 (0.340 sec/step)\n",
            "I0719 09:39:56.468142 140481689634688 learning.py:507] global step 3764: loss = 0.1629 (0.300 sec/step)\n",
            "I0719 09:39:56.815262 140481689634688 learning.py:507] global step 3765: loss = 0.1867 (0.346 sec/step)\n",
            "I0719 09:39:57.153670 140481689634688 learning.py:507] global step 3766: loss = 0.0338 (0.337 sec/step)\n",
            "I0719 09:39:57.466338 140481689634688 learning.py:507] global step 3767: loss = 0.1248 (0.311 sec/step)\n",
            "I0719 09:39:57.806195 140481689634688 learning.py:507] global step 3768: loss = 0.0849 (0.338 sec/step)\n",
            "I0719 09:39:58.125036 140481689634688 learning.py:507] global step 3769: loss = 0.1228 (0.317 sec/step)\n",
            "I0719 09:39:58.416581 140481689634688 learning.py:507] global step 3770: loss = 0.0415 (0.290 sec/step)\n",
            "I0719 09:39:58.739806 140481689634688 learning.py:507] global step 3771: loss = 0.0777 (0.322 sec/step)\n",
            "I0719 09:39:59.088422 140481689634688 learning.py:507] global step 3772: loss = 0.0625 (0.347 sec/step)\n",
            "I0719 09:39:59.428787 140481689634688 learning.py:507] global step 3773: loss = 0.1325 (0.339 sec/step)\n",
            "I0719 09:39:59.751523 140481689634688 learning.py:507] global step 3774: loss = 0.0854 (0.321 sec/step)\n",
            "I0719 09:40:00.037857 140481689634688 learning.py:507] global step 3775: loss = 0.1157 (0.284 sec/step)\n",
            "I0719 09:40:00.375417 140481689634688 learning.py:507] global step 3776: loss = 0.0767 (0.336 sec/step)\n",
            "I0719 09:40:00.680490 140481689634688 learning.py:507] global step 3777: loss = 0.1698 (0.303 sec/step)\n",
            "I0719 09:40:01.010933 140481689634688 learning.py:507] global step 3778: loss = 0.0480 (0.329 sec/step)\n",
            "I0719 09:40:01.324866 140481689634688 learning.py:507] global step 3779: loss = 0.2194 (0.312 sec/step)\n",
            "I0719 09:40:01.673125 140481689634688 learning.py:507] global step 3780: loss = 0.1440 (0.346 sec/step)\n",
            "I0719 09:40:02.029601 140481689634688 learning.py:507] global step 3781: loss = 0.0517 (0.355 sec/step)\n",
            "I0719 09:40:02.357760 140481689634688 learning.py:507] global step 3782: loss = 0.0849 (0.326 sec/step)\n",
            "I0719 09:40:02.687747 140481689634688 learning.py:507] global step 3783: loss = 0.4136 (0.328 sec/step)\n",
            "I0719 09:40:03.011124 140481689634688 learning.py:507] global step 3784: loss = 0.1224 (0.321 sec/step)\n",
            "I0719 09:40:03.343200 140481689634688 learning.py:507] global step 3785: loss = 0.0747 (0.330 sec/step)\n",
            "I0719 09:40:03.694489 140481689634688 learning.py:507] global step 3786: loss = 0.0310 (0.349 sec/step)\n",
            "I0719 09:40:04.040289 140481689634688 learning.py:507] global step 3787: loss = 0.0531 (0.344 sec/step)\n",
            "I0719 09:40:04.390708 140481689634688 learning.py:507] global step 3788: loss = 0.1485 (0.348 sec/step)\n",
            "I0719 09:40:04.700818 140481689634688 learning.py:507] global step 3789: loss = 0.0735 (0.308 sec/step)\n",
            "I0719 09:40:05.055162 140481689634688 learning.py:507] global step 3790: loss = 0.1588 (0.352 sec/step)\n",
            "I0719 09:40:05.404725 140481689634688 learning.py:507] global step 3791: loss = 0.2691 (0.348 sec/step)\n",
            "I0719 09:40:05.736000 140481689634688 learning.py:507] global step 3792: loss = 0.0602 (0.329 sec/step)\n",
            "I0719 09:40:06.079045 140481689634688 learning.py:507] global step 3793: loss = 0.2138 (0.341 sec/step)\n",
            "I0719 09:40:06.419832 140481689634688 learning.py:507] global step 3794: loss = 0.0911 (0.339 sec/step)\n",
            "I0719 09:40:06.754886 140481689634688 learning.py:507] global step 3795: loss = 0.0862 (0.333 sec/step)\n",
            "I0719 09:40:07.062521 140481689634688 learning.py:507] global step 3796: loss = 0.0776 (0.306 sec/step)\n",
            "I0719 09:40:07.391175 140481689634688 learning.py:507] global step 3797: loss = 0.0929 (0.327 sec/step)\n",
            "I0719 09:40:07.737133 140481689634688 learning.py:507] global step 3798: loss = 0.0832 (0.344 sec/step)\n",
            "I0719 09:40:08.072997 140481689634688 learning.py:507] global step 3799: loss = 0.1305 (0.334 sec/step)\n",
            "I0719 09:40:08.449384 140481689634688 learning.py:507] global step 3800: loss = 0.0390 (0.374 sec/step)\n",
            "I0719 09:40:08.796693 140481689634688 learning.py:507] global step 3801: loss = 0.1031 (0.345 sec/step)\n",
            "I0719 09:40:09.111536 140481689634688 learning.py:507] global step 3802: loss = 0.0997 (0.313 sec/step)\n",
            "I0719 09:40:09.425991 140481689634688 learning.py:507] global step 3803: loss = 0.0413 (0.312 sec/step)\n",
            "I0719 09:40:09.765908 140481689634688 learning.py:507] global step 3804: loss = 0.2563 (0.338 sec/step)\n",
            "I0719 09:40:10.102938 140481689634688 learning.py:507] global step 3805: loss = 0.1197 (0.335 sec/step)\n",
            "I0719 09:40:10.439975 140481689634688 learning.py:507] global step 3806: loss = 0.0595 (0.335 sec/step)\n",
            "I0719 09:40:10.738042 140481689634688 learning.py:507] global step 3807: loss = 0.0505 (0.296 sec/step)\n",
            "I0719 09:40:11.065833 140481689634688 learning.py:507] global step 3808: loss = 0.1083 (0.326 sec/step)\n",
            "I0719 09:40:11.393843 140481689634688 learning.py:507] global step 3809: loss = 0.0856 (0.326 sec/step)\n",
            "I0719 09:40:11.727116 140481689634688 learning.py:507] global step 3810: loss = 0.1407 (0.331 sec/step)\n",
            "I0719 09:40:12.077856 140481689634688 learning.py:507] global step 3811: loss = 0.1128 (0.349 sec/step)\n",
            "I0719 09:40:12.418740 140481689634688 learning.py:507] global step 3812: loss = 0.0847 (0.339 sec/step)\n",
            "I0719 09:40:12.720833 140481689634688 learning.py:507] global step 3813: loss = 0.0874 (0.300 sec/step)\n",
            "I0719 09:40:13.061800 140481689634688 learning.py:507] global step 3814: loss = 0.0836 (0.339 sec/step)\n",
            "I0719 09:40:13.347176 140481689634688 learning.py:507] global step 3815: loss = 0.1253 (0.284 sec/step)\n",
            "I0719 09:40:13.685386 140481689634688 learning.py:507] global step 3816: loss = 0.0926 (0.336 sec/step)\n",
            "I0719 09:40:14.038099 140481689634688 learning.py:507] global step 3817: loss = 0.0376 (0.351 sec/step)\n",
            "I0719 09:40:14.327679 140481689634688 learning.py:507] global step 3818: loss = 0.0805 (0.288 sec/step)\n",
            "I0719 09:40:14.669029 140481689634688 learning.py:507] global step 3819: loss = 0.0928 (0.340 sec/step)\n",
            "I0719 09:40:14.996723 140481689634688 learning.py:507] global step 3820: loss = 0.1261 (0.326 sec/step)\n",
            "I0719 09:40:15.323851 140481689634688 learning.py:507] global step 3821: loss = 0.1060 (0.325 sec/step)\n",
            "I0719 09:40:15.672043 140481689634688 learning.py:507] global step 3822: loss = 0.1119 (0.347 sec/step)\n",
            "I0719 09:40:15.986147 140481689634688 learning.py:507] global step 3823: loss = 0.1223 (0.312 sec/step)\n",
            "I0719 09:40:16.316559 140481689634688 learning.py:507] global step 3824: loss = 0.0810 (0.329 sec/step)\n",
            "I0719 09:40:16.600858 140481689634688 learning.py:507] global step 3825: loss = 0.0364 (0.283 sec/step)\n",
            "I0719 09:40:16.944988 140481689634688 learning.py:507] global step 3826: loss = 0.0647 (0.342 sec/step)\n",
            "I0719 09:40:17.262557 140481689634688 learning.py:507] global step 3827: loss = 0.0506 (0.316 sec/step)\n",
            "I0719 09:40:17.589911 140481689634688 learning.py:507] global step 3828: loss = 0.0877 (0.326 sec/step)\n",
            "I0719 09:40:17.890953 140481689634688 learning.py:507] global step 3829: loss = 0.0187 (0.297 sec/step)\n",
            "I0719 09:40:18.224932 140481689634688 learning.py:507] global step 3830: loss = 0.0605 (0.332 sec/step)\n",
            "I0719 09:40:18.569772 140481689634688 learning.py:507] global step 3831: loss = 0.1495 (0.343 sec/step)\n",
            "I0719 09:40:18.916512 140481689634688 learning.py:507] global step 3832: loss = 0.1080 (0.345 sec/step)\n",
            "I0719 09:40:19.236224 140481689634688 learning.py:507] global step 3833: loss = 0.0450 (0.318 sec/step)\n",
            "I0719 09:40:19.574883 140481689634688 learning.py:507] global step 3834: loss = 0.0803 (0.331 sec/step)\n",
            "I0719 09:40:19.906411 140481689634688 learning.py:507] global step 3835: loss = 0.0459 (0.328 sec/step)\n",
            "I0719 09:40:20.204968 140481689634688 learning.py:507] global step 3836: loss = 0.0312 (0.297 sec/step)\n",
            "I0719 09:40:20.553609 140481689634688 learning.py:507] global step 3837: loss = 0.0865 (0.347 sec/step)\n",
            "I0719 09:40:20.893395 140481689634688 learning.py:507] global step 3838: loss = 0.1863 (0.338 sec/step)\n",
            "I0719 09:40:21.226397 140481689634688 learning.py:507] global step 3839: loss = 0.0851 (0.331 sec/step)\n",
            "I0719 09:40:21.555161 140481689634688 learning.py:507] global step 3840: loss = 0.0724 (0.327 sec/step)\n",
            "I0719 09:40:21.898807 140481689634688 learning.py:507] global step 3841: loss = 0.1262 (0.342 sec/step)\n",
            "I0719 09:40:22.229348 140481689634688 learning.py:507] global step 3842: loss = 0.0250 (0.329 sec/step)\n",
            "I0719 09:40:22.573743 140481689634688 learning.py:507] global step 3843: loss = 0.0193 (0.343 sec/step)\n",
            "I0719 09:40:22.873129 140481689634688 learning.py:507] global step 3844: loss = 0.0922 (0.298 sec/step)\n",
            "I0719 09:40:23.193083 140481689634688 learning.py:507] global step 3845: loss = 0.0620 (0.318 sec/step)\n",
            "I0719 09:40:23.526523 140481689634688 learning.py:507] global step 3846: loss = 0.0848 (0.332 sec/step)\n",
            "I0719 09:40:23.833336 140481689634688 learning.py:507] global step 3847: loss = 0.0477 (0.305 sec/step)\n",
            "I0719 09:40:24.175942 140481689634688 learning.py:507] global step 3848: loss = 0.1796 (0.341 sec/step)\n",
            "I0719 09:40:24.467622 140481689634688 learning.py:507] global step 3849: loss = 0.0867 (0.289 sec/step)\n",
            "I0719 09:40:24.824146 140481689634688 learning.py:507] global step 3850: loss = 0.2275 (0.354 sec/step)\n",
            "I0719 09:40:25.170609 140481689634688 learning.py:507] global step 3851: loss = 0.1033 (0.345 sec/step)\n",
            "I0719 09:40:25.512106 140481689634688 learning.py:507] global step 3852: loss = 0.1322 (0.339 sec/step)\n",
            "I0719 09:40:25.846684 140481689634688 learning.py:507] global step 3853: loss = 0.0875 (0.333 sec/step)\n",
            "I0719 09:40:26.322618 140481689634688 learning.py:507] global step 3854: loss = 0.0628 (0.431 sec/step)\n",
            "I0719 09:40:26.823090 140481689634688 learning.py:507] global step 3855: loss = 0.0713 (0.492 sec/step)\n",
            "I0719 09:40:27.052129 140479018202880 supervisor.py:1050] Recording summary at step 3855.\n",
            "I0719 09:40:27.257371 140481689634688 learning.py:507] global step 3856: loss = 0.2146 (0.432 sec/step)\n",
            "I0719 09:40:27.590412 140481689634688 learning.py:507] global step 3857: loss = 0.0999 (0.331 sec/step)\n",
            "I0719 09:40:27.940819 140481689634688 learning.py:507] global step 3858: loss = 0.0946 (0.349 sec/step)\n",
            "I0719 09:40:28.279075 140481689634688 learning.py:507] global step 3859: loss = 0.1586 (0.336 sec/step)\n",
            "I0719 09:40:28.595710 140481689634688 learning.py:507] global step 3860: loss = 0.0574 (0.315 sec/step)\n",
            "I0719 09:40:28.928012 140481689634688 learning.py:507] global step 3861: loss = 0.0774 (0.330 sec/step)\n",
            "I0719 09:40:29.263193 140481689634688 learning.py:507] global step 3862: loss = 0.0206 (0.333 sec/step)\n",
            "I0719 09:40:29.611584 140481689634688 learning.py:507] global step 3863: loss = 0.0669 (0.347 sec/step)\n",
            "I0719 09:40:29.952518 140481689634688 learning.py:507] global step 3864: loss = 0.0908 (0.339 sec/step)\n",
            "I0719 09:40:30.313678 140481689634688 learning.py:507] global step 3865: loss = 0.0342 (0.359 sec/step)\n",
            "I0719 09:40:30.605336 140481689634688 learning.py:507] global step 3866: loss = 0.0783 (0.290 sec/step)\n",
            "I0719 09:40:30.945021 140481689634688 learning.py:507] global step 3867: loss = 0.0555 (0.338 sec/step)\n",
            "I0719 09:40:31.253380 140481689634688 learning.py:507] global step 3868: loss = 0.1156 (0.306 sec/step)\n",
            "I0719 09:40:31.594561 140481689634688 learning.py:507] global step 3869: loss = 0.1397 (0.340 sec/step)\n",
            "I0719 09:40:31.930600 140481689634688 learning.py:507] global step 3870: loss = 0.1084 (0.333 sec/step)\n",
            "I0719 09:40:32.233357 140481689634688 learning.py:507] global step 3871: loss = 0.1501 (0.300 sec/step)\n",
            "I0719 09:40:32.559928 140481689634688 learning.py:507] global step 3872: loss = 0.1050 (0.325 sec/step)\n",
            "I0719 09:40:32.891201 140481689634688 learning.py:507] global step 3873: loss = 0.0995 (0.329 sec/step)\n",
            "I0719 09:40:33.198528 140481689634688 learning.py:507] global step 3874: loss = 0.1180 (0.305 sec/step)\n",
            "I0719 09:40:33.538339 140481689634688 learning.py:507] global step 3875: loss = 0.0952 (0.338 sec/step)\n",
            "I0719 09:40:33.864140 140481689634688 learning.py:507] global step 3876: loss = 0.2999 (0.324 sec/step)\n",
            "I0719 09:40:34.216528 140481689634688 learning.py:507] global step 3877: loss = 0.0209 (0.351 sec/step)\n",
            "I0719 09:40:34.547871 140481689634688 learning.py:507] global step 3878: loss = 0.1044 (0.330 sec/step)\n",
            "I0719 09:40:34.881517 140481689634688 learning.py:507] global step 3879: loss = 0.0987 (0.332 sec/step)\n",
            "I0719 09:40:35.225733 140481689634688 learning.py:507] global step 3880: loss = 0.1530 (0.343 sec/step)\n",
            "I0719 09:40:35.572793 140481689634688 learning.py:507] global step 3881: loss = 0.0502 (0.345 sec/step)\n",
            "I0719 09:40:35.932627 140481689634688 learning.py:507] global step 3882: loss = 0.0489 (0.358 sec/step)\n",
            "I0719 09:40:36.267029 140481689634688 learning.py:507] global step 3883: loss = 0.1773 (0.333 sec/step)\n",
            "I0719 09:40:36.614206 140481689634688 learning.py:507] global step 3884: loss = 0.0875 (0.345 sec/step)\n",
            "I0719 09:40:36.953392 140481689634688 learning.py:507] global step 3885: loss = 0.1055 (0.337 sec/step)\n",
            "I0719 09:40:37.280465 140481689634688 learning.py:507] global step 3886: loss = 0.1054 (0.325 sec/step)\n",
            "I0719 09:40:37.579019 140481689634688 learning.py:507] global step 3887: loss = 0.0712 (0.297 sec/step)\n",
            "I0719 09:40:37.927044 140481689634688 learning.py:507] global step 3888: loss = 0.0459 (0.346 sec/step)\n",
            "I0719 09:40:38.263870 140481689634688 learning.py:507] global step 3889: loss = 0.0623 (0.335 sec/step)\n",
            "I0719 09:40:38.583964 140481689634688 learning.py:507] global step 3890: loss = 0.1112 (0.318 sec/step)\n",
            "I0719 09:40:38.925420 140481689634688 learning.py:507] global step 3891: loss = 0.0362 (0.340 sec/step)\n",
            "I0719 09:40:39.262073 140481689634688 learning.py:507] global step 3892: loss = 0.1270 (0.335 sec/step)\n",
            "I0719 09:40:39.588233 140481689634688 learning.py:507] global step 3893: loss = 0.0474 (0.324 sec/step)\n",
            "I0719 09:40:39.942014 140481689634688 learning.py:507] global step 3894: loss = 0.0739 (0.352 sec/step)\n",
            "I0719 09:40:40.280597 140481689634688 learning.py:507] global step 3895: loss = 0.0229 (0.337 sec/step)\n",
            "I0719 09:40:40.631388 140481689634688 learning.py:507] global step 3896: loss = 0.0680 (0.349 sec/step)\n",
            "I0719 09:40:40.956434 140481689634688 learning.py:507] global step 3897: loss = 0.1658 (0.323 sec/step)\n",
            "I0719 09:40:41.284422 140481689634688 learning.py:507] global step 3898: loss = 0.1226 (0.326 sec/step)\n",
            "I0719 09:40:41.627029 140481689634688 learning.py:507] global step 3899: loss = 0.0622 (0.341 sec/step)\n",
            "I0719 09:40:41.958089 140481689634688 learning.py:507] global step 3900: loss = 0.0777 (0.329 sec/step)\n",
            "I0719 09:40:42.289780 140481689634688 learning.py:507] global step 3901: loss = 0.0952 (0.330 sec/step)\n",
            "I0719 09:40:42.613766 140481689634688 learning.py:507] global step 3902: loss = 0.0447 (0.322 sec/step)\n",
            "I0719 09:40:42.966322 140481689634688 learning.py:507] global step 3903: loss = 0.0739 (0.351 sec/step)\n",
            "I0719 09:40:43.317313 140481689634688 learning.py:507] global step 3904: loss = 0.1583 (0.349 sec/step)\n",
            "I0719 09:40:43.646374 140481689634688 learning.py:507] global step 3905: loss = 0.0562 (0.327 sec/step)\n",
            "I0719 09:40:43.963003 140481689634688 learning.py:507] global step 3906: loss = 0.0478 (0.315 sec/step)\n",
            "I0719 09:40:44.286645 140481689634688 learning.py:507] global step 3907: loss = 0.0535 (0.322 sec/step)\n",
            "I0719 09:40:44.593744 140481689634688 learning.py:507] global step 3908: loss = 0.2327 (0.304 sec/step)\n",
            "I0719 09:40:44.930330 140481689634688 learning.py:507] global step 3909: loss = 0.0468 (0.335 sec/step)\n",
            "I0719 09:40:45.263347 140481689634688 learning.py:507] global step 3910: loss = 0.0153 (0.331 sec/step)\n",
            "I0719 09:40:45.604350 140481689634688 learning.py:507] global step 3911: loss = 0.0345 (0.339 sec/step)\n",
            "I0719 09:40:45.948301 140481689634688 learning.py:507] global step 3912: loss = 0.0589 (0.342 sec/step)\n",
            "I0719 09:40:46.240330 140481689634688 learning.py:507] global step 3913: loss = 0.0601 (0.290 sec/step)\n",
            "I0719 09:40:46.548924 140481689634688 learning.py:507] global step 3914: loss = 0.0556 (0.307 sec/step)\n",
            "I0719 09:40:46.874319 140481689634688 learning.py:507] global step 3915: loss = 0.0737 (0.324 sec/step)\n",
            "I0719 09:40:47.189064 140481689634688 learning.py:507] global step 3916: loss = 0.1170 (0.313 sec/step)\n",
            "I0719 09:40:47.540595 140481689634688 learning.py:507] global step 3917: loss = 0.0512 (0.350 sec/step)\n",
            "I0719 09:40:47.843116 140481689634688 learning.py:507] global step 3918: loss = 0.0565 (0.301 sec/step)\n",
            "I0719 09:40:48.168054 140481689634688 learning.py:507] global step 3919: loss = 0.1076 (0.323 sec/step)\n",
            "I0719 09:40:48.462159 140481689634688 learning.py:507] global step 3920: loss = 0.1103 (0.292 sec/step)\n",
            "I0719 09:40:48.797701 140481689634688 learning.py:507] global step 3921: loss = 0.0600 (0.334 sec/step)\n",
            "I0719 09:40:49.139209 140481689634688 learning.py:507] global step 3922: loss = 0.1301 (0.339 sec/step)\n",
            "I0719 09:40:49.493627 140481689634688 learning.py:507] global step 3923: loss = 0.0923 (0.352 sec/step)\n",
            "I0719 09:40:49.829694 140481689634688 learning.py:507] global step 3924: loss = 0.1464 (0.334 sec/step)\n",
            "I0719 09:40:50.164466 140481689634688 learning.py:507] global step 3925: loss = 0.1211 (0.333 sec/step)\n",
            "I0719 09:40:50.508936 140481689634688 learning.py:507] global step 3926: loss = 0.0937 (0.343 sec/step)\n",
            "I0719 09:40:50.854683 140481689634688 learning.py:507] global step 3927: loss = 0.2217 (0.344 sec/step)\n",
            "I0719 09:40:51.149350 140481689634688 learning.py:507] global step 3928: loss = 0.0648 (0.293 sec/step)\n",
            "I0719 09:40:51.494128 140481689634688 learning.py:507] global step 3929: loss = 0.1109 (0.343 sec/step)\n",
            "I0719 09:40:51.820450 140481689634688 learning.py:507] global step 3930: loss = 0.0317 (0.325 sec/step)\n",
            "I0719 09:40:52.143477 140481689634688 learning.py:507] global step 3931: loss = 0.0824 (0.321 sec/step)\n",
            "I0719 09:40:52.499667 140481689634688 learning.py:507] global step 3932: loss = 0.1480 (0.354 sec/step)\n",
            "I0719 09:40:52.846874 140481689634688 learning.py:507] global step 3933: loss = 0.0543 (0.345 sec/step)\n",
            "I0719 09:40:53.190910 140481689634688 learning.py:507] global step 3934: loss = 0.0274 (0.342 sec/step)\n",
            "I0719 09:40:53.529114 140481689634688 learning.py:507] global step 3935: loss = 0.0468 (0.334 sec/step)\n",
            "I0719 09:40:53.836192 140481689634688 learning.py:507] global step 3936: loss = 0.1387 (0.305 sec/step)\n",
            "I0719 09:40:54.160509 140481689634688 learning.py:507] global step 3937: loss = 0.0981 (0.323 sec/step)\n",
            "I0719 09:40:54.508590 140481689634688 learning.py:507] global step 3938: loss = 0.0898 (0.346 sec/step)\n",
            "I0719 09:40:54.856117 140481689634688 learning.py:507] global step 3939: loss = 0.1110 (0.345 sec/step)\n",
            "I0719 09:40:55.160738 140481689634688 learning.py:507] global step 3940: loss = 0.0872 (0.303 sec/step)\n",
            "I0719 09:40:55.498476 140481689634688 learning.py:507] global step 3941: loss = 0.0244 (0.336 sec/step)\n",
            "I0719 09:40:55.835719 140481689634688 learning.py:507] global step 3942: loss = 0.0855 (0.335 sec/step)\n",
            "I0719 09:40:56.119687 140481689634688 learning.py:507] global step 3943: loss = 0.1198 (0.280 sec/step)\n",
            "I0719 09:40:56.430607 140481689634688 learning.py:507] global step 3944: loss = 0.0467 (0.308 sec/step)\n",
            "I0719 09:40:56.748021 140481689634688 learning.py:507] global step 3945: loss = 0.1028 (0.316 sec/step)\n",
            "I0719 09:40:57.074785 140481689634688 learning.py:507] global step 3946: loss = 0.1115 (0.325 sec/step)\n",
            "I0719 09:40:57.416750 140481689634688 learning.py:507] global step 3947: loss = 0.0381 (0.340 sec/step)\n",
            "I0719 09:40:57.778922 140481689634688 learning.py:507] global step 3948: loss = 0.0883 (0.360 sec/step)\n",
            "I0719 09:40:58.152509 140481689634688 learning.py:507] global step 3949: loss = 0.0984 (0.372 sec/step)\n",
            "I0719 09:40:58.496801 140481689634688 learning.py:507] global step 3950: loss = 0.0484 (0.342 sec/step)\n",
            "I0719 09:40:58.799483 140481689634688 learning.py:507] global step 3951: loss = 0.0752 (0.301 sec/step)\n",
            "I0719 09:40:59.152060 140481689634688 learning.py:507] global step 3952: loss = 0.1705 (0.351 sec/step)\n",
            "I0719 09:40:59.500858 140481689634688 learning.py:507] global step 3953: loss = 0.0676 (0.347 sec/step)\n",
            "I0719 09:40:59.864024 140481689634688 learning.py:507] global step 3954: loss = 0.0349 (0.361 sec/step)\n",
            "I0719 09:41:00.175123 140481689634688 learning.py:507] global step 3955: loss = 0.2122 (0.309 sec/step)\n",
            "I0719 09:41:00.511200 140481689634688 learning.py:507] global step 3956: loss = 0.1939 (0.334 sec/step)\n",
            "I0719 09:41:00.850080 140481689634688 learning.py:507] global step 3957: loss = 0.1056 (0.337 sec/step)\n",
            "I0719 09:41:01.153904 140481689634688 learning.py:507] global step 3958: loss = 0.0993 (0.302 sec/step)\n",
            "I0719 09:41:01.512460 140481689634688 learning.py:507] global step 3959: loss = 0.1591 (0.357 sec/step)\n",
            "I0719 09:41:01.824680 140481689634688 learning.py:507] global step 3960: loss = 0.0834 (0.310 sec/step)\n",
            "I0719 09:41:02.121626 140481689634688 learning.py:507] global step 3961: loss = 0.0322 (0.295 sec/step)\n",
            "I0719 09:41:02.451526 140481689634688 learning.py:507] global step 3962: loss = 0.0612 (0.328 sec/step)\n",
            "I0719 09:41:02.766403 140481689634688 learning.py:507] global step 3963: loss = 0.0796 (0.313 sec/step)\n",
            "I0719 09:41:03.115410 140481689634688 learning.py:507] global step 3964: loss = 0.0298 (0.347 sec/step)\n",
            "I0719 09:41:03.409492 140481689634688 learning.py:507] global step 3965: loss = 0.0257 (0.292 sec/step)\n",
            "I0719 09:41:03.759767 140481689634688 learning.py:507] global step 3966: loss = 0.0522 (0.349 sec/step)\n",
            "I0719 09:41:04.104134 140481689634688 learning.py:507] global step 3967: loss = 0.0485 (0.342 sec/step)\n",
            "I0719 09:41:04.431168 140481689634688 learning.py:507] global step 3968: loss = 0.0520 (0.325 sec/step)\n",
            "I0719 09:41:04.723308 140481689634688 learning.py:507] global step 3969: loss = 0.1117 (0.290 sec/step)\n",
            "I0719 09:41:05.103970 140481689634688 learning.py:507] global step 3970: loss = 0.0766 (0.379 sec/step)\n",
            "I0719 09:41:05.448315 140481689634688 learning.py:507] global step 3971: loss = 0.0319 (0.343 sec/step)\n",
            "I0719 09:41:05.763257 140481689634688 learning.py:507] global step 3972: loss = 0.0990 (0.313 sec/step)\n",
            "I0719 09:41:06.107675 140481689634688 learning.py:507] global step 3973: loss = 0.0900 (0.343 sec/step)\n",
            "I0719 09:41:06.413305 140481689634688 learning.py:507] global step 3974: loss = 0.1154 (0.304 sec/step)\n",
            "I0719 09:41:06.718370 140481689634688 learning.py:507] global step 3975: loss = 0.1929 (0.303 sec/step)\n",
            "I0719 09:41:07.040887 140481689634688 learning.py:507] global step 3976: loss = 0.0491 (0.321 sec/step)\n",
            "I0719 09:41:07.374190 140481689634688 learning.py:507] global step 3977: loss = 0.0301 (0.331 sec/step)\n",
            "I0719 09:41:07.701544 140481689634688 learning.py:507] global step 3978: loss = 0.0842 (0.325 sec/step)\n",
            "I0719 09:41:08.042089 140481689634688 learning.py:507] global step 3979: loss = 0.0541 (0.339 sec/step)\n",
            "I0719 09:41:08.347843 140481689634688 learning.py:507] global step 3980: loss = 0.0410 (0.304 sec/step)\n",
            "I0719 09:41:08.679774 140481689634688 learning.py:507] global step 3981: loss = 0.0619 (0.330 sec/step)\n",
            "I0719 09:41:08.975084 140481689634688 learning.py:507] global step 3982: loss = 0.0706 (0.293 sec/step)\n",
            "I0719 09:41:09.307620 140481689634688 learning.py:507] global step 3983: loss = 0.1030 (0.331 sec/step)\n",
            "I0719 09:41:09.597953 140481689634688 learning.py:507] global step 3984: loss = 0.0458 (0.289 sec/step)\n",
            "I0719 09:41:09.932629 140481689634688 learning.py:507] global step 3985: loss = 0.1374 (0.333 sec/step)\n",
            "I0719 09:41:10.274518 140481689634688 learning.py:507] global step 3986: loss = 0.1731 (0.340 sec/step)\n",
            "I0719 09:41:10.626048 140481689634688 learning.py:507] global step 3987: loss = 0.0986 (0.350 sec/step)\n",
            "I0719 09:41:10.968737 140481689634688 learning.py:507] global step 3988: loss = 0.0733 (0.341 sec/step)\n",
            "I0719 09:41:11.301484 140481689634688 learning.py:507] global step 3989: loss = 0.1056 (0.330 sec/step)\n",
            "I0719 09:41:11.626548 140481689634688 learning.py:507] global step 3990: loss = 0.0285 (0.323 sec/step)\n",
            "I0719 09:41:11.980906 140481689634688 learning.py:507] global step 3991: loss = 0.1007 (0.353 sec/step)\n",
            "I0719 09:41:12.331757 140481689634688 learning.py:507] global step 3992: loss = 0.1356 (0.349 sec/step)\n",
            "I0719 09:41:12.655426 140481689634688 learning.py:507] global step 3993: loss = 0.1139 (0.322 sec/step)\n",
            "I0719 09:41:12.992516 140481689634688 learning.py:507] global step 3994: loss = 0.0227 (0.335 sec/step)\n",
            "I0719 09:41:13.288954 140481689634688 learning.py:507] global step 3995: loss = 0.0848 (0.295 sec/step)\n",
            "I0719 09:41:13.617870 140481689634688 learning.py:507] global step 3996: loss = 0.0655 (0.327 sec/step)\n",
            "I0719 09:41:13.942924 140481689634688 learning.py:507] global step 3997: loss = 0.0558 (0.323 sec/step)\n",
            "I0719 09:41:14.276411 140481689634688 learning.py:507] global step 3998: loss = 0.0730 (0.331 sec/step)\n",
            "I0719 09:41:14.607768 140481689634688 learning.py:507] global step 3999: loss = 0.1103 (0.330 sec/step)\n",
            "I0719 09:41:14.950692 140481689634688 learning.py:507] global step 4000: loss = 0.0438 (0.341 sec/step)\n",
            "I0719 09:41:15.333751 140481689634688 learning.py:507] global step 4001: loss = 0.0763 (0.381 sec/step)\n",
            "I0719 09:41:15.671082 140481689634688 learning.py:507] global step 4002: loss = 0.0507 (0.335 sec/step)\n",
            "I0719 09:41:16.004324 140481689634688 learning.py:507] global step 4003: loss = 0.0953 (0.332 sec/step)\n",
            "I0719 09:41:16.314174 140481689634688 learning.py:507] global step 4004: loss = 0.0436 (0.308 sec/step)\n",
            "I0719 09:41:16.652162 140481689634688 learning.py:507] global step 4005: loss = 0.0794 (0.336 sec/step)\n",
            "I0719 09:41:16.991759 140481689634688 learning.py:507] global step 4006: loss = 0.1333 (0.337 sec/step)\n",
            "I0719 09:41:17.346094 140481689634688 learning.py:507] global step 4007: loss = 0.0925 (0.353 sec/step)\n",
            "I0719 09:41:17.695074 140481689634688 learning.py:507] global step 4008: loss = 0.1056 (0.347 sec/step)\n",
            "I0719 09:41:18.028690 140481689634688 learning.py:507] global step 4009: loss = 0.1458 (0.332 sec/step)\n",
            "I0719 09:41:18.362130 140481689634688 learning.py:507] global step 4010: loss = 0.0782 (0.332 sec/step)\n",
            "I0719 09:41:18.684369 140481689634688 learning.py:507] global step 4011: loss = 0.0745 (0.320 sec/step)\n",
            "I0719 09:41:19.026905 140481689634688 learning.py:507] global step 4012: loss = 0.3192 (0.341 sec/step)\n",
            "I0719 09:41:19.348862 140481689634688 learning.py:507] global step 4013: loss = 0.0539 (0.320 sec/step)\n",
            "I0719 09:41:19.706318 140481689634688 learning.py:507] global step 4014: loss = 0.0962 (0.356 sec/step)\n",
            "I0719 09:41:20.036719 140481689634688 learning.py:507] global step 4015: loss = 0.0787 (0.329 sec/step)\n",
            "I0719 09:41:20.382713 140481689634688 learning.py:507] global step 4016: loss = 0.1188 (0.344 sec/step)\n",
            "I0719 09:41:20.711436 140481689634688 learning.py:507] global step 4017: loss = 0.0829 (0.327 sec/step)\n",
            "I0719 09:41:21.001772 140481689634688 learning.py:507] global step 4018: loss = 0.1415 (0.288 sec/step)\n",
            "I0719 09:41:21.321766 140481689634688 learning.py:507] global step 4019: loss = 0.1799 (0.316 sec/step)\n",
            "I0719 09:41:21.706060 140481689634688 learning.py:507] global step 4020: loss = 0.0552 (0.381 sec/step)\n",
            "I0719 09:41:22.062022 140481689634688 learning.py:507] global step 4021: loss = 0.0973 (0.354 sec/step)\n",
            "I0719 09:41:22.408366 140481689634688 learning.py:507] global step 4022: loss = 0.0425 (0.344 sec/step)\n",
            "I0719 09:41:22.760729 140481689634688 learning.py:507] global step 4023: loss = 0.0552 (0.351 sec/step)\n",
            "I0719 09:41:23.096013 140481689634688 learning.py:507] global step 4024: loss = 0.1432 (0.333 sec/step)\n",
            "I0719 09:41:23.427376 140481689634688 learning.py:507] global step 4025: loss = 0.0424 (0.329 sec/step)\n",
            "I0719 09:41:23.762578 140481689634688 learning.py:507] global step 4026: loss = 0.0581 (0.333 sec/step)\n",
            "I0719 09:41:24.092149 140481689634688 learning.py:507] global step 4027: loss = 0.0740 (0.328 sec/step)\n",
            "I0719 09:41:24.426671 140481689634688 learning.py:507] global step 4028: loss = 0.1206 (0.333 sec/step)\n",
            "I0719 09:41:24.749100 140481689634688 learning.py:507] global step 4029: loss = 0.0806 (0.321 sec/step)\n",
            "I0719 09:41:25.097018 140481689634688 learning.py:507] global step 4030: loss = 0.0651 (0.346 sec/step)\n",
            "I0719 09:41:25.399366 140481689634688 learning.py:507] global step 4031: loss = 0.1303 (0.300 sec/step)\n",
            "I0719 09:41:25.740803 140481689634688 learning.py:507] global step 4032: loss = 0.1320 (0.340 sec/step)\n",
            "I0719 09:41:26.069043 140481689634688 learning.py:507] global step 4033: loss = 0.1095 (0.326 sec/step)\n",
            "I0719 09:41:26.412389 140481689634688 learning.py:507] global step 4034: loss = 0.0562 (0.342 sec/step)\n",
            "I0719 09:41:26.764011 140481689634688 learning.py:507] global step 4035: loss = 0.1312 (0.350 sec/step)\n",
            "I0719 09:41:27.114540 140481689634688 learning.py:507] global step 4036: loss = 0.0720 (0.349 sec/step)\n",
            "I0719 09:41:27.411248 140481689634688 learning.py:507] global step 4037: loss = 0.1132 (0.295 sec/step)\n",
            "I0719 09:41:27.752431 140481689634688 learning.py:507] global step 4038: loss = 0.0510 (0.339 sec/step)\n",
            "I0719 09:41:28.093501 140481689634688 learning.py:507] global step 4039: loss = 0.0967 (0.335 sec/step)\n",
            "I0719 09:41:28.449872 140481689634688 learning.py:507] global step 4040: loss = 0.0427 (0.350 sec/step)\n",
            "I0719 09:41:28.807362 140481689634688 learning.py:507] global step 4041: loss = 0.0754 (0.356 sec/step)\n",
            "I0719 09:41:29.164467 140481689634688 learning.py:507] global step 4042: loss = 0.0954 (0.355 sec/step)\n",
            "I0719 09:41:29.496498 140481689634688 learning.py:507] global step 4043: loss = 0.0239 (0.330 sec/step)\n",
            "I0719 09:41:29.796979 140481689634688 learning.py:507] global step 4044: loss = 0.2236 (0.299 sec/step)\n",
            "I0719 09:41:30.113405 140481689634688 learning.py:507] global step 4045: loss = 0.3261 (0.315 sec/step)\n",
            "I0719 09:41:30.454340 140481689634688 learning.py:507] global step 4046: loss = 0.0602 (0.339 sec/step)\n",
            "I0719 09:41:30.775231 140481689634688 learning.py:507] global step 4047: loss = 0.1552 (0.319 sec/step)\n",
            "I0719 09:41:31.103672 140481689634688 learning.py:507] global step 4048: loss = 0.0172 (0.326 sec/step)\n",
            "I0719 09:41:31.457671 140481689634688 learning.py:507] global step 4049: loss = 0.0478 (0.352 sec/step)\n",
            "I0719 09:41:31.787085 140481689634688 learning.py:507] global step 4050: loss = 0.0720 (0.328 sec/step)\n",
            "I0719 09:41:32.142256 140481689634688 learning.py:507] global step 4051: loss = 0.0613 (0.353 sec/step)\n",
            "I0719 09:41:32.487364 140481689634688 learning.py:507] global step 4052: loss = 0.1241 (0.343 sec/step)\n",
            "I0719 09:41:32.831383 140481689634688 learning.py:507] global step 4053: loss = 0.1179 (0.342 sec/step)\n",
            "I0719 09:41:33.166747 140481689634688 learning.py:507] global step 4054: loss = 0.0791 (0.334 sec/step)\n",
            "I0719 09:41:33.496541 140481689634688 learning.py:507] global step 4055: loss = 0.0789 (0.328 sec/step)\n",
            "I0719 09:41:33.839168 140481689634688 learning.py:507] global step 4056: loss = 0.0659 (0.341 sec/step)\n",
            "I0719 09:41:34.149792 140481689634688 learning.py:507] global step 4057: loss = 0.0473 (0.309 sec/step)\n",
            "I0719 09:41:34.483422 140481689634688 learning.py:507] global step 4058: loss = 0.1011 (0.332 sec/step)\n",
            "I0719 09:41:34.836156 140481689634688 learning.py:507] global step 4059: loss = 0.0819 (0.351 sec/step)\n",
            "I0719 09:41:35.159715 140481689634688 learning.py:507] global step 4060: loss = 0.0949 (0.322 sec/step)\n",
            "I0719 09:41:35.505443 140481689634688 learning.py:507] global step 4061: loss = 0.0950 (0.344 sec/step)\n",
            "I0719 09:41:35.856019 140481689634688 learning.py:507] global step 4062: loss = 0.1192 (0.349 sec/step)\n",
            "I0719 09:41:36.175614 140481689634688 learning.py:507] global step 4063: loss = 0.0561 (0.318 sec/step)\n",
            "I0719 09:41:36.485352 140481689634688 learning.py:507] global step 4064: loss = 0.0353 (0.308 sec/step)\n",
            "I0719 09:41:36.834500 140481689634688 learning.py:507] global step 4065: loss = 0.0693 (0.347 sec/step)\n",
            "I0719 09:41:37.183729 140481689634688 learning.py:507] global step 4066: loss = 0.3248 (0.347 sec/step)\n",
            "I0719 09:41:37.497474 140481689634688 learning.py:507] global step 4067: loss = 0.0547 (0.311 sec/step)\n",
            "I0719 09:41:37.824697 140481689634688 learning.py:507] global step 4068: loss = 0.0969 (0.325 sec/step)\n",
            "I0719 09:41:38.157007 140481689634688 learning.py:507] global step 4069: loss = 0.1192 (0.330 sec/step)\n",
            "I0719 09:41:38.459048 140481689634688 learning.py:507] global step 4070: loss = 0.0429 (0.300 sec/step)\n",
            "I0719 09:41:38.795160 140481689634688 learning.py:507] global step 4071: loss = 0.0571 (0.334 sec/step)\n",
            "I0719 09:41:39.145676 140481689634688 learning.py:507] global step 4072: loss = 0.0797 (0.349 sec/step)\n",
            "I0719 09:41:39.475409 140481689634688 learning.py:507] global step 4073: loss = 0.0957 (0.328 sec/step)\n",
            "I0719 09:41:39.798031 140481689634688 learning.py:507] global step 4074: loss = 0.0854 (0.321 sec/step)\n",
            "I0719 09:41:40.116404 140481689634688 learning.py:507] global step 4075: loss = 0.0830 (0.317 sec/step)\n",
            "I0719 09:41:40.446687 140481689634688 learning.py:507] global step 4076: loss = 0.1153 (0.329 sec/step)\n",
            "I0719 09:41:40.792822 140481689634688 learning.py:507] global step 4077: loss = 0.1039 (0.344 sec/step)\n",
            "I0719 09:41:41.105197 140481689634688 learning.py:507] global step 4078: loss = 0.1466 (0.310 sec/step)\n",
            "I0719 09:41:41.432883 140481689634688 learning.py:507] global step 4079: loss = 0.1303 (0.325 sec/step)\n",
            "I0719 09:41:41.792224 140481689634688 learning.py:507] global step 4080: loss = 0.0469 (0.358 sec/step)\n",
            "I0719 09:41:42.143098 140481689634688 learning.py:507] global step 4081: loss = 0.1474 (0.349 sec/step)\n",
            "I0719 09:41:42.487301 140481689634688 learning.py:507] global step 4082: loss = 0.0458 (0.343 sec/step)\n",
            "I0719 09:41:42.814013 140481689634688 learning.py:507] global step 4083: loss = 0.1831 (0.325 sec/step)\n",
            "I0719 09:41:43.156998 140481689634688 learning.py:507] global step 4084: loss = 0.0772 (0.341 sec/step)\n",
            "I0719 09:41:43.436695 140481689634688 learning.py:507] global step 4085: loss = 0.1145 (0.278 sec/step)\n",
            "I0719 09:41:43.766911 140481689634688 learning.py:507] global step 4086: loss = 0.1754 (0.328 sec/step)\n",
            "I0719 09:41:44.071224 140481689634688 learning.py:507] global step 4087: loss = 0.1372 (0.303 sec/step)\n",
            "I0719 09:41:44.405631 140481689634688 learning.py:507] global step 4088: loss = 0.0155 (0.333 sec/step)\n",
            "I0719 09:41:44.743951 140481689634688 learning.py:507] global step 4089: loss = 0.0581 (0.336 sec/step)\n",
            "I0719 09:41:45.078608 140481689634688 learning.py:507] global step 4090: loss = 0.0477 (0.333 sec/step)\n",
            "I0719 09:41:45.423366 140481689634688 learning.py:507] global step 4091: loss = 0.1740 (0.343 sec/step)\n",
            "I0719 09:41:45.753307 140481689634688 learning.py:507] global step 4092: loss = 0.0846 (0.328 sec/step)\n",
            "I0719 09:41:46.093228 140481689634688 learning.py:507] global step 4093: loss = 0.0769 (0.338 sec/step)\n",
            "I0719 09:41:46.423494 140481689634688 learning.py:507] global step 4094: loss = 0.1072 (0.328 sec/step)\n",
            "I0719 09:41:46.747363 140481689634688 learning.py:507] global step 4095: loss = 0.0511 (0.322 sec/step)\n",
            "I0719 09:41:47.092070 140481689634688 learning.py:507] global step 4096: loss = 0.1607 (0.343 sec/step)\n",
            "I0719 09:41:47.441379 140481689634688 learning.py:507] global step 4097: loss = 0.2106 (0.348 sec/step)\n",
            "I0719 09:41:47.763066 140481689634688 learning.py:507] global step 4098: loss = 0.0767 (0.320 sec/step)\n",
            "I0719 09:41:48.087755 140481689634688 learning.py:507] global step 4099: loss = 0.1017 (0.322 sec/step)\n",
            "I0719 09:41:48.414061 140481689634688 learning.py:507] global step 4100: loss = 0.0777 (0.325 sec/step)\n",
            "I0719 09:41:48.749475 140481689634688 learning.py:507] global step 4101: loss = 0.0252 (0.334 sec/step)\n",
            "I0719 09:41:49.077620 140481689634688 learning.py:507] global step 4102: loss = 0.1050 (0.326 sec/step)\n",
            "I0719 09:41:49.416171 140481689634688 learning.py:507] global step 4103: loss = 0.0789 (0.337 sec/step)\n",
            "I0719 09:41:49.740435 140481689634688 learning.py:507] global step 4104: loss = 0.0821 (0.323 sec/step)\n",
            "I0719 09:41:50.096131 140481689634688 learning.py:507] global step 4105: loss = 0.0623 (0.354 sec/step)\n",
            "I0719 09:41:50.442827 140481689634688 learning.py:507] global step 4106: loss = 0.1488 (0.345 sec/step)\n",
            "I0719 09:41:50.794592 140481689634688 learning.py:507] global step 4107: loss = 0.0271 (0.350 sec/step)\n",
            "I0719 09:41:51.121523 140481689634688 learning.py:507] global step 4108: loss = 0.0832 (0.325 sec/step)\n",
            "I0719 09:41:51.449009 140481689634688 learning.py:507] global step 4109: loss = 0.1477 (0.326 sec/step)\n",
            "I0719 09:41:51.779756 140481689634688 learning.py:507] global step 4110: loss = 0.0649 (0.329 sec/step)\n",
            "I0719 09:41:52.117101 140481689634688 learning.py:507] global step 4111: loss = 0.0399 (0.336 sec/step)\n",
            "I0719 09:41:52.450301 140481689634688 learning.py:507] global step 4112: loss = 0.0790 (0.332 sec/step)\n",
            "I0719 09:41:52.778033 140481689634688 learning.py:507] global step 4113: loss = 0.0249 (0.326 sec/step)\n",
            "I0719 09:41:53.121338 140481689634688 learning.py:507] global step 4114: loss = 0.0715 (0.342 sec/step)\n",
            "I0719 09:41:53.468730 140481689634688 learning.py:507] global step 4115: loss = 0.0682 (0.346 sec/step)\n",
            "I0719 09:41:53.786218 140481689634688 learning.py:507] global step 4116: loss = 0.0171 (0.316 sec/step)\n",
            "I0719 09:41:54.138737 140481689634688 learning.py:507] global step 4117: loss = 0.1206 (0.351 sec/step)\n",
            "I0719 09:41:54.444604 140481689634688 learning.py:507] global step 4118: loss = 0.1339 (0.304 sec/step)\n",
            "I0719 09:41:54.767557 140481689634688 learning.py:507] global step 4119: loss = 0.1889 (0.321 sec/step)\n",
            "I0719 09:41:55.092846 140481689634688 learning.py:507] global step 4120: loss = 0.0531 (0.324 sec/step)\n",
            "I0719 09:41:55.432047 140481689634688 learning.py:507] global step 4121: loss = 0.1950 (0.337 sec/step)\n",
            "I0719 09:41:55.761340 140481689634688 learning.py:507] global step 4122: loss = 0.0950 (0.327 sec/step)\n",
            "I0719 09:41:56.112811 140481689634688 learning.py:507] global step 4123: loss = 0.2451 (0.350 sec/step)\n",
            "I0719 09:41:56.433758 140481689634688 learning.py:507] global step 4124: loss = 0.0162 (0.319 sec/step)\n",
            "I0719 09:41:56.781952 140481689634688 learning.py:507] global step 4125: loss = 0.0326 (0.346 sec/step)\n",
            "I0719 09:41:57.140389 140481689634688 learning.py:507] global step 4126: loss = 0.1219 (0.356 sec/step)\n",
            "I0719 09:41:57.479758 140481689634688 learning.py:507] global step 4127: loss = 0.1102 (0.338 sec/step)\n",
            "I0719 09:41:57.830700 140481689634688 learning.py:507] global step 4128: loss = 0.0789 (0.349 sec/step)\n",
            "I0719 09:41:58.122069 140481689634688 learning.py:507] global step 4129: loss = 0.0887 (0.289 sec/step)\n",
            "I0719 09:41:58.453127 140481689634688 learning.py:507] global step 4130: loss = 0.0464 (0.329 sec/step)\n",
            "I0719 09:41:58.743084 140481689634688 learning.py:507] global step 4131: loss = 0.1400 (0.288 sec/step)\n",
            "I0719 09:41:59.075955 140481689634688 learning.py:507] global step 4132: loss = 0.1688 (0.331 sec/step)\n",
            "I0719 09:41:59.385135 140481689634688 learning.py:507] global step 4133: loss = 0.0599 (0.307 sec/step)\n",
            "I0719 09:41:59.729248 140481689634688 learning.py:507] global step 4134: loss = 0.0876 (0.343 sec/step)\n",
            "I0719 09:42:00.045423 140481689634688 learning.py:507] global step 4135: loss = 0.0390 (0.314 sec/step)\n",
            "I0719 09:42:00.370338 140481689634688 learning.py:507] global step 4136: loss = 0.0764 (0.323 sec/step)\n",
            "I0719 09:42:00.694375 140481689634688 learning.py:507] global step 4137: loss = 0.0125 (0.322 sec/step)\n",
            "I0719 09:42:01.035982 140481689634688 learning.py:507] global step 4138: loss = 0.0425 (0.340 sec/step)\n",
            "I0719 09:42:01.379923 140481689634688 learning.py:507] global step 4139: loss = 0.0462 (0.342 sec/step)\n",
            "I0719 09:42:01.731349 140481689634688 learning.py:507] global step 4140: loss = 0.1688 (0.350 sec/step)\n",
            "I0719 09:42:02.063440 140481689634688 learning.py:507] global step 4141: loss = 0.0385 (0.330 sec/step)\n",
            "I0719 09:42:02.400987 140481689634688 learning.py:507] global step 4142: loss = 0.0918 (0.336 sec/step)\n",
            "I0719 09:42:02.715626 140481689634688 learning.py:507] global step 4143: loss = 0.1975 (0.313 sec/step)\n",
            "I0719 09:42:03.066362 140481689634688 learning.py:507] global step 4144: loss = 0.0377 (0.349 sec/step)\n",
            "I0719 09:42:03.395809 140481689634688 learning.py:507] global step 4145: loss = 0.0927 (0.328 sec/step)\n",
            "I0719 09:42:03.744406 140481689634688 learning.py:507] global step 4146: loss = 0.1323 (0.347 sec/step)\n",
            "I0719 09:42:04.073638 140481689634688 learning.py:507] global step 4147: loss = 0.0765 (0.328 sec/step)\n",
            "I0719 09:42:04.413342 140481689634688 learning.py:507] global step 4148: loss = 0.0503 (0.338 sec/step)\n",
            "I0719 09:42:04.765392 140481689634688 learning.py:507] global step 4149: loss = 0.0805 (0.350 sec/step)\n",
            "I0719 09:42:05.120510 140481689634688 learning.py:507] global step 4150: loss = 0.0732 (0.353 sec/step)\n",
            "I0719 09:42:05.459737 140481689634688 learning.py:507] global step 4151: loss = 0.0569 (0.338 sec/step)\n",
            "I0719 09:42:05.749693 140481689634688 learning.py:507] global step 4152: loss = 0.1206 (0.288 sec/step)\n",
            "I0719 09:42:06.088820 140481689634688 learning.py:507] global step 4153: loss = 0.1407 (0.337 sec/step)\n",
            "I0719 09:42:06.423851 140481689634688 learning.py:507] global step 4154: loss = 0.0439 (0.333 sec/step)\n",
            "I0719 09:42:06.764951 140481689634688 learning.py:507] global step 4155: loss = 0.1081 (0.339 sec/step)\n",
            "I0719 09:42:07.088470 140481689634688 learning.py:507] global step 4156: loss = 0.1112 (0.322 sec/step)\n",
            "I0719 09:42:07.438616 140481689634688 learning.py:507] global step 4157: loss = 0.0644 (0.348 sec/step)\n",
            "I0719 09:42:07.737039 140481689634688 learning.py:507] global step 4158: loss = 0.0559 (0.297 sec/step)\n",
            "I0719 09:42:08.077255 140481689634688 learning.py:507] global step 4159: loss = 0.0110 (0.338 sec/step)\n",
            "I0719 09:42:08.421076 140481689634688 learning.py:507] global step 4160: loss = 0.0622 (0.342 sec/step)\n",
            "I0719 09:42:08.751528 140481689634688 learning.py:507] global step 4161: loss = 0.0174 (0.329 sec/step)\n",
            "I0719 09:42:09.079776 140481689634688 learning.py:507] global step 4162: loss = 0.2089 (0.326 sec/step)\n",
            "I0719 09:42:09.417691 140481689634688 learning.py:507] global step 4163: loss = 0.1484 (0.336 sec/step)\n",
            "I0719 09:42:09.756370 140481689634688 learning.py:507] global step 4164: loss = 0.0836 (0.337 sec/step)\n",
            "I0719 09:42:10.103115 140481689634688 learning.py:507] global step 4165: loss = 0.0467 (0.345 sec/step)\n",
            "I0719 09:42:10.408561 140481689634688 learning.py:507] global step 4166: loss = 0.0731 (0.304 sec/step)\n",
            "I0719 09:42:10.686949 140481689634688 learning.py:507] global step 4167: loss = 0.0965 (0.277 sec/step)\n",
            "I0719 09:42:11.045905 140481689634688 learning.py:507] global step 4168: loss = 0.0845 (0.357 sec/step)\n",
            "I0719 09:42:11.369556 140481689634688 learning.py:507] global step 4169: loss = 0.0695 (0.322 sec/step)\n",
            "I0719 09:42:11.654512 140481689634688 learning.py:507] global step 4170: loss = 0.0662 (0.283 sec/step)\n",
            "I0719 09:42:11.993665 140481689634688 learning.py:507] global step 4171: loss = 0.1294 (0.337 sec/step)\n",
            "I0719 09:42:12.329536 140481689634688 learning.py:507] global step 4172: loss = 0.0776 (0.334 sec/step)\n",
            "I0719 09:42:12.657377 140481689634688 learning.py:507] global step 4173: loss = 0.0762 (0.326 sec/step)\n",
            "I0719 09:42:13.013248 140481689634688 learning.py:507] global step 4174: loss = 0.1023 (0.354 sec/step)\n",
            "I0719 09:42:13.346669 140481689634688 learning.py:507] global step 4175: loss = 0.0879 (0.332 sec/step)\n",
            "I0719 09:42:13.697836 140481689634688 learning.py:507] global step 4176: loss = 0.0768 (0.349 sec/step)\n",
            "I0719 09:42:13.992570 140481689634688 learning.py:507] global step 4177: loss = 0.1151 (0.293 sec/step)\n",
            "I0719 09:42:14.314869 140481689634688 learning.py:507] global step 4178: loss = 0.1273 (0.320 sec/step)\n",
            "I0719 09:42:14.667740 140481689634688 learning.py:507] global step 4179: loss = 0.0304 (0.351 sec/step)\n",
            "I0719 09:42:14.976737 140481689634688 learning.py:507] global step 4180: loss = 0.0426 (0.307 sec/step)\n",
            "I0719 09:42:15.295843 140481689634688 learning.py:507] global step 4181: loss = 0.0798 (0.317 sec/step)\n",
            "I0719 09:42:15.637196 140481689634688 learning.py:507] global step 4182: loss = 0.1186 (0.339 sec/step)\n",
            "I0719 09:42:15.982299 140481689634688 learning.py:507] global step 4183: loss = 0.0628 (0.343 sec/step)\n",
            "I0719 09:42:16.340912 140481689634688 learning.py:507] global step 4184: loss = 0.0571 (0.357 sec/step)\n",
            "I0719 09:42:16.672465 140481689634688 learning.py:507] global step 4185: loss = 0.1788 (0.330 sec/step)\n",
            "I0719 09:42:17.019950 140481689634688 learning.py:507] global step 4186: loss = 0.0819 (0.346 sec/step)\n",
            "I0719 09:42:17.349400 140481689634688 learning.py:507] global step 4187: loss = 0.1131 (0.328 sec/step)\n",
            "I0719 09:42:17.654078 140481689634688 learning.py:507] global step 4188: loss = 0.0934 (0.303 sec/step)\n",
            "I0719 09:42:18.013585 140481689634688 learning.py:507] global step 4189: loss = 0.0893 (0.357 sec/step)\n",
            "I0719 09:42:18.350389 140481689634688 learning.py:507] global step 4190: loss = 0.0480 (0.335 sec/step)\n",
            "I0719 09:42:18.631575 140481689634688 learning.py:507] global step 4191: loss = 0.1781 (0.279 sec/step)\n",
            "I0719 09:42:18.972077 140481689634688 learning.py:507] global step 4192: loss = 0.0834 (0.339 sec/step)\n",
            "I0719 09:42:19.283661 140481689634688 learning.py:507] global step 4193: loss = 0.0306 (0.310 sec/step)\n",
            "I0719 09:42:19.646626 140481689634688 learning.py:507] global step 4194: loss = 0.0925 (0.361 sec/step)\n",
            "I0719 09:42:19.986213 140481689634688 learning.py:507] global step 4195: loss = 0.0469 (0.338 sec/step)\n",
            "I0719 09:42:20.328036 140481689634688 learning.py:507] global step 4196: loss = 0.0906 (0.340 sec/step)\n",
            "I0719 09:42:20.669024 140481689634688 learning.py:507] global step 4197: loss = 0.0177 (0.339 sec/step)\n",
            "I0719 09:42:20.999598 140481689634688 learning.py:507] global step 4198: loss = 0.0894 (0.328 sec/step)\n",
            "I0719 09:42:21.334158 140481689634688 learning.py:507] global step 4199: loss = 0.0509 (0.332 sec/step)\n",
            "I0719 09:42:21.692543 140481689634688 learning.py:507] global step 4200: loss = 0.0320 (0.357 sec/step)\n",
            "I0719 09:42:22.036575 140481689634688 learning.py:507] global step 4201: loss = 0.2171 (0.342 sec/step)\n",
            "I0719 09:42:22.376535 140481689634688 learning.py:507] global step 4202: loss = 0.0460 (0.338 sec/step)\n",
            "I0719 09:42:22.662647 140481689634688 learning.py:507] global step 4203: loss = 0.0412 (0.284 sec/step)\n",
            "I0719 09:42:22.999801 140481689634688 learning.py:507] global step 4204: loss = 0.0869 (0.335 sec/step)\n",
            "I0719 09:42:23.327751 140481689634688 learning.py:507] global step 4205: loss = 0.0577 (0.326 sec/step)\n",
            "I0719 09:42:23.675095 140481689634688 learning.py:507] global step 4206: loss = 0.0204 (0.345 sec/step)\n",
            "I0719 09:42:23.994532 140481689634688 learning.py:507] global step 4207: loss = 0.0639 (0.317 sec/step)\n",
            "I0719 09:42:24.317625 140481689634688 learning.py:507] global step 4208: loss = 0.1025 (0.321 sec/step)\n",
            "I0719 09:42:24.646680 140481689634688 learning.py:507] global step 4209: loss = 0.1672 (0.327 sec/step)\n",
            "I0719 09:42:24.985734 140481689634688 learning.py:507] global step 4210: loss = 0.0326 (0.337 sec/step)\n",
            "I0719 09:42:25.309105 140481689634688 learning.py:507] global step 4211: loss = 0.0770 (0.322 sec/step)\n",
            "I0719 09:42:25.631128 140481689634688 learning.py:507] global step 4212: loss = 0.0960 (0.320 sec/step)\n",
            "I0719 09:42:26.010323 140481689634688 learning.py:507] global step 4213: loss = 0.0450 (0.371 sec/step)\n",
            "I0719 09:42:26.534289 140481689634688 learning.py:507] global step 4214: loss = 0.1489 (0.499 sec/step)\n",
            "I0719 09:42:27.080322 140481689634688 learning.py:507] global step 4215: loss = 0.0815 (0.529 sec/step)\n",
            "I0719 09:42:27.158582 140479018202880 supervisor.py:1050] Recording summary at step 4215.\n",
            "I0719 09:42:27.461455 140481689634688 learning.py:507] global step 4216: loss = 0.0673 (0.379 sec/step)\n",
            "I0719 09:42:27.771960 140481689634688 learning.py:507] global step 4217: loss = 0.2417 (0.309 sec/step)\n",
            "I0719 09:42:28.081966 140481689634688 learning.py:507] global step 4218: loss = 0.1383 (0.308 sec/step)\n",
            "I0719 09:42:28.445036 140481689634688 learning.py:507] global step 4219: loss = 0.0564 (0.361 sec/step)\n",
            "I0719 09:42:28.759630 140481689634688 learning.py:507] global step 4220: loss = 0.0828 (0.313 sec/step)\n",
            "I0719 09:42:29.094053 140481689634688 learning.py:507] global step 4221: loss = 0.0173 (0.333 sec/step)\n",
            "I0719 09:42:29.439064 140481689634688 learning.py:507] global step 4222: loss = 0.0486 (0.343 sec/step)\n",
            "I0719 09:42:29.764234 140481689634688 learning.py:507] global step 4223: loss = 0.0443 (0.323 sec/step)\n",
            "I0719 09:42:30.096222 140481689634688 learning.py:507] global step 4224: loss = 0.0474 (0.330 sec/step)\n",
            "I0719 09:42:30.440517 140481689634688 learning.py:507] global step 4225: loss = 0.0985 (0.343 sec/step)\n",
            "I0719 09:42:30.731594 140481689634688 learning.py:507] global step 4226: loss = 0.1338 (0.289 sec/step)\n",
            "I0719 09:42:31.065126 140481689634688 learning.py:507] global step 4227: loss = 0.0594 (0.332 sec/step)\n",
            "I0719 09:42:31.413619 140481689634688 learning.py:507] global step 4228: loss = 0.0932 (0.346 sec/step)\n",
            "I0719 09:42:31.776230 140481689634688 learning.py:507] global step 4229: loss = 0.0265 (0.361 sec/step)\n",
            "I0719 09:42:32.090956 140481689634688 learning.py:507] global step 4230: loss = 0.1339 (0.312 sec/step)\n",
            "I0719 09:42:32.450361 140481689634688 learning.py:507] global step 4231: loss = 0.1438 (0.358 sec/step)\n",
            "I0719 09:42:32.766557 140481689634688 learning.py:507] global step 4232: loss = 0.0894 (0.315 sec/step)\n",
            "I0719 09:42:33.113089 140481689634688 learning.py:507] global step 4233: loss = 0.1337 (0.345 sec/step)\n",
            "I0719 09:42:33.407491 140481689634688 learning.py:507] global step 4234: loss = 0.0520 (0.293 sec/step)\n",
            "I0719 09:42:33.694094 140481689634688 learning.py:507] global step 4235: loss = 0.1291 (0.285 sec/step)\n",
            "I0719 09:42:34.046301 140481689634688 learning.py:507] global step 4236: loss = 0.2278 (0.350 sec/step)\n",
            "I0719 09:42:34.325711 140481689634688 learning.py:507] global step 4237: loss = 0.1225 (0.278 sec/step)\n",
            "I0719 09:42:34.648802 140481689634688 learning.py:507] global step 4238: loss = 0.0934 (0.321 sec/step)\n",
            "I0719 09:42:34.987090 140481689634688 learning.py:507] global step 4239: loss = 0.0338 (0.337 sec/step)\n",
            "I0719 09:42:35.317725 140481689634688 learning.py:507] global step 4240: loss = 0.0888 (0.329 sec/step)\n",
            "I0719 09:42:35.668745 140481689634688 learning.py:507] global step 4241: loss = 0.0507 (0.349 sec/step)\n",
            "I0719 09:42:35.982733 140481689634688 learning.py:507] global step 4242: loss = 0.0940 (0.312 sec/step)\n",
            "I0719 09:42:36.346582 140481689634688 learning.py:507] global step 4243: loss = 0.0807 (0.362 sec/step)\n",
            "I0719 09:42:36.680379 140481689634688 learning.py:507] global step 4244: loss = 0.0617 (0.332 sec/step)\n",
            "I0719 09:42:37.009106 140481689634688 learning.py:507] global step 4245: loss = 0.1124 (0.327 sec/step)\n",
            "I0719 09:42:37.325896 140481689634688 learning.py:507] global step 4246: loss = 0.1451 (0.315 sec/step)\n",
            "I0719 09:42:37.629922 140481689634688 learning.py:507] global step 4247: loss = 0.1151 (0.302 sec/step)\n",
            "I0719 09:42:37.962955 140481689634688 learning.py:507] global step 4248: loss = 0.1014 (0.331 sec/step)\n",
            "I0719 09:42:38.291128 140481689634688 learning.py:507] global step 4249: loss = 0.0898 (0.326 sec/step)\n",
            "I0719 09:42:38.653432 140481689634688 learning.py:507] global step 4250: loss = 0.1071 (0.361 sec/step)\n",
            "I0719 09:42:38.973983 140481689634688 learning.py:507] global step 4251: loss = 0.0646 (0.319 sec/step)\n",
            "I0719 09:42:39.313705 140481689634688 learning.py:507] global step 4252: loss = 0.0582 (0.338 sec/step)\n",
            "I0719 09:42:39.612110 140481689634688 learning.py:507] global step 4253: loss = 0.0681 (0.297 sec/step)\n",
            "I0719 09:42:39.921770 140481689634688 learning.py:507] global step 4254: loss = 0.1334 (0.308 sec/step)\n",
            "I0719 09:42:40.255248 140481689634688 learning.py:507] global step 4255: loss = 0.0842 (0.331 sec/step)\n",
            "I0719 09:42:40.588668 140481689634688 learning.py:507] global step 4256: loss = 0.2265 (0.331 sec/step)\n",
            "I0719 09:42:40.944460 140481689634688 learning.py:507] global step 4257: loss = 0.0612 (0.354 sec/step)\n",
            "I0719 09:42:41.267832 140481689634688 learning.py:507] global step 4258: loss = 0.1167 (0.321 sec/step)\n",
            "I0719 09:42:41.618199 140481689634688 learning.py:507] global step 4259: loss = 0.0322 (0.349 sec/step)\n",
            "I0719 09:42:41.950666 140481689634688 learning.py:507] global step 4260: loss = 0.1686 (0.330 sec/step)\n",
            "I0719 09:42:42.296544 140481689634688 learning.py:507] global step 4261: loss = 0.0827 (0.344 sec/step)\n",
            "I0719 09:42:42.642684 140481689634688 learning.py:507] global step 4262: loss = 0.1503 (0.344 sec/step)\n",
            "I0719 09:42:42.965526 140481689634688 learning.py:507] global step 4263: loss = 0.1531 (0.321 sec/step)\n",
            "I0719 09:42:43.315953 140481689634688 learning.py:507] global step 4264: loss = 0.0692 (0.348 sec/step)\n",
            "I0719 09:42:43.662618 140481689634688 learning.py:507] global step 4265: loss = 0.0370 (0.345 sec/step)\n",
            "I0719 09:42:44.006691 140481689634688 learning.py:507] global step 4266: loss = 0.0877 (0.342 sec/step)\n",
            "I0719 09:42:44.331484 140481689634688 learning.py:507] global step 4267: loss = 0.1500 (0.323 sec/step)\n",
            "I0719 09:42:44.648471 140481689634688 learning.py:507] global step 4268: loss = 0.0778 (0.315 sec/step)\n",
            "I0719 09:42:44.967053 140481689634688 learning.py:507] global step 4269: loss = 0.1080 (0.317 sec/step)\n",
            "I0719 09:42:45.328413 140481689634688 learning.py:507] global step 4270: loss = 0.0630 (0.359 sec/step)\n",
            "I0719 09:42:45.671613 140481689634688 learning.py:507] global step 4271: loss = 0.2061 (0.341 sec/step)\n",
            "I0719 09:42:46.004845 140481689634688 learning.py:507] global step 4272: loss = 0.1163 (0.331 sec/step)\n",
            "I0719 09:42:46.344424 140481689634688 learning.py:507] global step 4273: loss = 0.0342 (0.338 sec/step)\n",
            "I0719 09:42:46.680603 140481689634688 learning.py:507] global step 4274: loss = 0.0798 (0.334 sec/step)\n",
            "I0719 09:42:47.034059 140481689634688 learning.py:507] global step 4275: loss = 0.0432 (0.352 sec/step)\n",
            "I0719 09:42:47.363194 140481689634688 learning.py:507] global step 4276: loss = 0.0650 (0.327 sec/step)\n",
            "I0719 09:42:47.683503 140481689634688 learning.py:507] global step 4277: loss = 0.1390 (0.318 sec/step)\n",
            "I0719 09:42:48.020763 140481689634688 learning.py:507] global step 4278: loss = 0.0618 (0.335 sec/step)\n",
            "I0719 09:42:48.366003 140481689634688 learning.py:507] global step 4279: loss = 0.0269 (0.343 sec/step)\n",
            "I0719 09:42:48.697484 140481689634688 learning.py:507] global step 4280: loss = 0.1007 (0.329 sec/step)\n",
            "I0719 09:42:49.046811 140481689634688 learning.py:507] global step 4281: loss = 0.0187 (0.347 sec/step)\n",
            "I0719 09:42:49.389390 140481689634688 learning.py:507] global step 4282: loss = 0.0642 (0.341 sec/step)\n",
            "I0719 09:42:49.731654 140481689634688 learning.py:507] global step 4283: loss = 0.0816 (0.341 sec/step)\n",
            "I0719 09:42:50.058209 140481689634688 learning.py:507] global step 4284: loss = 0.0829 (0.325 sec/step)\n",
            "I0719 09:42:50.359443 140481689634688 learning.py:507] global step 4285: loss = 0.0543 (0.299 sec/step)\n",
            "I0719 09:42:50.699633 140481689634688 learning.py:507] global step 4286: loss = 0.1055 (0.338 sec/step)\n",
            "I0719 09:42:51.016697 140481689634688 learning.py:507] global step 4287: loss = 0.1297 (0.315 sec/step)\n",
            "I0719 09:42:51.351012 140481689634688 learning.py:507] global step 4288: loss = 0.1633 (0.333 sec/step)\n",
            "I0719 09:42:51.682109 140481689634688 learning.py:507] global step 4289: loss = 0.1103 (0.329 sec/step)\n",
            "I0719 09:42:52.031837 140481689634688 learning.py:507] global step 4290: loss = 0.1451 (0.348 sec/step)\n",
            "I0719 09:42:52.353621 140481689634688 learning.py:507] global step 4291: loss = 0.1052 (0.320 sec/step)\n",
            "I0719 09:42:52.732509 140481689634688 learning.py:507] global step 4292: loss = 0.0326 (0.377 sec/step)\n",
            "I0719 09:42:53.062345 140481689634688 learning.py:507] global step 4293: loss = 0.0610 (0.327 sec/step)\n",
            "I0719 09:42:53.396982 140481689634688 learning.py:507] global step 4294: loss = 0.0890 (0.333 sec/step)\n",
            "I0719 09:42:53.721341 140481689634688 learning.py:507] global step 4295: loss = 0.1787 (0.323 sec/step)\n",
            "I0719 09:42:54.073096 140481689634688 learning.py:507] global step 4296: loss = 0.1899 (0.350 sec/step)\n",
            "I0719 09:42:54.399677 140481689634688 learning.py:507] global step 4297: loss = 0.0311 (0.325 sec/step)\n",
            "I0719 09:42:54.758616 140481689634688 learning.py:507] global step 4298: loss = 0.1030 (0.357 sec/step)\n",
            "I0719 09:42:55.090079 140481689634688 learning.py:507] global step 4299: loss = 0.1200 (0.330 sec/step)\n",
            "I0719 09:42:55.404705 140481689634688 learning.py:507] global step 4300: loss = 0.0666 (0.313 sec/step)\n",
            "I0719 09:42:55.706901 140481689634688 learning.py:507] global step 4301: loss = 0.0722 (0.301 sec/step)\n",
            "I0719 09:42:56.054640 140481689634688 learning.py:507] global step 4302: loss = 0.0654 (0.346 sec/step)\n",
            "I0719 09:42:56.419209 140481689634688 learning.py:507] global step 4303: loss = 0.0468 (0.363 sec/step)\n",
            "I0719 09:42:56.776700 140481689634688 learning.py:507] global step 4304: loss = 0.0692 (0.356 sec/step)\n",
            "I0719 09:42:57.108402 140481689634688 learning.py:507] global step 4305: loss = 0.1489 (0.328 sec/step)\n",
            "I0719 09:42:57.430302 140481689634688 learning.py:507] global step 4306: loss = 0.1518 (0.320 sec/step)\n",
            "I0719 09:42:57.796262 140481689634688 learning.py:507] global step 4307: loss = 0.0419 (0.364 sec/step)\n",
            "I0719 09:42:58.123352 140481689634688 learning.py:507] global step 4308: loss = 0.0841 (0.326 sec/step)\n",
            "I0719 09:42:58.476073 140481689634688 learning.py:507] global step 4309: loss = 0.0837 (0.351 sec/step)\n",
            "I0719 09:42:58.807923 140481689634688 learning.py:507] global step 4310: loss = 0.1644 (0.330 sec/step)\n",
            "I0719 09:42:59.144117 140481689634688 learning.py:507] global step 4311: loss = 0.1072 (0.334 sec/step)\n",
            "I0719 09:42:59.533649 140481689634688 learning.py:507] global step 4312: loss = 0.1427 (0.388 sec/step)\n",
            "I0719 09:42:59.846225 140481689634688 learning.py:507] global step 4313: loss = 0.0877 (0.311 sec/step)\n",
            "I0719 09:43:00.201626 140481689634688 learning.py:507] global step 4314: loss = 0.0373 (0.352 sec/step)\n",
            "I0719 09:43:00.539959 140481689634688 learning.py:507] global step 4315: loss = 0.1088 (0.337 sec/step)\n",
            "I0719 09:43:00.881360 140481689634688 learning.py:507] global step 4316: loss = 0.1055 (0.340 sec/step)\n",
            "I0719 09:43:01.213541 140481689634688 learning.py:507] global step 4317: loss = 0.0378 (0.331 sec/step)\n",
            "I0719 09:43:01.553622 140481689634688 learning.py:507] global step 4318: loss = 0.1051 (0.338 sec/step)\n",
            "I0719 09:43:01.852176 140481689634688 learning.py:507] global step 4319: loss = 0.1415 (0.296 sec/step)\n",
            "I0719 09:43:02.124187 140481689634688 learning.py:507] global step 4320: loss = 0.1415 (0.270 sec/step)\n",
            "I0719 09:43:02.461633 140481689634688 learning.py:507] global step 4321: loss = 0.1907 (0.336 sec/step)\n",
            "I0719 09:43:02.787370 140481689634688 learning.py:507] global step 4322: loss = 0.0154 (0.324 sec/step)\n",
            "I0719 09:43:03.124957 140481689634688 learning.py:507] global step 4323: loss = 0.1773 (0.336 sec/step)\n",
            "I0719 09:43:03.453569 140481689634688 learning.py:507] global step 4324: loss = 0.0120 (0.327 sec/step)\n",
            "I0719 09:43:03.797607 140481689634688 learning.py:507] global step 4325: loss = 0.1271 (0.342 sec/step)\n",
            "I0719 09:43:04.149880 140481689634688 learning.py:507] global step 4326: loss = 0.0617 (0.350 sec/step)\n",
            "I0719 09:43:04.477395 140481689634688 learning.py:507] global step 4327: loss = 0.0170 (0.326 sec/step)\n",
            "I0719 09:43:04.828447 140481689634688 learning.py:507] global step 4328: loss = 0.0922 (0.349 sec/step)\n",
            "I0719 09:43:05.165466 140481689634688 learning.py:507] global step 4329: loss = 0.1238 (0.335 sec/step)\n",
            "I0719 09:43:05.506145 140481689634688 learning.py:507] global step 4330: loss = 0.0804 (0.339 sec/step)\n",
            "I0719 09:43:05.851822 140481689634688 learning.py:507] global step 4331: loss = 0.0883 (0.344 sec/step)\n",
            "I0719 09:43:06.180653 140481689634688 learning.py:507] global step 4332: loss = 0.0924 (0.327 sec/step)\n",
            "I0719 09:43:06.497287 140481689634688 learning.py:507] global step 4333: loss = 0.1557 (0.315 sec/step)\n",
            "I0719 09:43:06.803831 140481689634688 learning.py:507] global step 4334: loss = 0.0630 (0.305 sec/step)\n",
            "I0719 09:43:07.131136 140481689634688 learning.py:507] global step 4335: loss = 0.0876 (0.326 sec/step)\n",
            "I0719 09:43:07.464703 140481689634688 learning.py:507] global step 4336: loss = 0.0641 (0.332 sec/step)\n",
            "I0719 09:43:07.770114 140481689634688 learning.py:507] global step 4337: loss = 0.1331 (0.304 sec/step)\n",
            "I0719 09:43:08.117441 140481689634688 learning.py:507] global step 4338: loss = 0.0867 (0.345 sec/step)\n",
            "I0719 09:43:08.409744 140481689634688 learning.py:507] global step 4339: loss = 0.0680 (0.291 sec/step)\n",
            "I0719 09:43:08.751220 140481689634688 learning.py:507] global step 4340: loss = 0.0441 (0.340 sec/step)\n",
            "I0719 09:43:09.089901 140481689634688 learning.py:507] global step 4341: loss = 0.0667 (0.337 sec/step)\n",
            "I0719 09:43:09.402242 140481689634688 learning.py:507] global step 4342: loss = 0.0938 (0.310 sec/step)\n",
            "I0719 09:43:09.767206 140481689634688 learning.py:507] global step 4343: loss = 0.0808 (0.363 sec/step)\n",
            "I0719 09:43:10.070527 140481689634688 learning.py:507] global step 4344: loss = 0.0269 (0.301 sec/step)\n",
            "I0719 09:43:10.393060 140481689634688 learning.py:507] global step 4345: loss = 0.0777 (0.321 sec/step)\n",
            "I0719 09:43:10.728577 140481689634688 learning.py:507] global step 4346: loss = 0.0749 (0.333 sec/step)\n",
            "I0719 09:43:11.066790 140481689634688 learning.py:507] global step 4347: loss = 0.1197 (0.336 sec/step)\n",
            "I0719 09:43:11.369091 140481689634688 learning.py:507] global step 4348: loss = 0.1233 (0.301 sec/step)\n",
            "I0719 09:43:11.714897 140481689634688 learning.py:507] global step 4349: loss = 0.0804 (0.344 sec/step)\n",
            "I0719 09:43:12.051167 140481689634688 learning.py:507] global step 4350: loss = 0.0753 (0.334 sec/step)\n",
            "I0719 09:43:12.398764 140481689634688 learning.py:507] global step 4351: loss = 0.1983 (0.346 sec/step)\n",
            "I0719 09:43:12.754086 140481689634688 learning.py:507] global step 4352: loss = 0.1310 (0.353 sec/step)\n",
            "I0719 09:43:13.107620 140481689634688 learning.py:507] global step 4353: loss = 0.1359 (0.352 sec/step)\n",
            "I0719 09:43:13.445885 140481689634688 learning.py:507] global step 4354: loss = 0.0228 (0.336 sec/step)\n",
            "I0719 09:43:13.776204 140481689634688 learning.py:507] global step 4355: loss = 0.1260 (0.328 sec/step)\n",
            "I0719 09:43:14.106878 140481689634688 learning.py:507] global step 4356: loss = 0.1405 (0.329 sec/step)\n",
            "I0719 09:43:14.387208 140481689634688 learning.py:507] global step 4357: loss = 0.1824 (0.279 sec/step)\n",
            "I0719 09:43:14.732454 140481689634688 learning.py:507] global step 4358: loss = 0.1089 (0.343 sec/step)\n",
            "I0719 09:43:15.062148 140481689634688 learning.py:507] global step 4359: loss = 0.0793 (0.328 sec/step)\n",
            "I0719 09:43:15.406187 140481689634688 learning.py:507] global step 4360: loss = 0.0403 (0.342 sec/step)\n",
            "I0719 09:43:15.717060 140481689634688 learning.py:507] global step 4361: loss = 0.0676 (0.309 sec/step)\n",
            "I0719 09:43:16.022557 140481689634688 learning.py:507] global step 4362: loss = 0.0463 (0.304 sec/step)\n",
            "I0719 09:43:16.312608 140481689634688 learning.py:507] global step 4363: loss = 0.0459 (0.288 sec/step)\n",
            "I0719 09:43:16.658907 140481689634688 learning.py:507] global step 4364: loss = 0.1201 (0.345 sec/step)\n",
            "I0719 09:43:16.973470 140481689634688 learning.py:507] global step 4365: loss = 0.0768 (0.313 sec/step)\n",
            "I0719 09:43:17.261349 140481689634688 learning.py:507] global step 4366: loss = 0.0704 (0.286 sec/step)\n",
            "I0719 09:43:17.607362 140481689634688 learning.py:507] global step 4367: loss = 0.0908 (0.345 sec/step)\n",
            "I0719 09:43:17.948251 140481689634688 learning.py:507] global step 4368: loss = 0.0805 (0.339 sec/step)\n",
            "I0719 09:43:18.309747 140481689634688 learning.py:507] global step 4369: loss = 0.1316 (0.360 sec/step)\n",
            "I0719 09:43:18.637262 140481689634688 learning.py:507] global step 4370: loss = 0.0477 (0.326 sec/step)\n",
            "I0719 09:43:18.976909 140481689634688 learning.py:507] global step 4371: loss = 0.1061 (0.338 sec/step)\n",
            "I0719 09:43:19.310715 140481689634688 learning.py:507] global step 4372: loss = 0.0984 (0.332 sec/step)\n",
            "I0719 09:43:19.621568 140481689634688 learning.py:507] global step 4373: loss = 0.1197 (0.309 sec/step)\n",
            "I0719 09:43:19.964415 140481689634688 learning.py:507] global step 4374: loss = 0.0655 (0.341 sec/step)\n",
            "I0719 09:43:20.278744 140481689634688 learning.py:507] global step 4375: loss = 0.0668 (0.313 sec/step)\n",
            "I0719 09:43:20.632755 140481689634688 learning.py:507] global step 4376: loss = 0.0579 (0.352 sec/step)\n",
            "I0719 09:43:20.949090 140481689634688 learning.py:507] global step 4377: loss = 0.0226 (0.314 sec/step)\n",
            "I0719 09:43:21.270702 140481689634688 learning.py:507] global step 4378: loss = 0.0232 (0.320 sec/step)\n",
            "I0719 09:43:21.601841 140481689634688 learning.py:507] global step 4379: loss = 0.0380 (0.326 sec/step)\n",
            "I0719 09:43:21.962732 140481689634688 learning.py:507] global step 4380: loss = 0.1368 (0.359 sec/step)\n",
            "I0719 09:43:22.268872 140481689634688 learning.py:507] global step 4381: loss = 0.1246 (0.304 sec/step)\n",
            "I0719 09:43:22.577888 140481689634688 learning.py:507] global step 4382: loss = 0.0350 (0.307 sec/step)\n",
            "I0719 09:43:22.885080 140481689634688 learning.py:507] global step 4383: loss = 0.0717 (0.305 sec/step)\n",
            "I0719 09:43:23.192171 140481689634688 learning.py:507] global step 4384: loss = 0.1705 (0.305 sec/step)\n",
            "I0719 09:43:23.507793 140481689634688 learning.py:507] global step 4385: loss = 0.0317 (0.314 sec/step)\n",
            "I0719 09:43:23.853024 140481689634688 learning.py:507] global step 4386: loss = 0.1687 (0.343 sec/step)\n",
            "I0719 09:43:24.186670 140481689634688 learning.py:507] global step 4387: loss = 0.1034 (0.332 sec/step)\n",
            "I0719 09:43:24.486226 140481689634688 learning.py:507] global step 4388: loss = 0.0482 (0.297 sec/step)\n",
            "I0719 09:43:24.834186 140481689634688 learning.py:507] global step 4389: loss = 0.0614 (0.346 sec/step)\n",
            "I0719 09:43:25.136809 140481689634688 learning.py:507] global step 4390: loss = 0.0806 (0.301 sec/step)\n",
            "I0719 09:43:25.488759 140481689634688 learning.py:507] global step 4391: loss = 0.0591 (0.350 sec/step)\n",
            "I0719 09:43:25.837894 140481689634688 learning.py:507] global step 4392: loss = 0.0660 (0.347 sec/step)\n",
            "I0719 09:43:26.181352 140481689634688 learning.py:507] global step 4393: loss = 0.1111 (0.342 sec/step)\n",
            "I0719 09:43:26.527055 140481689634688 learning.py:507] global step 4394: loss = 0.0438 (0.344 sec/step)\n",
            "I0719 09:43:26.811730 140481689634688 learning.py:507] global step 4395: loss = 0.0679 (0.283 sec/step)\n",
            "I0719 09:43:27.120114 140481689634688 learning.py:507] global step 4396: loss = 0.1709 (0.307 sec/step)\n",
            "I0719 09:43:27.463303 140481689634688 learning.py:507] global step 4397: loss = 0.1012 (0.342 sec/step)\n",
            "I0719 09:43:27.794402 140481689634688 learning.py:507] global step 4398: loss = 0.0496 (0.329 sec/step)\n",
            "I0719 09:43:28.165123 140481689634688 learning.py:507] global step 4399: loss = 0.0987 (0.369 sec/step)\n",
            "I0719 09:43:28.464980 140481689634688 learning.py:507] global step 4400: loss = 0.0633 (0.298 sec/step)\n",
            "I0719 09:43:28.759554 140481689634688 learning.py:507] global step 4401: loss = 0.0656 (0.293 sec/step)\n",
            "I0719 09:43:29.100905 140481689634688 learning.py:507] global step 4402: loss = 0.0551 (0.340 sec/step)\n",
            "I0719 09:43:29.452960 140481689634688 learning.py:507] global step 4403: loss = 0.0300 (0.349 sec/step)\n",
            "I0719 09:43:29.818691 140481689634688 learning.py:507] global step 4404: loss = 0.0543 (0.364 sec/step)\n",
            "I0719 09:43:30.101087 140481689634688 learning.py:507] global step 4405: loss = 0.0307 (0.281 sec/step)\n",
            "I0719 09:43:30.455330 140481689634688 learning.py:507] global step 4406: loss = 0.0521 (0.352 sec/step)\n",
            "I0719 09:43:30.801971 140481689634688 learning.py:507] global step 4407: loss = 0.0757 (0.345 sec/step)\n",
            "I0719 09:43:31.159676 140481689634688 learning.py:507] global step 4408: loss = 0.0665 (0.356 sec/step)\n",
            "I0719 09:43:31.488487 140481689634688 learning.py:507] global step 4409: loss = 0.0700 (0.327 sec/step)\n",
            "I0719 09:43:31.838829 140481689634688 learning.py:507] global step 4410: loss = 0.0873 (0.349 sec/step)\n",
            "I0719 09:43:32.147088 140481689634688 learning.py:507] global step 4411: loss = 0.1103 (0.306 sec/step)\n",
            "I0719 09:43:32.486662 140481689634688 learning.py:507] global step 4412: loss = 0.0820 (0.338 sec/step)\n",
            "I0719 09:43:32.805541 140481689634688 learning.py:507] global step 4413: loss = 0.0239 (0.316 sec/step)\n",
            "I0719 09:43:33.120040 140481689634688 learning.py:507] global step 4414: loss = 0.0726 (0.313 sec/step)\n",
            "I0719 09:43:33.453748 140481689634688 learning.py:507] global step 4415: loss = 0.1204 (0.332 sec/step)\n",
            "I0719 09:43:33.734635 140481689634688 learning.py:507] global step 4416: loss = 0.0334 (0.279 sec/step)\n",
            "I0719 09:43:34.058602 140481689634688 learning.py:507] global step 4417: loss = 0.0716 (0.322 sec/step)\n",
            "I0719 09:43:34.400418 140481689634688 learning.py:507] global step 4418: loss = 0.0939 (0.339 sec/step)\n",
            "I0719 09:43:34.756181 140481689634688 learning.py:507] global step 4419: loss = 0.0910 (0.354 sec/step)\n",
            "I0719 09:43:35.089730 140481689634688 learning.py:507] global step 4420: loss = 0.1112 (0.332 sec/step)\n",
            "I0719 09:43:35.443788 140481689634688 learning.py:507] global step 4421: loss = 0.0960 (0.352 sec/step)\n",
            "I0719 09:43:35.767362 140481689634688 learning.py:507] global step 4422: loss = 0.1099 (0.322 sec/step)\n",
            "I0719 09:43:36.110627 140481689634688 learning.py:507] global step 4423: loss = 0.0335 (0.341 sec/step)\n",
            "I0719 09:43:36.455686 140481689634688 learning.py:507] global step 4424: loss = 0.1424 (0.343 sec/step)\n",
            "I0719 09:43:36.784475 140481689634688 learning.py:507] global step 4425: loss = 0.0567 (0.327 sec/step)\n",
            "I0719 09:43:37.137881 140481689634688 learning.py:507] global step 4426: loss = 0.1933 (0.352 sec/step)\n",
            "I0719 09:43:37.466697 140481689634688 learning.py:507] global step 4427: loss = 0.0302 (0.327 sec/step)\n",
            "I0719 09:43:37.815067 140481689634688 learning.py:507] global step 4428: loss = 0.0305 (0.347 sec/step)\n",
            "I0719 09:43:38.167799 140481689634688 learning.py:507] global step 4429: loss = 0.1107 (0.351 sec/step)\n",
            "I0719 09:43:38.484328 140481689634688 learning.py:507] global step 4430: loss = 0.0451 (0.315 sec/step)\n",
            "I0719 09:43:38.813941 140481689634688 learning.py:507] global step 4431: loss = 0.0390 (0.328 sec/step)\n",
            "I0719 09:43:39.139093 140481689634688 learning.py:507] global step 4432: loss = 0.0764 (0.323 sec/step)\n",
            "I0719 09:43:39.502095 140481689634688 learning.py:507] global step 4433: loss = 0.0368 (0.361 sec/step)\n",
            "I0719 09:43:39.834383 140481689634688 learning.py:507] global step 4434: loss = 0.0787 (0.331 sec/step)\n",
            "I0719 09:43:40.153451 140481689634688 learning.py:507] global step 4435: loss = 0.0783 (0.317 sec/step)\n",
            "I0719 09:43:40.495149 140481689634688 learning.py:507] global step 4436: loss = 0.0305 (0.340 sec/step)\n",
            "I0719 09:43:40.815656 140481689634688 learning.py:507] global step 4437: loss = 0.0747 (0.319 sec/step)\n",
            "I0719 09:43:41.135822 140481689634688 learning.py:507] global step 4438: loss = 0.0593 (0.318 sec/step)\n",
            "I0719 09:43:41.472181 140481689634688 learning.py:507] global step 4439: loss = 0.0969 (0.335 sec/step)\n",
            "I0719 09:43:41.807115 140481689634688 learning.py:507] global step 4440: loss = 0.0942 (0.333 sec/step)\n",
            "I0719 09:43:42.125607 140481689634688 learning.py:507] global step 4441: loss = 0.0455 (0.317 sec/step)\n",
            "I0719 09:43:42.418588 140481689634688 learning.py:507] global step 4442: loss = 0.0372 (0.291 sec/step)\n",
            "I0719 09:43:42.701650 140481689634688 learning.py:507] global step 4443: loss = 0.1163 (0.281 sec/step)\n",
            "I0719 09:43:42.981909 140481689634688 learning.py:507] global step 4444: loss = 0.0716 (0.278 sec/step)\n",
            "I0719 09:43:43.318559 140481689634688 learning.py:507] global step 4445: loss = 0.0793 (0.335 sec/step)\n",
            "I0719 09:43:43.672869 140481689634688 learning.py:507] global step 4446: loss = 0.0630 (0.353 sec/step)\n",
            "I0719 09:43:44.013590 140481689634688 learning.py:507] global step 4447: loss = 0.0399 (0.339 sec/step)\n",
            "I0719 09:43:44.329635 140481689634688 learning.py:507] global step 4448: loss = 0.0475 (0.314 sec/step)\n",
            "I0719 09:43:44.662842 140481689634688 learning.py:507] global step 4449: loss = 0.0638 (0.331 sec/step)\n",
            "I0719 09:43:45.021689 140481689634688 learning.py:507] global step 4450: loss = 0.0324 (0.357 sec/step)\n",
            "I0719 09:43:45.372045 140481689634688 learning.py:507] global step 4451: loss = 0.0853 (0.349 sec/step)\n",
            "I0719 09:43:45.748402 140481689634688 learning.py:507] global step 4452: loss = 0.0874 (0.374 sec/step)\n",
            "I0719 09:43:46.088119 140481689634688 learning.py:507] global step 4453: loss = 0.0924 (0.337 sec/step)\n",
            "I0719 09:43:46.423826 140481689634688 learning.py:507] global step 4454: loss = 0.0124 (0.334 sec/step)\n",
            "I0719 09:43:46.773865 140481689634688 learning.py:507] global step 4455: loss = 0.0707 (0.348 sec/step)\n",
            "I0719 09:43:47.122839 140481689634688 learning.py:507] global step 4456: loss = 0.0897 (0.347 sec/step)\n",
            "I0719 09:43:47.427450 140481689634688 learning.py:507] global step 4457: loss = 0.1386 (0.303 sec/step)\n",
            "I0719 09:43:47.762044 140481689634688 learning.py:507] global step 4458: loss = 0.0212 (0.333 sec/step)\n",
            "I0719 09:43:48.061434 140481689634688 learning.py:507] global step 4459: loss = 0.1136 (0.298 sec/step)\n",
            "I0719 09:43:48.424536 140481689634688 learning.py:507] global step 4460: loss = 0.0317 (0.362 sec/step)\n",
            "I0719 09:43:48.782183 140481689634688 learning.py:507] global step 4461: loss = 0.0736 (0.356 sec/step)\n",
            "I0719 09:43:49.117612 140481689634688 learning.py:507] global step 4462: loss = 0.0658 (0.334 sec/step)\n",
            "I0719 09:43:49.444349 140481689634688 learning.py:507] global step 4463: loss = 0.1282 (0.325 sec/step)\n",
            "I0719 09:43:49.815485 140481689634688 learning.py:507] global step 4464: loss = 0.0837 (0.369 sec/step)\n",
            "I0719 09:43:50.149096 140481689634688 learning.py:507] global step 4465: loss = 0.0826 (0.332 sec/step)\n",
            "I0719 09:43:50.481842 140481689634688 learning.py:507] global step 4466: loss = 0.0999 (0.331 sec/step)\n",
            "I0719 09:43:50.841799 140481689634688 learning.py:507] global step 4467: loss = 0.0882 (0.358 sec/step)\n",
            "I0719 09:43:51.150550 140481689634688 learning.py:507] global step 4468: loss = 0.0751 (0.307 sec/step)\n",
            "I0719 09:43:51.482460 140481689634688 learning.py:507] global step 4469: loss = 0.0921 (0.330 sec/step)\n",
            "I0719 09:43:51.802477 140481689634688 learning.py:507] global step 4470: loss = 0.1008 (0.318 sec/step)\n",
            "I0719 09:43:52.144011 140481689634688 learning.py:507] global step 4471: loss = 0.0907 (0.340 sec/step)\n",
            "I0719 09:43:52.496775 140481689634688 learning.py:507] global step 4472: loss = 0.0823 (0.351 sec/step)\n",
            "I0719 09:43:52.861376 140481689634688 learning.py:507] global step 4473: loss = 0.1149 (0.362 sec/step)\n",
            "I0719 09:43:53.204856 140481689634688 learning.py:507] global step 4474: loss = 0.1221 (0.342 sec/step)\n",
            "I0719 09:43:53.530626 140481689634688 learning.py:507] global step 4475: loss = 0.0527 (0.324 sec/step)\n",
            "I0719 09:43:53.882017 140481689634688 learning.py:507] global step 4476: loss = 0.1078 (0.350 sec/step)\n",
            "I0719 09:43:54.204360 140481689634688 learning.py:507] global step 4477: loss = 0.0961 (0.321 sec/step)\n",
            "I0719 09:43:54.548799 140481689634688 learning.py:507] global step 4478: loss = 0.0817 (0.343 sec/step)\n",
            "I0719 09:43:54.905248 140481689634688 learning.py:507] global step 4479: loss = 0.1304 (0.355 sec/step)\n",
            "I0719 09:43:55.231058 140481689634688 learning.py:507] global step 4480: loss = 0.1280 (0.324 sec/step)\n",
            "I0719 09:43:55.561170 140481689634688 learning.py:507] global step 4481: loss = 0.0659 (0.328 sec/step)\n",
            "I0719 09:43:55.886163 140481689634688 learning.py:507] global step 4482: loss = 0.1247 (0.323 sec/step)\n",
            "I0719 09:43:56.244132 140481689634688 learning.py:507] global step 4483: loss = 0.1100 (0.356 sec/step)\n",
            "I0719 09:43:56.578565 140481689634688 learning.py:507] global step 4484: loss = 0.1453 (0.332 sec/step)\n",
            "I0719 09:43:56.928951 140481689634688 learning.py:507] global step 4485: loss = 0.0318 (0.349 sec/step)\n",
            "I0719 09:43:57.247692 140481689634688 learning.py:507] global step 4486: loss = 0.0833 (0.317 sec/step)\n",
            "I0719 09:43:57.576843 140481689634688 learning.py:507] global step 4487: loss = 0.1044 (0.327 sec/step)\n",
            "I0719 09:43:57.901635 140481689634688 learning.py:507] global step 4488: loss = 0.1272 (0.323 sec/step)\n",
            "I0719 09:43:58.200249 140481689634688 learning.py:507] global step 4489: loss = 0.1170 (0.297 sec/step)\n",
            "I0719 09:43:58.526658 140481689634688 learning.py:507] global step 4490: loss = 0.0787 (0.325 sec/step)\n",
            "I0719 09:43:58.851097 140481689634688 learning.py:507] global step 4491: loss = 0.0924 (0.323 sec/step)\n",
            "I0719 09:43:59.190697 140481689634688 learning.py:507] global step 4492: loss = 0.1945 (0.338 sec/step)\n",
            "I0719 09:43:59.518975 140481689634688 learning.py:507] global step 4493: loss = 0.0191 (0.327 sec/step)\n",
            "I0719 09:43:59.845352 140481689634688 learning.py:507] global step 4494: loss = 0.0639 (0.324 sec/step)\n",
            "I0719 09:44:00.169424 140481689634688 learning.py:507] global step 4495: loss = 0.0586 (0.322 sec/step)\n",
            "I0719 09:44:00.513496 140481689634688 learning.py:507] global step 4496: loss = 0.0441 (0.342 sec/step)\n",
            "I0719 09:44:00.847650 140481689634688 learning.py:507] global step 4497: loss = 0.1310 (0.332 sec/step)\n",
            "I0719 09:44:01.146717 140481689634688 learning.py:507] global step 4498: loss = 0.0733 (0.297 sec/step)\n",
            "I0719 09:44:01.463055 140481689634688 learning.py:507] global step 4499: loss = 0.0473 (0.315 sec/step)\n",
            "I0719 09:44:01.801210 140481689634688 learning.py:507] global step 4500: loss = 0.1297 (0.336 sec/step)\n",
            "I0719 09:44:02.093148 140481689634688 learning.py:507] global step 4501: loss = 0.0296 (0.290 sec/step)\n",
            "I0719 09:44:02.423592 140481689634688 learning.py:507] global step 4502: loss = 0.0217 (0.329 sec/step)\n",
            "I0719 09:44:02.714925 140481689634688 learning.py:507] global step 4503: loss = 0.0710 (0.290 sec/step)\n",
            "I0719 09:44:03.039088 140481689634688 learning.py:507] global step 4504: loss = 0.1060 (0.322 sec/step)\n",
            "I0719 09:44:03.385162 140481689634688 learning.py:507] global step 4505: loss = 0.0436 (0.344 sec/step)\n",
            "I0719 09:44:03.723611 140481689634688 learning.py:507] global step 4506: loss = 0.1128 (0.337 sec/step)\n",
            "I0719 09:44:04.046139 140481689634688 learning.py:507] global step 4507: loss = 0.0752 (0.321 sec/step)\n",
            "I0719 09:44:04.328570 140481689634688 learning.py:507] global step 4508: loss = 0.0541 (0.281 sec/step)\n",
            "I0719 09:44:04.689857 140481689634688 learning.py:507] global step 4509: loss = 0.0563 (0.360 sec/step)\n",
            "I0719 09:44:05.059888 140481689634688 learning.py:507] global step 4510: loss = 0.0959 (0.368 sec/step)\n",
            "I0719 09:44:05.409376 140481689634688 learning.py:507] global step 4511: loss = 0.0180 (0.348 sec/step)\n",
            "I0719 09:44:05.721478 140481689634688 learning.py:507] global step 4512: loss = 0.0437 (0.310 sec/step)\n",
            "I0719 09:44:06.066828 140481689634688 learning.py:507] global step 4513: loss = 0.1466 (0.343 sec/step)\n",
            "I0719 09:44:06.412727 140481689634688 learning.py:507] global step 4514: loss = 0.1770 (0.344 sec/step)\n",
            "I0719 09:44:06.746191 140481689634688 learning.py:507] global step 4515: loss = 0.0154 (0.331 sec/step)\n",
            "I0719 09:44:07.087970 140481689634688 learning.py:507] global step 4516: loss = 0.1172 (0.340 sec/step)\n",
            "I0719 09:44:07.443574 140481689634688 learning.py:507] global step 4517: loss = 0.0554 (0.354 sec/step)\n",
            "I0719 09:44:07.781677 140481689634688 learning.py:507] global step 4518: loss = 0.0661 (0.336 sec/step)\n",
            "I0719 09:44:08.104374 140481689634688 learning.py:507] global step 4519: loss = 0.0152 (0.321 sec/step)\n",
            "I0719 09:44:08.438514 140481689634688 learning.py:507] global step 4520: loss = 0.0977 (0.332 sec/step)\n",
            "I0719 09:44:08.753221 140481689634688 learning.py:507] global step 4521: loss = 0.0478 (0.313 sec/step)\n",
            "I0719 09:44:09.095205 140481689634688 learning.py:507] global step 4522: loss = 0.1301 (0.340 sec/step)\n",
            "I0719 09:44:09.416444 140481689634688 learning.py:507] global step 4523: loss = 0.0389 (0.320 sec/step)\n",
            "I0719 09:44:09.756097 140481689634688 learning.py:507] global step 4524: loss = 0.0496 (0.338 sec/step)\n",
            "I0719 09:44:10.110062 140481689634688 learning.py:507] global step 4525: loss = 0.1017 (0.352 sec/step)\n",
            "I0719 09:44:10.497409 140481689634688 learning.py:507] global step 4526: loss = 0.0903 (0.385 sec/step)\n",
            "I0719 09:44:10.828176 140481689634688 learning.py:507] global step 4527: loss = 0.0558 (0.329 sec/step)\n",
            "I0719 09:44:11.177857 140481689634688 learning.py:507] global step 4528: loss = 0.1965 (0.348 sec/step)\n",
            "I0719 09:44:11.529782 140481689634688 learning.py:507] global step 4529: loss = 0.0666 (0.350 sec/step)\n",
            "I0719 09:44:11.879756 140481689634688 learning.py:507] global step 4530: loss = 0.0571 (0.348 sec/step)\n",
            "I0719 09:44:12.248697 140481689634688 learning.py:507] global step 4531: loss = 0.0291 (0.367 sec/step)\n",
            "I0719 09:44:12.554437 140481689634688 learning.py:507] global step 4532: loss = 0.0913 (0.304 sec/step)\n",
            "I0719 09:44:12.905510 140481689634688 learning.py:507] global step 4533: loss = 0.0894 (0.349 sec/step)\n",
            "I0719 09:44:13.243693 140481689634688 learning.py:507] global step 4534: loss = 0.0861 (0.336 sec/step)\n",
            "I0719 09:44:13.580197 140481689634688 learning.py:507] global step 4535: loss = 0.0165 (0.335 sec/step)\n",
            "I0719 09:44:13.928235 140481689634688 learning.py:507] global step 4536: loss = 0.0978 (0.346 sec/step)\n",
            "I0719 09:44:14.282185 140481689634688 learning.py:507] global step 4537: loss = 0.1269 (0.352 sec/step)\n",
            "I0719 09:44:14.617561 140481689634688 learning.py:507] global step 4538: loss = 0.0974 (0.334 sec/step)\n",
            "I0719 09:44:14.939538 140481689634688 learning.py:507] global step 4539: loss = 0.0354 (0.320 sec/step)\n",
            "I0719 09:44:15.259957 140481689634688 learning.py:507] global step 4540: loss = 0.0731 (0.319 sec/step)\n",
            "I0719 09:44:15.586332 140481689634688 learning.py:507] global step 4541: loss = 0.0275 (0.325 sec/step)\n",
            "I0719 09:44:15.941487 140481689634688 learning.py:507] global step 4542: loss = 0.1500 (0.353 sec/step)\n",
            "I0719 09:44:16.286062 140481689634688 learning.py:507] global step 4543: loss = 0.0546 (0.343 sec/step)\n",
            "I0719 09:44:16.625735 140481689634688 learning.py:507] global step 4544: loss = 0.0937 (0.338 sec/step)\n",
            "I0719 09:44:16.960194 140481689634688 learning.py:507] global step 4545: loss = 0.0509 (0.333 sec/step)\n",
            "I0719 09:44:17.273461 140481689634688 learning.py:507] global step 4546: loss = 0.0486 (0.312 sec/step)\n",
            "I0719 09:44:17.583005 140481689634688 learning.py:507] global step 4547: loss = 0.1630 (0.308 sec/step)\n",
            "I0719 09:44:17.878729 140481689634688 learning.py:507] global step 4548: loss = 0.1720 (0.294 sec/step)\n",
            "I0719 09:44:18.221657 140481689634688 learning.py:507] global step 4549: loss = 0.0339 (0.341 sec/step)\n",
            "I0719 09:44:18.563661 140481689634688 learning.py:507] global step 4550: loss = 0.1111 (0.340 sec/step)\n",
            "I0719 09:44:18.866724 140481689634688 learning.py:507] global step 4551: loss = 0.0819 (0.301 sec/step)\n",
            "I0719 09:44:19.205316 140481689634688 learning.py:507] global step 4552: loss = 0.1819 (0.337 sec/step)\n",
            "I0719 09:44:19.530689 140481689634688 learning.py:507] global step 4553: loss = 0.0654 (0.324 sec/step)\n",
            "I0719 09:44:19.854231 140481689634688 learning.py:507] global step 4554: loss = 0.0180 (0.322 sec/step)\n",
            "I0719 09:44:20.178508 140481689634688 learning.py:507] global step 4555: loss = 0.1524 (0.322 sec/step)\n",
            "I0719 09:44:20.535822 140481689634688 learning.py:507] global step 4556: loss = 0.0806 (0.355 sec/step)\n",
            "I0719 09:44:20.924625 140481689634688 learning.py:507] global step 4557: loss = 0.1072 (0.387 sec/step)\n",
            "I0719 09:44:21.274148 140481689634688 learning.py:507] global step 4558: loss = 0.1162 (0.348 sec/step)\n",
            "I0719 09:44:21.567850 140481689634688 learning.py:507] global step 4559: loss = 0.0455 (0.292 sec/step)\n",
            "I0719 09:44:21.913550 140481689634688 learning.py:507] global step 4560: loss = 0.0632 (0.344 sec/step)\n",
            "I0719 09:44:22.245100 140481689634688 learning.py:507] global step 4561: loss = 0.1018 (0.330 sec/step)\n",
            "I0719 09:44:22.582337 140481689634688 learning.py:507] global step 4562: loss = 0.0960 (0.335 sec/step)\n",
            "I0719 09:44:22.922425 140481689634688 learning.py:507] global step 4563: loss = 0.0342 (0.338 sec/step)\n",
            "I0719 09:44:23.271585 140481689634688 learning.py:507] global step 4564: loss = 0.0600 (0.347 sec/step)\n",
            "I0719 09:44:23.610895 140481689634688 learning.py:507] global step 4565: loss = 0.0681 (0.338 sec/step)\n",
            "I0719 09:44:23.947766 140481689634688 learning.py:507] global step 4566: loss = 0.0856 (0.335 sec/step)\n",
            "I0719 09:44:24.301433 140481689634688 learning.py:507] global step 4567: loss = 0.0845 (0.352 sec/step)\n",
            "I0719 09:44:24.661011 140481689634688 learning.py:507] global step 4568: loss = 0.0299 (0.355 sec/step)\n",
            "I0719 09:44:24.953414 140481689634688 learning.py:507] global step 4569: loss = 0.1134 (0.288 sec/step)\n",
            "I0719 09:44:25.296947 140481689634688 learning.py:507] global step 4570: loss = 0.0386 (0.342 sec/step)\n",
            "I0719 09:44:25.640342 140481689634688 learning.py:507] global step 4571: loss = 0.0836 (0.341 sec/step)\n",
            "I0719 09:44:25.963528 140481689634688 learning.py:507] global step 4572: loss = 0.0813 (0.321 sec/step)\n",
            "I0719 09:44:26.513225 140481689634688 learning.py:507] global step 4573: loss = 0.0723 (0.524 sec/step)\n",
            "I0719 09:44:27.002047 140481689634688 learning.py:507] global step 4574: loss = 0.0701 (0.451 sec/step)\n",
            "I0719 09:44:27.074201 140479018202880 supervisor.py:1050] Recording summary at step 4574.\n",
            "I0719 09:44:27.380358 140481689634688 learning.py:507] global step 4575: loss = 0.1144 (0.376 sec/step)\n",
            "I0719 09:44:27.693838 140481689634688 learning.py:507] global step 4576: loss = 0.1176 (0.312 sec/step)\n",
            "I0719 09:44:28.020298 140481689634688 learning.py:507] global step 4577: loss = 0.0807 (0.325 sec/step)\n",
            "I0719 09:44:28.376352 140481689634688 learning.py:507] global step 4578: loss = 0.1142 (0.354 sec/step)\n",
            "I0719 09:44:28.718129 140481689634688 learning.py:507] global step 4579: loss = 0.0758 (0.340 sec/step)\n",
            "I0719 09:44:29.083333 140481689634688 learning.py:507] global step 4580: loss = 0.0623 (0.363 sec/step)\n",
            "I0719 09:44:29.421183 140481689634688 learning.py:507] global step 4581: loss = 0.1220 (0.336 sec/step)\n",
            "I0719 09:44:29.756960 140481689634688 learning.py:507] global step 4582: loss = 0.1085 (0.334 sec/step)\n",
            "I0719 09:44:30.109498 140481689634688 learning.py:507] global step 4583: loss = 0.0606 (0.351 sec/step)\n",
            "I0719 09:44:30.447703 140481689634688 learning.py:507] global step 4584: loss = 0.0153 (0.336 sec/step)\n",
            "I0719 09:44:30.785503 140481689634688 learning.py:507] global step 4585: loss = 0.0512 (0.336 sec/step)\n",
            "I0719 09:44:31.124356 140481689634688 learning.py:507] global step 4586: loss = 0.3466 (0.337 sec/step)\n",
            "I0719 09:44:31.464402 140481689634688 learning.py:507] global step 4587: loss = 0.0821 (0.338 sec/step)\n",
            "I0719 09:44:31.806710 140481689634688 learning.py:507] global step 4588: loss = 0.0577 (0.340 sec/step)\n",
            "I0719 09:44:32.164395 140481689634688 learning.py:507] global step 4589: loss = 0.0302 (0.356 sec/step)\n",
            "I0719 09:44:32.444542 140481689634688 learning.py:507] global step 4590: loss = 0.0377 (0.278 sec/step)\n",
            "I0719 09:44:32.785808 140481689634688 learning.py:507] global step 4591: loss = 0.0517 (0.340 sec/step)\n",
            "I0719 09:44:33.128718 140481689634688 learning.py:507] global step 4592: loss = 0.1331 (0.341 sec/step)\n",
            "I0719 09:44:33.448797 140481689634688 learning.py:507] global step 4593: loss = 0.0987 (0.318 sec/step)\n",
            "I0719 09:44:33.768537 140481689634688 learning.py:507] global step 4594: loss = 0.0749 (0.318 sec/step)\n",
            "I0719 09:44:34.105894 140481689634688 learning.py:507] global step 4595: loss = 0.1633 (0.336 sec/step)\n",
            "I0719 09:44:34.450699 140481689634688 learning.py:507] global step 4596: loss = 0.0629 (0.343 sec/step)\n",
            "I0719 09:44:34.718419 140481689634688 learning.py:507] global step 4597: loss = 0.0614 (0.266 sec/step)\n",
            "I0719 09:44:35.046439 140481689634688 learning.py:507] global step 4598: loss = 0.1005 (0.326 sec/step)\n",
            "I0719 09:44:35.389729 140481689634688 learning.py:507] global step 4599: loss = 0.0720 (0.341 sec/step)\n",
            "I0719 09:44:35.739656 140481689634688 learning.py:507] global step 4600: loss = 0.0538 (0.348 sec/step)\n",
            "I0719 09:44:36.082671 140481689634688 learning.py:507] global step 4601: loss = 0.0945 (0.341 sec/step)\n",
            "I0719 09:44:36.406237 140481689634688 learning.py:507] global step 4602: loss = 0.0379 (0.322 sec/step)\n",
            "I0719 09:44:36.723622 140481689634688 learning.py:507] global step 4603: loss = 0.1060 (0.316 sec/step)\n",
            "I0719 09:44:37.064806 140481689634688 learning.py:507] global step 4604: loss = 0.1045 (0.339 sec/step)\n",
            "I0719 09:44:37.393779 140481689634688 learning.py:507] global step 4605: loss = 0.0773 (0.327 sec/step)\n",
            "I0719 09:44:37.717623 140481689634688 learning.py:507] global step 4606: loss = 0.0796 (0.322 sec/step)\n",
            "I0719 09:44:38.072450 140481689634688 learning.py:507] global step 4607: loss = 0.1088 (0.353 sec/step)\n",
            "I0719 09:44:38.411262 140481689634688 learning.py:507] global step 4608: loss = 0.0642 (0.337 sec/step)\n",
            "I0719 09:44:38.757153 140481689634688 learning.py:507] global step 4609: loss = 0.0325 (0.344 sec/step)\n",
            "I0719 09:44:39.076583 140481689634688 learning.py:507] global step 4610: loss = 0.0191 (0.317 sec/step)\n",
            "I0719 09:44:39.379705 140481689634688 learning.py:507] global step 4611: loss = 0.0570 (0.301 sec/step)\n",
            "I0719 09:44:39.704714 140481689634688 learning.py:507] global step 4612: loss = 0.1123 (0.323 sec/step)\n",
            "I0719 09:44:40.048403 140481689634688 learning.py:507] global step 4613: loss = 0.0445 (0.342 sec/step)\n",
            "I0719 09:44:40.384350 140481689634688 learning.py:507] global step 4614: loss = 0.1384 (0.334 sec/step)\n",
            "I0719 09:44:40.673471 140481689634688 learning.py:507] global step 4615: loss = 0.1124 (0.287 sec/step)\n",
            "I0719 09:44:41.030688 140481689634688 learning.py:507] global step 4616: loss = 0.1902 (0.355 sec/step)\n",
            "I0719 09:44:41.363219 140481689634688 learning.py:507] global step 4617: loss = 0.1083 (0.331 sec/step)\n",
            "I0719 09:44:41.646834 140481689634688 learning.py:507] global step 4618: loss = 0.0475 (0.282 sec/step)\n",
            "I0719 09:44:41.965028 140481689634688 learning.py:507] global step 4619: loss = 0.0734 (0.316 sec/step)\n",
            "I0719 09:44:42.313999 140481689634688 learning.py:507] global step 4620: loss = 0.0789 (0.347 sec/step)\n",
            "I0719 09:44:42.677022 140481689634688 learning.py:507] global step 4621: loss = 0.2644 (0.361 sec/step)\n",
            "I0719 09:44:43.033162 140481689634688 learning.py:507] global step 4622: loss = 0.1474 (0.354 sec/step)\n",
            "I0719 09:44:43.357443 140481689634688 learning.py:507] global step 4623: loss = 0.0707 (0.323 sec/step)\n",
            "I0719 09:44:43.706368 140481689634688 learning.py:507] global step 4624: loss = 0.0659 (0.347 sec/step)\n",
            "I0719 09:44:44.060564 140481689634688 learning.py:507] global step 4625: loss = 0.0736 (0.352 sec/step)\n",
            "I0719 09:44:44.347484 140481689634688 learning.py:507] global step 4626: loss = 0.0676 (0.285 sec/step)\n",
            "I0719 09:44:44.697674 140481689634688 learning.py:507] global step 4627: loss = 0.0674 (0.348 sec/step)\n",
            "I0719 09:44:45.047634 140481689634688 learning.py:507] global step 4628: loss = 0.0913 (0.348 sec/step)\n",
            "I0719 09:44:45.395786 140481689634688 learning.py:507] global step 4629: loss = 0.1888 (0.346 sec/step)\n",
            "I0719 09:44:45.747729 140481689634688 learning.py:507] global step 4630: loss = 0.1589 (0.350 sec/step)\n",
            "I0719 09:44:46.075249 140481689634688 learning.py:507] global step 4631: loss = 0.0160 (0.326 sec/step)\n",
            "I0719 09:44:46.395532 140481689634688 learning.py:507] global step 4632: loss = 0.0423 (0.318 sec/step)\n",
            "I0719 09:44:46.751313 140481689634688 learning.py:507] global step 4633: loss = 0.0432 (0.354 sec/step)\n",
            "I0719 09:44:47.076713 140481689634688 learning.py:507] global step 4634: loss = 0.0514 (0.323 sec/step)\n",
            "I0719 09:44:47.410406 140481689634688 learning.py:507] global step 4635: loss = 0.0694 (0.332 sec/step)\n",
            "I0719 09:44:47.736798 140481689634688 learning.py:507] global step 4636: loss = 0.0207 (0.325 sec/step)\n",
            "I0719 09:44:48.110389 140481689634688 learning.py:507] global step 4637: loss = 0.0773 (0.372 sec/step)\n",
            "I0719 09:44:48.439881 140481689634688 learning.py:507] global step 4638: loss = 0.0702 (0.328 sec/step)\n",
            "I0719 09:44:48.740056 140481689634688 learning.py:507] global step 4639: loss = 0.0659 (0.298 sec/step)\n",
            "I0719 09:44:49.064949 140481689634688 learning.py:507] global step 4640: loss = 0.0769 (0.323 sec/step)\n",
            "I0719 09:44:49.416720 140481689634688 learning.py:507] global step 4641: loss = 0.0378 (0.347 sec/step)\n",
            "I0719 09:44:49.736636 140481689634688 learning.py:507] global step 4642: loss = 0.1022 (0.318 sec/step)\n",
            "I0719 09:44:50.096234 140481689634688 learning.py:507] global step 4643: loss = 0.0562 (0.358 sec/step)\n",
            "I0719 09:44:50.445461 140481689634688 learning.py:507] global step 4644: loss = 0.0801 (0.347 sec/step)\n",
            "I0719 09:44:50.794616 140481689634688 learning.py:507] global step 4645: loss = 0.1666 (0.348 sec/step)\n",
            "I0719 09:44:51.118340 140481689634688 learning.py:507] global step 4646: loss = 0.0213 (0.322 sec/step)\n",
            "I0719 09:44:51.450201 140481689634688 learning.py:507] global step 4647: loss = 0.0574 (0.330 sec/step)\n",
            "I0719 09:44:51.787399 140481689634688 learning.py:507] global step 4648: loss = 0.0773 (0.335 sec/step)\n",
            "I0719 09:44:52.104393 140481689634688 learning.py:507] global step 4649: loss = 0.0517 (0.315 sec/step)\n",
            "I0719 09:44:52.476715 140481689634688 learning.py:507] global step 4650: loss = 0.0356 (0.370 sec/step)\n",
            "I0719 09:44:52.827826 140481689634688 learning.py:507] global step 4651: loss = 0.0141 (0.349 sec/step)\n",
            "I0719 09:44:53.191447 140481689634688 learning.py:507] global step 4652: loss = 0.1085 (0.362 sec/step)\n",
            "I0719 09:44:53.535431 140481689634688 learning.py:507] global step 4653: loss = 0.0623 (0.342 sec/step)\n",
            "I0719 09:44:53.883599 140481689634688 learning.py:507] global step 4654: loss = 0.1590 (0.346 sec/step)\n",
            "I0719 09:44:54.226681 140481689634688 learning.py:507] global step 4655: loss = 0.0960 (0.341 sec/step)\n",
            "I0719 09:44:54.561897 140481689634688 learning.py:507] global step 4656: loss = 0.0646 (0.333 sec/step)\n",
            "I0719 09:44:54.902333 140481689634688 learning.py:507] global step 4657: loss = 0.0952 (0.338 sec/step)\n",
            "I0719 09:44:55.235505 140481689634688 learning.py:507] global step 4658: loss = 0.0675 (0.331 sec/step)\n",
            "I0719 09:44:55.543874 140481689634688 learning.py:507] global step 4659: loss = 0.0608 (0.306 sec/step)\n",
            "I0719 09:44:55.897466 140481689634688 learning.py:507] global step 4660: loss = 0.0480 (0.351 sec/step)\n",
            "I0719 09:44:56.237348 140481689634688 learning.py:507] global step 4661: loss = 0.1683 (0.338 sec/step)\n",
            "I0719 09:44:56.576853 140481689634688 learning.py:507] global step 4662: loss = 0.0305 (0.338 sec/step)\n",
            "I0719 09:44:56.859643 140481689634688 learning.py:507] global step 4663: loss = 0.1105 (0.281 sec/step)\n",
            "I0719 09:44:57.195463 140481689634688 learning.py:507] global step 4664: loss = 0.0211 (0.334 sec/step)\n",
            "I0719 09:44:57.536064 140481689634688 learning.py:507] global step 4665: loss = 0.0559 (0.339 sec/step)\n",
            "I0719 09:44:57.876489 140481689634688 learning.py:507] global step 4666: loss = 0.0899 (0.338 sec/step)\n",
            "I0719 09:44:58.213170 140481689634688 learning.py:507] global step 4667: loss = 0.1574 (0.335 sec/step)\n",
            "I0719 09:44:58.553448 140481689634688 learning.py:507] global step 4668: loss = 0.1392 (0.338 sec/step)\n",
            "I0719 09:44:58.875145 140481689634688 learning.py:507] global step 4669: loss = 0.0320 (0.320 sec/step)\n",
            "I0719 09:44:59.219616 140481689634688 learning.py:507] global step 4670: loss = 0.0389 (0.343 sec/step)\n",
            "I0719 09:44:59.556792 140481689634688 learning.py:507] global step 4671: loss = 0.0945 (0.335 sec/step)\n",
            "I0719 09:44:59.858034 140481689634688 learning.py:507] global step 4672: loss = 0.0472 (0.299 sec/step)\n",
            "I0719 09:45:00.139117 140481689634688 learning.py:507] global step 4673: loss = 0.0262 (0.279 sec/step)\n",
            "I0719 09:45:00.490365 140481689634688 learning.py:507] global step 4674: loss = 0.0741 (0.350 sec/step)\n",
            "I0719 09:45:00.827974 140481689634688 learning.py:507] global step 4675: loss = 0.0689 (0.335 sec/step)\n",
            "I0719 09:45:01.168137 140481689634688 learning.py:507] global step 4676: loss = 0.0752 (0.338 sec/step)\n",
            "I0719 09:45:01.499962 140481689634688 learning.py:507] global step 4677: loss = 0.1269 (0.330 sec/step)\n",
            "I0719 09:45:01.830868 140481689634688 learning.py:507] global step 4678: loss = 0.0403 (0.329 sec/step)\n",
            "I0719 09:45:02.169231 140481689634688 learning.py:507] global step 4679: loss = 0.0693 (0.337 sec/step)\n",
            "I0719 09:45:02.519514 140481689634688 learning.py:507] global step 4680: loss = 0.0703 (0.349 sec/step)\n",
            "I0719 09:45:02.876966 140481689634688 learning.py:507] global step 4681: loss = 0.0644 (0.356 sec/step)\n",
            "I0719 09:45:03.157702 140481689634688 learning.py:507] global step 4682: loss = 0.0877 (0.279 sec/step)\n",
            "I0719 09:45:03.511693 140481689634688 learning.py:507] global step 4683: loss = 0.0838 (0.352 sec/step)\n",
            "I0719 09:45:03.849740 140481689634688 learning.py:507] global step 4684: loss = 0.1049 (0.336 sec/step)\n",
            "I0719 09:45:04.189361 140481689634688 learning.py:507] global step 4685: loss = 0.0842 (0.338 sec/step)\n",
            "I0719 09:45:04.515985 140481689634688 learning.py:507] global step 4686: loss = 0.0717 (0.325 sec/step)\n",
            "I0719 09:45:04.859723 140481689634688 learning.py:507] global step 4687: loss = 0.0575 (0.342 sec/step)\n",
            "I0719 09:45:05.199614 140481689634688 learning.py:507] global step 4688: loss = 0.1145 (0.338 sec/step)\n",
            "I0719 09:45:05.527807 140481689634688 learning.py:507] global step 4689: loss = 0.0757 (0.326 sec/step)\n",
            "I0719 09:45:05.843913 140481689634688 learning.py:507] global step 4690: loss = 0.0970 (0.313 sec/step)\n",
            "I0719 09:45:06.182457 140481689634688 learning.py:507] global step 4691: loss = 0.0576 (0.336 sec/step)\n",
            "I0719 09:45:06.496975 140481689634688 learning.py:507] global step 4692: loss = 0.0197 (0.313 sec/step)\n",
            "I0719 09:45:06.831356 140481689634688 learning.py:507] global step 4693: loss = 0.0701 (0.333 sec/step)\n",
            "I0719 09:45:07.180258 140481689634688 learning.py:507] global step 4694: loss = 0.0926 (0.347 sec/step)\n",
            "I0719 09:45:07.535060 140481689634688 learning.py:507] global step 4695: loss = 0.0769 (0.353 sec/step)\n",
            "I0719 09:45:07.868251 140481689634688 learning.py:507] global step 4696: loss = 0.0587 (0.331 sec/step)\n",
            "I0719 09:45:08.221952 140481689634688 learning.py:507] global step 4697: loss = 0.0195 (0.352 sec/step)\n",
            "I0719 09:45:08.545286 140481689634688 learning.py:507] global step 4698: loss = 0.0513 (0.322 sec/step)\n",
            "I0719 09:45:08.880677 140481689634688 learning.py:507] global step 4699: loss = 0.0729 (0.334 sec/step)\n",
            "I0719 09:45:09.224965 140481689634688 learning.py:507] global step 4700: loss = 0.1169 (0.342 sec/step)\n",
            "I0719 09:45:09.556448 140481689634688 learning.py:507] global step 4701: loss = 0.1997 (0.330 sec/step)\n",
            "I0719 09:45:09.841824 140481689634688 learning.py:507] global step 4702: loss = 0.1276 (0.283 sec/step)\n",
            "I0719 09:45:10.205629 140481689634688 learning.py:507] global step 4703: loss = 0.1431 (0.362 sec/step)\n",
            "I0719 09:45:10.566650 140481689634688 learning.py:507] global step 4704: loss = 0.0457 (0.359 sec/step)\n",
            "I0719 09:45:10.907750 140481689634688 learning.py:507] global step 4705: loss = 0.0412 (0.339 sec/step)\n",
            "I0719 09:45:11.227433 140481689634688 learning.py:507] global step 4706: loss = 0.0691 (0.318 sec/step)\n",
            "I0719 09:45:11.568475 140481689634688 learning.py:507] global step 4707: loss = 0.0868 (0.339 sec/step)\n",
            "I0719 09:45:11.910029 140481689634688 learning.py:507] global step 4708: loss = 0.1445 (0.340 sec/step)\n",
            "I0719 09:45:12.244934 140481689634688 learning.py:507] global step 4709: loss = 0.1186 (0.333 sec/step)\n",
            "I0719 09:45:12.573343 140481689634688 learning.py:507] global step 4710: loss = 0.2166 (0.327 sec/step)\n",
            "I0719 09:45:12.891767 140481689634688 learning.py:507] global step 4711: loss = 0.0488 (0.316 sec/step)\n",
            "I0719 09:45:13.245457 140481689634688 learning.py:507] global step 4712: loss = 0.0647 (0.352 sec/step)\n",
            "I0719 09:45:13.580502 140481689634688 learning.py:507] global step 4713: loss = 0.0888 (0.333 sec/step)\n",
            "I0719 09:45:13.893565 140481689634688 learning.py:507] global step 4714: loss = 0.1675 (0.311 sec/step)\n",
            "I0719 09:45:14.190758 140481689634688 learning.py:507] global step 4715: loss = 0.1454 (0.295 sec/step)\n",
            "I0719 09:45:14.482734 140481689634688 learning.py:507] global step 4716: loss = 0.0739 (0.290 sec/step)\n",
            "I0719 09:45:14.782721 140481689634688 learning.py:507] global step 4717: loss = 0.1098 (0.298 sec/step)\n",
            "I0719 09:45:15.070689 140481689634688 learning.py:507] global step 4718: loss = 0.0568 (0.286 sec/step)\n",
            "I0719 09:45:15.413533 140481689634688 learning.py:507] global step 4719: loss = 0.0988 (0.341 sec/step)\n",
            "I0719 09:45:15.767613 140481689634688 learning.py:507] global step 4720: loss = 0.0738 (0.352 sec/step)\n",
            "I0719 09:45:16.123879 140481689634688 learning.py:507] global step 4721: loss = 0.1326 (0.354 sec/step)\n",
            "I0719 09:45:16.461118 140481689634688 learning.py:507] global step 4722: loss = 0.1208 (0.335 sec/step)\n",
            "I0719 09:45:16.799345 140481689634688 learning.py:507] global step 4723: loss = 0.0567 (0.336 sec/step)\n",
            "I0719 09:45:17.125486 140481689634688 learning.py:507] global step 4724: loss = 0.0579 (0.324 sec/step)\n",
            "I0719 09:45:17.455571 140481689634688 learning.py:507] global step 4725: loss = 0.0546 (0.328 sec/step)\n",
            "I0719 09:45:17.793231 140481689634688 learning.py:507] global step 4726: loss = 0.0550 (0.336 sec/step)\n",
            "I0719 09:45:18.125608 140481689634688 learning.py:507] global step 4727: loss = 0.0674 (0.330 sec/step)\n",
            "I0719 09:45:18.472366 140481689634688 learning.py:507] global step 4728: loss = 0.0469 (0.345 sec/step)\n",
            "I0719 09:45:18.798252 140481689634688 learning.py:507] global step 4729: loss = 0.1439 (0.324 sec/step)\n",
            "I0719 09:45:19.138741 140481689634688 learning.py:507] global step 4730: loss = 0.0946 (0.339 sec/step)\n",
            "I0719 09:45:19.448842 140481689634688 learning.py:507] global step 4731: loss = 0.0560 (0.308 sec/step)\n",
            "I0719 09:45:19.771918 140481689634688 learning.py:507] global step 4732: loss = 0.0351 (0.321 sec/step)\n",
            "I0719 09:45:20.092566 140481689634688 learning.py:507] global step 4733: loss = 0.0841 (0.319 sec/step)\n",
            "I0719 09:45:20.424378 140481689634688 learning.py:507] global step 4734: loss = 0.0676 (0.330 sec/step)\n",
            "I0719 09:45:20.718472 140481689634688 learning.py:507] global step 4735: loss = 0.0543 (0.293 sec/step)\n",
            "I0719 09:45:21.026771 140481689634688 learning.py:507] global step 4736: loss = 0.0824 (0.305 sec/step)\n",
            "I0719 09:45:21.323745 140481689634688 learning.py:507] global step 4737: loss = 0.0718 (0.295 sec/step)\n",
            "I0719 09:45:21.648316 140481689634688 learning.py:507] global step 4738: loss = 0.0239 (0.322 sec/step)\n",
            "I0719 09:45:21.997644 140481689634688 learning.py:507] global step 4739: loss = 0.0761 (0.348 sec/step)\n",
            "I0719 09:45:22.297422 140481689634688 learning.py:507] global step 4740: loss = 0.0605 (0.298 sec/step)\n",
            "I0719 09:45:22.663351 140481689634688 learning.py:507] global step 4741: loss = 0.0474 (0.364 sec/step)\n",
            "I0719 09:45:22.994309 140481689634688 learning.py:507] global step 4742: loss = 0.0537 (0.329 sec/step)\n",
            "I0719 09:45:23.272719 140481689634688 learning.py:507] global step 4743: loss = 0.0705 (0.276 sec/step)\n",
            "I0719 09:45:23.628220 140481689634688 learning.py:507] global step 4744: loss = 0.0874 (0.354 sec/step)\n",
            "I0719 09:45:23.965459 140481689634688 learning.py:507] global step 4745: loss = 0.1034 (0.335 sec/step)\n",
            "I0719 09:45:24.307158 140481689634688 learning.py:507] global step 4746: loss = 0.0899 (0.340 sec/step)\n",
            "I0719 09:45:24.649934 140481689634688 learning.py:507] global step 4747: loss = 0.0312 (0.341 sec/step)\n",
            "I0719 09:45:24.969128 140481689634688 learning.py:507] global step 4748: loss = 0.2429 (0.317 sec/step)\n",
            "I0719 09:45:25.284779 140481689634688 learning.py:507] global step 4749: loss = 0.0753 (0.314 sec/step)\n",
            "I0719 09:45:25.642805 140481689634688 learning.py:507] global step 4750: loss = 0.1337 (0.356 sec/step)\n",
            "I0719 09:45:25.938159 140481689634688 learning.py:507] global step 4751: loss = 0.0463 (0.293 sec/step)\n",
            "I0719 09:45:26.266761 140481689634688 learning.py:507] global step 4752: loss = 0.0948 (0.327 sec/step)\n",
            "I0719 09:45:26.581995 140481689634688 learning.py:507] global step 4753: loss = 0.0645 (0.313 sec/step)\n",
            "I0719 09:45:26.937631 140481689634688 learning.py:507] global step 4754: loss = 0.0212 (0.353 sec/step)\n",
            "I0719 09:45:27.275747 140481689634688 learning.py:507] global step 4755: loss = 0.0201 (0.336 sec/step)\n",
            "I0719 09:45:27.602515 140481689634688 learning.py:507] global step 4756: loss = 0.0406 (0.325 sec/step)\n",
            "I0719 09:45:27.931311 140481689634688 learning.py:507] global step 4757: loss = 0.1062 (0.325 sec/step)\n",
            "I0719 09:45:28.272815 140481689634688 learning.py:507] global step 4758: loss = 0.0861 (0.339 sec/step)\n",
            "I0719 09:45:28.596124 140481689634688 learning.py:507] global step 4759: loss = 0.1255 (0.321 sec/step)\n",
            "I0719 09:45:28.924025 140481689634688 learning.py:507] global step 4760: loss = 0.0773 (0.326 sec/step)\n",
            "I0719 09:45:29.252500 140481689634688 learning.py:507] global step 4761: loss = 0.1404 (0.327 sec/step)\n",
            "I0719 09:45:29.586623 140481689634688 learning.py:507] global step 4762: loss = 0.2421 (0.332 sec/step)\n",
            "I0719 09:45:29.929184 140481689634688 learning.py:507] global step 4763: loss = 0.0450 (0.341 sec/step)\n",
            "I0719 09:45:30.265889 140481689634688 learning.py:507] global step 4764: loss = 0.0687 (0.335 sec/step)\n",
            "I0719 09:45:30.613873 140481689634688 learning.py:507] global step 4765: loss = 0.0972 (0.346 sec/step)\n",
            "I0719 09:45:30.943852 140481689634688 learning.py:507] global step 4766: loss = 0.0545 (0.328 sec/step)\n",
            "I0719 09:45:31.272687 140481689634688 learning.py:507] global step 4767: loss = 0.0796 (0.327 sec/step)\n",
            "I0719 09:45:31.620287 140481689634688 learning.py:507] global step 4768: loss = 0.0755 (0.346 sec/step)\n",
            "I0719 09:45:31.993649 140481689634688 learning.py:507] global step 4769: loss = 0.1297 (0.372 sec/step)\n",
            "I0719 09:45:32.306474 140481689634688 learning.py:507] global step 4770: loss = 0.0568 (0.311 sec/step)\n",
            "I0719 09:45:32.609987 140481689634688 learning.py:507] global step 4771: loss = 0.0574 (0.301 sec/step)\n",
            "I0719 09:45:32.972033 140481689634688 learning.py:507] global step 4772: loss = 0.1585 (0.360 sec/step)\n",
            "I0719 09:45:33.267786 140481689634688 learning.py:507] global step 4773: loss = 0.0772 (0.294 sec/step)\n",
            "I0719 09:45:33.599443 140481689634688 learning.py:507] global step 4774: loss = 0.0220 (0.330 sec/step)\n",
            "I0719 09:45:33.921696 140481689634688 learning.py:507] global step 4775: loss = 0.0123 (0.321 sec/step)\n",
            "I0719 09:45:34.266587 140481689634688 learning.py:507] global step 4776: loss = 0.1357 (0.343 sec/step)\n",
            "I0719 09:45:34.551852 140481689634688 learning.py:507] global step 4777: loss = 0.1158 (0.284 sec/step)\n",
            "I0719 09:45:34.882559 140481689634688 learning.py:507] global step 4778: loss = 0.0933 (0.329 sec/step)\n",
            "I0719 09:45:35.215501 140481689634688 learning.py:507] global step 4779: loss = 0.0190 (0.331 sec/step)\n",
            "I0719 09:45:35.572330 140481689634688 learning.py:507] global step 4780: loss = 0.0278 (0.355 sec/step)\n",
            "I0719 09:45:35.907216 140481689634688 learning.py:507] global step 4781: loss = 0.2541 (0.333 sec/step)\n",
            "I0719 09:45:36.235424 140481689634688 learning.py:507] global step 4782: loss = 0.1133 (0.325 sec/step)\n",
            "I0719 09:45:36.571629 140481689634688 learning.py:507] global step 4783: loss = 0.0743 (0.335 sec/step)\n",
            "I0719 09:45:36.891553 140481689634688 learning.py:507] global step 4784: loss = 0.0957 (0.318 sec/step)\n",
            "I0719 09:45:37.229946 140481689634688 learning.py:507] global step 4785: loss = 0.0400 (0.336 sec/step)\n",
            "I0719 09:45:37.562635 140481689634688 learning.py:507] global step 4786: loss = 0.0711 (0.331 sec/step)\n",
            "I0719 09:45:37.910821 140481689634688 learning.py:507] global step 4787: loss = 0.0678 (0.346 sec/step)\n",
            "I0719 09:45:38.245488 140481689634688 learning.py:507] global step 4788: loss = 0.0470 (0.333 sec/step)\n",
            "I0719 09:45:38.558333 140481689634688 learning.py:507] global step 4789: loss = 0.0575 (0.311 sec/step)\n",
            "I0719 09:45:38.897164 140481689634688 learning.py:507] global step 4790: loss = 0.0376 (0.337 sec/step)\n",
            "I0719 09:45:39.233556 140481689634688 learning.py:507] global step 4791: loss = 0.0898 (0.334 sec/step)\n",
            "I0719 09:45:39.524221 140481689634688 learning.py:507] global step 4792: loss = 0.0425 (0.289 sec/step)\n",
            "I0719 09:45:39.833333 140481689634688 learning.py:507] global step 4793: loss = 0.1058 (0.307 sec/step)\n",
            "I0719 09:45:40.193843 140481689634688 learning.py:507] global step 4794: loss = 0.0296 (0.359 sec/step)\n",
            "I0719 09:45:40.524697 140481689634688 learning.py:507] global step 4795: loss = 0.0755 (0.329 sec/step)\n",
            "I0719 09:45:40.843472 140481689634688 learning.py:507] global step 4796: loss = 0.0374 (0.317 sec/step)\n",
            "I0719 09:45:41.157914 140481689634688 learning.py:507] global step 4797: loss = 0.0175 (0.313 sec/step)\n",
            "I0719 09:45:41.480959 140481689634688 learning.py:507] global step 4798: loss = 0.0522 (0.321 sec/step)\n",
            "I0719 09:45:41.817474 140481689634688 learning.py:507] global step 4799: loss = 0.0671 (0.335 sec/step)\n",
            "I0719 09:45:42.169532 140481689634688 learning.py:507] global step 4800: loss = 0.1868 (0.350 sec/step)\n",
            "I0719 09:45:42.512663 140481689634688 learning.py:507] global step 4801: loss = 0.0891 (0.341 sec/step)\n",
            "I0719 09:45:42.814105 140481689634688 learning.py:507] global step 4802: loss = 0.0264 (0.300 sec/step)\n",
            "I0719 09:45:43.156404 140481689634688 learning.py:507] global step 4803: loss = 0.1028 (0.340 sec/step)\n",
            "I0719 09:45:43.487626 140481689634688 learning.py:507] global step 4804: loss = 0.0686 (0.329 sec/step)\n",
            "I0719 09:45:43.812349 140481689634688 learning.py:507] global step 4805: loss = 0.0496 (0.323 sec/step)\n",
            "I0719 09:45:44.169114 140481689634688 learning.py:507] global step 4806: loss = 0.0665 (0.355 sec/step)\n",
            "I0719 09:45:44.511224 140481689634688 learning.py:507] global step 4807: loss = 0.1001 (0.340 sec/step)\n",
            "I0719 09:45:44.854581 140481689634688 learning.py:507] global step 4808: loss = 0.0365 (0.342 sec/step)\n",
            "I0719 09:45:45.196691 140481689634688 learning.py:507] global step 4809: loss = 0.0304 (0.340 sec/step)\n",
            "I0719 09:45:45.534293 140481689634688 learning.py:507] global step 4810: loss = 0.1620 (0.336 sec/step)\n",
            "I0719 09:45:45.885496 140481689634688 learning.py:507] global step 4811: loss = 0.0626 (0.350 sec/step)\n",
            "I0719 09:45:46.222479 140481689634688 learning.py:507] global step 4812: loss = 0.0706 (0.335 sec/step)\n",
            "I0719 09:45:46.516036 140481689634688 learning.py:507] global step 4813: loss = 0.0293 (0.292 sec/step)\n",
            "I0719 09:45:46.845375 140481689634688 learning.py:507] global step 4814: loss = 0.0239 (0.328 sec/step)\n",
            "I0719 09:45:47.169113 140481689634688 learning.py:507] global step 4815: loss = 0.0720 (0.322 sec/step)\n",
            "I0719 09:45:47.510917 140481689634688 learning.py:507] global step 4816: loss = 0.0738 (0.340 sec/step)\n",
            "I0719 09:45:47.850958 140481689634688 learning.py:507] global step 4817: loss = 0.0929 (0.338 sec/step)\n",
            "I0719 09:45:48.177989 140481689634688 learning.py:507] global step 4818: loss = 0.0859 (0.325 sec/step)\n",
            "I0719 09:45:48.516692 140481689634688 learning.py:507] global step 4819: loss = 0.0310 (0.337 sec/step)\n",
            "I0719 09:45:48.844262 140481689634688 learning.py:507] global step 4820: loss = 0.0162 (0.326 sec/step)\n",
            "I0719 09:45:49.175215 140481689634688 learning.py:507] global step 4821: loss = 0.0131 (0.329 sec/step)\n",
            "I0719 09:45:49.483958 140481689634688 learning.py:507] global step 4822: loss = 0.0529 (0.307 sec/step)\n",
            "I0719 09:45:49.824568 140481689634688 learning.py:507] global step 4823: loss = 0.0293 (0.339 sec/step)\n",
            "I0719 09:45:50.156594 140481689634688 learning.py:507] global step 4824: loss = 0.0722 (0.330 sec/step)\n",
            "I0719 09:45:50.494890 140481689634688 learning.py:507] global step 4825: loss = 0.0592 (0.337 sec/step)\n",
            "I0719 09:45:50.830177 140481689634688 learning.py:507] global step 4826: loss = 0.0566 (0.334 sec/step)\n",
            "I0719 09:45:51.170542 140481689634688 learning.py:507] global step 4827: loss = 0.0845 (0.339 sec/step)\n",
            "I0719 09:45:51.471045 140481689634688 learning.py:507] global step 4828: loss = 0.0513 (0.299 sec/step)\n",
            "I0719 09:45:51.823667 140481689634688 learning.py:507] global step 4829: loss = 0.0671 (0.351 sec/step)\n",
            "I0719 09:45:52.168310 140481689634688 learning.py:507] global step 4830: loss = 0.0789 (0.343 sec/step)\n",
            "I0719 09:45:52.507340 140481689634688 learning.py:507] global step 4831: loss = 0.0892 (0.337 sec/step)\n",
            "I0719 09:45:52.842251 140481689634688 learning.py:507] global step 4832: loss = 0.0455 (0.333 sec/step)\n",
            "I0719 09:45:53.230096 140481689634688 learning.py:507] global step 4833: loss = 0.0342 (0.386 sec/step)\n",
            "I0719 09:45:53.575009 140481689634688 learning.py:507] global step 4834: loss = 0.0791 (0.343 sec/step)\n",
            "I0719 09:45:53.903514 140481689634688 learning.py:507] global step 4835: loss = 0.0285 (0.327 sec/step)\n",
            "I0719 09:45:54.268919 140481689634688 learning.py:507] global step 4836: loss = 0.0614 (0.363 sec/step)\n",
            "I0719 09:45:54.587710 140481689634688 learning.py:507] global step 4837: loss = 0.0579 (0.317 sec/step)\n",
            "I0719 09:45:54.946932 140481689634688 learning.py:507] global step 4838: loss = 0.0551 (0.357 sec/step)\n",
            "I0719 09:45:55.299958 140481689634688 learning.py:507] global step 4839: loss = 0.2011 (0.351 sec/step)\n",
            "I0719 09:45:55.621166 140481689634688 learning.py:507] global step 4840: loss = 0.1003 (0.319 sec/step)\n",
            "I0719 09:45:55.917675 140481689634688 learning.py:507] global step 4841: loss = 0.1875 (0.295 sec/step)\n",
            "I0719 09:45:56.226553 140481689634688 learning.py:507] global step 4842: loss = 0.0394 (0.307 sec/step)\n",
            "I0719 09:45:56.571348 140481689634688 learning.py:507] global step 4843: loss = 0.0258 (0.343 sec/step)\n",
            "I0719 09:45:56.909794 140481689634688 learning.py:507] global step 4844: loss = 0.0420 (0.337 sec/step)\n",
            "I0719 09:45:57.243528 140481689634688 learning.py:507] global step 4845: loss = 0.0505 (0.332 sec/step)\n",
            "I0719 09:45:57.597195 140481689634688 learning.py:507] global step 4846: loss = 0.0389 (0.352 sec/step)\n",
            "I0719 09:45:57.923541 140481689634688 learning.py:507] global step 4847: loss = 0.0809 (0.325 sec/step)\n",
            "I0719 09:45:58.269644 140481689634688 learning.py:507] global step 4848: loss = 0.0752 (0.344 sec/step)\n",
            "I0719 09:45:58.664511 140481689634688 learning.py:507] global step 4849: loss = 0.0677 (0.393 sec/step)\n",
            "I0719 09:45:58.968647 140481689634688 learning.py:507] global step 4850: loss = 0.0702 (0.302 sec/step)\n",
            "I0719 09:45:59.307007 140481689634688 learning.py:507] global step 4851: loss = 0.1181 (0.337 sec/step)\n",
            "I0719 09:45:59.645063 140481689634688 learning.py:507] global step 4852: loss = 0.0562 (0.336 sec/step)\n",
            "I0719 09:45:59.989780 140481689634688 learning.py:507] global step 4853: loss = 0.0659 (0.343 sec/step)\n",
            "I0719 09:46:00.294343 140481689634688 learning.py:507] global step 4854: loss = 0.0568 (0.303 sec/step)\n",
            "I0719 09:46:00.647802 140481689634688 learning.py:507] global step 4855: loss = 0.0625 (0.352 sec/step)\n",
            "I0719 09:46:00.979965 140481689634688 learning.py:507] global step 4856: loss = 0.0498 (0.330 sec/step)\n",
            "I0719 09:46:01.327856 140481689634688 learning.py:507] global step 4857: loss = 0.0655 (0.346 sec/step)\n",
            "I0719 09:46:01.660447 140481689634688 learning.py:507] global step 4858: loss = 0.0370 (0.331 sec/step)\n",
            "I0719 09:46:02.008064 140481689634688 learning.py:507] global step 4859: loss = 0.1531 (0.346 sec/step)\n",
            "I0719 09:46:02.340996 140481689634688 learning.py:507] global step 4860: loss = 0.1781 (0.331 sec/step)\n",
            "I0719 09:46:02.683412 140481689634688 learning.py:507] global step 4861: loss = 0.1074 (0.341 sec/step)\n",
            "I0719 09:46:02.957669 140481689634688 learning.py:507] global step 4862: loss = 0.0568 (0.272 sec/step)\n",
            "I0719 09:46:03.300996 140481689634688 learning.py:507] global step 4863: loss = 0.0624 (0.342 sec/step)\n",
            "I0719 09:46:03.627633 140481689634688 learning.py:507] global step 4864: loss = 0.0707 (0.325 sec/step)\n",
            "I0719 09:46:03.960105 140481689634688 learning.py:507] global step 4865: loss = 0.0563 (0.331 sec/step)\n",
            "I0719 09:46:04.299616 140481689634688 learning.py:507] global step 4866: loss = 0.0383 (0.338 sec/step)\n",
            "I0719 09:46:04.634721 140481689634688 learning.py:507] global step 4867: loss = 0.0645 (0.333 sec/step)\n",
            "I0719 09:46:04.969989 140481689634688 learning.py:507] global step 4868: loss = 0.0693 (0.334 sec/step)\n",
            "I0719 09:46:05.307096 140481689634688 learning.py:507] global step 4869: loss = 0.0619 (0.335 sec/step)\n",
            "I0719 09:46:05.666956 140481689634688 learning.py:507] global step 4870: loss = 0.0648 (0.358 sec/step)\n",
            "I0719 09:46:06.038002 140481689634688 learning.py:507] global step 4871: loss = 0.0354 (0.369 sec/step)\n",
            "I0719 09:46:06.324925 140481689634688 learning.py:507] global step 4872: loss = 0.0377 (0.285 sec/step)\n",
            "I0719 09:46:06.651901 140481689634688 learning.py:507] global step 4873: loss = 0.0960 (0.325 sec/step)\n",
            "I0719 09:46:06.983068 140481689634688 learning.py:507] global step 4874: loss = 0.0685 (0.329 sec/step)\n",
            "I0719 09:46:07.320885 140481689634688 learning.py:507] global step 4875: loss = 0.0774 (0.336 sec/step)\n",
            "I0719 09:46:07.668092 140481689634688 learning.py:507] global step 4876: loss = 0.0453 (0.345 sec/step)\n",
            "I0719 09:46:08.016502 140481689634688 learning.py:507] global step 4877: loss = 0.0576 (0.347 sec/step)\n",
            "I0719 09:46:08.363906 140481689634688 learning.py:507] global step 4878: loss = 0.0964 (0.346 sec/step)\n",
            "I0719 09:46:08.647847 140481689634688 learning.py:507] global step 4879: loss = 0.0344 (0.282 sec/step)\n",
            "I0719 09:46:08.980132 140481689634688 learning.py:507] global step 4880: loss = 0.0976 (0.331 sec/step)\n",
            "I0719 09:46:09.311208 140481689634688 learning.py:507] global step 4881: loss = 0.0434 (0.329 sec/step)\n",
            "I0719 09:46:09.657665 140481689634688 learning.py:507] global step 4882: loss = 0.0406 (0.345 sec/step)\n",
            "I0719 09:46:09.992590 140481689634688 learning.py:507] global step 4883: loss = 0.0896 (0.333 sec/step)\n",
            "I0719 09:46:10.350152 140481689634688 learning.py:507] global step 4884: loss = 0.1066 (0.356 sec/step)\n",
            "I0719 09:46:10.686206 140481689634688 learning.py:507] global step 4885: loss = 0.0277 (0.334 sec/step)\n",
            "I0719 09:46:11.021493 140481689634688 learning.py:507] global step 4886: loss = 0.0917 (0.334 sec/step)\n",
            "I0719 09:46:11.374425 140481689634688 learning.py:507] global step 4887: loss = 0.0337 (0.351 sec/step)\n",
            "I0719 09:46:11.684499 140481689634688 learning.py:507] global step 4888: loss = 0.1470 (0.305 sec/step)\n",
            "I0719 09:46:12.046461 140481689634688 learning.py:507] global step 4889: loss = 0.0467 (0.360 sec/step)\n",
            "I0719 09:46:12.404924 140481689634688 learning.py:507] global step 4890: loss = 0.0703 (0.357 sec/step)\n",
            "I0719 09:46:12.724833 140481689634688 learning.py:507] global step 4891: loss = 0.0478 (0.317 sec/step)\n",
            "I0719 09:46:13.013678 140481689634688 learning.py:507] global step 4892: loss = 0.0470 (0.287 sec/step)\n",
            "I0719 09:46:13.331135 140481689634688 learning.py:507] global step 4893: loss = 0.0428 (0.316 sec/step)\n",
            "I0719 09:46:13.661062 140481689634688 learning.py:507] global step 4894: loss = 0.0650 (0.328 sec/step)\n",
            "I0719 09:46:13.986940 140481689634688 learning.py:507] global step 4895: loss = 0.0965 (0.324 sec/step)\n",
            "I0719 09:46:14.339982 140481689634688 learning.py:507] global step 4896: loss = 0.0479 (0.351 sec/step)\n",
            "I0719 09:46:14.637849 140481689634688 learning.py:507] global step 4897: loss = 0.0862 (0.296 sec/step)\n",
            "I0719 09:46:14.963680 140481689634688 learning.py:507] global step 4898: loss = 0.1019 (0.324 sec/step)\n",
            "I0719 09:46:15.298030 140481689634688 learning.py:507] global step 4899: loss = 0.0781 (0.332 sec/step)\n",
            "I0719 09:46:15.608385 140481689634688 learning.py:507] global step 4900: loss = 0.0486 (0.309 sec/step)\n",
            "I0719 09:46:15.925810 140481689634688 learning.py:507] global step 4901: loss = 0.1402 (0.316 sec/step)\n",
            "I0719 09:46:16.269758 140481689634688 learning.py:507] global step 4902: loss = 0.0614 (0.342 sec/step)\n",
            "I0719 09:46:16.626154 140481689634688 learning.py:507] global step 4903: loss = 0.0942 (0.354 sec/step)\n",
            "I0719 09:46:16.978151 140481689634688 learning.py:507] global step 4904: loss = 0.0316 (0.350 sec/step)\n",
            "I0719 09:46:17.292136 140481689634688 learning.py:507] global step 4905: loss = 0.0538 (0.312 sec/step)\n",
            "I0719 09:46:17.649325 140481689634688 learning.py:507] global step 4906: loss = 0.0855 (0.355 sec/step)\n",
            "I0719 09:46:17.988296 140481689634688 learning.py:507] global step 4907: loss = 0.1240 (0.337 sec/step)\n",
            "I0719 09:46:18.302509 140481689634688 learning.py:507] global step 4908: loss = 0.1341 (0.312 sec/step)\n",
            "I0719 09:46:18.587470 140481689634688 learning.py:507] global step 4909: loss = 0.0338 (0.283 sec/step)\n",
            "I0719 09:46:18.917036 140481689634688 learning.py:507] global step 4910: loss = 0.0118 (0.328 sec/step)\n",
            "I0719 09:46:19.236310 140481689634688 learning.py:507] global step 4911: loss = 0.0534 (0.317 sec/step)\n",
            "I0719 09:46:19.572477 140481689634688 learning.py:507] global step 4912: loss = 0.0884 (0.334 sec/step)\n",
            "I0719 09:46:19.884839 140481689634688 learning.py:507] global step 4913: loss = 0.1683 (0.311 sec/step)\n",
            "I0719 09:46:20.230713 140481689634688 learning.py:507] global step 4914: loss = 0.2689 (0.344 sec/step)\n",
            "I0719 09:46:20.585480 140481689634688 learning.py:507] global step 4915: loss = 0.0688 (0.353 sec/step)\n",
            "I0719 09:46:20.915689 140481689634688 learning.py:507] global step 4916: loss = 0.0429 (0.329 sec/step)\n",
            "I0719 09:46:21.225094 140481689634688 learning.py:507] global step 4917: loss = 0.0493 (0.308 sec/step)\n",
            "I0719 09:46:21.540436 140481689634688 learning.py:507] global step 4918: loss = 0.0600 (0.314 sec/step)\n",
            "I0719 09:46:21.859670 140481689634688 learning.py:507] global step 4919: loss = 0.0674 (0.317 sec/step)\n",
            "I0719 09:46:22.214652 140481689634688 learning.py:507] global step 4920: loss = 0.0772 (0.353 sec/step)\n",
            "I0719 09:46:22.553817 140481689634688 learning.py:507] global step 4921: loss = 0.1723 (0.337 sec/step)\n",
            "I0719 09:46:22.842050 140481689634688 learning.py:507] global step 4922: loss = 0.0258 (0.286 sec/step)\n",
            "I0719 09:46:23.185675 140481689634688 learning.py:507] global step 4923: loss = 0.0558 (0.342 sec/step)\n",
            "I0719 09:46:23.519304 140481689634688 learning.py:507] global step 4924: loss = 0.0453 (0.332 sec/step)\n",
            "I0719 09:46:23.863407 140481689634688 learning.py:507] global step 4925: loss = 0.0820 (0.342 sec/step)\n",
            "I0719 09:46:24.206099 140481689634688 learning.py:507] global step 4926: loss = 0.1697 (0.341 sec/step)\n",
            "I0719 09:46:24.545076 140481689634688 learning.py:507] global step 4927: loss = 0.0363 (0.337 sec/step)\n",
            "I0719 09:46:24.905892 140481689634688 learning.py:507] global step 4928: loss = 0.0477 (0.359 sec/step)\n",
            "I0719 09:46:25.247658 140481689634688 learning.py:507] global step 4929: loss = 0.0217 (0.340 sec/step)\n",
            "I0719 09:46:25.591767 140481689634688 learning.py:507] global step 4930: loss = 0.1039 (0.342 sec/step)\n",
            "I0719 09:46:25.906747 140479034988288 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0719 09:46:26.310086 140481689634688 learning.py:507] global step 4931: loss = 0.0613 (0.592 sec/step)\n",
            "I0719 09:46:26.849564 140481689634688 learning.py:507] global step 4932: loss = 0.1225 (0.491 sec/step)\n",
            "I0719 09:46:27.444324 140481689634688 learning.py:507] global step 4933: loss = 0.1248 (0.547 sec/step)\n",
            "I0719 09:46:27.833031 140479018202880 supervisor.py:1050] Recording summary at step 4933.\n",
            "I0719 09:46:27.855422 140481689634688 learning.py:507] global step 4934: loss = 0.0570 (0.389 sec/step)\n",
            "I0719 09:46:28.383227 140481689634688 learning.py:507] global step 4935: loss = 0.0660 (0.505 sec/step)\n",
            "I0719 09:46:28.913085 140481689634688 learning.py:507] global step 4936: loss = 0.0353 (0.390 sec/step)\n",
            "I0719 09:46:29.207613 140481689634688 learning.py:507] global step 4937: loss = 0.0798 (0.293 sec/step)\n",
            "I0719 09:46:29.505858 140481689634688 learning.py:507] global step 4938: loss = 0.0322 (0.297 sec/step)\n",
            "I0719 09:46:29.848132 140481689634688 learning.py:507] global step 4939: loss = 0.0795 (0.340 sec/step)\n",
            "I0719 09:46:30.186491 140481689634688 learning.py:507] global step 4940: loss = 0.1163 (0.336 sec/step)\n",
            "I0719 09:46:30.544902 140481689634688 learning.py:507] global step 4941: loss = 0.0900 (0.357 sec/step)\n",
            "I0719 09:46:30.907052 140481689634688 learning.py:507] global step 4942: loss = 0.1402 (0.360 sec/step)\n",
            "I0719 09:46:31.233355 140481689634688 learning.py:507] global step 4943: loss = 0.1324 (0.324 sec/step)\n",
            "I0719 09:46:31.561027 140481689634688 learning.py:507] global step 4944: loss = 0.0485 (0.326 sec/step)\n",
            "I0719 09:46:31.874903 140481689634688 learning.py:507] global step 4945: loss = 0.0707 (0.312 sec/step)\n",
            "I0719 09:46:32.213718 140481689634688 learning.py:507] global step 4946: loss = 0.1524 (0.337 sec/step)\n",
            "I0719 09:46:32.592016 140481689634688 learning.py:507] global step 4947: loss = 0.1144 (0.377 sec/step)\n",
            "I0719 09:46:32.917695 140481689634688 learning.py:507] global step 4948: loss = 0.0603 (0.324 sec/step)\n",
            "I0719 09:46:33.266611 140481689634688 learning.py:507] global step 4949: loss = 0.0726 (0.347 sec/step)\n",
            "I0719 09:46:33.616691 140481689634688 learning.py:507] global step 4950: loss = 0.0464 (0.348 sec/step)\n",
            "I0719 09:46:33.949614 140481689634688 learning.py:507] global step 4951: loss = 0.0585 (0.331 sec/step)\n",
            "I0719 09:46:34.302872 140481689634688 learning.py:507] global step 4952: loss = 0.0628 (0.351 sec/step)\n",
            "I0719 09:46:34.616510 140481689634688 learning.py:507] global step 4953: loss = 0.0465 (0.312 sec/step)\n",
            "I0719 09:46:34.949214 140481689634688 learning.py:507] global step 4954: loss = 0.1012 (0.331 sec/step)\n",
            "I0719 09:46:35.314736 140481689634688 learning.py:507] global step 4955: loss = 0.0433 (0.364 sec/step)\n",
            "I0719 09:46:35.660406 140481689634688 learning.py:507] global step 4956: loss = 0.0966 (0.344 sec/step)\n",
            "I0719 09:46:36.004225 140481689634688 learning.py:507] global step 4957: loss = 0.1054 (0.342 sec/step)\n",
            "I0719 09:46:36.280827 140481689634688 learning.py:507] global step 4958: loss = 0.1205 (0.275 sec/step)\n",
            "I0719 09:46:36.623119 140481689634688 learning.py:507] global step 4959: loss = 0.1213 (0.340 sec/step)\n",
            "I0719 09:46:36.950683 140481689634688 learning.py:507] global step 4960: loss = 0.0570 (0.326 sec/step)\n",
            "I0719 09:46:37.289187 140481689634688 learning.py:507] global step 4961: loss = 0.1094 (0.337 sec/step)\n",
            "I0719 09:46:37.640762 140481689634688 learning.py:507] global step 4962: loss = 0.0827 (0.350 sec/step)\n",
            "I0719 09:46:37.972046 140481689634688 learning.py:507] global step 4963: loss = 0.0685 (0.329 sec/step)\n",
            "I0719 09:46:38.314146 140481689634688 learning.py:507] global step 4964: loss = 0.0692 (0.340 sec/step)\n",
            "I0719 09:46:38.649840 140481689634688 learning.py:507] global step 4965: loss = 0.0138 (0.333 sec/step)\n",
            "I0719 09:46:38.997443 140481689634688 learning.py:507] global step 4966: loss = 0.0199 (0.346 sec/step)\n",
            "I0719 09:46:39.309065 140481689634688 learning.py:507] global step 4967: loss = 0.1763 (0.309 sec/step)\n",
            "I0719 09:46:39.663430 140481689634688 learning.py:507] global step 4968: loss = 0.0514 (0.353 sec/step)\n",
            "I0719 09:46:39.977772 140481689634688 learning.py:507] global step 4969: loss = 0.0579 (0.313 sec/step)\n",
            "I0719 09:46:40.318973 140481689634688 learning.py:507] global step 4970: loss = 0.0725 (0.339 sec/step)\n",
            "I0719 09:46:40.622436 140481689634688 learning.py:507] global step 4971: loss = 0.1286 (0.301 sec/step)\n",
            "I0719 09:46:40.929086 140481689634688 learning.py:507] global step 4972: loss = 0.0329 (0.304 sec/step)\n",
            "I0719 09:46:41.276685 140481689634688 learning.py:507] global step 4973: loss = 0.0748 (0.346 sec/step)\n",
            "I0719 09:46:41.624071 140481689634688 learning.py:507] global step 4974: loss = 0.0946 (0.346 sec/step)\n",
            "I0719 09:46:41.950870 140481689634688 learning.py:507] global step 4975: loss = 0.0725 (0.325 sec/step)\n",
            "I0719 09:46:42.279255 140481689634688 learning.py:507] global step 4976: loss = 0.1103 (0.327 sec/step)\n",
            "I0719 09:46:42.636989 140481689634688 learning.py:507] global step 4977: loss = 0.0581 (0.355 sec/step)\n",
            "I0719 09:46:42.929332 140481689634688 learning.py:507] global step 4978: loss = 0.1017 (0.290 sec/step)\n",
            "I0719 09:46:43.227981 140481689634688 learning.py:507] global step 4979: loss = 0.1224 (0.297 sec/step)\n",
            "I0719 09:46:43.554764 140481689634688 learning.py:507] global step 4980: loss = 0.0681 (0.325 sec/step)\n",
            "I0719 09:46:43.882642 140481689634688 learning.py:507] global step 4981: loss = 0.0495 (0.326 sec/step)\n",
            "I0719 09:46:44.235784 140481689634688 learning.py:507] global step 4982: loss = 0.0967 (0.351 sec/step)\n",
            "I0719 09:46:44.574904 140481689634688 learning.py:507] global step 4983: loss = 0.0686 (0.337 sec/step)\n",
            "I0719 09:46:44.945142 140481689634688 learning.py:507] global step 4984: loss = 0.0287 (0.368 sec/step)\n",
            "I0719 09:46:45.264682 140481689634688 learning.py:507] global step 4985: loss = 0.0812 (0.318 sec/step)\n",
            "I0719 09:46:45.607317 140481689634688 learning.py:507] global step 4986: loss = 0.0592 (0.341 sec/step)\n",
            "I0719 09:46:45.979535 140481689634688 learning.py:507] global step 4987: loss = 0.0888 (0.370 sec/step)\n",
            "I0719 09:46:46.301214 140481689634688 learning.py:507] global step 4988: loss = 0.0703 (0.320 sec/step)\n",
            "I0719 09:46:46.652899 140481689634688 learning.py:507] global step 4989: loss = 0.0470 (0.350 sec/step)\n",
            "I0719 09:46:47.011567 140481689634688 learning.py:507] global step 4990: loss = 0.1378 (0.357 sec/step)\n",
            "I0719 09:46:47.343381 140481689634688 learning.py:507] global step 4991: loss = 0.0939 (0.330 sec/step)\n",
            "I0719 09:46:47.674721 140481689634688 learning.py:507] global step 4992: loss = 0.1353 (0.330 sec/step)\n",
            "I0719 09:46:48.025694 140481689634688 learning.py:507] global step 4993: loss = 0.0421 (0.349 sec/step)\n",
            "I0719 09:46:48.335919 140481689634688 learning.py:507] global step 4994: loss = 0.0403 (0.308 sec/step)\n",
            "I0719 09:46:48.658338 140481689634688 learning.py:507] global step 4995: loss = 0.0259 (0.321 sec/step)\n",
            "I0719 09:46:49.046890 140481689634688 learning.py:507] global step 4996: loss = 0.0843 (0.387 sec/step)\n",
            "I0719 09:46:49.358908 140481689634688 learning.py:507] global step 4997: loss = 0.0344 (0.310 sec/step)\n",
            "I0719 09:46:49.694862 140481689634688 learning.py:507] global step 4998: loss = 0.0326 (0.334 sec/step)\n",
            "I0719 09:46:50.001262 140481689634688 learning.py:507] global step 4999: loss = 0.0349 (0.305 sec/step)\n",
            "I0719 09:46:50.343058 140481689634688 learning.py:507] global step 5000: loss = 0.1183 (0.340 sec/step)\n",
            "I0719 09:46:50.682188 140481689634688 learning.py:507] global step 5001: loss = 0.0848 (0.337 sec/step)\n",
            "I0719 09:46:51.022988 140481689634688 learning.py:507] global step 5002: loss = 0.1008 (0.339 sec/step)\n",
            "I0719 09:46:51.319355 140481689634688 learning.py:507] global step 5003: loss = 0.1284 (0.295 sec/step)\n",
            "I0719 09:46:51.656377 140481689634688 learning.py:507] global step 5004: loss = 0.0210 (0.335 sec/step)\n",
            "I0719 09:46:52.006001 140481689634688 learning.py:507] global step 5005: loss = 0.0364 (0.348 sec/step)\n",
            "I0719 09:46:52.339109 140481689634688 learning.py:507] global step 5006: loss = 0.0896 (0.331 sec/step)\n",
            "I0719 09:46:52.666343 140481689634688 learning.py:507] global step 5007: loss = 0.0589 (0.325 sec/step)\n",
            "I0719 09:46:53.002695 140481689634688 learning.py:507] global step 5008: loss = 0.0620 (0.335 sec/step)\n",
            "I0719 09:46:53.300120 140481689634688 learning.py:507] global step 5009: loss = 0.0458 (0.295 sec/step)\n",
            "I0719 09:46:53.648006 140481689634688 learning.py:507] global step 5010: loss = 0.2654 (0.346 sec/step)\n",
            "I0719 09:46:53.980323 140481689634688 learning.py:507] global step 5011: loss = 0.1189 (0.330 sec/step)\n",
            "I0719 09:46:54.330599 140481689634688 learning.py:507] global step 5012: loss = 0.0666 (0.349 sec/step)\n",
            "I0719 09:46:54.666698 140481689634688 learning.py:507] global step 5013: loss = 0.0828 (0.334 sec/step)\n",
            "I0719 09:46:55.035047 140481689634688 learning.py:507] global step 5014: loss = 0.2000 (0.366 sec/step)\n",
            "I0719 09:46:55.366710 140481689634688 learning.py:507] global step 5015: loss = 0.0699 (0.330 sec/step)\n",
            "I0719 09:46:55.700665 140481689634688 learning.py:507] global step 5016: loss = 0.0741 (0.332 sec/step)\n",
            "I0719 09:46:56.021516 140481689634688 learning.py:507] global step 5017: loss = 0.0650 (0.319 sec/step)\n",
            "I0719 09:46:56.366093 140481689634688 learning.py:507] global step 5018: loss = 0.0677 (0.342 sec/step)\n",
            "I0719 09:46:56.685119 140481689634688 learning.py:507] global step 5019: loss = 0.0637 (0.317 sec/step)\n",
            "I0719 09:46:57.011419 140481689634688 learning.py:507] global step 5020: loss = 0.0669 (0.324 sec/step)\n",
            "I0719 09:46:57.381887 140481689634688 learning.py:507] global step 5021: loss = 0.0779 (0.369 sec/step)\n",
            "I0719 09:46:57.665894 140481689634688 learning.py:507] global step 5022: loss = 0.1106 (0.282 sec/step)\n",
            "I0719 09:46:58.015939 140481689634688 learning.py:507] global step 5023: loss = 0.1155 (0.348 sec/step)\n",
            "I0719 09:46:58.306064 140481689634688 learning.py:507] global step 5024: loss = 0.0802 (0.288 sec/step)\n",
            "I0719 09:46:58.595552 140481689634688 learning.py:507] global step 5025: loss = 0.1595 (0.288 sec/step)\n",
            "I0719 09:46:58.920369 140481689634688 learning.py:507] global step 5026: loss = 0.0970 (0.323 sec/step)\n",
            "I0719 09:46:59.249061 140481689634688 learning.py:507] global step 5027: loss = 0.0910 (0.327 sec/step)\n",
            "I0719 09:46:59.604718 140481689634688 learning.py:507] global step 5028: loss = 0.0784 (0.354 sec/step)\n",
            "I0719 09:46:59.939692 140481689634688 learning.py:507] global step 5029: loss = 0.0350 (0.333 sec/step)\n",
            "I0719 09:47:00.280516 140481689634688 learning.py:507] global step 5030: loss = 0.1056 (0.339 sec/step)\n",
            "I0719 09:47:00.616092 140481689634688 learning.py:507] global step 5031: loss = 0.0404 (0.334 sec/step)\n",
            "I0719 09:47:00.960144 140481689634688 learning.py:507] global step 5032: loss = 0.0743 (0.343 sec/step)\n",
            "I0719 09:47:01.299968 140481689634688 learning.py:507] global step 5033: loss = 0.0871 (0.338 sec/step)\n",
            "I0719 09:47:01.629543 140481689634688 learning.py:507] global step 5034: loss = 0.0526 (0.328 sec/step)\n",
            "I0719 09:47:01.935807 140481689634688 learning.py:507] global step 5035: loss = 0.0547 (0.305 sec/step)\n",
            "I0719 09:47:02.297861 140481689634688 learning.py:507] global step 5036: loss = 0.0264 (0.360 sec/step)\n",
            "I0719 09:47:02.634715 140481689634688 learning.py:507] global step 5037: loss = 0.0992 (0.335 sec/step)\n",
            "I0719 09:47:02.924703 140481689634688 learning.py:507] global step 5038: loss = 0.0254 (0.288 sec/step)\n",
            "I0719 09:47:03.250515 140481689634688 learning.py:507] global step 5039: loss = 0.0751 (0.324 sec/step)\n",
            "I0719 09:47:03.572541 140481689634688 learning.py:507] global step 5040: loss = 0.0440 (0.320 sec/step)\n",
            "I0719 09:47:03.900145 140481689634688 learning.py:507] global step 5041: loss = 0.1029 (0.326 sec/step)\n",
            "I0719 09:47:04.258410 140481689634688 learning.py:507] global step 5042: loss = 0.1070 (0.356 sec/step)\n",
            "I0719 09:47:04.624431 140481689634688 learning.py:507] global step 5043: loss = 0.0473 (0.364 sec/step)\n",
            "I0719 09:47:04.984200 140481689634688 learning.py:507] global step 5044: loss = 0.0577 (0.358 sec/step)\n",
            "I0719 09:47:05.337725 140481689634688 learning.py:507] global step 5045: loss = 0.0956 (0.352 sec/step)\n",
            "I0719 09:47:05.667832 140481689634688 learning.py:507] global step 5046: loss = 0.0768 (0.328 sec/step)\n",
            "I0719 09:47:05.997312 140481689634688 learning.py:507] global step 5047: loss = 0.0661 (0.327 sec/step)\n",
            "I0719 09:47:06.333200 140481689634688 learning.py:507] global step 5048: loss = 0.0665 (0.334 sec/step)\n",
            "I0719 09:47:06.660031 140481689634688 learning.py:507] global step 5049: loss = 0.0356 (0.325 sec/step)\n",
            "I0719 09:47:07.010822 140481689634688 learning.py:507] global step 5050: loss = 0.0725 (0.349 sec/step)\n",
            "I0719 09:47:07.359343 140481689634688 learning.py:507] global step 5051: loss = 0.0314 (0.347 sec/step)\n",
            "I0719 09:47:07.695215 140481689634688 learning.py:507] global step 5052: loss = 0.0642 (0.334 sec/step)\n",
            "I0719 09:47:08.035695 140481689634688 learning.py:507] global step 5053: loss = 0.0479 (0.339 sec/step)\n",
            "I0719 09:47:08.363090 140481689634688 learning.py:507] global step 5054: loss = 0.0485 (0.326 sec/step)\n",
            "I0719 09:47:08.691748 140481689634688 learning.py:507] global step 5055: loss = 0.1003 (0.327 sec/step)\n",
            "I0719 09:47:09.037323 140481689634688 learning.py:507] global step 5056: loss = 0.0661 (0.344 sec/step)\n",
            "I0719 09:47:09.368348 140481689634688 learning.py:507] global step 5057: loss = 0.0487 (0.329 sec/step)\n",
            "I0719 09:47:09.679460 140481689634688 learning.py:507] global step 5058: loss = 0.0563 (0.309 sec/step)\n",
            "I0719 09:47:09.991465 140481689634688 learning.py:507] global step 5059: loss = 0.0270 (0.310 sec/step)\n",
            "I0719 09:47:10.332652 140481689634688 learning.py:507] global step 5060: loss = 0.1356 (0.340 sec/step)\n",
            "I0719 09:47:10.670653 140481689634688 learning.py:507] global step 5061: loss = 0.0985 (0.336 sec/step)\n",
            "I0719 09:47:11.033692 140481689634688 learning.py:507] global step 5062: loss = 0.1067 (0.361 sec/step)\n",
            "I0719 09:47:11.344910 140481689634688 learning.py:507] global step 5063: loss = 0.0390 (0.309 sec/step)\n",
            "I0719 09:47:11.677020 140481689634688 learning.py:507] global step 5064: loss = 0.1337 (0.330 sec/step)\n",
            "I0719 09:47:12.020409 140481689634688 learning.py:507] global step 5065: loss = 0.1107 (0.342 sec/step)\n",
            "I0719 09:47:12.347791 140481689634688 learning.py:507] global step 5066: loss = 0.1363 (0.326 sec/step)\n",
            "I0719 09:47:12.685250 140481689634688 learning.py:507] global step 5067: loss = 0.0989 (0.336 sec/step)\n",
            "I0719 09:47:13.029244 140481689634688 learning.py:507] global step 5068: loss = 0.0474 (0.342 sec/step)\n",
            "I0719 09:47:13.359845 140481689634688 learning.py:507] global step 5069: loss = 0.0188 (0.329 sec/step)\n",
            "I0719 09:47:13.709210 140481689634688 learning.py:507] global step 5070: loss = 0.0678 (0.348 sec/step)\n",
            "I0719 09:47:14.039146 140481689634688 learning.py:507] global step 5071: loss = 0.0399 (0.328 sec/step)\n",
            "I0719 09:47:14.406527 140481689634688 learning.py:507] global step 5072: loss = 0.1448 (0.365 sec/step)\n",
            "I0719 09:47:14.730938 140481689634688 learning.py:507] global step 5073: loss = 0.0249 (0.323 sec/step)\n",
            "I0719 09:47:15.059009 140481689634688 learning.py:507] global step 5074: loss = 0.1212 (0.326 sec/step)\n",
            "I0719 09:47:15.380195 140481689634688 learning.py:507] global step 5075: loss = 0.0451 (0.319 sec/step)\n",
            "I0719 09:47:15.708262 140481689634688 learning.py:507] global step 5076: loss = 0.1202 (0.326 sec/step)\n",
            "I0719 09:47:16.071621 140481689634688 learning.py:507] global step 5077: loss = 0.1868 (0.361 sec/step)\n",
            "I0719 09:47:16.415352 140481689634688 learning.py:507] global step 5078: loss = 0.0517 (0.342 sec/step)\n",
            "I0719 09:47:16.762257 140481689634688 learning.py:507] global step 5079: loss = 0.0379 (0.345 sec/step)\n",
            "I0719 09:47:17.124757 140481689634688 learning.py:507] global step 5080: loss = 0.0436 (0.361 sec/step)\n",
            "I0719 09:47:17.461615 140481689634688 learning.py:507] global step 5081: loss = 0.0980 (0.335 sec/step)\n",
            "I0719 09:47:17.774632 140481689634688 learning.py:507] global step 5082: loss = 0.0400 (0.311 sec/step)\n",
            "I0719 09:47:18.111726 140481689634688 learning.py:507] global step 5083: loss = 0.1270 (0.335 sec/step)\n",
            "I0719 09:47:18.406766 140481689634688 learning.py:507] global step 5084: loss = 0.1231 (0.293 sec/step)\n",
            "I0719 09:47:18.728149 140481689634688 learning.py:507] global step 5085: loss = 0.0433 (0.320 sec/step)\n",
            "I0719 09:47:19.080051 140481689634688 learning.py:507] global step 5086: loss = 0.0475 (0.350 sec/step)\n",
            "I0719 09:47:19.394160 140481689634688 learning.py:507] global step 5087: loss = 0.1209 (0.312 sec/step)\n",
            "I0719 09:47:19.737432 140481689634688 learning.py:507] global step 5088: loss = 0.0290 (0.341 sec/step)\n",
            "I0719 09:47:20.013137 140481689634688 learning.py:507] global step 5089: loss = 0.0673 (0.274 sec/step)\n",
            "I0719 09:47:20.346406 140481689634688 learning.py:507] global step 5090: loss = 0.0282 (0.331 sec/step)\n",
            "I0719 09:47:20.690618 140481689634688 learning.py:507] global step 5091: loss = 0.0853 (0.342 sec/step)\n",
            "I0719 09:47:21.026376 140481689634688 learning.py:507] global step 5092: loss = 0.0594 (0.334 sec/step)\n",
            "I0719 09:47:21.371749 140481689634688 learning.py:507] global step 5093: loss = 0.0304 (0.343 sec/step)\n",
            "I0719 09:47:21.720355 140481689634688 learning.py:507] global step 5094: loss = 0.0372 (0.346 sec/step)\n",
            "I0719 09:47:22.062040 140481689634688 learning.py:507] global step 5095: loss = 0.0354 (0.339 sec/step)\n",
            "I0719 09:47:22.415622 140481689634688 learning.py:507] global step 5096: loss = 0.0608 (0.347 sec/step)\n",
            "I0719 09:47:22.756669 140481689634688 learning.py:507] global step 5097: loss = 0.0802 (0.339 sec/step)\n",
            "I0719 09:47:23.067163 140481689634688 learning.py:507] global step 5098: loss = 0.0410 (0.309 sec/step)\n",
            "I0719 09:47:23.407976 140481689634688 learning.py:507] global step 5099: loss = 0.0313 (0.339 sec/step)\n",
            "I0719 09:47:23.756452 140481689634688 learning.py:507] global step 5100: loss = 0.1201 (0.346 sec/step)\n",
            "I0719 09:47:24.153642 140481689634688 learning.py:507] global step 5101: loss = 0.0403 (0.395 sec/step)\n",
            "I0719 09:47:24.506501 140481689634688 learning.py:507] global step 5102: loss = 0.0815 (0.351 sec/step)\n",
            "I0719 09:47:24.821998 140481689634688 learning.py:507] global step 5103: loss = 0.0609 (0.314 sec/step)\n",
            "I0719 09:47:25.122609 140481689634688 learning.py:507] global step 5104: loss = 0.1134 (0.298 sec/step)\n",
            "I0719 09:47:25.474734 140481689634688 learning.py:507] global step 5105: loss = 0.0785 (0.350 sec/step)\n",
            "I0719 09:47:25.835510 140481689634688 learning.py:507] global step 5106: loss = 0.1255 (0.359 sec/step)\n",
            "I0719 09:47:26.134487 140481689634688 learning.py:507] global step 5107: loss = 0.1021 (0.297 sec/step)\n",
            "I0719 09:47:26.499501 140481689634688 learning.py:507] global step 5108: loss = 0.0327 (0.363 sec/step)\n",
            "I0719 09:47:26.821740 140481689634688 learning.py:507] global step 5109: loss = 0.0470 (0.320 sec/step)\n",
            "I0719 09:47:27.129085 140481689634688 learning.py:507] global step 5110: loss = 0.0692 (0.306 sec/step)\n",
            "I0719 09:47:27.492223 140481689634688 learning.py:507] global step 5111: loss = 0.0416 (0.361 sec/step)\n",
            "I0719 09:47:27.857066 140481689634688 learning.py:507] global step 5112: loss = 0.1718 (0.363 sec/step)\n",
            "I0719 09:47:28.191347 140481689634688 learning.py:507] global step 5113: loss = 0.0368 (0.333 sec/step)\n",
            "I0719 09:47:28.518826 140481689634688 learning.py:507] global step 5114: loss = 0.0722 (0.326 sec/step)\n",
            "I0719 09:47:28.809197 140481689634688 learning.py:507] global step 5115: loss = 0.0445 (0.289 sec/step)\n",
            "I0719 09:47:29.149492 140481689634688 learning.py:507] global step 5116: loss = 0.0545 (0.339 sec/step)\n",
            "I0719 09:47:29.482884 140481689634688 learning.py:507] global step 5117: loss = 0.1112 (0.332 sec/step)\n",
            "I0719 09:47:29.805349 140481689634688 learning.py:507] global step 5118: loss = 0.0854 (0.321 sec/step)\n",
            "I0719 09:47:30.147830 140481689634688 learning.py:507] global step 5119: loss = 0.1010 (0.341 sec/step)\n",
            "I0719 09:47:30.463888 140481689634688 learning.py:507] global step 5120: loss = 0.1408 (0.314 sec/step)\n",
            "I0719 09:47:30.808723 140481689634688 learning.py:507] global step 5121: loss = 0.0387 (0.343 sec/step)\n",
            "I0719 09:47:31.155529 140481689634688 learning.py:507] global step 5122: loss = 0.0088 (0.345 sec/step)\n",
            "I0719 09:47:31.517200 140481689634688 learning.py:507] global step 5123: loss = 0.0276 (0.360 sec/step)\n",
            "I0719 09:47:31.873419 140481689634688 learning.py:507] global step 5124: loss = 0.0549 (0.354 sec/step)\n",
            "I0719 09:47:32.205927 140481689634688 learning.py:507] global step 5125: loss = 0.1074 (0.331 sec/step)\n",
            "I0719 09:47:32.547169 140481689634688 learning.py:507] global step 5126: loss = 0.1415 (0.339 sec/step)\n",
            "I0719 09:47:32.885565 140481689634688 learning.py:507] global step 5127: loss = 0.0990 (0.337 sec/step)\n",
            "I0719 09:47:33.225610 140481689634688 learning.py:507] global step 5128: loss = 0.0177 (0.338 sec/step)\n",
            "I0719 09:47:33.569726 140481689634688 learning.py:507] global step 5129: loss = 0.1713 (0.343 sec/step)\n",
            "I0719 09:47:33.884400 140481689634688 learning.py:507] global step 5130: loss = 0.0351 (0.313 sec/step)\n",
            "I0719 09:47:34.236898 140481689634688 learning.py:507] global step 5131: loss = 0.0741 (0.351 sec/step)\n",
            "I0719 09:47:34.544375 140481689634688 learning.py:507] global step 5132: loss = 0.0329 (0.305 sec/step)\n",
            "I0719 09:47:34.882131 140481689634688 learning.py:507] global step 5133: loss = 0.0957 (0.336 sec/step)\n",
            "I0719 09:47:35.221454 140481689634688 learning.py:507] global step 5134: loss = 0.0682 (0.337 sec/step)\n",
            "I0719 09:47:35.538669 140481689634688 learning.py:507] global step 5135: loss = 0.0587 (0.315 sec/step)\n",
            "I0719 09:47:35.878240 140481689634688 learning.py:507] global step 5136: loss = 0.1905 (0.338 sec/step)\n",
            "I0719 09:47:36.204951 140481689634688 learning.py:507] global step 5137: loss = 0.0172 (0.324 sec/step)\n",
            "I0719 09:47:36.487041 140481689634688 learning.py:507] global step 5138: loss = 0.0485 (0.280 sec/step)\n",
            "I0719 09:47:36.841388 140481689634688 learning.py:507] global step 5139: loss = 0.0519 (0.353 sec/step)\n",
            "I0719 09:47:37.176260 140481689634688 learning.py:507] global step 5140: loss = 0.0463 (0.333 sec/step)\n",
            "I0719 09:47:37.488253 140481689634688 learning.py:507] global step 5141: loss = 0.1604 (0.310 sec/step)\n",
            "I0719 09:47:37.821825 140481689634688 learning.py:507] global step 5142: loss = 0.0748 (0.332 sec/step)\n",
            "I0719 09:47:38.108626 140481689634688 learning.py:507] global step 5143: loss = 0.0566 (0.285 sec/step)\n",
            "I0719 09:47:38.439566 140481689634688 learning.py:507] global step 5144: loss = 0.0939 (0.329 sec/step)\n",
            "I0719 09:47:38.801183 140481689634688 learning.py:507] global step 5145: loss = 0.0705 (0.360 sec/step)\n",
            "I0719 09:47:39.165216 140481689634688 learning.py:507] global step 5146: loss = 0.0864 (0.362 sec/step)\n",
            "I0719 09:47:39.508887 140481689634688 learning.py:507] global step 5147: loss = 0.2342 (0.342 sec/step)\n",
            "I0719 09:47:39.850112 140481689634688 learning.py:507] global step 5148: loss = 0.0803 (0.339 sec/step)\n",
            "I0719 09:47:40.179724 140481689634688 learning.py:507] global step 5149: loss = 0.0654 (0.328 sec/step)\n",
            "I0719 09:47:40.516983 140481689634688 learning.py:507] global step 5150: loss = 0.1262 (0.335 sec/step)\n",
            "I0719 09:47:40.859580 140481689634688 learning.py:507] global step 5151: loss = 0.0485 (0.341 sec/step)\n",
            "I0719 09:47:41.203125 140481689634688 learning.py:507] global step 5152: loss = 0.1278 (0.342 sec/step)\n",
            "I0719 09:47:41.570163 140481689634688 learning.py:507] global step 5153: loss = 0.0671 (0.365 sec/step)\n",
            "I0719 09:47:41.943531 140481689634688 learning.py:507] global step 5154: loss = 0.1057 (0.372 sec/step)\n",
            "I0719 09:47:42.262660 140481689634688 learning.py:507] global step 5155: loss = 0.0357 (0.318 sec/step)\n",
            "I0719 09:47:42.611844 140481689634688 learning.py:507] global step 5156: loss = 0.0510 (0.347 sec/step)\n",
            "I0719 09:47:42.929168 140481689634688 learning.py:507] global step 5157: loss = 0.0185 (0.316 sec/step)\n",
            "I0719 09:47:43.268506 140481689634688 learning.py:507] global step 5158: loss = 0.0140 (0.337 sec/step)\n",
            "I0719 09:47:43.593173 140481689634688 learning.py:507] global step 5159: loss = 0.0903 (0.323 sec/step)\n",
            "I0719 09:47:43.938780 140481689634688 learning.py:507] global step 5160: loss = 0.0171 (0.344 sec/step)\n",
            "I0719 09:47:44.267567 140481689634688 learning.py:507] global step 5161: loss = 0.0760 (0.327 sec/step)\n",
            "I0719 09:47:44.608861 140481689634688 learning.py:507] global step 5162: loss = 0.0886 (0.339 sec/step)\n",
            "I0719 09:47:44.899422 140481689634688 learning.py:507] global step 5163: loss = 0.0692 (0.289 sec/step)\n",
            "I0719 09:47:45.237789 140481689634688 learning.py:507] global step 5164: loss = 0.1260 (0.336 sec/step)\n",
            "I0719 09:47:45.580185 140481689634688 learning.py:507] global step 5165: loss = 0.0894 (0.341 sec/step)\n",
            "I0719 09:47:45.898536 140481689634688 learning.py:507] global step 5166: loss = 0.1822 (0.316 sec/step)\n",
            "I0719 09:47:46.243529 140481689634688 learning.py:507] global step 5167: loss = 0.0230 (0.343 sec/step)\n",
            "I0719 09:47:46.573804 140481689634688 learning.py:507] global step 5168: loss = 0.1125 (0.327 sec/step)\n",
            "I0719 09:47:46.906464 140481689634688 learning.py:507] global step 5169: loss = 0.0631 (0.331 sec/step)\n",
            "I0719 09:47:47.229730 140481689634688 learning.py:507] global step 5170: loss = 0.0603 (0.322 sec/step)\n",
            "I0719 09:47:47.581456 140481689634688 learning.py:507] global step 5171: loss = 0.0786 (0.350 sec/step)\n",
            "I0719 09:47:47.930759 140481689634688 learning.py:507] global step 5172: loss = 0.0876 (0.347 sec/step)\n",
            "I0719 09:47:48.267619 140481689634688 learning.py:507] global step 5173: loss = 0.0597 (0.335 sec/step)\n",
            "I0719 09:47:48.607985 140481689634688 learning.py:507] global step 5174: loss = 0.0141 (0.339 sec/step)\n",
            "I0719 09:47:48.909542 140481689634688 learning.py:507] global step 5175: loss = 0.0579 (0.299 sec/step)\n",
            "I0719 09:47:49.256971 140481689634688 learning.py:507] global step 5176: loss = 0.0579 (0.346 sec/step)\n",
            "I0719 09:47:49.593582 140481689634688 learning.py:507] global step 5177: loss = 0.0649 (0.334 sec/step)\n",
            "I0719 09:47:49.986319 140481689634688 learning.py:507] global step 5178: loss = 0.0553 (0.391 sec/step)\n",
            "I0719 09:47:50.345451 140481689634688 learning.py:507] global step 5179: loss = 0.0818 (0.357 sec/step)\n",
            "I0719 09:47:50.681321 140481689634688 learning.py:507] global step 5180: loss = 0.0090 (0.334 sec/step)\n",
            "I0719 09:47:51.031336 140481689634688 learning.py:507] global step 5181: loss = 0.1114 (0.348 sec/step)\n",
            "I0719 09:47:51.365016 140481689634688 learning.py:507] global step 5182: loss = 0.0460 (0.332 sec/step)\n",
            "I0719 09:47:51.715150 140481689634688 learning.py:507] global step 5183: loss = 0.0166 (0.348 sec/step)\n",
            "I0719 09:47:52.037743 140481689634688 learning.py:507] global step 5184: loss = 0.0690 (0.320 sec/step)\n",
            "I0719 09:47:52.377991 140481689634688 learning.py:507] global step 5185: loss = 0.0428 (0.338 sec/step)\n",
            "I0719 09:47:52.676774 140481689634688 learning.py:507] global step 5186: loss = 0.0370 (0.297 sec/step)\n",
            "I0719 09:47:53.013974 140481689634688 learning.py:507] global step 5187: loss = 0.0145 (0.334 sec/step)\n",
            "I0719 09:47:53.366876 140481689634688 learning.py:507] global step 5188: loss = 0.0419 (0.350 sec/step)\n",
            "I0719 09:47:53.676081 140481689634688 learning.py:507] global step 5189: loss = 0.1295 (0.306 sec/step)\n",
            "I0719 09:47:53.998184 140481689634688 learning.py:507] global step 5190: loss = 0.0374 (0.320 sec/step)\n",
            "I0719 09:47:54.350001 140481689634688 learning.py:507] global step 5191: loss = 0.0366 (0.349 sec/step)\n",
            "I0719 09:47:54.665977 140481689634688 learning.py:507] global step 5192: loss = 0.0325 (0.314 sec/step)\n",
            "I0719 09:47:55.003296 140481689634688 learning.py:507] global step 5193: loss = 0.0704 (0.335 sec/step)\n",
            "I0719 09:47:55.324172 140481689634688 learning.py:507] global step 5194: loss = 0.0537 (0.319 sec/step)\n",
            "I0719 09:47:55.656498 140481689634688 learning.py:507] global step 5195: loss = 0.1050 (0.331 sec/step)\n",
            "I0719 09:47:55.995116 140481689634688 learning.py:507] global step 5196: loss = 0.1215 (0.336 sec/step)\n",
            "I0719 09:47:56.316664 140481689634688 learning.py:507] global step 5197: loss = 0.1068 (0.319 sec/step)\n",
            "I0719 09:47:56.650174 140481689634688 learning.py:507] global step 5198: loss = 0.0287 (0.331 sec/step)\n",
            "I0719 09:47:56.939505 140481689634688 learning.py:507] global step 5199: loss = 0.0881 (0.287 sec/step)\n",
            "I0719 09:47:57.293341 140481689634688 learning.py:507] global step 5200: loss = 0.1097 (0.352 sec/step)\n",
            "I0719 09:47:57.618201 140481689634688 learning.py:507] global step 5201: loss = 0.0144 (0.323 sec/step)\n",
            "I0719 09:47:57.930160 140481689634688 learning.py:507] global step 5202: loss = 0.1174 (0.310 sec/step)\n",
            "I0719 09:47:58.252998 140481689634688 learning.py:507] global step 5203: loss = 0.0351 (0.321 sec/step)\n",
            "I0719 09:47:58.572661 140481689634688 learning.py:507] global step 5204: loss = 0.0728 (0.318 sec/step)\n",
            "I0719 09:47:58.922400 140481689634688 learning.py:507] global step 5205: loss = 0.0670 (0.348 sec/step)\n",
            "I0719 09:47:59.266671 140481689634688 learning.py:507] global step 5206: loss = 0.0559 (0.342 sec/step)\n",
            "I0719 09:47:59.607699 140481689634688 learning.py:507] global step 5207: loss = 0.0922 (0.339 sec/step)\n",
            "I0719 09:47:59.949016 140481689634688 learning.py:507] global step 5208: loss = 0.0668 (0.338 sec/step)\n",
            "I0719 09:48:00.267959 140481689634688 learning.py:507] global step 5209: loss = 0.1364 (0.317 sec/step)\n",
            "I0719 09:48:00.625049 140481689634688 learning.py:507] global step 5210: loss = 0.0419 (0.355 sec/step)\n",
            "I0719 09:48:00.971349 140481689634688 learning.py:507] global step 5211: loss = 0.1224 (0.344 sec/step)\n",
            "I0719 09:48:01.296045 140481689634688 learning.py:507] global step 5212: loss = 0.1137 (0.323 sec/step)\n",
            "I0719 09:48:01.630495 140481689634688 learning.py:507] global step 5213: loss = 0.0958 (0.333 sec/step)\n",
            "I0719 09:48:01.958688 140481689634688 learning.py:507] global step 5214: loss = 0.1332 (0.326 sec/step)\n",
            "I0719 09:48:02.290064 140481689634688 learning.py:507] global step 5215: loss = 0.0303 (0.329 sec/step)\n",
            "I0719 09:48:02.652812 140481689634688 learning.py:507] global step 5216: loss = 0.0194 (0.361 sec/step)\n",
            "I0719 09:48:02.950606 140481689634688 learning.py:507] global step 5217: loss = 0.0369 (0.296 sec/step)\n",
            "I0719 09:48:03.288298 140481689634688 learning.py:507] global step 5218: loss = 0.0513 (0.336 sec/step)\n",
            "I0719 09:48:03.658417 140481689634688 learning.py:507] global step 5219: loss = 0.0428 (0.368 sec/step)\n",
            "I0719 09:48:03.993646 140481689634688 learning.py:507] global step 5220: loss = 0.0502 (0.333 sec/step)\n",
            "I0719 09:48:04.345026 140481689634688 learning.py:507] global step 5221: loss = 0.0974 (0.350 sec/step)\n",
            "I0719 09:48:04.665555 140481689634688 learning.py:507] global step 5222: loss = 0.0215 (0.319 sec/step)\n",
            "I0719 09:48:05.012701 140481689634688 learning.py:507] global step 5223: loss = 0.0898 (0.345 sec/step)\n",
            "I0719 09:48:05.345685 140481689634688 learning.py:507] global step 5224: loss = 0.0513 (0.331 sec/step)\n",
            "I0719 09:48:05.687212 140481689634688 learning.py:507] global step 5225: loss = 0.1146 (0.340 sec/step)\n",
            "I0719 09:48:05.993710 140481689634688 learning.py:507] global step 5226: loss = 0.0638 (0.305 sec/step)\n",
            "I0719 09:48:06.301384 140481689634688 learning.py:507] global step 5227: loss = 0.0464 (0.306 sec/step)\n",
            "I0719 09:48:06.639600 140481689634688 learning.py:507] global step 5228: loss = 0.0436 (0.337 sec/step)\n",
            "I0719 09:48:06.978307 140481689634688 learning.py:507] global step 5229: loss = 0.0389 (0.337 sec/step)\n",
            "I0719 09:48:07.339294 140481689634688 learning.py:507] global step 5230: loss = 0.0523 (0.359 sec/step)\n",
            "I0719 09:48:07.695127 140481689634688 learning.py:507] global step 5231: loss = 0.0615 (0.354 sec/step)\n",
            "I0719 09:48:07.979850 140481689634688 learning.py:507] global step 5232: loss = 0.1257 (0.283 sec/step)\n",
            "I0719 09:48:08.301661 140481689634688 learning.py:507] global step 5233: loss = 0.0978 (0.320 sec/step)\n",
            "I0719 09:48:08.648042 140481689634688 learning.py:507] global step 5234: loss = 0.0411 (0.345 sec/step)\n",
            "I0719 09:48:08.995752 140481689634688 learning.py:507] global step 5235: loss = 0.1282 (0.346 sec/step)\n",
            "I0719 09:48:09.353118 140481689634688 learning.py:507] global step 5236: loss = 0.0710 (0.356 sec/step)\n",
            "I0719 09:48:09.696654 140481689634688 learning.py:507] global step 5237: loss = 0.1415 (0.342 sec/step)\n",
            "I0719 09:48:10.029547 140481689634688 learning.py:507] global step 5238: loss = 0.0764 (0.331 sec/step)\n",
            "I0719 09:48:10.365181 140481689634688 learning.py:507] global step 5239: loss = 0.0204 (0.334 sec/step)\n",
            "I0719 09:48:10.686347 140481689634688 learning.py:507] global step 5240: loss = 0.1500 (0.319 sec/step)\n",
            "I0719 09:48:11.028489 140481689634688 learning.py:507] global step 5241: loss = 0.0853 (0.340 sec/step)\n",
            "I0719 09:48:11.332536 140481689634688 learning.py:507] global step 5242: loss = 0.0759 (0.299 sec/step)\n",
            "I0719 09:48:11.658233 140481689634688 learning.py:507] global step 5243: loss = 0.0948 (0.323 sec/step)\n",
            "I0719 09:48:11.986987 140481689634688 learning.py:507] global step 5244: loss = 0.0561 (0.327 sec/step)\n",
            "I0719 09:48:12.324331 140481689634688 learning.py:507] global step 5245: loss = 0.0555 (0.336 sec/step)\n",
            "I0719 09:48:12.638231 140481689634688 learning.py:507] global step 5246: loss = 0.0802 (0.312 sec/step)\n",
            "I0719 09:48:13.006769 140481689634688 learning.py:507] global step 5247: loss = 0.1776 (0.367 sec/step)\n",
            "I0719 09:48:13.348126 140481689634688 learning.py:507] global step 5248: loss = 0.0160 (0.340 sec/step)\n",
            "I0719 09:48:13.631610 140481689634688 learning.py:507] global step 5249: loss = 0.0587 (0.282 sec/step)\n",
            "I0719 09:48:13.946637 140481689634688 learning.py:507] global step 5250: loss = 0.0376 (0.313 sec/step)\n",
            "I0719 09:48:14.284729 140481689634688 learning.py:507] global step 5251: loss = 0.1854 (0.336 sec/step)\n",
            "I0719 09:48:14.625208 140481689634688 learning.py:507] global step 5252: loss = 0.0475 (0.339 sec/step)\n",
            "I0719 09:48:14.974946 140481689634688 learning.py:507] global step 5253: loss = 0.0522 (0.348 sec/step)\n",
            "I0719 09:48:15.320501 140481689634688 learning.py:507] global step 5254: loss = 0.0629 (0.344 sec/step)\n",
            "I0719 09:48:15.660771 140481689634688 learning.py:507] global step 5255: loss = 0.0946 (0.339 sec/step)\n",
            "I0719 09:48:15.992696 140481689634688 learning.py:507] global step 5256: loss = 0.0914 (0.330 sec/step)\n",
            "I0719 09:48:16.312767 140481689634688 learning.py:507] global step 5257: loss = 0.0225 (0.318 sec/step)\n",
            "I0719 09:48:16.636321 140481689634688 learning.py:507] global step 5258: loss = 0.1603 (0.322 sec/step)\n",
            "I0719 09:48:16.990668 140481689634688 learning.py:507] global step 5259: loss = 0.0487 (0.353 sec/step)\n",
            "I0719 09:48:17.318491 140481689634688 learning.py:507] global step 5260: loss = 0.0553 (0.326 sec/step)\n",
            "I0719 09:48:17.664626 140481689634688 learning.py:507] global step 5261: loss = 0.0417 (0.344 sec/step)\n",
            "I0719 09:48:17.988968 140481689634688 learning.py:507] global step 5262: loss = 0.0583 (0.322 sec/step)\n",
            "I0719 09:48:18.328641 140481689634688 learning.py:507] global step 5263: loss = 0.1372 (0.338 sec/step)\n",
            "I0719 09:48:18.691692 140481689634688 learning.py:507] global step 5264: loss = 0.0886 (0.361 sec/step)\n",
            "I0719 09:48:19.040778 140481689634688 learning.py:507] global step 5265: loss = 0.0396 (0.347 sec/step)\n",
            "I0719 09:48:19.394332 140481689634688 learning.py:507] global step 5266: loss = 0.0560 (0.352 sec/step)\n",
            "I0719 09:48:19.679313 140481689634688 learning.py:507] global step 5267: loss = 0.0702 (0.283 sec/step)\n",
            "I0719 09:48:20.011694 140481689634688 learning.py:507] global step 5268: loss = 0.0519 (0.331 sec/step)\n",
            "I0719 09:48:20.360503 140481689634688 learning.py:507] global step 5269: loss = 0.0432 (0.347 sec/step)\n",
            "I0719 09:48:20.711516 140481689634688 learning.py:507] global step 5270: loss = 0.0592 (0.349 sec/step)\n",
            "I0719 09:48:21.008709 140481689634688 learning.py:507] global step 5271: loss = 0.1136 (0.295 sec/step)\n",
            "I0719 09:48:21.355099 140481689634688 learning.py:507] global step 5272: loss = 0.0653 (0.345 sec/step)\n",
            "I0719 09:48:21.693345 140481689634688 learning.py:507] global step 5273: loss = 0.0781 (0.336 sec/step)\n",
            "I0719 09:48:22.027596 140481689634688 learning.py:507] global step 5274: loss = 0.0613 (0.333 sec/step)\n",
            "I0719 09:48:22.368775 140481689634688 learning.py:507] global step 5275: loss = 0.0701 (0.339 sec/step)\n",
            "I0719 09:48:22.739608 140481689634688 learning.py:507] global step 5276: loss = 0.0480 (0.369 sec/step)\n",
            "I0719 09:48:23.072489 140481689634688 learning.py:507] global step 5277: loss = 0.0858 (0.331 sec/step)\n",
            "I0719 09:48:23.428897 140481689634688 learning.py:507] global step 5278: loss = 0.0799 (0.355 sec/step)\n",
            "I0719 09:48:23.767490 140481689634688 learning.py:507] global step 5279: loss = 0.0649 (0.337 sec/step)\n",
            "I0719 09:48:24.100657 140481689634688 learning.py:507] global step 5280: loss = 0.0495 (0.331 sec/step)\n",
            "I0719 09:48:24.441189 140481689634688 learning.py:507] global step 5281: loss = 0.0656 (0.339 sec/step)\n",
            "I0719 09:48:24.830847 140481689634688 learning.py:507] global step 5282: loss = 0.0863 (0.388 sec/step)\n",
            "I0719 09:48:25.097429 140481689634688 learning.py:507] global step 5283: loss = 0.1185 (0.265 sec/step)\n",
            "I0719 09:48:25.442160 140481689634688 learning.py:507] global step 5284: loss = 0.0563 (0.343 sec/step)\n",
            "I0719 09:48:25.797060 140481689634688 learning.py:507] global step 5285: loss = 0.1147 (0.353 sec/step)\n",
            "I0719 09:48:26.223476 140481689634688 learning.py:507] global step 5286: loss = 0.0470 (0.410 sec/step)\n",
            "I0719 09:48:26.786303 140481689634688 learning.py:507] global step 5287: loss = 0.0686 (0.554 sec/step)\n",
            "I0719 09:48:27.212155 140481689634688 learning.py:507] global step 5288: loss = 0.0559 (0.418 sec/step)\n",
            "I0719 09:48:27.266244 140479018202880 supervisor.py:1050] Recording summary at step 5288.\n",
            "I0719 09:48:27.574607 140481689634688 learning.py:507] global step 5289: loss = 0.1206 (0.360 sec/step)\n",
            "I0719 09:48:27.892777 140481689634688 learning.py:507] global step 5290: loss = 0.0619 (0.316 sec/step)\n",
            "I0719 09:48:28.243244 140481689634688 learning.py:507] global step 5291: loss = 0.1282 (0.348 sec/step)\n",
            "I0719 09:48:28.575031 140481689634688 learning.py:507] global step 5292: loss = 0.0495 (0.330 sec/step)\n",
            "I0719 09:48:28.879358 140481689634688 learning.py:507] global step 5293: loss = 0.1138 (0.302 sec/step)\n",
            "I0719 09:48:29.210340 140481689634688 learning.py:507] global step 5294: loss = 0.0648 (0.329 sec/step)\n",
            "I0719 09:48:29.546554 140481689634688 learning.py:507] global step 5295: loss = 0.1571 (0.334 sec/step)\n",
            "I0719 09:48:29.906909 140481689634688 learning.py:507] global step 5296: loss = 0.0548 (0.359 sec/step)\n",
            "I0719 09:48:30.193390 140481689634688 learning.py:507] global step 5297: loss = 0.0594 (0.284 sec/step)\n",
            "I0719 09:48:30.490474 140481689634688 learning.py:507] global step 5298: loss = 0.1103 (0.295 sec/step)\n",
            "I0719 09:48:30.838623 140481689634688 learning.py:507] global step 5299: loss = 0.0815 (0.346 sec/step)\n",
            "I0719 09:48:31.214808 140481689634688 learning.py:507] global step 5300: loss = 0.0417 (0.374 sec/step)\n",
            "I0719 09:48:31.559498 140481689634688 learning.py:507] global step 5301: loss = 0.0554 (0.343 sec/step)\n",
            "I0719 09:48:31.911187 140481689634688 learning.py:507] global step 5302: loss = 0.0498 (0.350 sec/step)\n",
            "I0719 09:48:32.272305 140481689634688 learning.py:507] global step 5303: loss = 0.0281 (0.359 sec/step)\n",
            "I0719 09:48:32.589838 140481689634688 learning.py:507] global step 5304: loss = 0.0607 (0.316 sec/step)\n",
            "I0719 09:48:32.915291 140481689634688 learning.py:507] global step 5305: loss = 0.0717 (0.324 sec/step)\n",
            "I0719 09:48:33.217777 140481689634688 learning.py:507] global step 5306: loss = 0.1615 (0.301 sec/step)\n",
            "I0719 09:48:33.542741 140481689634688 learning.py:507] global step 5307: loss = 0.0761 (0.323 sec/step)\n",
            "I0719 09:48:33.863216 140481689634688 learning.py:507] global step 5308: loss = 0.0844 (0.319 sec/step)\n",
            "I0719 09:48:34.210012 140481689634688 learning.py:507] global step 5309: loss = 0.0263 (0.345 sec/step)\n",
            "I0719 09:48:34.540820 140481689634688 learning.py:507] global step 5310: loss = 0.1611 (0.329 sec/step)\n",
            "I0719 09:48:34.893468 140481689634688 learning.py:507] global step 5311: loss = 0.0472 (0.351 sec/step)\n",
            "I0719 09:48:35.232899 140481689634688 learning.py:507] global step 5312: loss = 0.0416 (0.338 sec/step)\n",
            "I0719 09:48:35.565719 140481689634688 learning.py:507] global step 5313: loss = 0.0249 (0.331 sec/step)\n",
            "I0719 09:48:35.910382 140481689634688 learning.py:507] global step 5314: loss = 0.1038 (0.343 sec/step)\n",
            "I0719 09:48:36.247080 140481689634688 learning.py:507] global step 5315: loss = 0.0451 (0.335 sec/step)\n",
            "I0719 09:48:36.553394 140481689634688 learning.py:507] global step 5316: loss = 0.1443 (0.305 sec/step)\n",
            "I0719 09:48:36.868170 140481689634688 learning.py:507] global step 5317: loss = 0.0139 (0.313 sec/step)\n",
            "I0719 09:48:37.229525 140481689634688 learning.py:507] global step 5318: loss = 0.0806 (0.359 sec/step)\n",
            "I0719 09:48:37.518344 140481689634688 learning.py:507] global step 5319: loss = 0.0644 (0.287 sec/step)\n",
            "I0719 09:48:37.862632 140481689634688 learning.py:507] global step 5320: loss = 0.0633 (0.343 sec/step)\n",
            "I0719 09:48:38.161669 140481689634688 learning.py:507] global step 5321: loss = 0.1316 (0.297 sec/step)\n",
            "I0719 09:48:38.496872 140481689634688 learning.py:507] global step 5322: loss = 0.0108 (0.333 sec/step)\n",
            "I0719 09:48:38.832621 140481689634688 learning.py:507] global step 5323: loss = 0.0920 (0.334 sec/step)\n",
            "I0719 09:48:39.174512 140481689634688 learning.py:507] global step 5324: loss = 0.0277 (0.340 sec/step)\n",
            "I0719 09:48:39.524724 140481689634688 learning.py:507] global step 5325: loss = 0.0511 (0.349 sec/step)\n",
            "I0719 09:48:39.862627 140481689634688 learning.py:507] global step 5326: loss = 0.1148 (0.336 sec/step)\n",
            "I0719 09:48:40.196877 140481689634688 learning.py:507] global step 5327: loss = 0.0925 (0.332 sec/step)\n",
            "I0719 09:48:40.549009 140481689634688 learning.py:507] global step 5328: loss = 0.0861 (0.350 sec/step)\n",
            "I0719 09:48:40.837218 140481689634688 learning.py:507] global step 5329: loss = 0.0613 (0.286 sec/step)\n",
            "I0719 09:48:41.163192 140481689634688 learning.py:507] global step 5330: loss = 0.0566 (0.324 sec/step)\n",
            "I0719 09:48:41.518742 140481689634688 learning.py:507] global step 5331: loss = 0.0939 (0.354 sec/step)\n",
            "I0719 09:48:41.812869 140481689634688 learning.py:507] global step 5332: loss = 0.0765 (0.292 sec/step)\n",
            "I0719 09:48:42.113506 140481689634688 learning.py:507] global step 5333: loss = 0.1206 (0.299 sec/step)\n",
            "I0719 09:48:42.485645 140481689634688 learning.py:507] global step 5334: loss = 0.1648 (0.370 sec/step)\n",
            "I0719 09:48:42.824792 140481689634688 learning.py:507] global step 5335: loss = 0.0840 (0.337 sec/step)\n",
            "I0719 09:48:43.090829 140481689634688 learning.py:507] global step 5336: loss = 0.1050 (0.264 sec/step)\n",
            "I0719 09:48:43.414951 140481689634688 learning.py:507] global step 5337: loss = 0.0214 (0.322 sec/step)\n",
            "I0719 09:48:43.703306 140481689634688 learning.py:507] global step 5338: loss = 0.0301 (0.287 sec/step)\n",
            "I0719 09:48:44.023260 140481689634688 learning.py:507] global step 5339: loss = 0.1207 (0.318 sec/step)\n",
            "I0719 09:48:44.363206 140481689634688 learning.py:507] global step 5340: loss = 0.1586 (0.338 sec/step)\n",
            "I0719 09:48:44.757759 140481689634688 learning.py:507] global step 5341: loss = 0.1516 (0.393 sec/step)\n",
            "I0719 09:48:45.083745 140481689634688 learning.py:507] global step 5342: loss = 0.0333 (0.324 sec/step)\n",
            "I0719 09:48:45.398248 140481689634688 learning.py:507] global step 5343: loss = 0.0708 (0.313 sec/step)\n",
            "I0719 09:48:45.718482 140481689634688 learning.py:507] global step 5344: loss = 0.0744 (0.318 sec/step)\n",
            "I0719 09:48:46.038736 140481689634688 learning.py:507] global step 5345: loss = 0.0618 (0.318 sec/step)\n",
            "I0719 09:48:46.395874 140481689634688 learning.py:507] global step 5346: loss = 0.0857 (0.355 sec/step)\n",
            "I0719 09:48:46.765260 140481689634688 learning.py:507] global step 5347: loss = 0.0981 (0.368 sec/step)\n",
            "I0719 09:48:47.073864 140481689634688 learning.py:507] global step 5348: loss = 0.0699 (0.307 sec/step)\n",
            "I0719 09:48:47.427792 140481689634688 learning.py:507] global step 5349: loss = 0.0815 (0.352 sec/step)\n",
            "I0719 09:48:47.722297 140481689634688 learning.py:507] global step 5350: loss = 0.1082 (0.293 sec/step)\n",
            "I0719 09:48:48.054636 140481689634688 learning.py:507] global step 5351: loss = 0.0608 (0.331 sec/step)\n",
            "I0719 09:48:48.390189 140481689634688 learning.py:507] global step 5352: loss = 0.1274 (0.334 sec/step)\n",
            "I0719 09:48:48.725455 140481689634688 learning.py:507] global step 5353: loss = 0.0393 (0.334 sec/step)\n",
            "I0719 09:48:49.021589 140481689634688 learning.py:507] global step 5354: loss = 0.1035 (0.295 sec/step)\n",
            "I0719 09:48:49.345815 140481689634688 learning.py:507] global step 5355: loss = 0.0628 (0.322 sec/step)\n",
            "I0719 09:48:49.687527 140481689634688 learning.py:507] global step 5356: loss = 0.0897 (0.340 sec/step)\n",
            "I0719 09:48:50.018117 140481689634688 learning.py:507] global step 5357: loss = 0.0481 (0.329 sec/step)\n",
            "I0719 09:48:50.353348 140481689634688 learning.py:507] global step 5358: loss = 0.0290 (0.333 sec/step)\n",
            "I0719 09:48:50.706685 140481689634688 learning.py:507] global step 5359: loss = 0.0800 (0.352 sec/step)\n",
            "I0719 09:48:51.015249 140481689634688 learning.py:507] global step 5360: loss = 0.0560 (0.307 sec/step)\n",
            "I0719 09:48:51.343135 140481689634688 learning.py:507] global step 5361: loss = 0.0436 (0.326 sec/step)\n",
            "I0719 09:48:51.693872 140481689634688 learning.py:507] global step 5362: loss = 0.0743 (0.349 sec/step)\n",
            "I0719 09:48:52.022398 140481689634688 learning.py:507] global step 5363: loss = 0.0678 (0.326 sec/step)\n",
            "I0719 09:48:52.346809 140481689634688 learning.py:507] global step 5364: loss = 0.0979 (0.322 sec/step)\n",
            "I0719 09:48:52.672438 140481689634688 learning.py:507] global step 5365: loss = 0.0700 (0.324 sec/step)\n",
            "I0719 09:48:53.018441 140481689634688 learning.py:507] global step 5366: loss = 0.0989 (0.344 sec/step)\n",
            "I0719 09:48:53.338197 140481689634688 learning.py:507] global step 5367: loss = 0.0481 (0.318 sec/step)\n",
            "I0719 09:48:53.690312 140481689634688 learning.py:507] global step 5368: loss = 0.0814 (0.350 sec/step)\n",
            "I0719 09:48:54.022890 140481689634688 learning.py:507] global step 5369: loss = 0.0686 (0.331 sec/step)\n",
            "I0719 09:48:54.339067 140481689634688 learning.py:507] global step 5370: loss = 0.0955 (0.314 sec/step)\n",
            "I0719 09:48:54.673780 140481689634688 learning.py:507] global step 5371: loss = 0.0440 (0.333 sec/step)\n",
            "I0719 09:48:55.002243 140481689634688 learning.py:507] global step 5372: loss = 0.0480 (0.327 sec/step)\n",
            "I0719 09:48:55.326448 140481689634688 learning.py:507] global step 5373: loss = 0.0646 (0.323 sec/step)\n",
            "I0719 09:48:55.670599 140481689634688 learning.py:507] global step 5374: loss = 0.0517 (0.342 sec/step)\n",
            "I0719 09:48:55.998406 140481689634688 learning.py:507] global step 5375: loss = 0.0495 (0.326 sec/step)\n",
            "I0719 09:48:56.343003 140481689634688 learning.py:507] global step 5376: loss = 0.0572 (0.343 sec/step)\n",
            "I0719 09:48:56.704646 140481689634688 learning.py:507] global step 5377: loss = 0.0577 (0.360 sec/step)\n",
            "I0719 09:48:57.056210 140481689634688 learning.py:507] global step 5378: loss = 0.1282 (0.350 sec/step)\n",
            "I0719 09:48:57.380504 140481689634688 learning.py:507] global step 5379: loss = 0.0215 (0.323 sec/step)\n",
            "I0719 09:48:57.725225 140481689634688 learning.py:507] global step 5380: loss = 0.0844 (0.343 sec/step)\n",
            "I0719 09:48:58.065078 140481689634688 learning.py:507] global step 5381: loss = 0.0410 (0.338 sec/step)\n",
            "I0719 09:48:58.402685 140481689634688 learning.py:507] global step 5382: loss = 0.0448 (0.336 sec/step)\n",
            "I0719 09:48:58.728327 140481689634688 learning.py:507] global step 5383: loss = 0.0789 (0.324 sec/step)\n",
            "I0719 09:48:59.043797 140481689634688 learning.py:507] global step 5384: loss = 0.0749 (0.314 sec/step)\n",
            "I0719 09:48:59.333973 140481689634688 learning.py:507] global step 5385: loss = 0.0324 (0.288 sec/step)\n",
            "I0719 09:48:59.647644 140481689634688 learning.py:507] global step 5386: loss = 0.1285 (0.312 sec/step)\n",
            "I0719 09:48:59.924943 140481689634688 learning.py:507] global step 5387: loss = 0.0633 (0.275 sec/step)\n",
            "I0719 09:49:00.207146 140481689634688 learning.py:507] global step 5388: loss = 0.0908 (0.280 sec/step)\n",
            "I0719 09:49:00.522645 140481689634688 learning.py:507] global step 5389: loss = 0.1020 (0.314 sec/step)\n",
            "I0719 09:49:00.867390 140481689634688 learning.py:507] global step 5390: loss = 0.1192 (0.343 sec/step)\n",
            "I0719 09:49:01.196036 140481689634688 learning.py:507] global step 5391: loss = 0.0858 (0.327 sec/step)\n",
            "I0719 09:49:01.542203 140481689634688 learning.py:507] global step 5392: loss = 0.0563 (0.344 sec/step)\n",
            "I0719 09:49:01.899542 140481689634688 learning.py:507] global step 5393: loss = 0.0760 (0.356 sec/step)\n",
            "I0719 09:49:02.240047 140481689634688 learning.py:507] global step 5394: loss = 0.0746 (0.339 sec/step)\n",
            "I0719 09:49:02.597637 140481689634688 learning.py:507] global step 5395: loss = 0.0446 (0.356 sec/step)\n",
            "I0719 09:49:02.945809 140481689634688 learning.py:507] global step 5396: loss = 0.1384 (0.346 sec/step)\n",
            "I0719 09:49:03.278610 140481689634688 learning.py:507] global step 5397: loss = 0.0616 (0.331 sec/step)\n",
            "I0719 09:49:03.606736 140481689634688 learning.py:507] global step 5398: loss = 0.0403 (0.326 sec/step)\n",
            "I0719 09:49:03.957820 140481689634688 learning.py:507] global step 5399: loss = 0.0916 (0.349 sec/step)\n",
            "I0719 09:49:04.283422 140481689634688 learning.py:507] global step 5400: loss = 0.0957 (0.324 sec/step)\n",
            "I0719 09:49:04.623073 140481689634688 learning.py:507] global step 5401: loss = 0.0892 (0.338 sec/step)\n",
            "I0719 09:49:04.966436 140481689634688 learning.py:507] global step 5402: loss = 0.0527 (0.342 sec/step)\n",
            "I0719 09:49:05.303070 140481689634688 learning.py:507] global step 5403: loss = 0.0416 (0.335 sec/step)\n",
            "I0719 09:49:05.669153 140481689634688 learning.py:507] global step 5404: loss = 0.0544 (0.363 sec/step)\n",
            "I0719 09:49:06.017824 140481689634688 learning.py:507] global step 5405: loss = 0.1280 (0.347 sec/step)\n",
            "I0719 09:49:06.367740 140481689634688 learning.py:507] global step 5406: loss = 0.0772 (0.348 sec/step)\n",
            "I0719 09:49:06.694957 140481689634688 learning.py:507] global step 5407: loss = 0.0800 (0.325 sec/step)\n",
            "I0719 09:49:07.031665 140481689634688 learning.py:507] global step 5408: loss = 0.0775 (0.335 sec/step)\n",
            "I0719 09:49:07.353377 140481689634688 learning.py:507] global step 5409: loss = 0.0589 (0.320 sec/step)\n",
            "I0719 09:49:07.687760 140481689634688 learning.py:507] global step 5410: loss = 0.0180 (0.333 sec/step)\n",
            "I0719 09:49:08.042660 140481689634688 learning.py:507] global step 5411: loss = 0.1095 (0.353 sec/step)\n",
            "I0719 09:49:08.384601 140481689634688 learning.py:507] global step 5412: loss = 0.0515 (0.340 sec/step)\n",
            "I0719 09:49:08.736375 140481689634688 learning.py:507] global step 5413: loss = 0.0750 (0.350 sec/step)\n",
            "I0719 09:49:09.076401 140481689634688 learning.py:507] global step 5414: loss = 0.0610 (0.338 sec/step)\n",
            "I0719 09:49:09.432509 140481689634688 learning.py:507] global step 5415: loss = 0.0552 (0.354 sec/step)\n",
            "I0719 09:49:09.770407 140481689634688 learning.py:507] global step 5416: loss = 0.0232 (0.336 sec/step)\n",
            "I0719 09:49:10.110448 140481689634688 learning.py:507] global step 5417: loss = 0.0695 (0.338 sec/step)\n",
            "I0719 09:49:10.451293 140481689634688 learning.py:507] global step 5418: loss = 0.0741 (0.339 sec/step)\n",
            "I0719 09:49:10.783323 140481689634688 learning.py:507] global step 5419: loss = 0.0379 (0.329 sec/step)\n",
            "I0719 09:49:11.140917 140481689634688 learning.py:507] global step 5420: loss = 0.1156 (0.355 sec/step)\n",
            "I0719 09:49:11.476314 140481689634688 learning.py:507] global step 5421: loss = 0.0607 (0.333 sec/step)\n",
            "I0719 09:49:11.765383 140481689634688 learning.py:507] global step 5422: loss = 0.1093 (0.287 sec/step)\n",
            "I0719 09:49:12.115774 140481689634688 learning.py:507] global step 5423: loss = 0.1573 (0.347 sec/step)\n",
            "I0719 09:49:12.458430 140481689634688 learning.py:507] global step 5424: loss = 0.0239 (0.341 sec/step)\n",
            "I0719 09:49:12.804461 140481689634688 learning.py:507] global step 5425: loss = 0.0991 (0.344 sec/step)\n",
            "I0719 09:49:13.138118 140481689634688 learning.py:507] global step 5426: loss = 0.0710 (0.329 sec/step)\n",
            "I0719 09:49:13.477084 140481689634688 learning.py:507] global step 5427: loss = 0.0425 (0.337 sec/step)\n",
            "I0719 09:49:13.802812 140481689634688 learning.py:507] global step 5428: loss = 0.0826 (0.324 sec/step)\n",
            "I0719 09:49:14.121172 140481689634688 learning.py:507] global step 5429: loss = 0.0931 (0.316 sec/step)\n",
            "I0719 09:49:14.481413 140481689634688 learning.py:507] global step 5430: loss = 0.0522 (0.358 sec/step)\n",
            "I0719 09:49:14.826422 140481689634688 learning.py:507] global step 5431: loss = 0.0823 (0.343 sec/step)\n",
            "I0719 09:49:15.163500 140481689634688 learning.py:507] global step 5432: loss = 0.0953 (0.336 sec/step)\n",
            "I0719 09:49:15.488869 140481689634688 learning.py:507] global step 5433: loss = 0.0374 (0.323 sec/step)\n",
            "I0719 09:49:15.825486 140481689634688 learning.py:507] global step 5434: loss = 0.0462 (0.335 sec/step)\n",
            "I0719 09:49:16.175817 140481689634688 learning.py:507] global step 5435: loss = 0.0720 (0.348 sec/step)\n",
            "I0719 09:49:16.485604 140481689634688 learning.py:507] global step 5436: loss = 0.2164 (0.308 sec/step)\n",
            "I0719 09:49:16.780505 140481689634688 learning.py:507] global step 5437: loss = 0.1142 (0.293 sec/step)\n",
            "I0719 09:49:17.121404 140481689634688 learning.py:507] global step 5438: loss = 0.0319 (0.339 sec/step)\n",
            "I0719 09:49:17.500118 140481689634688 learning.py:507] global step 5439: loss = 0.1496 (0.377 sec/step)\n",
            "I0719 09:49:17.811039 140481689634688 learning.py:507] global step 5440: loss = 0.1394 (0.309 sec/step)\n",
            "I0719 09:49:18.151291 140481689634688 learning.py:507] global step 5441: loss = 0.1064 (0.339 sec/step)\n",
            "I0719 09:49:18.463833 140481689634688 learning.py:507] global step 5442: loss = 0.1314 (0.311 sec/step)\n",
            "I0719 09:49:18.766351 140481689634688 learning.py:507] global step 5443: loss = 0.0347 (0.301 sec/step)\n",
            "I0719 09:49:19.065409 140481689634688 learning.py:507] global step 5444: loss = 0.2488 (0.297 sec/step)\n",
            "I0719 09:49:19.439391 140481689634688 learning.py:507] global step 5445: loss = 0.1756 (0.372 sec/step)\n",
            "I0719 09:49:19.784468 140481689634688 learning.py:507] global step 5446: loss = 0.1054 (0.343 sec/step)\n",
            "I0719 09:49:20.150351 140481689634688 learning.py:507] global step 5447: loss = 0.1082 (0.364 sec/step)\n",
            "I0719 09:49:20.455198 140481689634688 learning.py:507] global step 5448: loss = 0.1072 (0.303 sec/step)\n",
            "I0719 09:49:20.779639 140481689634688 learning.py:507] global step 5449: loss = 0.0438 (0.323 sec/step)\n",
            "I0719 09:49:21.106536 140481689634688 learning.py:507] global step 5450: loss = 0.1229 (0.325 sec/step)\n",
            "I0719 09:49:21.452953 140481689634688 learning.py:507] global step 5451: loss = 0.0998 (0.345 sec/step)\n",
            "I0719 09:49:21.747362 140481689634688 learning.py:507] global step 5452: loss = 0.0334 (0.293 sec/step)\n",
            "I0719 09:49:22.089298 140481689634688 learning.py:507] global step 5453: loss = 0.0496 (0.340 sec/step)\n",
            "I0719 09:49:22.412258 140481689634688 learning.py:507] global step 5454: loss = 0.0988 (0.321 sec/step)\n",
            "I0719 09:49:22.763894 140481689634688 learning.py:507] global step 5455: loss = 0.0811 (0.350 sec/step)\n",
            "I0719 09:49:23.107202 140481689634688 learning.py:507] global step 5456: loss = 0.0923 (0.342 sec/step)\n",
            "I0719 09:49:23.449558 140481689634688 learning.py:507] global step 5457: loss = 0.0575 (0.341 sec/step)\n",
            "I0719 09:49:23.831199 140481689634688 learning.py:507] global step 5458: loss = 0.2314 (0.380 sec/step)\n",
            "I0719 09:49:24.180848 140481689634688 learning.py:507] global step 5459: loss = 0.1185 (0.348 sec/step)\n",
            "I0719 09:49:24.531892 140481689634688 learning.py:507] global step 5460: loss = 0.0834 (0.349 sec/step)\n",
            "I0719 09:49:24.856186 140481689634688 learning.py:507] global step 5461: loss = 0.0593 (0.322 sec/step)\n",
            "I0719 09:49:25.135508 140481689634688 learning.py:507] global step 5462: loss = 0.0653 (0.278 sec/step)\n",
            "I0719 09:49:25.469453 140481689634688 learning.py:507] global step 5463: loss = 0.0422 (0.332 sec/step)\n",
            "I0719 09:49:25.828891 140481689634688 learning.py:507] global step 5464: loss = 0.0540 (0.357 sec/step)\n",
            "I0719 09:49:26.156502 140481689634688 learning.py:507] global step 5465: loss = 0.0499 (0.326 sec/step)\n",
            "I0719 09:49:26.502407 140481689634688 learning.py:507] global step 5466: loss = 0.0900 (0.343 sec/step)\n",
            "I0719 09:49:26.853063 140481689634688 learning.py:507] global step 5467: loss = 0.0758 (0.349 sec/step)\n",
            "I0719 09:49:27.158127 140481689634688 learning.py:507] global step 5468: loss = 0.0305 (0.303 sec/step)\n",
            "I0719 09:49:27.505744 140481689634688 learning.py:507] global step 5469: loss = 0.0911 (0.346 sec/step)\n",
            "I0719 09:49:27.845444 140481689634688 learning.py:507] global step 5470: loss = 0.0798 (0.338 sec/step)\n",
            "I0719 09:49:28.203240 140481689634688 learning.py:507] global step 5471: loss = 0.1201 (0.356 sec/step)\n",
            "I0719 09:49:28.540862 140481689634688 learning.py:507] global step 5472: loss = 0.0256 (0.336 sec/step)\n",
            "I0719 09:49:28.902116 140481689634688 learning.py:507] global step 5473: loss = 0.1204 (0.360 sec/step)\n",
            "I0719 09:49:29.234133 140481689634688 learning.py:507] global step 5474: loss = 0.1231 (0.330 sec/step)\n",
            "I0719 09:49:29.535580 140481689634688 learning.py:507] global step 5475: loss = 0.0723 (0.300 sec/step)\n",
            "I0719 09:49:29.888242 140481689634688 learning.py:507] global step 5476: loss = 0.0280 (0.351 sec/step)\n",
            "I0719 09:49:30.225078 140481689634688 learning.py:507] global step 5477: loss = 0.0495 (0.335 sec/step)\n",
            "I0719 09:49:30.569809 140481689634688 learning.py:507] global step 5478: loss = 0.0439 (0.343 sec/step)\n",
            "I0719 09:49:30.941442 140481689634688 learning.py:507] global step 5479: loss = 0.0267 (0.370 sec/step)\n",
            "I0719 09:49:31.254535 140481689634688 learning.py:507] global step 5480: loss = 0.0597 (0.311 sec/step)\n",
            "I0719 09:49:31.591935 140481689634688 learning.py:507] global step 5481: loss = 0.0656 (0.336 sec/step)\n",
            "I0719 09:49:31.888309 140481689634688 learning.py:507] global step 5482: loss = 0.1229 (0.294 sec/step)\n",
            "I0719 09:49:32.225396 140481689634688 learning.py:507] global step 5483: loss = 0.0864 (0.335 sec/step)\n",
            "I0719 09:49:32.545583 140481689634688 learning.py:507] global step 5484: loss = 0.1003 (0.318 sec/step)\n",
            "I0719 09:49:32.871651 140481689634688 learning.py:507] global step 5485: loss = 0.1796 (0.324 sec/step)\n",
            "I0719 09:49:33.194536 140481689634688 learning.py:507] global step 5486: loss = 0.0896 (0.321 sec/step)\n",
            "I0719 09:49:33.538065 140481689634688 learning.py:507] global step 5487: loss = 0.0597 (0.342 sec/step)\n",
            "I0719 09:49:33.863415 140481689634688 learning.py:507] global step 5488: loss = 0.1000 (0.323 sec/step)\n",
            "I0719 09:49:34.193386 140481689634688 learning.py:507] global step 5489: loss = 0.1096 (0.328 sec/step)\n",
            "I0719 09:49:34.513234 140481689634688 learning.py:507] global step 5490: loss = 0.0889 (0.317 sec/step)\n",
            "I0719 09:49:34.845066 140481689634688 learning.py:507] global step 5491: loss = 0.0410 (0.330 sec/step)\n",
            "I0719 09:49:35.195168 140481689634688 learning.py:507] global step 5492: loss = 0.0281 (0.348 sec/step)\n",
            "I0719 09:49:35.533859 140481689634688 learning.py:507] global step 5493: loss = 0.0531 (0.337 sec/step)\n",
            "I0719 09:49:35.840042 140481689634688 learning.py:507] global step 5494: loss = 0.1119 (0.304 sec/step)\n",
            "I0719 09:49:36.183108 140481689634688 learning.py:507] global step 5495: loss = 0.0842 (0.341 sec/step)\n",
            "I0719 09:49:36.541333 140481689634688 learning.py:507] global step 5496: loss = 0.0491 (0.356 sec/step)\n",
            "I0719 09:49:36.833564 140481689634688 learning.py:507] global step 5497: loss = 0.1174 (0.290 sec/step)\n",
            "I0719 09:49:37.178976 140481689634688 learning.py:507] global step 5498: loss = 0.0235 (0.344 sec/step)\n",
            "I0719 09:49:37.512535 140481689634688 learning.py:507] global step 5499: loss = 0.0454 (0.332 sec/step)\n",
            "I0719 09:49:37.834226 140481689634688 learning.py:507] global step 5500: loss = 0.0972 (0.320 sec/step)\n",
            "I0719 09:49:38.167237 140481689634688 learning.py:507] global step 5501: loss = 0.0837 (0.331 sec/step)\n",
            "I0719 09:49:38.456226 140481689634688 learning.py:507] global step 5502: loss = 0.0632 (0.287 sec/step)\n",
            "I0719 09:49:38.771574 140481689634688 learning.py:507] global step 5503: loss = 0.0373 (0.313 sec/step)\n",
            "I0719 09:49:39.114738 140481689634688 learning.py:507] global step 5504: loss = 0.0113 (0.341 sec/step)\n",
            "I0719 09:49:39.458999 140481689634688 learning.py:507] global step 5505: loss = 0.0193 (0.342 sec/step)\n",
            "I0719 09:49:39.787767 140481689634688 learning.py:507] global step 5506: loss = 0.0502 (0.327 sec/step)\n",
            "I0719 09:49:40.100644 140481689634688 learning.py:507] global step 5507: loss = 0.0532 (0.311 sec/step)\n",
            "I0719 09:49:40.444986 140481689634688 learning.py:507] global step 5508: loss = 0.0750 (0.343 sec/step)\n",
            "I0719 09:49:40.748254 140481689634688 learning.py:507] global step 5509: loss = 0.0102 (0.301 sec/step)\n",
            "I0719 09:49:41.118735 140481689634688 learning.py:507] global step 5510: loss = 0.0610 (0.368 sec/step)\n",
            "I0719 09:49:41.434957 140481689634688 learning.py:507] global step 5511: loss = 0.0559 (0.314 sec/step)\n",
            "I0719 09:49:41.769045 140481689634688 learning.py:507] global step 5512: loss = 0.0319 (0.332 sec/step)\n",
            "I0719 09:49:42.096563 140481689634688 learning.py:507] global step 5513: loss = 0.2008 (0.326 sec/step)\n",
            "I0719 09:49:42.405083 140481689634688 learning.py:507] global step 5514: loss = 0.0808 (0.305 sec/step)\n",
            "I0719 09:49:42.742341 140481689634688 learning.py:507] global step 5515: loss = 0.0311 (0.335 sec/step)\n",
            "I0719 09:49:43.081821 140481689634688 learning.py:507] global step 5516: loss = 0.0694 (0.338 sec/step)\n",
            "I0719 09:49:43.431952 140481689634688 learning.py:507] global step 5517: loss = 0.0404 (0.348 sec/step)\n",
            "I0719 09:49:43.753678 140481689634688 learning.py:507] global step 5518: loss = 0.0702 (0.320 sec/step)\n",
            "I0719 09:49:44.058936 140481689634688 learning.py:507] global step 5519: loss = 0.0705 (0.304 sec/step)\n",
            "I0719 09:49:44.402090 140481689634688 learning.py:507] global step 5520: loss = 0.1109 (0.341 sec/step)\n",
            "I0719 09:49:44.719958 140481689634688 learning.py:507] global step 5521: loss = 0.0372 (0.316 sec/step)\n",
            "I0719 09:49:45.011899 140481689634688 learning.py:507] global step 5522: loss = 0.1241 (0.290 sec/step)\n",
            "I0719 09:49:45.393186 140481689634688 learning.py:507] global step 5523: loss = 0.0668 (0.380 sec/step)\n",
            "I0719 09:49:45.705241 140481689634688 learning.py:507] global step 5524: loss = 0.0720 (0.310 sec/step)\n",
            "I0719 09:49:46.033225 140481689634688 learning.py:507] global step 5525: loss = 0.1321 (0.326 sec/step)\n",
            "I0719 09:49:46.397200 140481689634688 learning.py:507] global step 5526: loss = 0.0548 (0.362 sec/step)\n",
            "I0719 09:49:46.748667 140481689634688 learning.py:507] global step 5527: loss = 0.0681 (0.350 sec/step)\n",
            "I0719 09:49:47.102333 140481689634688 learning.py:507] global step 5528: loss = 0.1580 (0.352 sec/step)\n",
            "I0719 09:49:47.405196 140481689634688 learning.py:507] global step 5529: loss = 0.0495 (0.301 sec/step)\n",
            "I0719 09:49:47.747763 140481689634688 learning.py:507] global step 5530: loss = 0.0340 (0.341 sec/step)\n",
            "I0719 09:49:48.075512 140481689634688 learning.py:507] global step 5531: loss = 0.0380 (0.326 sec/step)\n",
            "I0719 09:49:48.406305 140481689634688 learning.py:507] global step 5532: loss = 0.0395 (0.329 sec/step)\n",
            "I0719 09:49:48.737242 140481689634688 learning.py:507] global step 5533: loss = 0.1019 (0.329 sec/step)\n",
            "I0719 09:49:49.071313 140481689634688 learning.py:507] global step 5534: loss = 0.0653 (0.332 sec/step)\n",
            "I0719 09:49:49.389336 140481689634688 learning.py:507] global step 5535: loss = 0.0476 (0.316 sec/step)\n",
            "I0719 09:49:49.671309 140481689634688 learning.py:507] global step 5536: loss = 0.1113 (0.280 sec/step)\n",
            "I0719 09:49:50.010019 140481689634688 learning.py:507] global step 5537: loss = 0.1177 (0.337 sec/step)\n",
            "I0719 09:49:50.344357 140481689634688 learning.py:507] global step 5538: loss = 0.1120 (0.333 sec/step)\n",
            "I0719 09:49:50.681293 140481689634688 learning.py:507] global step 5539: loss = 0.1234 (0.335 sec/step)\n",
            "I0719 09:49:51.020253 140481689634688 learning.py:507] global step 5540: loss = 0.1167 (0.337 sec/step)\n",
            "I0719 09:49:51.354702 140481689634688 learning.py:507] global step 5541: loss = 0.0679 (0.333 sec/step)\n",
            "I0719 09:49:51.700590 140481689634688 learning.py:507] global step 5542: loss = 0.0902 (0.344 sec/step)\n",
            "I0719 09:49:52.057289 140481689634688 learning.py:507] global step 5543: loss = 0.0862 (0.355 sec/step)\n",
            "I0719 09:49:52.347441 140481689634688 learning.py:507] global step 5544: loss = 0.0424 (0.288 sec/step)\n",
            "I0719 09:49:52.666951 140481689634688 learning.py:507] global step 5545: loss = 0.1005 (0.318 sec/step)\n",
            "I0719 09:49:52.988841 140481689634688 learning.py:507] global step 5546: loss = 0.0186 (0.320 sec/step)\n",
            "I0719 09:49:53.307920 140481689634688 learning.py:507] global step 5547: loss = 0.0331 (0.317 sec/step)\n",
            "I0719 09:49:53.582813 140481689634688 learning.py:507] global step 5548: loss = 0.1076 (0.273 sec/step)\n",
            "I0719 09:49:53.925165 140481689634688 learning.py:507] global step 5549: loss = 0.0484 (0.340 sec/step)\n",
            "I0719 09:49:54.276980 140481689634688 learning.py:507] global step 5550: loss = 0.1085 (0.350 sec/step)\n",
            "I0719 09:49:54.609597 140481689634688 learning.py:507] global step 5551: loss = 0.0443 (0.331 sec/step)\n",
            "I0719 09:49:54.934757 140481689634688 learning.py:507] global step 5552: loss = 0.0538 (0.323 sec/step)\n",
            "I0719 09:49:55.287562 140481689634688 learning.py:507] global step 5553: loss = 0.0521 (0.351 sec/step)\n",
            "I0719 09:49:55.627429 140481689634688 learning.py:507] global step 5554: loss = 0.0312 (0.338 sec/step)\n",
            "I0719 09:49:55.958993 140481689634688 learning.py:507] global step 5555: loss = 0.0329 (0.330 sec/step)\n",
            "I0719 09:49:56.274957 140481689634688 learning.py:507] global step 5556: loss = 0.0445 (0.314 sec/step)\n",
            "I0719 09:49:56.592539 140481689634688 learning.py:507] global step 5557: loss = 0.0379 (0.316 sec/step)\n",
            "I0719 09:49:56.911942 140481689634688 learning.py:507] global step 5558: loss = 0.1161 (0.318 sec/step)\n",
            "I0719 09:49:57.275447 140481689634688 learning.py:507] global step 5559: loss = 0.0410 (0.361 sec/step)\n",
            "I0719 09:49:57.572671 140481689634688 learning.py:507] global step 5560: loss = 0.0413 (0.296 sec/step)\n",
            "I0719 09:49:57.856294 140481689634688 learning.py:507] global step 5561: loss = 0.0969 (0.282 sec/step)\n",
            "I0719 09:49:58.184997 140481689634688 learning.py:507] global step 5562: loss = 0.0451 (0.327 sec/step)\n",
            "I0719 09:49:58.532896 140481689634688 learning.py:507] global step 5563: loss = 0.0748 (0.346 sec/step)\n",
            "I0719 09:49:58.849431 140481689634688 learning.py:507] global step 5564: loss = 0.2406 (0.315 sec/step)\n",
            "I0719 09:49:59.149466 140481689634688 learning.py:507] global step 5565: loss = 0.0433 (0.298 sec/step)\n",
            "I0719 09:49:59.486605 140481689634688 learning.py:507] global step 5566: loss = 0.2519 (0.335 sec/step)\n",
            "I0719 09:49:59.826384 140481689634688 learning.py:507] global step 5567: loss = 0.0729 (0.338 sec/step)\n",
            "I0719 09:50:00.159197 140481689634688 learning.py:507] global step 5568: loss = 0.0411 (0.331 sec/step)\n",
            "I0719 09:50:00.478495 140481689634688 learning.py:507] global step 5569: loss = 0.0893 (0.317 sec/step)\n",
            "I0719 09:50:00.832944 140481689634688 learning.py:507] global step 5570: loss = 0.0911 (0.353 sec/step)\n",
            "I0719 09:50:01.207846 140481689634688 learning.py:507] global step 5571: loss = 0.0439 (0.373 sec/step)\n",
            "I0719 09:50:01.554736 140481689634688 learning.py:507] global step 5572: loss = 0.0400 (0.345 sec/step)\n",
            "I0719 09:50:01.854877 140481689634688 learning.py:507] global step 5573: loss = 0.0470 (0.298 sec/step)\n",
            "I0719 09:50:02.198805 140481689634688 learning.py:507] global step 5574: loss = 0.0489 (0.342 sec/step)\n",
            "I0719 09:50:02.504644 140481689634688 learning.py:507] global step 5575: loss = 0.0972 (0.304 sec/step)\n",
            "I0719 09:50:02.819003 140481689634688 learning.py:507] global step 5576: loss = 0.0292 (0.313 sec/step)\n",
            "I0719 09:50:03.172966 140481689634688 learning.py:507] global step 5577: loss = 0.0998 (0.352 sec/step)\n",
            "I0719 09:50:03.536202 140481689634688 learning.py:507] global step 5578: loss = 0.0670 (0.361 sec/step)\n",
            "I0719 09:50:03.894786 140481689634688 learning.py:507] global step 5579: loss = 0.0526 (0.357 sec/step)\n",
            "I0719 09:50:04.233589 140481689634688 learning.py:507] global step 5580: loss = 0.0392 (0.337 sec/step)\n",
            "I0719 09:50:04.578933 140481689634688 learning.py:507] global step 5581: loss = 0.0758 (0.344 sec/step)\n",
            "I0719 09:50:04.871749 140481689634688 learning.py:507] global step 5582: loss = 0.0649 (0.291 sec/step)\n",
            "I0719 09:50:05.208614 140481689634688 learning.py:507] global step 5583: loss = 0.0889 (0.335 sec/step)\n",
            "I0719 09:50:05.548396 140481689634688 learning.py:507] global step 5584: loss = 0.0705 (0.338 sec/step)\n",
            "I0719 09:50:05.889854 140481689634688 learning.py:507] global step 5585: loss = 0.0452 (0.340 sec/step)\n",
            "I0719 09:50:06.212395 140481689634688 learning.py:507] global step 5586: loss = 0.0571 (0.321 sec/step)\n",
            "I0719 09:50:06.579482 140481689634688 learning.py:507] global step 5587: loss = 0.0853 (0.365 sec/step)\n",
            "I0719 09:50:06.918156 140481689634688 learning.py:507] global step 5588: loss = 0.0447 (0.337 sec/step)\n",
            "I0719 09:50:07.266063 140481689634688 learning.py:507] global step 5589: loss = 0.1529 (0.346 sec/step)\n",
            "I0719 09:50:07.600185 140481689634688 learning.py:507] global step 5590: loss = 0.0370 (0.332 sec/step)\n",
            "I0719 09:50:07.929042 140481689634688 learning.py:507] global step 5591: loss = 0.2379 (0.327 sec/step)\n",
            "I0719 09:50:08.214569 140481689634688 learning.py:507] global step 5592: loss = 0.0404 (0.284 sec/step)\n",
            "I0719 09:50:08.566661 140481689634688 learning.py:507] global step 5593: loss = 0.1380 (0.350 sec/step)\n",
            "I0719 09:50:08.930515 140481689634688 learning.py:507] global step 5594: loss = 0.0356 (0.362 sec/step)\n",
            "I0719 09:50:09.273905 140481689634688 learning.py:507] global step 5595: loss = 0.0604 (0.342 sec/step)\n",
            "I0719 09:50:09.593455 140481689634688 learning.py:507] global step 5596: loss = 0.0342 (0.318 sec/step)\n",
            "I0719 09:50:09.930932 140481689634688 learning.py:507] global step 5597: loss = 0.0300 (0.336 sec/step)\n",
            "I0719 09:50:10.263972 140481689634688 learning.py:507] global step 5598: loss = 0.1297 (0.331 sec/step)\n",
            "I0719 09:50:10.624428 140481689634688 learning.py:507] global step 5599: loss = 0.0514 (0.359 sec/step)\n",
            "I0719 09:50:10.960348 140481689634688 learning.py:507] global step 5600: loss = 0.0710 (0.334 sec/step)\n",
            "I0719 09:50:11.315653 140481689634688 learning.py:507] global step 5601: loss = 0.0420 (0.353 sec/step)\n",
            "I0719 09:50:11.618142 140481689634688 learning.py:507] global step 5602: loss = 0.0536 (0.301 sec/step)\n",
            "I0719 09:50:11.969860 140481689634688 learning.py:507] global step 5603: loss = 0.1072 (0.350 sec/step)\n",
            "I0719 09:50:12.253678 140481689634688 learning.py:507] global step 5604: loss = 0.0700 (0.282 sec/step)\n",
            "I0719 09:50:12.587535 140481689634688 learning.py:507] global step 5605: loss = 0.0734 (0.332 sec/step)\n",
            "I0719 09:50:12.931549 140481689634688 learning.py:507] global step 5606: loss = 0.0845 (0.342 sec/step)\n",
            "I0719 09:50:13.265038 140481689634688 learning.py:507] global step 5607: loss = 0.0638 (0.332 sec/step)\n",
            "I0719 09:50:13.599958 140481689634688 learning.py:507] global step 5608: loss = 0.0390 (0.333 sec/step)\n",
            "I0719 09:50:13.936827 140481689634688 learning.py:507] global step 5609: loss = 0.1137 (0.335 sec/step)\n",
            "I0719 09:50:14.266907 140481689634688 learning.py:507] global step 5610: loss = 0.0110 (0.329 sec/step)\n",
            "I0719 09:50:14.606590 140481689634688 learning.py:507] global step 5611: loss = 0.0767 (0.338 sec/step)\n",
            "I0719 09:50:14.917045 140481689634688 learning.py:507] global step 5612: loss = 0.0519 (0.309 sec/step)\n",
            "I0719 09:50:15.241118 140481689634688 learning.py:507] global step 5613: loss = 0.0238 (0.322 sec/step)\n",
            "I0719 09:50:15.597859 140481689634688 learning.py:507] global step 5614: loss = 0.0205 (0.355 sec/step)\n",
            "I0719 09:50:15.935414 140481689634688 learning.py:507] global step 5615: loss = 0.1042 (0.335 sec/step)\n",
            "I0719 09:50:16.268790 140481689634688 learning.py:507] global step 5616: loss = 0.0486 (0.331 sec/step)\n",
            "I0719 09:50:16.615427 140481689634688 learning.py:507] global step 5617: loss = 0.0334 (0.345 sec/step)\n",
            "I0719 09:50:16.957213 140481689634688 learning.py:507] global step 5618: loss = 0.1385 (0.339 sec/step)\n",
            "I0719 09:50:17.275523 140481689634688 learning.py:507] global step 5619: loss = 0.1083 (0.316 sec/step)\n",
            "I0719 09:50:17.617402 140481689634688 learning.py:507] global step 5620: loss = 0.0409 (0.340 sec/step)\n",
            "I0719 09:50:17.953645 140481689634688 learning.py:507] global step 5621: loss = 0.0605 (0.335 sec/step)\n",
            "I0719 09:50:18.309098 140481689634688 learning.py:507] global step 5622: loss = 0.0970 (0.354 sec/step)\n",
            "I0719 09:50:18.595144 140481689634688 learning.py:507] global step 5623: loss = 0.0259 (0.284 sec/step)\n",
            "I0719 09:50:18.937953 140481689634688 learning.py:507] global step 5624: loss = 0.0356 (0.341 sec/step)\n",
            "I0719 09:50:19.274096 140481689634688 learning.py:507] global step 5625: loss = 0.0974 (0.334 sec/step)\n",
            "I0719 09:50:19.614751 140481689634688 learning.py:507] global step 5626: loss = 0.0394 (0.339 sec/step)\n",
            "I0719 09:50:19.912959 140481689634688 learning.py:507] global step 5627: loss = 0.0894 (0.296 sec/step)\n",
            "I0719 09:50:20.239461 140481689634688 learning.py:507] global step 5628: loss = 0.0591 (0.325 sec/step)\n",
            "I0719 09:50:20.547691 140481689634688 learning.py:507] global step 5629: loss = 0.0462 (0.306 sec/step)\n",
            "I0719 09:50:20.881299 140481689634688 learning.py:507] global step 5630: loss = 0.0521 (0.331 sec/step)\n",
            "I0719 09:50:21.222097 140481689634688 learning.py:507] global step 5631: loss = 0.1785 (0.339 sec/step)\n",
            "I0719 09:50:21.546468 140481689634688 learning.py:507] global step 5632: loss = 0.0866 (0.322 sec/step)\n",
            "I0719 09:50:21.877025 140481689634688 learning.py:507] global step 5633: loss = 0.0327 (0.329 sec/step)\n",
            "I0719 09:50:22.220393 140481689634688 learning.py:507] global step 5634: loss = 0.0707 (0.339 sec/step)\n",
            "I0719 09:50:22.579338 140481689634688 learning.py:507] global step 5635: loss = 0.0503 (0.356 sec/step)\n",
            "I0719 09:50:22.907063 140481689634688 learning.py:507] global step 5636: loss = 0.0669 (0.325 sec/step)\n",
            "I0719 09:50:23.255222 140481689634688 learning.py:507] global step 5637: loss = 0.0484 (0.346 sec/step)\n",
            "I0719 09:50:23.582542 140481689634688 learning.py:507] global step 5638: loss = 0.0307 (0.326 sec/step)\n",
            "I0719 09:50:23.922675 140481689634688 learning.py:507] global step 5639: loss = 0.0359 (0.338 sec/step)\n",
            "I0719 09:50:24.281184 140481689634688 learning.py:507] global step 5640: loss = 0.0941 (0.357 sec/step)\n",
            "I0719 09:50:24.617757 140481689634688 learning.py:507] global step 5641: loss = 0.1268 (0.335 sec/step)\n",
            "I0719 09:50:24.944307 140481689634688 learning.py:507] global step 5642: loss = 0.0163 (0.325 sec/step)\n",
            "I0719 09:50:25.306764 140481689634688 learning.py:507] global step 5643: loss = 0.0579 (0.358 sec/step)\n",
            "I0719 09:50:25.633741 140481689634688 learning.py:507] global step 5644: loss = 0.0345 (0.325 sec/step)\n",
            "I0719 09:50:25.986391 140481689634688 learning.py:507] global step 5645: loss = 0.1111 (0.351 sec/step)\n",
            "I0719 09:50:26.568697 140481689634688 learning.py:507] global step 5646: loss = 0.0598 (0.536 sec/step)\n",
            "I0719 09:50:27.038561 140481689634688 learning.py:507] global step 5647: loss = 0.0955 (0.467 sec/step)\n",
            "I0719 09:50:27.097232 140479018202880 supervisor.py:1050] Recording summary at step 5647.\n",
            "I0719 09:50:27.373376 140481689634688 learning.py:507] global step 5648: loss = 0.1099 (0.326 sec/step)\n",
            "I0719 09:50:27.726769 140481689634688 learning.py:507] global step 5649: loss = 0.0448 (0.351 sec/step)\n",
            "I0719 09:50:28.073608 140481689634688 learning.py:507] global step 5650: loss = 0.1834 (0.345 sec/step)\n",
            "I0719 09:50:28.355855 140481689634688 learning.py:507] global step 5651: loss = 0.1055 (0.281 sec/step)\n",
            "I0719 09:50:28.695345 140481689634688 learning.py:507] global step 5652: loss = 0.1359 (0.338 sec/step)\n",
            "I0719 09:50:29.021927 140481689634688 learning.py:507] global step 5653: loss = 0.0756 (0.325 sec/step)\n",
            "I0719 09:50:29.364700 140481689634688 learning.py:507] global step 5654: loss = 0.0396 (0.341 sec/step)\n",
            "I0719 09:50:29.662662 140481689634688 learning.py:507] global step 5655: loss = 0.0358 (0.296 sec/step)\n",
            "I0719 09:50:29.952674 140481689634688 learning.py:507] global step 5656: loss = 0.0377 (0.288 sec/step)\n",
            "I0719 09:50:30.282378 140481689634688 learning.py:507] global step 5657: loss = 0.1048 (0.328 sec/step)\n",
            "I0719 09:50:30.586758 140481689634688 learning.py:507] global step 5658: loss = 0.0365 (0.303 sec/step)\n",
            "I0719 09:50:30.933068 140481689634688 learning.py:507] global step 5659: loss = 0.1125 (0.345 sec/step)\n",
            "I0719 09:50:31.237543 140481689634688 learning.py:507] global step 5660: loss = 0.0322 (0.302 sec/step)\n",
            "I0719 09:50:31.554032 140481689634688 learning.py:507] global step 5661: loss = 0.0646 (0.315 sec/step)\n",
            "I0719 09:50:31.896524 140481689634688 learning.py:507] global step 5662: loss = 0.0887 (0.341 sec/step)\n",
            "I0719 09:50:32.238417 140481689634688 learning.py:507] global step 5663: loss = 0.0439 (0.340 sec/step)\n",
            "I0719 09:50:32.575233 140481689634688 learning.py:507] global step 5664: loss = 0.0446 (0.335 sec/step)\n",
            "I0719 09:50:32.911954 140481689634688 learning.py:507] global step 5665: loss = 0.1382 (0.335 sec/step)\n",
            "I0719 09:50:33.291615 140481689634688 learning.py:507] global step 5666: loss = 0.2154 (0.378 sec/step)\n",
            "I0719 09:50:33.650755 140481689634688 learning.py:507] global step 5667: loss = 0.1174 (0.357 sec/step)\n",
            "I0719 09:50:34.009924 140481689634688 learning.py:507] global step 5668: loss = 0.0492 (0.357 sec/step)\n",
            "I0719 09:50:34.342601 140481689634688 learning.py:507] global step 5669: loss = 0.0352 (0.331 sec/step)\n",
            "I0719 09:50:34.694532 140481689634688 learning.py:507] global step 5670: loss = 0.0663 (0.350 sec/step)\n",
            "I0719 09:50:35.033320 140481689634688 learning.py:507] global step 5671: loss = 0.0818 (0.337 sec/step)\n",
            "I0719 09:50:35.369957 140481689634688 learning.py:507] global step 5672: loss = 0.0745 (0.335 sec/step)\n",
            "I0719 09:50:35.713231 140481689634688 learning.py:507] global step 5673: loss = 0.0587 (0.342 sec/step)\n",
            "I0719 09:50:36.053574 140481689634688 learning.py:507] global step 5674: loss = 0.0786 (0.339 sec/step)\n",
            "I0719 09:50:36.372517 140481689634688 learning.py:507] global step 5675: loss = 0.0795 (0.317 sec/step)\n",
            "I0719 09:50:36.719583 140481689634688 learning.py:507] global step 5676: loss = 0.0392 (0.345 sec/step)\n",
            "I0719 09:50:37.019448 140481689634688 learning.py:507] global step 5677: loss = 0.0731 (0.298 sec/step)\n",
            "I0719 09:50:37.357721 140481689634688 learning.py:507] global step 5678: loss = 0.0361 (0.335 sec/step)\n",
            "I0719 09:50:37.677938 140481689634688 learning.py:507] global step 5679: loss = 0.0325 (0.318 sec/step)\n",
            "I0719 09:50:38.021188 140481689634688 learning.py:507] global step 5680: loss = 0.0540 (0.342 sec/step)\n",
            "I0719 09:50:38.310305 140481689634688 learning.py:507] global step 5681: loss = 0.0526 (0.288 sec/step)\n",
            "I0719 09:50:38.645840 140481689634688 learning.py:507] global step 5682: loss = 0.0697 (0.334 sec/step)\n",
            "I0719 09:50:38.997145 140481689634688 learning.py:507] global step 5683: loss = 0.0610 (0.349 sec/step)\n",
            "I0719 09:50:39.327960 140481689634688 learning.py:507] global step 5684: loss = 0.0556 (0.329 sec/step)\n",
            "I0719 09:50:39.629660 140481689634688 learning.py:507] global step 5685: loss = 0.1426 (0.300 sec/step)\n",
            "I0719 09:50:39.990398 140481689634688 learning.py:507] global step 5686: loss = 0.0392 (0.359 sec/step)\n",
            "I0719 09:50:40.339630 140481689634688 learning.py:507] global step 5687: loss = 0.0375 (0.347 sec/step)\n",
            "I0719 09:50:40.670189 140481689634688 learning.py:507] global step 5688: loss = 0.0657 (0.329 sec/step)\n",
            "I0719 09:50:40.983702 140481689634688 learning.py:507] global step 5689: loss = 0.0423 (0.312 sec/step)\n",
            "I0719 09:50:41.342527 140481689634688 learning.py:507] global step 5690: loss = 0.0374 (0.357 sec/step)\n",
            "I0719 09:50:41.688710 140481689634688 learning.py:507] global step 5691: loss = 0.0789 (0.344 sec/step)\n",
            "I0719 09:50:42.024757 140481689634688 learning.py:507] global step 5692: loss = 0.0718 (0.334 sec/step)\n",
            "I0719 09:50:42.369827 140481689634688 learning.py:507] global step 5693: loss = 0.0666 (0.343 sec/step)\n",
            "I0719 09:50:42.684109 140481689634688 learning.py:507] global step 5694: loss = 0.0761 (0.312 sec/step)\n",
            "I0719 09:50:43.012138 140481689634688 learning.py:507] global step 5695: loss = 0.0298 (0.324 sec/step)\n",
            "I0719 09:50:43.355392 140481689634688 learning.py:507] global step 5696: loss = 0.0832 (0.341 sec/step)\n",
            "I0719 09:50:43.718228 140481689634688 learning.py:507] global step 5697: loss = 0.0374 (0.361 sec/step)\n",
            "I0719 09:50:44.050307 140481689634688 learning.py:507] global step 5698: loss = 0.0556 (0.330 sec/step)\n",
            "I0719 09:50:44.395953 140481689634688 learning.py:507] global step 5699: loss = 0.0458 (0.344 sec/step)\n",
            "I0719 09:50:44.718514 140481689634688 learning.py:507] global step 5700: loss = 0.0220 (0.321 sec/step)\n",
            "I0719 09:50:45.059363 140481689634688 learning.py:507] global step 5701: loss = 0.0420 (0.339 sec/step)\n",
            "I0719 09:50:45.406519 140481689634688 learning.py:507] global step 5702: loss = 0.0532 (0.346 sec/step)\n",
            "I0719 09:50:45.728702 140481689634688 learning.py:507] global step 5703: loss = 0.0313 (0.320 sec/step)\n",
            "I0719 09:50:46.054687 140481689634688 learning.py:507] global step 5704: loss = 0.0263 (0.324 sec/step)\n",
            "I0719 09:50:46.399094 140481689634688 learning.py:507] global step 5705: loss = 0.0547 (0.343 sec/step)\n",
            "I0719 09:50:46.687603 140481689634688 learning.py:507] global step 5706: loss = 0.0350 (0.287 sec/step)\n",
            "I0719 09:50:47.037835 140481689634688 learning.py:507] global step 5707: loss = 0.0279 (0.348 sec/step)\n",
            "I0719 09:50:47.373162 140481689634688 learning.py:507] global step 5708: loss = 0.0646 (0.334 sec/step)\n",
            "I0719 09:50:47.693637 140481689634688 learning.py:507] global step 5709: loss = 0.0501 (0.319 sec/step)\n",
            "I0719 09:50:48.017097 140481689634688 learning.py:507] global step 5710: loss = 0.1276 (0.321 sec/step)\n",
            "I0719 09:50:48.342244 140481689634688 learning.py:507] global step 5711: loss = 0.0839 (0.324 sec/step)\n",
            "I0719 09:50:48.701983 140481689634688 learning.py:507] global step 5712: loss = 0.1144 (0.358 sec/step)\n",
            "I0719 09:50:49.063609 140481689634688 learning.py:507] global step 5713: loss = 0.0587 (0.360 sec/step)\n",
            "I0719 09:50:49.388023 140481689634688 learning.py:507] global step 5714: loss = 0.0562 (0.322 sec/step)\n",
            "I0719 09:50:49.723376 140481689634688 learning.py:507] global step 5715: loss = 0.0681 (0.334 sec/step)\n",
            "I0719 09:50:50.084657 140481689634688 learning.py:507] global step 5716: loss = 0.0800 (0.360 sec/step)\n",
            "I0719 09:50:50.408926 140481689634688 learning.py:507] global step 5717: loss = 0.0642 (0.323 sec/step)\n",
            "I0719 09:50:50.748997 140481689634688 learning.py:507] global step 5718: loss = 0.0930 (0.338 sec/step)\n",
            "I0719 09:50:51.083412 140481689634688 learning.py:507] global step 5719: loss = 0.0688 (0.333 sec/step)\n",
            "I0719 09:50:51.401355 140481689634688 learning.py:507] global step 5720: loss = 0.0510 (0.316 sec/step)\n",
            "I0719 09:50:51.722441 140481689634688 learning.py:507] global step 5721: loss = 0.0603 (0.319 sec/step)\n",
            "I0719 09:50:52.064429 140481689634688 learning.py:507] global step 5722: loss = 0.0611 (0.340 sec/step)\n",
            "I0719 09:50:52.369047 140481689634688 learning.py:507] global step 5723: loss = 0.0404 (0.303 sec/step)\n",
            "I0719 09:50:52.700008 140481689634688 learning.py:507] global step 5724: loss = 0.0316 (0.329 sec/step)\n",
            "I0719 09:50:53.016479 140481689634688 learning.py:507] global step 5725: loss = 0.0118 (0.314 sec/step)\n",
            "I0719 09:50:53.337853 140481689634688 learning.py:507] global step 5726: loss = 0.0488 (0.317 sec/step)\n",
            "I0719 09:50:53.694646 140481689634688 learning.py:507] global step 5727: loss = 0.2107 (0.355 sec/step)\n",
            "I0719 09:50:54.046987 140481689634688 learning.py:507] global step 5728: loss = 0.0653 (0.350 sec/step)\n",
            "I0719 09:50:54.381697 140481689634688 learning.py:507] global step 5729: loss = 0.0561 (0.333 sec/step)\n",
            "I0719 09:50:54.669046 140481689634688 learning.py:507] global step 5730: loss = 0.1101 (0.286 sec/step)\n",
            "I0719 09:50:54.969639 140481689634688 learning.py:507] global step 5731: loss = 0.1519 (0.299 sec/step)\n",
            "I0719 09:50:55.297360 140481689634688 learning.py:507] global step 5732: loss = 0.0524 (0.326 sec/step)\n",
            "I0719 09:50:55.626340 140481689634688 learning.py:507] global step 5733: loss = 0.0387 (0.327 sec/step)\n",
            "I0719 09:50:55.945909 140481689634688 learning.py:507] global step 5734: loss = 0.0674 (0.318 sec/step)\n",
            "I0719 09:50:56.262250 140481689634688 learning.py:507] global step 5735: loss = 0.0510 (0.315 sec/step)\n",
            "I0719 09:50:56.611178 140481689634688 learning.py:507] global step 5736: loss = 0.0778 (0.347 sec/step)\n",
            "I0719 09:50:56.967046 140481689634688 learning.py:507] global step 5737: loss = 0.1532 (0.354 sec/step)\n",
            "I0719 09:50:57.282162 140481689634688 learning.py:507] global step 5738: loss = 0.0987 (0.313 sec/step)\n",
            "I0719 09:50:57.618113 140481689634688 learning.py:507] global step 5739: loss = 0.0107 (0.334 sec/step)\n",
            "I0719 09:50:57.963650 140481689634688 learning.py:507] global step 5740: loss = 0.0303 (0.342 sec/step)\n",
            "I0719 09:50:58.309487 140481689634688 learning.py:507] global step 5741: loss = 0.0749 (0.344 sec/step)\n",
            "I0719 09:50:58.652745 140481689634688 learning.py:507] global step 5742: loss = 0.0438 (0.341 sec/step)\n",
            "I0719 09:50:58.995916 140481689634688 learning.py:507] global step 5743: loss = 0.1493 (0.341 sec/step)\n",
            "I0719 09:50:59.345481 140481689634688 learning.py:507] global step 5744: loss = 0.0953 (0.348 sec/step)\n",
            "I0719 09:50:59.690649 140481689634688 learning.py:507] global step 5745: loss = 0.0487 (0.343 sec/step)\n",
            "I0719 09:51:00.023708 140481689634688 learning.py:507] global step 5746: loss = 0.0675 (0.331 sec/step)\n",
            "I0719 09:51:00.346053 140481689634688 learning.py:507] global step 5747: loss = 0.0482 (0.321 sec/step)\n",
            "I0719 09:51:00.714436 140481689634688 learning.py:507] global step 5748: loss = 0.0518 (0.367 sec/step)\n",
            "I0719 09:51:01.038149 140481689634688 learning.py:507] global step 5749: loss = 0.0644 (0.322 sec/step)\n",
            "I0719 09:51:01.353372 140481689634688 learning.py:507] global step 5750: loss = 0.0670 (0.314 sec/step)\n",
            "I0719 09:51:01.716297 140481689634688 learning.py:507] global step 5751: loss = 0.0649 (0.361 sec/step)\n",
            "I0719 09:51:02.008630 140481689634688 learning.py:507] global step 5752: loss = 0.0474 (0.290 sec/step)\n",
            "I0719 09:51:02.369657 140481689634688 learning.py:507] global step 5753: loss = 0.0756 (0.359 sec/step)\n",
            "I0719 09:51:02.707191 140481689634688 learning.py:507] global step 5754: loss = 0.0334 (0.336 sec/step)\n",
            "I0719 09:51:03.009787 140481689634688 learning.py:507] global step 5755: loss = 0.1009 (0.301 sec/step)\n",
            "I0719 09:51:03.337687 140481689634688 learning.py:507] global step 5756: loss = 0.0851 (0.325 sec/step)\n",
            "I0719 09:51:03.671261 140481689634688 learning.py:507] global step 5757: loss = 0.0752 (0.330 sec/step)\n",
            "I0719 09:51:04.004492 140481689634688 learning.py:507] global step 5758: loss = 0.0389 (0.332 sec/step)\n",
            "I0719 09:51:04.340911 140481689634688 learning.py:507] global step 5759: loss = 0.0645 (0.335 sec/step)\n",
            "I0719 09:51:04.684351 140481689634688 learning.py:507] global step 5760: loss = 0.0157 (0.342 sec/step)\n",
            "I0719 09:51:05.013649 140481689634688 learning.py:507] global step 5761: loss = 0.0493 (0.327 sec/step)\n",
            "I0719 09:51:05.331851 140481689634688 learning.py:507] global step 5762: loss = 0.0574 (0.316 sec/step)\n",
            "I0719 09:51:05.655989 140481689634688 learning.py:507] global step 5763: loss = 0.0643 (0.322 sec/step)\n",
            "I0719 09:51:05.936921 140481689634688 learning.py:507] global step 5764: loss = 0.0319 (0.279 sec/step)\n",
            "I0719 09:51:06.272337 140481689634688 learning.py:507] global step 5765: loss = 0.0612 (0.334 sec/step)\n",
            "I0719 09:51:06.639823 140481689634688 learning.py:507] global step 5766: loss = 0.0746 (0.366 sec/step)\n",
            "I0719 09:51:06.961961 140481689634688 learning.py:507] global step 5767: loss = 0.0547 (0.320 sec/step)\n",
            "I0719 09:51:07.272375 140481689634688 learning.py:507] global step 5768: loss = 0.0360 (0.308 sec/step)\n",
            "I0719 09:51:07.631209 140481689634688 learning.py:507] global step 5769: loss = 0.0240 (0.357 sec/step)\n",
            "I0719 09:51:07.972436 140481689634688 learning.py:507] global step 5770: loss = 0.0909 (0.340 sec/step)\n",
            "I0719 09:51:08.280986 140481689634688 learning.py:507] global step 5771: loss = 0.0272 (0.307 sec/step)\n",
            "I0719 09:51:08.614052 140481689634688 learning.py:507] global step 5772: loss = 0.0146 (0.331 sec/step)\n",
            "I0719 09:51:08.894559 140481689634688 learning.py:507] global step 5773: loss = 0.0700 (0.279 sec/step)\n",
            "I0719 09:51:09.238202 140481689634688 learning.py:507] global step 5774: loss = 0.0258 (0.342 sec/step)\n",
            "I0719 09:51:09.572824 140481689634688 learning.py:507] global step 5775: loss = 0.0703 (0.333 sec/step)\n",
            "I0719 09:51:09.909350 140481689634688 learning.py:507] global step 5776: loss = 0.0654 (0.335 sec/step)\n",
            "I0719 09:51:10.235657 140481689634688 learning.py:507] global step 5777: loss = 0.0589 (0.324 sec/step)\n",
            "I0719 09:51:10.561152 140481689634688 learning.py:507] global step 5778: loss = 0.0822 (0.324 sec/step)\n",
            "I0719 09:51:10.898438 140481689634688 learning.py:507] global step 5779: loss = 0.0658 (0.335 sec/step)\n",
            "I0719 09:51:11.231814 140481689634688 learning.py:507] global step 5780: loss = 0.0762 (0.332 sec/step)\n",
            "I0719 09:51:11.588763 140481689634688 learning.py:507] global step 5781: loss = 0.0432 (0.355 sec/step)\n",
            "I0719 09:51:11.897045 140481689634688 learning.py:507] global step 5782: loss = 0.1062 (0.304 sec/step)\n",
            "I0719 09:51:12.225842 140481689634688 learning.py:507] global step 5783: loss = 0.0139 (0.327 sec/step)\n",
            "I0719 09:51:12.557450 140481689634688 learning.py:507] global step 5784: loss = 0.0784 (0.330 sec/step)\n",
            "I0719 09:51:12.928457 140481689634688 learning.py:507] global step 5785: loss = 0.0431 (0.369 sec/step)\n",
            "I0719 09:51:13.273654 140481689634688 learning.py:507] global step 5786: loss = 0.0757 (0.343 sec/step)\n",
            "I0719 09:51:13.559084 140481689634688 learning.py:507] global step 5787: loss = 0.0736 (0.282 sec/step)\n",
            "I0719 09:51:13.914415 140481689634688 learning.py:507] global step 5788: loss = 0.1268 (0.353 sec/step)\n",
            "I0719 09:51:14.258555 140481689634688 learning.py:507] global step 5789: loss = 0.1549 (0.342 sec/step)\n",
            "I0719 09:51:14.590708 140481689634688 learning.py:507] global step 5790: loss = 0.0571 (0.331 sec/step)\n",
            "I0719 09:51:14.930814 140481689634688 learning.py:507] global step 5791: loss = 0.0461 (0.338 sec/step)\n",
            "I0719 09:51:15.262846 140481689634688 learning.py:507] global step 5792: loss = 0.0224 (0.330 sec/step)\n",
            "I0719 09:51:15.613990 140481689634688 learning.py:507] global step 5793: loss = 0.1488 (0.349 sec/step)\n",
            "I0719 09:51:15.961475 140481689634688 learning.py:507] global step 5794: loss = 0.0873 (0.346 sec/step)\n",
            "I0719 09:51:16.305488 140481689634688 learning.py:507] global step 5795: loss = 0.0635 (0.342 sec/step)\n",
            "I0719 09:51:16.630414 140481689634688 learning.py:507] global step 5796: loss = 0.0824 (0.323 sec/step)\n",
            "I0719 09:51:16.963849 140481689634688 learning.py:507] global step 5797: loss = 0.1041 (0.332 sec/step)\n",
            "I0719 09:51:17.298190 140481689634688 learning.py:507] global step 5798: loss = 0.1382 (0.333 sec/step)\n",
            "I0719 09:51:17.624138 140481689634688 learning.py:507] global step 5799: loss = 0.0348 (0.324 sec/step)\n",
            "I0719 09:51:17.933795 140481689634688 learning.py:507] global step 5800: loss = 0.0289 (0.308 sec/step)\n",
            "I0719 09:51:18.241899 140481689634688 learning.py:507] global step 5801: loss = 0.0605 (0.306 sec/step)\n",
            "I0719 09:51:18.579297 140481689634688 learning.py:507] global step 5802: loss = 0.0898 (0.336 sec/step)\n",
            "I0719 09:51:18.912554 140481689634688 learning.py:507] global step 5803: loss = 0.0533 (0.332 sec/step)\n",
            "I0719 09:51:19.238334 140481689634688 learning.py:507] global step 5804: loss = 0.0883 (0.324 sec/step)\n",
            "I0719 09:51:19.562876 140481689634688 learning.py:507] global step 5805: loss = 0.0186 (0.323 sec/step)\n",
            "I0719 09:51:19.892553 140481689634688 learning.py:507] global step 5806: loss = 0.0568 (0.328 sec/step)\n",
            "I0719 09:51:20.206837 140481689634688 learning.py:507] global step 5807: loss = 0.0738 (0.313 sec/step)\n",
            "I0719 09:51:20.544526 140481689634688 learning.py:507] global step 5808: loss = 0.0679 (0.336 sec/step)\n",
            "I0719 09:51:20.928905 140481689634688 learning.py:507] global step 5809: loss = 0.2122 (0.383 sec/step)\n",
            "I0719 09:51:21.252303 140481689634688 learning.py:507] global step 5810: loss = 0.0332 (0.321 sec/step)\n",
            "I0719 09:51:21.553140 140481689634688 learning.py:507] global step 5811: loss = 0.0320 (0.299 sec/step)\n",
            "I0719 09:51:21.886360 140481689634688 learning.py:507] global step 5812: loss = 0.0714 (0.332 sec/step)\n",
            "I0719 09:51:22.245905 140481689634688 learning.py:507] global step 5813: loss = 0.0531 (0.358 sec/step)\n",
            "I0719 09:51:22.603679 140481689634688 learning.py:507] global step 5814: loss = 0.0613 (0.356 sec/step)\n",
            "I0719 09:51:22.941685 140481689634688 learning.py:507] global step 5815: loss = 0.0548 (0.336 sec/step)\n",
            "I0719 09:51:23.265229 140481689634688 learning.py:507] global step 5816: loss = 0.0699 (0.322 sec/step)\n",
            "I0719 09:51:23.564799 140481689634688 learning.py:507] global step 5817: loss = 0.0712 (0.298 sec/step)\n",
            "I0719 09:51:23.892527 140481689634688 learning.py:507] global step 5818: loss = 0.0809 (0.326 sec/step)\n",
            "I0719 09:51:24.241587 140481689634688 learning.py:507] global step 5819: loss = 0.0193 (0.347 sec/step)\n",
            "I0719 09:51:24.547752 140481689634688 learning.py:507] global step 5820: loss = 0.0385 (0.304 sec/step)\n",
            "I0719 09:51:24.882096 140481689634688 learning.py:507] global step 5821: loss = 0.0459 (0.333 sec/step)\n",
            "I0719 09:51:25.219111 140481689634688 learning.py:507] global step 5822: loss = 0.1549 (0.335 sec/step)\n",
            "I0719 09:51:25.556043 140481689634688 learning.py:507] global step 5823: loss = 0.0332 (0.335 sec/step)\n",
            "I0719 09:51:25.878851 140481689634688 learning.py:507] global step 5824: loss = 0.0315 (0.321 sec/step)\n",
            "I0719 09:51:26.201951 140481689634688 learning.py:507] global step 5825: loss = 0.0360 (0.321 sec/step)\n",
            "I0719 09:51:26.548551 140481689634688 learning.py:507] global step 5826: loss = 0.0745 (0.345 sec/step)\n",
            "I0719 09:51:26.877627 140481689634688 learning.py:507] global step 5827: loss = 0.0538 (0.327 sec/step)\n",
            "I0719 09:51:27.225889 140481689634688 learning.py:507] global step 5828: loss = 0.1242 (0.347 sec/step)\n",
            "I0719 09:51:27.544633 140481689634688 learning.py:507] global step 5829: loss = 0.0488 (0.317 sec/step)\n",
            "I0719 09:51:27.894846 140481689634688 learning.py:507] global step 5830: loss = 0.0747 (0.348 sec/step)\n",
            "I0719 09:51:28.227527 140481689634688 learning.py:507] global step 5831: loss = 0.1021 (0.331 sec/step)\n",
            "I0719 09:51:28.530155 140481689634688 learning.py:507] global step 5832: loss = 0.0405 (0.301 sec/step)\n",
            "I0719 09:51:28.839822 140481689634688 learning.py:507] global step 5833: loss = 0.0876 (0.308 sec/step)\n",
            "I0719 09:51:29.142891 140481689634688 learning.py:507] global step 5834: loss = 0.0407 (0.301 sec/step)\n",
            "I0719 09:51:29.491634 140481689634688 learning.py:507] global step 5835: loss = 0.0582 (0.346 sec/step)\n",
            "I0719 09:51:29.835218 140481689634688 learning.py:507] global step 5836: loss = 0.0697 (0.342 sec/step)\n",
            "I0719 09:51:30.160876 140481689634688 learning.py:507] global step 5837: loss = 0.0472 (0.324 sec/step)\n",
            "I0719 09:51:30.485991 140481689634688 learning.py:507] global step 5838: loss = 0.0122 (0.323 sec/step)\n",
            "I0719 09:51:30.778585 140481689634688 learning.py:507] global step 5839: loss = 0.0784 (0.291 sec/step)\n",
            "I0719 09:51:31.117753 140481689634688 learning.py:507] global step 5840: loss = 0.0265 (0.338 sec/step)\n",
            "I0719 09:51:31.461552 140481689634688 learning.py:507] global step 5841: loss = 0.0446 (0.342 sec/step)\n",
            "I0719 09:51:31.745522 140481689634688 learning.py:507] global step 5842: loss = 0.0668 (0.282 sec/step)\n",
            "I0719 09:51:32.089003 140481689634688 learning.py:507] global step 5843: loss = 0.0681 (0.342 sec/step)\n",
            "I0719 09:51:32.428583 140481689634688 learning.py:507] global step 5844: loss = 0.0216 (0.338 sec/step)\n",
            "I0719 09:51:32.767019 140481689634688 learning.py:507] global step 5845: loss = 0.0529 (0.337 sec/step)\n",
            "I0719 09:51:33.029115 140481689634688 learning.py:507] global step 5846: loss = 0.0342 (0.260 sec/step)\n",
            "I0719 09:51:33.367355 140481689634688 learning.py:507] global step 5847: loss = 0.0707 (0.337 sec/step)\n",
            "I0719 09:51:33.637598 140481689634688 learning.py:507] global step 5848: loss = 0.0273 (0.269 sec/step)\n",
            "I0719 09:51:33.976369 140481689634688 learning.py:507] global step 5849: loss = 0.1348 (0.337 sec/step)\n",
            "I0719 09:51:34.322057 140481689634688 learning.py:507] global step 5850: loss = 0.0392 (0.343 sec/step)\n",
            "I0719 09:51:34.656702 140481689634688 learning.py:507] global step 5851: loss = 0.0548 (0.333 sec/step)\n",
            "I0719 09:51:35.020685 140481689634688 learning.py:507] global step 5852: loss = 0.0831 (0.362 sec/step)\n",
            "I0719 09:51:35.357843 140481689634688 learning.py:507] global step 5853: loss = 0.0565 (0.335 sec/step)\n",
            "I0719 09:51:35.679722 140481689634688 learning.py:507] global step 5854: loss = 0.0549 (0.320 sec/step)\n",
            "I0719 09:51:35.987572 140481689634688 learning.py:507] global step 5855: loss = 0.0867 (0.306 sec/step)\n",
            "I0719 09:51:36.336542 140481689634688 learning.py:507] global step 5856: loss = 0.0108 (0.347 sec/step)\n",
            "I0719 09:51:36.657775 140481689634688 learning.py:507] global step 5857: loss = 0.0760 (0.319 sec/step)\n",
            "I0719 09:51:37.002538 140481689634688 learning.py:507] global step 5858: loss = 0.0847 (0.343 sec/step)\n",
            "I0719 09:51:37.334384 140481689634688 learning.py:507] global step 5859: loss = 0.1253 (0.330 sec/step)\n",
            "I0719 09:51:37.663001 140481689634688 learning.py:507] global step 5860: loss = 0.0143 (0.327 sec/step)\n",
            "I0719 09:51:38.034843 140481689634688 learning.py:507] global step 5861: loss = 0.0195 (0.370 sec/step)\n",
            "I0719 09:51:38.378608 140481689634688 learning.py:507] global step 5862: loss = 0.0487 (0.342 sec/step)\n",
            "I0719 09:51:38.665861 140481689634688 learning.py:507] global step 5863: loss = 0.0755 (0.285 sec/step)\n",
            "I0719 09:51:38.990353 140481689634688 learning.py:507] global step 5864: loss = 0.0433 (0.323 sec/step)\n",
            "I0719 09:51:39.328230 140481689634688 learning.py:507] global step 5865: loss = 0.0661 (0.335 sec/step)\n",
            "I0719 09:51:39.653584 140481689634688 learning.py:507] global step 5866: loss = 0.0639 (0.323 sec/step)\n",
            "I0719 09:51:39.973558 140481689634688 learning.py:507] global step 5867: loss = 0.0491 (0.318 sec/step)\n",
            "I0719 09:51:40.303559 140481689634688 learning.py:507] global step 5868: loss = 0.0301 (0.328 sec/step)\n",
            "I0719 09:51:40.636251 140481689634688 learning.py:507] global step 5869: loss = 0.0455 (0.331 sec/step)\n",
            "I0719 09:51:40.943005 140481689634688 learning.py:507] global step 5870: loss = 0.1467 (0.305 sec/step)\n",
            "I0719 09:51:41.256563 140481689634688 learning.py:507] global step 5871: loss = 0.0567 (0.312 sec/step)\n",
            "I0719 09:51:41.573831 140481689634688 learning.py:507] global step 5872: loss = 0.0108 (0.315 sec/step)\n",
            "I0719 09:51:41.910825 140481689634688 learning.py:507] global step 5873: loss = 0.0503 (0.335 sec/step)\n",
            "I0719 09:51:42.237205 140481689634688 learning.py:507] global step 5874: loss = 0.0517 (0.324 sec/step)\n",
            "I0719 09:51:42.587234 140481689634688 learning.py:507] global step 5875: loss = 0.0443 (0.348 sec/step)\n",
            "I0719 09:51:42.915586 140481689634688 learning.py:507] global step 5876: loss = 0.0900 (0.327 sec/step)\n",
            "I0719 09:51:43.242632 140481689634688 learning.py:507] global step 5877: loss = 0.0450 (0.325 sec/step)\n",
            "I0719 09:51:43.554351 140481689634688 learning.py:507] global step 5878: loss = 0.0566 (0.310 sec/step)\n",
            "I0719 09:51:43.876425 140481689634688 learning.py:507] global step 5879: loss = 0.0337 (0.320 sec/step)\n",
            "I0719 09:51:44.197858 140481689634688 learning.py:507] global step 5880: loss = 0.0658 (0.320 sec/step)\n",
            "I0719 09:51:44.529875 140481689634688 learning.py:507] global step 5881: loss = 0.0512 (0.330 sec/step)\n",
            "I0719 09:51:44.863016 140481689634688 learning.py:507] global step 5882: loss = 0.1128 (0.331 sec/step)\n",
            "I0719 09:51:45.171388 140481689634688 learning.py:507] global step 5883: loss = 0.0242 (0.307 sec/step)\n",
            "I0719 09:51:45.501137 140481689634688 learning.py:507] global step 5884: loss = 0.0472 (0.328 sec/step)\n",
            "I0719 09:51:45.813153 140481689634688 learning.py:507] global step 5885: loss = 0.0820 (0.310 sec/step)\n",
            "I0719 09:51:46.161170 140481689634688 learning.py:507] global step 5886: loss = 0.0874 (0.346 sec/step)\n",
            "I0719 09:51:46.450353 140481689634688 learning.py:507] global step 5887: loss = 0.0670 (0.287 sec/step)\n",
            "I0719 09:51:46.735701 140481689634688 learning.py:507] global step 5888: loss = 0.0689 (0.284 sec/step)\n",
            "I0719 09:51:47.031360 140481689634688 learning.py:507] global step 5889: loss = 0.0714 (0.294 sec/step)\n",
            "I0719 09:51:47.357021 140481689634688 learning.py:507] global step 5890: loss = 0.0601 (0.324 sec/step)\n",
            "I0719 09:51:47.708577 140481689634688 learning.py:507] global step 5891: loss = 0.0487 (0.350 sec/step)\n",
            "I0719 09:51:48.016975 140481689634688 learning.py:507] global step 5892: loss = 0.1348 (0.307 sec/step)\n",
            "I0719 09:51:48.348036 140481689634688 learning.py:507] global step 5893: loss = 0.0604 (0.329 sec/step)\n",
            "I0719 09:51:48.694985 140481689634688 learning.py:507] global step 5894: loss = 0.0518 (0.345 sec/step)\n",
            "I0719 09:51:49.062879 140481689634688 learning.py:507] global step 5895: loss = 0.0462 (0.366 sec/step)\n",
            "I0719 09:51:49.380658 140481689634688 learning.py:507] global step 5896: loss = 0.0178 (0.316 sec/step)\n",
            "I0719 09:51:49.721539 140481689634688 learning.py:507] global step 5897: loss = 0.0854 (0.339 sec/step)\n",
            "I0719 09:51:50.079353 140481689634688 learning.py:507] global step 5898: loss = 0.0489 (0.356 sec/step)\n",
            "I0719 09:51:50.365355 140481689634688 learning.py:507] global step 5899: loss = 0.1111 (0.284 sec/step)\n",
            "I0719 09:51:50.670707 140481689634688 learning.py:507] global step 5900: loss = 0.0598 (0.304 sec/step)\n",
            "I0719 09:51:51.030491 140481689634688 learning.py:507] global step 5901: loss = 0.0401 (0.358 sec/step)\n",
            "I0719 09:51:51.369618 140481689634688 learning.py:507] global step 5902: loss = 0.0737 (0.337 sec/step)\n",
            "I0719 09:51:51.714225 140481689634688 learning.py:507] global step 5903: loss = 0.0646 (0.343 sec/step)\n",
            "I0719 09:51:52.073779 140481689634688 learning.py:507] global step 5904: loss = 0.0595 (0.358 sec/step)\n",
            "I0719 09:51:52.393676 140481689634688 learning.py:507] global step 5905: loss = 0.0587 (0.318 sec/step)\n",
            "I0719 09:51:52.684592 140481689634688 learning.py:507] global step 5906: loss = 0.0477 (0.289 sec/step)\n",
            "I0719 09:51:53.011799 140481689634688 learning.py:507] global step 5907: loss = 0.0213 (0.325 sec/step)\n",
            "I0719 09:51:53.368184 140481689634688 learning.py:507] global step 5908: loss = 0.0477 (0.354 sec/step)\n",
            "I0719 09:51:53.703391 140481689634688 learning.py:507] global step 5909: loss = 0.0490 (0.334 sec/step)\n",
            "I0719 09:51:54.067748 140481689634688 learning.py:507] global step 5910: loss = 0.0683 (0.363 sec/step)\n",
            "I0719 09:51:54.410540 140481689634688 learning.py:507] global step 5911: loss = 0.0756 (0.341 sec/step)\n",
            "I0719 09:51:54.718515 140481689634688 learning.py:507] global step 5912: loss = 0.0449 (0.306 sec/step)\n",
            "I0719 09:51:55.037001 140481689634688 learning.py:507] global step 5913: loss = 0.0732 (0.317 sec/step)\n",
            "I0719 09:51:55.402625 140481689634688 learning.py:507] global step 5914: loss = 0.0341 (0.364 sec/step)\n",
            "I0719 09:51:55.756604 140481689634688 learning.py:507] global step 5915: loss = 0.0383 (0.352 sec/step)\n",
            "I0719 09:51:56.106629 140481689634688 learning.py:507] global step 5916: loss = 0.0407 (0.348 sec/step)\n",
            "I0719 09:51:56.389764 140481689634688 learning.py:507] global step 5917: loss = 0.0503 (0.281 sec/step)\n",
            "I0719 09:51:56.744764 140481689634688 learning.py:507] global step 5918: loss = 0.1013 (0.353 sec/step)\n",
            "I0719 09:51:57.078437 140481689634688 learning.py:507] global step 5919: loss = 0.0545 (0.332 sec/step)\n",
            "I0719 09:51:57.423852 140481689634688 learning.py:507] global step 5920: loss = 0.0478 (0.344 sec/step)\n",
            "I0719 09:51:57.751076 140481689634688 learning.py:507] global step 5921: loss = 0.1046 (0.326 sec/step)\n",
            "I0719 09:51:58.088485 140481689634688 learning.py:507] global step 5922: loss = 0.0488 (0.336 sec/step)\n",
            "I0719 09:51:58.427484 140481689634688 learning.py:507] global step 5923: loss = 0.0569 (0.337 sec/step)\n",
            "I0719 09:51:58.707463 140481689634688 learning.py:507] global step 5924: loss = 0.0247 (0.278 sec/step)\n",
            "I0719 09:51:59.057950 140481689634688 learning.py:507] global step 5925: loss = 0.1233 (0.345 sec/step)\n",
            "I0719 09:51:59.371991 140481689634688 learning.py:507] global step 5926: loss = 0.0231 (0.312 sec/step)\n",
            "I0719 09:51:59.703312 140481689634688 learning.py:507] global step 5927: loss = 0.0812 (0.329 sec/step)\n",
            "I0719 09:52:00.032627 140481689634688 learning.py:507] global step 5928: loss = 0.0702 (0.328 sec/step)\n",
            "I0719 09:52:00.377974 140481689634688 learning.py:507] global step 5929: loss = 0.0734 (0.344 sec/step)\n",
            "I0719 09:52:00.723259 140481689634688 learning.py:507] global step 5930: loss = 0.0494 (0.343 sec/step)\n",
            "I0719 09:52:01.075111 140481689634688 learning.py:507] global step 5931: loss = 0.0914 (0.347 sec/step)\n",
            "I0719 09:52:01.350404 140481689634688 learning.py:507] global step 5932: loss = 0.0512 (0.273 sec/step)\n",
            "I0719 09:52:01.660967 140481689634688 learning.py:507] global step 5933: loss = 0.0423 (0.309 sec/step)\n",
            "I0719 09:52:02.001075 140481689634688 learning.py:507] global step 5934: loss = 0.0412 (0.338 sec/step)\n",
            "I0719 09:52:02.341845 140481689634688 learning.py:507] global step 5935: loss = 0.0649 (0.339 sec/step)\n",
            "I0719 09:52:02.699671 140481689634688 learning.py:507] global step 5936: loss = 0.0880 (0.356 sec/step)\n",
            "I0719 09:52:03.023624 140481689634688 learning.py:507] global step 5937: loss = 0.0830 (0.322 sec/step)\n",
            "I0719 09:52:03.361868 140481689634688 learning.py:507] global step 5938: loss = 0.0746 (0.336 sec/step)\n",
            "I0719 09:52:03.669552 140481689634688 learning.py:507] global step 5939: loss = 0.0379 (0.306 sec/step)\n",
            "I0719 09:52:04.019533 140481689634688 learning.py:507] global step 5940: loss = 0.0539 (0.348 sec/step)\n",
            "I0719 09:52:04.357520 140481689634688 learning.py:507] global step 5941: loss = 0.0672 (0.336 sec/step)\n",
            "I0719 09:52:04.684708 140481689634688 learning.py:507] global step 5942: loss = 0.0783 (0.326 sec/step)\n",
            "I0719 09:52:05.013802 140481689634688 learning.py:507] global step 5943: loss = 0.0234 (0.327 sec/step)\n",
            "I0719 09:52:05.331724 140481689634688 learning.py:507] global step 5944: loss = 0.0454 (0.316 sec/step)\n",
            "I0719 09:52:05.667778 140481689634688 learning.py:507] global step 5945: loss = 0.0672 (0.334 sec/step)\n",
            "I0719 09:52:05.982683 140481689634688 learning.py:507] global step 5946: loss = 0.0201 (0.313 sec/step)\n",
            "I0719 09:52:06.316949 140481689634688 learning.py:507] global step 5947: loss = 0.0225 (0.332 sec/step)\n",
            "I0719 09:52:06.618747 140481689634688 learning.py:507] global step 5948: loss = 0.0562 (0.300 sec/step)\n",
            "I0719 09:52:06.939465 140481689634688 learning.py:507] global step 5949: loss = 0.0667 (0.319 sec/step)\n",
            "I0719 09:52:07.285022 140481689634688 learning.py:507] global step 5950: loss = 0.0543 (0.344 sec/step)\n",
            "I0719 09:52:07.600633 140481689634688 learning.py:507] global step 5951: loss = 0.1024 (0.314 sec/step)\n",
            "I0719 09:52:07.947734 140481689634688 learning.py:507] global step 5952: loss = 0.0886 (0.345 sec/step)\n",
            "I0719 09:52:08.289875 140481689634688 learning.py:507] global step 5953: loss = 0.0346 (0.340 sec/step)\n",
            "I0719 09:52:08.593825 140481689634688 learning.py:507] global step 5954: loss = 0.0234 (0.302 sec/step)\n",
            "I0719 09:52:08.952122 140481689634688 learning.py:507] global step 5955: loss = 0.0330 (0.357 sec/step)\n",
            "I0719 09:52:09.312543 140481689634688 learning.py:507] global step 5956: loss = 0.0499 (0.359 sec/step)\n",
            "I0719 09:52:09.664073 140481689634688 learning.py:507] global step 5957: loss = 0.0751 (0.350 sec/step)\n",
            "I0719 09:52:10.004431 140481689634688 learning.py:507] global step 5958: loss = 0.0796 (0.339 sec/step)\n",
            "I0719 09:52:10.349009 140481689634688 learning.py:507] global step 5959: loss = 0.0196 (0.343 sec/step)\n",
            "I0719 09:52:10.679613 140481689634688 learning.py:507] global step 5960: loss = 0.0819 (0.329 sec/step)\n",
            "I0719 09:52:11.006787 140481689634688 learning.py:507] global step 5961: loss = 0.0537 (0.325 sec/step)\n",
            "I0719 09:52:11.361299 140481689634688 learning.py:507] global step 5962: loss = 0.0660 (0.353 sec/step)\n",
            "I0719 09:52:11.696607 140481689634688 learning.py:507] global step 5963: loss = 0.0869 (0.334 sec/step)\n",
            "I0719 09:52:12.051398 140481689634688 learning.py:507] global step 5964: loss = 0.1392 (0.353 sec/step)\n",
            "I0719 09:52:12.393779 140481689634688 learning.py:507] global step 5965: loss = 0.0394 (0.341 sec/step)\n",
            "I0719 09:52:12.721190 140481689634688 learning.py:507] global step 5966: loss = 0.0273 (0.326 sec/step)\n",
            "I0719 09:52:13.062487 140481689634688 learning.py:507] global step 5967: loss = 0.0940 (0.339 sec/step)\n",
            "I0719 09:52:13.416357 140481689634688 learning.py:507] global step 5968: loss = 0.0626 (0.352 sec/step)\n",
            "I0719 09:52:13.776857 140481689634688 learning.py:507] global step 5969: loss = 0.0304 (0.359 sec/step)\n",
            "I0719 09:52:14.102387 140481689634688 learning.py:507] global step 5970: loss = 0.1128 (0.324 sec/step)\n",
            "I0719 09:52:14.437405 140481689634688 learning.py:507] global step 5971: loss = 0.0582 (0.333 sec/step)\n",
            "I0719 09:52:14.777082 140481689634688 learning.py:507] global step 5972: loss = 0.0700 (0.338 sec/step)\n",
            "I0719 09:52:15.120753 140481689634688 learning.py:507] global step 5973: loss = 0.0650 (0.342 sec/step)\n",
            "I0719 09:52:15.472261 140481689634688 learning.py:507] global step 5974: loss = 0.0936 (0.350 sec/step)\n",
            "I0719 09:52:15.823047 140481689634688 learning.py:507] global step 5975: loss = 0.0376 (0.349 sec/step)\n",
            "I0719 09:52:16.169497 140481689634688 learning.py:507] global step 5976: loss = 0.1567 (0.345 sec/step)\n",
            "I0719 09:52:16.508080 140481689634688 learning.py:507] global step 5977: loss = 0.0804 (0.337 sec/step)\n",
            "I0719 09:52:16.815509 140481689634688 learning.py:507] global step 5978: loss = 0.0468 (0.306 sec/step)\n",
            "I0719 09:52:17.169303 140481689634688 learning.py:507] global step 5979: loss = 0.1302 (0.352 sec/step)\n",
            "I0719 09:52:17.498777 140481689634688 learning.py:507] global step 5980: loss = 0.0728 (0.328 sec/step)\n",
            "I0719 09:52:17.836968 140481689634688 learning.py:507] global step 5981: loss = 0.0610 (0.336 sec/step)\n",
            "I0719 09:52:18.191806 140481689634688 learning.py:507] global step 5982: loss = 0.1153 (0.353 sec/step)\n",
            "I0719 09:52:18.513897 140481689634688 learning.py:507] global step 5983: loss = 0.0545 (0.320 sec/step)\n",
            "I0719 09:52:18.841041 140481689634688 learning.py:507] global step 5984: loss = 0.0870 (0.325 sec/step)\n",
            "I0719 09:52:19.175261 140481689634688 learning.py:507] global step 5985: loss = 0.0228 (0.332 sec/step)\n",
            "I0719 09:52:19.514618 140481689634688 learning.py:507] global step 5986: loss = 0.0406 (0.337 sec/step)\n",
            "I0719 09:52:19.843588 140481689634688 learning.py:507] global step 5987: loss = 0.0739 (0.327 sec/step)\n",
            "I0719 09:52:20.165992 140481689634688 learning.py:507] global step 5988: loss = 0.0555 (0.321 sec/step)\n",
            "I0719 09:52:20.500105 140481689634688 learning.py:507] global step 5989: loss = 0.0752 (0.332 sec/step)\n",
            "I0719 09:52:20.815027 140481689634688 learning.py:507] global step 5990: loss = 0.1538 (0.313 sec/step)\n",
            "I0719 09:52:21.156904 140481689634688 learning.py:507] global step 5991: loss = 0.0697 (0.340 sec/step)\n",
            "I0719 09:52:21.461331 140481689634688 learning.py:507] global step 5992: loss = 0.0991 (0.303 sec/step)\n",
            "I0719 09:52:21.791160 140481689634688 learning.py:507] global step 5993: loss = 0.0508 (0.328 sec/step)\n",
            "I0719 09:52:22.116480 140481689634688 learning.py:507] global step 5994: loss = 0.0264 (0.324 sec/step)\n",
            "I0719 09:52:22.448225 140481689634688 learning.py:507] global step 5995: loss = 0.0590 (0.329 sec/step)\n",
            "I0719 09:52:22.786039 140481689634688 learning.py:507] global step 5996: loss = 0.0648 (0.335 sec/step)\n",
            "I0719 09:52:23.051016 140481689634688 learning.py:507] global step 5997: loss = 0.1174 (0.263 sec/step)\n",
            "I0719 09:52:23.386557 140481689634688 learning.py:507] global step 5998: loss = 0.0511 (0.334 sec/step)\n",
            "I0719 09:52:23.723838 140481689634688 learning.py:507] global step 5999: loss = 0.0616 (0.336 sec/step)\n",
            "I0719 09:52:24.054679 140481689634688 learning.py:507] global step 6000: loss = 0.0288 (0.327 sec/step)\n",
            "I0719 09:52:24.375331 140481689634688 learning.py:507] global step 6001: loss = 0.0354 (0.319 sec/step)\n",
            "I0719 09:52:24.715340 140481689634688 learning.py:507] global step 6002: loss = 0.0591 (0.338 sec/step)\n",
            "I0719 09:52:25.060512 140481689634688 learning.py:507] global step 6003: loss = 0.0256 (0.343 sec/step)\n",
            "I0719 09:52:25.401945 140481689634688 learning.py:507] global step 6004: loss = 0.0789 (0.339 sec/step)\n",
            "I0719 09:52:25.680334 140481689634688 learning.py:507] global step 6005: loss = 0.0304 (0.276 sec/step)\n",
            "I0719 09:52:26.047294 140481689634688 learning.py:507] global step 6006: loss = 0.0588 (0.365 sec/step)\n",
            "I0719 09:52:26.551018 140481689634688 learning.py:507] global step 6007: loss = 0.0261 (0.454 sec/step)\n",
            "I0719 09:52:27.031625 140481689634688 learning.py:507] global step 6008: loss = 0.1475 (0.475 sec/step)\n",
            "I0719 09:52:27.417870 140481689634688 learning.py:507] global step 6009: loss = 0.0471 (0.367 sec/step)\n",
            "I0719 09:52:27.422966 140479018202880 supervisor.py:1050] Recording summary at step 6009.\n",
            "I0719 09:52:27.772857 140481689634688 learning.py:507] global step 6010: loss = 0.0503 (0.353 sec/step)\n",
            "I0719 09:52:28.106187 140481689634688 learning.py:507] global step 6011: loss = 0.0704 (0.332 sec/step)\n",
            "I0719 09:52:28.450552 140481689634688 learning.py:507] global step 6012: loss = 0.0559 (0.343 sec/step)\n",
            "I0719 09:52:28.778825 140481689634688 learning.py:507] global step 6013: loss = 0.0194 (0.326 sec/step)\n",
            "I0719 09:52:29.122660 140481689634688 learning.py:507] global step 6014: loss = 0.0357 (0.342 sec/step)\n",
            "I0719 09:52:29.473435 140481689634688 learning.py:507] global step 6015: loss = 0.0385 (0.349 sec/step)\n",
            "I0719 09:52:29.830040 140481689634688 learning.py:507] global step 6016: loss = 0.1012 (0.354 sec/step)\n",
            "I0719 09:52:30.175928 140481689634688 learning.py:507] global step 6017: loss = 0.0217 (0.344 sec/step)\n",
            "I0719 09:52:30.513305 140481689634688 learning.py:507] global step 6018: loss = 0.0394 (0.336 sec/step)\n",
            "I0719 09:52:30.842455 140481689634688 learning.py:507] global step 6019: loss = 0.0420 (0.327 sec/step)\n",
            "I0719 09:52:31.218101 140481689634688 learning.py:507] global step 6020: loss = 0.0283 (0.374 sec/step)\n",
            "I0719 09:52:31.560514 140481689634688 learning.py:507] global step 6021: loss = 0.0740 (0.341 sec/step)\n",
            "I0719 09:52:31.884995 140481689634688 learning.py:507] global step 6022: loss = 0.0737 (0.321 sec/step)\n",
            "I0719 09:52:32.255641 140481689634688 learning.py:507] global step 6023: loss = 0.0409 (0.369 sec/step)\n",
            "I0719 09:52:32.607731 140481689634688 learning.py:507] global step 6024: loss = 0.0465 (0.350 sec/step)\n",
            "I0719 09:52:32.949417 140481689634688 learning.py:507] global step 6025: loss = 0.0870 (0.340 sec/step)\n",
            "I0719 09:52:33.290720 140481689634688 learning.py:507] global step 6026: loss = 0.0902 (0.340 sec/step)\n",
            "I0719 09:52:33.632336 140481689634688 learning.py:507] global step 6027: loss = 0.0602 (0.340 sec/step)\n",
            "I0719 09:52:33.956951 140481689634688 learning.py:507] global step 6028: loss = 0.0538 (0.323 sec/step)\n",
            "I0719 09:52:34.255635 140481689634688 learning.py:507] global step 6029: loss = 0.0702 (0.297 sec/step)\n",
            "I0719 09:52:34.612741 140481689634688 learning.py:507] global step 6030: loss = 0.0288 (0.355 sec/step)\n",
            "I0719 09:52:34.938806 140481689634688 learning.py:507] global step 6031: loss = 0.0465 (0.324 sec/step)\n",
            "I0719 09:52:35.308955 140481689634688 learning.py:507] global step 6032: loss = 0.0323 (0.368 sec/step)\n",
            "I0719 09:52:35.625924 140481689634688 learning.py:507] global step 6033: loss = 0.0511 (0.315 sec/step)\n",
            "I0719 09:52:35.969869 140481689634688 learning.py:507] global step 6034: loss = 0.0426 (0.342 sec/step)\n",
            "I0719 09:52:36.333079 140481689634688 learning.py:507] global step 6035: loss = 0.0456 (0.362 sec/step)\n",
            "I0719 09:52:36.683741 140481689634688 learning.py:507] global step 6036: loss = 0.1034 (0.349 sec/step)\n",
            "I0719 09:52:37.011318 140481689634688 learning.py:507] global step 6037: loss = 0.0657 (0.326 sec/step)\n",
            "I0719 09:52:37.367059 140481689634688 learning.py:507] global step 6038: loss = 0.0606 (0.354 sec/step)\n",
            "I0719 09:52:37.706293 140481689634688 learning.py:507] global step 6039: loss = 0.0716 (0.337 sec/step)\n",
            "I0719 09:52:38.051744 140481689634688 learning.py:507] global step 6040: loss = 0.0374 (0.344 sec/step)\n",
            "I0719 09:52:38.363625 140481689634688 learning.py:507] global step 6041: loss = 0.0275 (0.310 sec/step)\n",
            "I0719 09:52:38.724734 140481689634688 learning.py:507] global step 6042: loss = 0.0531 (0.359 sec/step)\n",
            "I0719 09:52:39.073848 140481689634688 learning.py:507] global step 6043: loss = 0.0275 (0.347 sec/step)\n",
            "I0719 09:52:39.426099 140481689634688 learning.py:507] global step 6044: loss = 0.0428 (0.351 sec/step)\n",
            "I0719 09:52:39.770232 140481689634688 learning.py:507] global step 6045: loss = 0.0436 (0.342 sec/step)\n",
            "I0719 09:52:40.109176 140481689634688 learning.py:507] global step 6046: loss = 0.0554 (0.337 sec/step)\n",
            "I0719 09:52:40.444026 140481689634688 learning.py:507] global step 6047: loss = 0.1219 (0.333 sec/step)\n",
            "I0719 09:52:40.775933 140481689634688 learning.py:507] global step 6048: loss = 0.0633 (0.330 sec/step)\n",
            "I0719 09:52:41.105361 140481689634688 learning.py:507] global step 6049: loss = 0.0484 (0.328 sec/step)\n",
            "I0719 09:52:41.439460 140481689634688 learning.py:507] global step 6050: loss = 0.0323 (0.332 sec/step)\n",
            "I0719 09:52:41.754425 140481689634688 learning.py:507] global step 6051: loss = 0.0895 (0.313 sec/step)\n",
            "I0719 09:52:42.134954 140481689634688 learning.py:507] global step 6052: loss = 0.0328 (0.379 sec/step)\n",
            "I0719 09:52:42.491631 140481689634688 learning.py:507] global step 6053: loss = 0.0812 (0.355 sec/step)\n",
            "I0719 09:52:42.822944 140481689634688 learning.py:507] global step 6054: loss = 0.0681 (0.329 sec/step)\n",
            "I0719 09:52:43.182544 140481689634688 learning.py:507] global step 6055: loss = 0.0389 (0.358 sec/step)\n",
            "I0719 09:52:43.498873 140481689634688 learning.py:507] global step 6056: loss = 0.0497 (0.315 sec/step)\n",
            "I0719 09:52:43.822063 140481689634688 learning.py:507] global step 6057: loss = 0.0300 (0.321 sec/step)\n",
            "I0719 09:52:44.173541 140481689634688 learning.py:507] global step 6058: loss = 0.0567 (0.350 sec/step)\n",
            "I0719 09:52:44.512217 140481689634688 learning.py:507] global step 6059: loss = 0.0720 (0.337 sec/step)\n",
            "I0719 09:52:44.809786 140481689634688 learning.py:507] global step 6060: loss = 0.0275 (0.296 sec/step)\n",
            "I0719 09:52:45.161429 140481689634688 learning.py:507] global step 6061: loss = 0.0449 (0.350 sec/step)\n",
            "I0719 09:52:45.496789 140481689634688 learning.py:507] global step 6062: loss = 0.0767 (0.334 sec/step)\n",
            "I0719 09:52:45.836963 140481689634688 learning.py:507] global step 6063: loss = 0.0535 (0.338 sec/step)\n",
            "I0719 09:52:46.189041 140481689634688 learning.py:507] global step 6064: loss = 0.0425 (0.350 sec/step)\n",
            "I0719 09:52:46.536177 140481689634688 learning.py:507] global step 6065: loss = 0.0344 (0.346 sec/step)\n",
            "I0719 09:52:46.815148 140481689634688 learning.py:507] global step 6066: loss = 0.0583 (0.277 sec/step)\n",
            "I0719 09:52:47.160322 140481689634688 learning.py:507] global step 6067: loss = 0.0440 (0.343 sec/step)\n",
            "I0719 09:52:47.479421 140481689634688 learning.py:507] global step 6068: loss = 0.0351 (0.317 sec/step)\n",
            "I0719 09:52:47.821085 140481689634688 learning.py:507] global step 6069: loss = 0.0553 (0.340 sec/step)\n",
            "I0719 09:52:48.165936 140481689634688 learning.py:507] global step 6070: loss = 0.0509 (0.343 sec/step)\n",
            "I0719 09:52:48.484992 140481689634688 learning.py:507] global step 6071: loss = 0.1328 (0.317 sec/step)\n",
            "I0719 09:52:48.838568 140481689634688 learning.py:507] global step 6072: loss = 0.0124 (0.352 sec/step)\n",
            "I0719 09:52:49.172127 140481689634688 learning.py:507] global step 6073: loss = 0.1017 (0.332 sec/step)\n",
            "I0719 09:52:49.507055 140481689634688 learning.py:507] global step 6074: loss = 0.0659 (0.333 sec/step)\n",
            "I0719 09:52:49.866974 140481689634688 learning.py:507] global step 6075: loss = 0.0407 (0.358 sec/step)\n",
            "I0719 09:52:50.197304 140481689634688 learning.py:507] global step 6076: loss = 0.1616 (0.329 sec/step)\n",
            "I0719 09:52:50.527663 140481689634688 learning.py:507] global step 6077: loss = 0.0336 (0.328 sec/step)\n",
            "I0719 09:52:50.900461 140481689634688 learning.py:507] global step 6078: loss = 0.1116 (0.371 sec/step)\n",
            "I0719 09:52:51.239613 140481689634688 learning.py:507] global step 6079: loss = 0.0491 (0.337 sec/step)\n",
            "I0719 09:52:51.593538 140481689634688 learning.py:507] global step 6080: loss = 0.0608 (0.352 sec/step)\n",
            "I0719 09:52:51.936422 140481689634688 learning.py:507] global step 6081: loss = 0.0584 (0.341 sec/step)\n",
            "I0719 09:52:52.260461 140481689634688 learning.py:507] global step 6082: loss = 0.0481 (0.322 sec/step)\n",
            "I0719 09:52:52.600258 140481689634688 learning.py:507] global step 6083: loss = 0.0386 (0.338 sec/step)\n",
            "I0719 09:52:52.930423 140481689634688 learning.py:507] global step 6084: loss = 0.0938 (0.328 sec/step)\n",
            "I0719 09:52:53.210637 140481689634688 learning.py:507] global step 6085: loss = 0.0809 (0.278 sec/step)\n",
            "I0719 09:52:53.551381 140481689634688 learning.py:507] global step 6086: loss = 0.0114 (0.339 sec/step)\n",
            "I0719 09:52:53.916394 140481689634688 learning.py:507] global step 6087: loss = 0.0335 (0.363 sec/step)\n",
            "I0719 09:52:54.235073 140481689634688 learning.py:507] global step 6088: loss = 0.0568 (0.317 sec/step)\n",
            "I0719 09:52:54.575051 140481689634688 learning.py:507] global step 6089: loss = 0.0442 (0.338 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 416, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "outputId": "59d2350e-5697-497d-c4ee-34bf72629ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!tensorboard --logdir=training"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorBoard 1.14.0 at http://6489ddc33fb0:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "06f75744-5269-4bfe-e63e-6a543529dcfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-4930 --output_directory inference_graph"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 09:55:21.548138 140258274916224 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0719 09:55:21.561512 140258274916224 deprecation_wrapper.py:119] From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0719 09:55:21.574060 140258274916224 deprecation_wrapper.py:119] From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0719 09:55:21.574779 140258274916224 deprecation_wrapper.py:119] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0719 09:55:21.581574 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0719 09:55:21.581812 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0719 09:55:21.639301 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2412: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0719 09:55:21.708781 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py:166: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0719 09:55:21.722885 140258274916224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0719 09:55:24.565886 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "I0719 09:55:24.577324 140258274916224 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "W0719 09:55:24.577721 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py:553: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "I0719 09:55:24.598898 140258274916224 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "W0719 09:55:24.599366 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0719 09:55:24.599502 140258274916224 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0719 09:55:24.803832 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:141: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0719 09:55:25.656151 140258274916224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/spatial_transform_ops.py:418: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "W0719 09:55:26.471783 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "I0719 09:55:26.724979 140258274916224 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "I0719 09:55:27.234455 140258274916224 regularizers.py:98] Scale of 0 disables regularizer.\n",
            "W0719 09:55:28.444162 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0719 09:55:28.448751 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0719 09:55:28.450110 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "247 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/12.84m params)\n",
            "  Conv (--/2.65m params)\n",
            "    Conv/biases (512, 512/512 params)\n",
            "    Conv/weights (3x3x576x512, 2.65m/2.65m params)\n",
            "  FirstStageBoxPredictor (--/36.94k params)\n",
            "    FirstStageBoxPredictor/BoxEncodingPredictor (--/24.62k params)\n",
            "      FirstStageBoxPredictor/BoxEncodingPredictor/biases (48, 48/48 params)\n",
            "      FirstStageBoxPredictor/BoxEncodingPredictor/weights (1x1x512x48, 24.58k/24.58k params)\n",
            "    FirstStageBoxPredictor/ClassPredictor (--/12.31k params)\n",
            "      FirstStageBoxPredictor/ClassPredictor/biases (24, 24/24 params)\n",
            "      FirstStageBoxPredictor/ClassPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "  FirstStageFeatureExtractor (--/4.25m params)\n",
            "    FirstStageFeatureExtractor/InceptionV2 (--/4.25m params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7 (--/2.71k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm (--/0 params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights (7x7x3x8, 1.18k/1.18k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights (1x1x24x64, 1.54k/1.54k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1 (--/4.10k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm (--/0 params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights (1x1x64x64, 4.10k/4.10k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3 (--/110.59k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm (--/0 params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights (3x3x64x192, 110.59k/110.59k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_3b (--/218.11k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0 (--/12.29k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1 (--/12.29k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1 (--/49.15k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1 (--/12.29k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3 (--/36.86k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x64, 36.86k/36.86k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2 (--/150.53k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1 (--/12.29k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3 (--/6.14k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1 (--/6.14k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_3c (--/259.07k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0 (--/16.38k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1 (--/16.38k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1 (--/71.68k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1 (--/16.38k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2 (--/154.62k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1 (--/16.38k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3 (--/16.38k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1 (--/16.38k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_4a (--/384.00k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0 (--/225.28k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1 (--/40.96k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights (1x1x320x128, 40.96k/40.96k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3 (--/184.32k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1 (--/158.72k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1 (--/20.48k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights (1x1x320x64, 20.48k/20.48k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3 (--/82.94k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_4b (--/608.26k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0 (--/129.02k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1 (--/129.02k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights (1x1x576x224, 129.02k/129.02k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1 (--/92.16k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1 (--/36.86k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights (1x1x576x64, 36.86k/36.86k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2 (--/313.34k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3 (--/73.73k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_4c (--/663.55k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0 (--/110.59k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1 (--/110.59k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1 (--/165.89k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3 (--/110.59k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2 (--/313.34k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3 (--/73.73k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_4d (--/893.95k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0 (--/92.16k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1 (--/92.16k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1 (--/258.05k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3 (--/184.32k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2 (--/488.45k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1 (--/73.73k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3 (--/184.32k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3 (--/230.40k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights (3x3x160x160, 230.40k/230.40k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3 (--/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FirstStageFeatureExtractor/InceptionV2/Mixed_4e (--/1.11m params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0 (--/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1 (--/294.91k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3 (--/221.18k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2 (--/700.42k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1 (--/92.16k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3 (--/276.48k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights (3x3x160x192, 276.48k/276.48k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3 (--/331.78k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights (3x3x192x192, 331.78k/331.78k params)\n",
            "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3 (--/55.30k params)\n",
            "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "  SecondStageBoxPredictor (--/6.15k params)\n",
            "    SecondStageBoxPredictor/BoxEncodingPredictor (--/4.10k params)\n",
            "      SecondStageBoxPredictor/BoxEncodingPredictor/biases (4, 4/4 params)\n",
            "      SecondStageBoxPredictor/BoxEncodingPredictor/weights (1024x4, 4.10k/4.10k params)\n",
            "    SecondStageBoxPredictor/ClassPredictor (--/2.05k params)\n",
            "      SecondStageBoxPredictor/ClassPredictor/biases (2, 2/2 params)\n",
            "      SecondStageBoxPredictor/ClassPredictor/weights (1024x2, 2.05k/2.05k params)\n",
            "  SecondStageFeatureExtractor (--/5.89m params)\n",
            "    SecondStageFeatureExtractor/InceptionV2 (--/5.89m params)\n",
            "      SecondStageFeatureExtractor/InceptionV2/Mixed_5a (--/1.44m params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0 (--/294.91k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1 (--/73.73k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3 (--/221.18k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1 (--/1.14m params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1 (--/110.59k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3 (--/442.37k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights (3x3x192x256, 442.37k/442.37k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3 (--/589.82k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      SecondStageFeatureExtractor/InceptionV2/Mixed_5b (--/2.18m params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0 (--/360.45k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1 (--/749.57k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2 (--/937.98k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1 (--/163.84k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x160, 163.84k/163.84k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3 (--/322.56k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights (3x3x160x224, 322.56k/322.56k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3 (--/131.07k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n",
            "      SecondStageFeatureExtractor/InceptionV2/Mixed_5c (--/2.28m params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0 (--/360.45k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1 (--/749.57k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2 (--/1.04m params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1 (--/196.61k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3 (--/387.07k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights (3x3x192x224, 387.07k/387.07k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n",
            "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3 (--/131.07k params)\n",
            "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
            "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "247 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/6.18k flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_3 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n",
            "  map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n",
            "  map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n",
            "  map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n",
            "  map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_3 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_2 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_1 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_2 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_1 (300/300 flops)\n",
            "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum (300/300 flops)\n",
            "  map_2/while/mul (300/300 flops)\n",
            "  map_2/while/mul_1 (300/300 flops)\n",
            "  map_2/while/mul_2 (300/300 flops)\n",
            "  map_2/while/mul_3 (300/300 flops)\n",
            "  GridAnchorGenerator/mul (12/12 flops)\n",
            "  GridAnchorGenerator/mul_1 (12/12 flops)\n",
            "  GridAnchorGenerator/mul_2 (12/12 flops)\n",
            "  GridAnchorGenerator/truediv (12/12 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
            "  mul (1/1 flops)\n",
            "  map_2/while/add_1 (1/1 flops)\n",
            "  map_2/while/add (1/1 flops)\n",
            "  map_2/while/Less_1 (1/1 flops)\n",
            "  map_2/while/Less (1/1 flops)\n",
            "  map_1/while/add_1 (1/1 flops)\n",
            "  map_1/while/add (1/1 flops)\n",
            "  map_1/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
            "  map_1/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
            "  map_1/while/Less_1 (1/1 flops)\n",
            "  map_1/while/Less (1/1 flops)\n",
            "  map/while/add_1 (1/1 flops)\n",
            "  map/while/add (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
            "  map/while/Less_1 (1/1 flops)\n",
            "  map/while/Less (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/add_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/add (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/Less_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/map/while/Less (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul_1 (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv_1 (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  SecondStageDetectionFeaturesExtract/mul (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize_1/Minimum (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  GridAnchorGenerator/zeros/Less (1/1 flops)\n",
            "  BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  FirstStageFeatureExtractor/GreaterEqual (1/1 flops)\n",
            "  FirstStageFeatureExtractor/GreaterEqual_1 (1/1 flops)\n",
            "  GridAnchorGenerator/add_3 (1/1 flops)\n",
            "  GridAnchorGenerator/add_4 (1/1 flops)\n",
            "  GridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  GridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  GridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/Less (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize/mul (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize/mul_1 (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize/truediv (1/1 flops)\n",
            "  Preprocessor/map/while/ResizeToRange/cond/resize/truediv_1 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "W0719 09:55:30.013377 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-07-19 09:55:31.372681: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-19 09:55:31.391700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.392139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-19 09:55:31.392496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:55:31.393837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:55:31.395136: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-19 09:55:31.395526: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-19 09:55:31.397169: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-19 09:55:31.398541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-19 09:55:31.402171: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-19 09:55:31.402347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.402859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.403305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-19 09:55:31.409311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-19 09:55:31.409615: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29f72c0 executing computations on platform Host. Devices:\n",
            "2019-07-19 09:55:31.409654: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-19 09:55:31.481435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.481959: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29f5dc0 executing computations on platform CUDA. Devices:\n",
            "2019-07-19 09:55:31.481996: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-19 09:55:31.482253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.482693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-19 09:55:31.482786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:55:31.482829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:55:31.482871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-19 09:55:31.482916: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-19 09:55:31.482955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-19 09:55:31.482992: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-19 09:55:31.483030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-19 09:55:31.483155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.483656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.484013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-19 09:55:31.484093: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:55:31.485366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-19 09:55:31.485401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-19 09:55:31.485419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-19 09:55:31.485748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.486238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:31.486624: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-19 09:55:31.486686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0719 09:55:31.487786 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0719 09:55:31.489303 140258274916224 saver.py:1280] Restoring parameters from training/model.ckpt-4930\n",
            "2019-07-19 09:55:34.429572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:34.430007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-19 09:55:34.430092: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:55:34.430136: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:55:34.430202: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-19 09:55:34.430244: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-19 09:55:34.430306: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-19 09:55:34.430360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-19 09:55:34.430398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-19 09:55:34.430529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:34.430989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:34.431369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-19 09:55:34.431421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-19 09:55:34.431445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-19 09:55:34.431464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-19 09:55:34.431764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:34.432215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:34.432610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "I0719 09:55:34.433911 140258274916224 saver.py:1280] Restoring parameters from training/model.ckpt-4930\n",
            "W0719 09:55:35.151761 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0719 09:55:35.152040 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "I0719 09:55:35.531397 140258274916224 graph_util_impl.py:311] Froze 356 variables.\n",
            "I0719 09:55:35.671965 140258274916224 graph_util_impl.py:364] Converted 356 variables to const ops.\n",
            "2019-07-19 09:55:35.906934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:35.907419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-19 09:55:35.907510: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-19 09:55:35.907554: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-19 09:55:35.907594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-19 09:55:35.907637: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-19 09:55:35.907687: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-19 09:55:35.907728: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-19 09:55:35.907767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-19 09:55:35.907898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:35.908374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:35.908737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-19 09:55:35.908788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-19 09:55:35.908810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-19 09:55:35.908825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-19 09:55:35.909134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:35.909758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-19 09:55:35.910124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0719 09:55:36.670038 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0719 09:55:36.670666 140258274916224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0719 09:55:36.671259 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0719 09:55:36.671470 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0719 09:55:36.671707 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0719 09:55:36.671858 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:307: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0719 09:55:36.672176 140258274916224 builder_impl.py:636] No assets to save.\n",
            "I0719 09:55:36.672311 140258274916224 builder_impl.py:456] No assets to write.\n",
            "I0719 09:55:37.159800 140258274916224 builder_impl.py:421] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "W0719 09:55:37.217640 140258274916224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0719 09:55:37.218010 140258274916224 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cat /content/models/research/object_detection/Object_detection_image.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /content/models/research/object_detection/in/1.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "outputId": "7e33bfdb-fcfd-42c7-97a1-ea5a6f8b4f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = '1.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=8,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAJcCAIAAAAuNv2aAAEAAElEQVR4nOz9eZAkWX7fB/5+v/ee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\nrONutPX/DyPxhV1YYi0AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=900x604 at 0x7F944241F5C0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7zIhZ6Oki6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /content/models/research/object_detection/Train-Object-Detection-Classifier/in/1.jpg /content/models/research/object_detection/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}