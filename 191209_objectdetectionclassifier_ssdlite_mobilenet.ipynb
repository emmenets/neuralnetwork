{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmenets/neuralnetwork/blob/master/191209_objectdetectionclassifier_ssdlite_mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7Dc-iLJzbt",
        "colab_type": "code",
        "outputId": "ef70dd6f-37b5-4b3b-a1b9-7bdd30812920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXT3U_hX49xr",
        "colab_type": "code",
        "outputId": "dc4e38d8-7b7c-45e0-bf6a-11f287dad16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtUXvjZr7tSF",
        "colab_type": "code",
        "outputId": "aee65525-36b5-47f2-dcfe-2fb0d40a4c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbBtNJnYqZx",
        "colab_type": "code",
        "outputId": "2a893864-6841-4bc7-8113-473695f50f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-09 10:32:57--  http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.119.128, 2a00:1450:4013:c00::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.119.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51025348 (49M) [application/x-tar]\n",
            "Saving to: ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’\n",
            "\n",
            "\r          ssdlite_m   0%[                    ]       0  --.-KB/s               \r         ssdlite_mo   8%[>                   ]   4.01M  14.9MB/s               \r        ssdlite_mob  70%[=============>      ]  34.37M  73.4MB/s               \rssdlite_mobilenet_v 100%[===================>]  48.66M  82.9MB/s    in 0.6s    \n",
            "\n",
            "2019-12-09 10:32:58 (82.9 MB/s) - ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz’ saved [51025348/51025348]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVw6V940Y5Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawmnNo8lsSA",
        "colab_type": "code",
        "outputId": "150061a8-3c4c-4e2e-d02b-74735b26a66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!git clone https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10'...\n",
            "remote: Enumerating objects: 1129, done.\u001b[K\n",
            "remote: Total 1129 (delta 0), reused 0 (delta 0), pack-reused 1129\u001b[K\n",
            "Receiving objects: 100% (1129/1129), 57.63 MiB | 20.08 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvKNQUWBHMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/doc /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images /content/models/research/object_detection\n",
        "#!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/inference_graph /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/training /content/models/research/object_detection\n",
        "!cp -R /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/translate /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/generate_tfrecord.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_image.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_video.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/Object_detection_webcam.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/resizer.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test.mov /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/test1.JPG /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/xml_to_csv.py /content/models/research/object_detection\n",
        "!cp  /content/models/research/object_detection/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/README.md /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNvAfoLAzM4j",
        "colab_type": "code",
        "outputId": "5e8692c9-7077-4560-fd0d-00d04f17358f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdPn1OSJnXWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanL9KzkDpYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ2muKeD2BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQsHlyfvM11K",
        "colab_type": "code",
        "outputId": "601cf861-2ebb-41a5-aa6c-05dc6ce198ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/models/research/object_detection"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c43cQ2UVyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images\n",
        "!mkdir inference_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6n8GqZS-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/test /content/models/research/object_detection/images\n",
        "!cp -r /gdrive/My\\ Drive/colabfiles/lektion31/images/train /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/test_labels.csv /content/models/research/object_detection/images\n",
        "!cp /gdrive/My\\ Drive/colabfiles/lektion31/train_labels.csv /content/models/research/object_detection/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEgw5L1C3Xw",
        "colab_type": "code",
        "outputId": "3d693251-3801-4353-e547-d55b66d304e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile generate_tfrecord.py\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=images/test_labels.csv  --image_dir=images/test --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
        "flags.DEFINE_string('image_dir', '', 'Path to the image directory')\n",
        "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'rasti':\n",
        "        return 1\n",
        "\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
        "    path = os.path.join(os.getcwd(), FLAGS.image_dir)\n",
        "    examples = pd.read_csv(FLAGS.csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting generate_tfrecord.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsxFKOByIQcz",
        "colab_type": "code",
        "outputId": "3ace2579-8fd5-4b27-fcc3-6391f32f647e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1209 10:42:54.428231 140685356410752 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1209 10:42:54.483097 140685356410752 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_6ZGWPJTZ3",
        "colab_type": "code",
        "outputId": "16815228-35f4-484f-d608-5114c42b0c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From generate_tfrecord.py:102: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W1209 10:43:00.452557 140646703986560 module_wrapper.py:139] From generate_tfrecord.py:88: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1209 10:43:00.496000 140646703986560 module_wrapper.py:139] From generate_tfrecord.py:47: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82tXS2NDgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/models/research/object_detection/training/faster_rcnn_inception_v2_pets.config\n",
        "!rm /content/models/research/object_detection/training/labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jictvdm5SqHU",
        "colab_type": "code",
        "outputId": "0794c530-20a2-4d5a-f6c4-67c77e357df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'rasti'\n",
        "}\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ayapmyjQt4W",
        "colab_type": "code",
        "outputId": "79fffcc6-8a9e-47b6-d2c8-b10bee6c0093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training/ssdlite_mobilenet_v2_coco.config\n",
        "\n",
        "# SSDLite with Mobilenet v2 configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 3\n",
        "        use_depthwise: true\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      use_depthwise: true\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/models/research/object_detection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/models/research/object_detection/training/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training/ssdlite_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v90gM0m8xfDB",
        "colab_type": "code",
        "outputId": "831bdd8c-618e-4830-e3e9-4f940b102d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Training executable for detection models.\n",
        "\n",
        "This executable is used to train DetectionModels. There are two ways of\n",
        "configuring the training job:\n",
        "\n",
        "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
        "can be specified by --pipeline_config_path.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --pipeline_config_path=pipeline_config.pbtxt\n",
        "\n",
        "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
        "configuration file to define what type of DetectionModel is being trained, an\n",
        "input_reader_pb2.InputReader file to specify what training data will be used and\n",
        "a train_pb2.TrainConfig file to configure training parameters.\n",
        "\n",
        "Example usage:\n",
        "    ./train \\\n",
        "        --logtostderr \\\n",
        "        --train_dir=path/to/train_dir \\\n",
        "        --model_config_path=model_config.pbtxt \\\n",
        "        --train_config_path=train_config.pbtxt \\\n",
        "        --input_config_path=train_input_config.pbtxt\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.builders import dataset_builder\n",
        "from object_detection.builders import graph_rewriter_builder\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.legacy import trainer\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
        "flags.DEFINE_integer('task', 0, 'task id')\n",
        "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
        "flags.DEFINE_boolean('clone_on_cpu', False,\n",
        "                     'Force clones to be deployed on CPU.  Note that even if '\n",
        "                     'set to False (allowing ops to run on gpu), some ops may '\n",
        "                     'still be run on the CPU if they have no GPU kernel.')\n",
        "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
        "                     'replicas.')\n",
        "flags.DEFINE_integer('ps_tasks', 0,\n",
        "                     'Number of parameter server tasks. If None, does not use '\n",
        "                     'a parameter server.')\n",
        "flags.DEFINE_string('train_dir', '',\n",
        "                    'Directory to save the checkpoints and training summaries.')\n",
        "\n",
        "flags.DEFINE_string('pipeline_config_path', '',\n",
        "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
        "                    'file. If provided, other configs are ignored')\n",
        "\n",
        "flags.DEFINE_string('train_config_path', '',\n",
        "                    'Path to a train_pb2.TrainConfig config file.')\n",
        "flags.DEFINE_string('input_config_path', '',\n",
        "                    'Path to an input_reader_pb2.InputReader config file.')\n",
        "flags.DEFINE_string('model_config_path', '',\n",
        "                    'Path to a model_pb2.DetectionModel config file.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
        "def main(_):\n",
        "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
        "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  if FLAGS.pipeline_config_path:\n",
        "    configs = config_util.get_configs_from_pipeline_file(\n",
        "        FLAGS.pipeline_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
        "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
        "                    overwrite=True)\n",
        "  else:\n",
        "    configs = config_util.get_configs_from_multiple_files(\n",
        "        model_config_path=FLAGS.model_config_path,\n",
        "        train_config_path=FLAGS.train_config_path,\n",
        "        train_input_config_path=FLAGS.input_config_path)\n",
        "    if FLAGS.task == 0:\n",
        "      for name, config in [('model.config', FLAGS.model_config_path),\n",
        "                           ('train.config', FLAGS.train_config_path),\n",
        "                           ('input.config', FLAGS.input_config_path)]:\n",
        "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
        "                      overwrite=True)\n",
        "\n",
        "  model_config = configs['model']\n",
        "  train_config = configs['train_config']\n",
        "  input_config = configs['train_input_config']\n",
        "\n",
        "  model_fn = functools.partial(\n",
        "      model_builder.build,\n",
        "      model_config=model_config,\n",
        "      is_training=True)\n",
        "\n",
        "  def get_next(config):\n",
        "    return dataset_builder.make_initializable_iterator(\n",
        "        dataset_builder.build(config)).get_next()\n",
        "\n",
        "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
        "\n",
        "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
        "  cluster_data = env.get('cluster', None)\n",
        "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
        "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
        "  task_info = type('TaskSpec', (object,), task_data)\n",
        "\n",
        "  # Parameters for a single worker.\n",
        "  ps_tasks = 0\n",
        "  worker_replicas = 1\n",
        "  worker_job_name = 'lonely_worker'\n",
        "  task = 0\n",
        "  is_chief = True\n",
        "  master = ''\n",
        "\n",
        "  if cluster_data and 'worker' in cluster_data:\n",
        "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
        "    worker_replicas = len(cluster_data['worker']) + 1\n",
        "  if cluster_data and 'ps' in cluster_data:\n",
        "    ps_tasks = len(cluster_data['ps'])\n",
        "\n",
        "  if worker_replicas > 1 and ps_tasks < 1:\n",
        "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
        "\n",
        "  if worker_replicas >= 1 and ps_tasks > 0:\n",
        "    # Set up distributed training.\n",
        "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
        "                             job_name=task_info.type,\n",
        "                             task_index=task_info.index)\n",
        "    if task_info.type == 'ps':\n",
        "      server.join()\n",
        "      return\n",
        "\n",
        "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
        "    task = task_info.index\n",
        "    is_chief = (task_info.type == 'master')\n",
        "    master = server.target\n",
        "\n",
        "  graph_rewriter_fn = None\n",
        "  if 'graph_rewriter_config' in configs:\n",
        "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
        "        configs['graph_rewriter_config'], is_training=True)\n",
        "\n",
        "  trainer.train(\n",
        "      create_input_dict_fn,\n",
        "      model_fn,\n",
        "      train_config,\n",
        "      master,\n",
        "      task,\n",
        "      FLAGS.num_clones,\n",
        "      worker_replicas,\n",
        "      FLAGS.clone_on_cpu,\n",
        "      ps_tasks,\n",
        "      worker_job_name,\n",
        "      is_chief,\n",
        "      FLAGS.train_dir,\n",
        "      graph_hook_fn=graph_rewriter_fn)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQIfvm7y7Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -R /content/models/research/slim/nets /content/models/research/object_detection\n",
        "!cp -R /content/models/research/slim/deployment /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtxSZ_veqzNJ",
        "colab_type": "code",
        "outputId": "0e29e621-3921-4c28-b5d0-d65ba998482f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from tensorboardcolab import *\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://d64201da.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu4A4zayA8Z",
        "colab_type": "code",
        "outputId": "be680958-f513-41d1-cd6a-d9b3af5cdc50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:56: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:185: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W1209 10:43:58.690779 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "WARNING:tensorflow:From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1209 10:43:58.690981 139904075134848 module_wrapper.py:139] From train.py:91: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1209 10:43:58.691246 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W1209 10:43:58.694739 139904075134848 module_wrapper.py:139] From train.py:96: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W1209 10:43:58.703479 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:267: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W1209 10:43:58.707057 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W1209 10:43:58.707245 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W1209 10:43:58.717875 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W1209 10:43:58.719090 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1209 10:43:58.719224 139904075134848 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W1209 10:43:58.726024 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W1209 10:43:58.726164 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1209 10:43:58.749183 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W1209 10:43:59.250872 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1209 10:43:59.256919 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W1209 10:43:59.263804 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:627: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1209 10:43:59.310992 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:197: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1209 10:43:59.320358 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W1209 10:43:59.921222 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1209 10:43:59.924579 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W1209 10:43:59.925694 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W1209 10:43:59.930266 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:51: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W1209 10:43:59.933307 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W1209 10:43:59.935944 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:286: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1209 10:43:59.936265 139904075134848 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W1209 10:43:59.936379 139904075134848 module_wrapper.py:139] From /content/models/research/object_detection/deployment/model_deploy.py:192: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1209 10:44:00.663384 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1209 10:44:00.929795 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1209 10:44:03.386373 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1209 10:44:03.395743 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.395915 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.481380 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.695258 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.782810 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.867088 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 10:44:03.953488 139904075134848 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W1209 10:44:04.174598 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W1209 10:44:07.504658 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:79: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W1209 10:44:07.505776 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W1209 10:44:07.506840 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W1209 10:44:07.980891 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:209: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W1209 10:44:07.981812 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:157: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W1209 10:44:07.982069 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W1209 10:44:07.989928 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "W1209 10:44:10.058974 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:323: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W1209 10:44:10.061015 139904075134848 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W1209 10:44:12.190496 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W1209 10:44:16.343004 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:354: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W1209 10:44:16.660087 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:356: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W1209 10:44:16.662092 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:360: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W1209 10:44:16.665686 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:369: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1209 10:44:16.673149 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:372: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1209 10:44:16.673346 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:377: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W1209 10:44:17.609269 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1209 10:44:17.612066 139904075134848 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W1209 10:44:17.614592 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.614723 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.614781 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.614837 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.614889 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.614943 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615003 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615051 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615097 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615147 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615192 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615236 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615293 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615336 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615380 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615436 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615484 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615530 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615578 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615637 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615682 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615730 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615774 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615818 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.615873 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.615923 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.615974 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616023 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616068 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616112 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616159 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616203 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616247 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616302 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616351 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616397 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616445 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616490 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616533 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616582 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616641 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616692 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616754 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616806 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.616859 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.616919 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.616983 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.617039 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.617100 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.617152 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.617205 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.617271 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.617330 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.617386 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.617449 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.617509 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.617567 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.617654 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.617720 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.617779 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.617877 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.617945 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618006 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.618072 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.618131 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618201 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.618266 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.618324 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618380 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.618452 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.618515 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618571 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.618650 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.618712 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618771 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.618851 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.618909 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.618977 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.619052 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.619114 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.619173 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.619238 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.619299 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.619356 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.619422 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.619484 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.619541 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.619628 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.619695 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.619752 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.619818 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.619879 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.619945 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620014 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.620074 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.620132 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620204 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.620264 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.620322 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620386 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.620445 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.620502 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620567 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.620642 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.620702 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620777 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.620839 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.620900 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.620985 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.621051 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.621111 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.621175 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.621234 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.621293 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.621367 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.621429 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.621494 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.621558 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.621632 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.621694 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.621759 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.621819 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.621877 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.621962 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622028 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.622087 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.622151 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622213 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.622272 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.622338 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622398 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.622456 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.622529 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622591 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.622664 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.622728 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622788 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.622845 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.622911 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.622988 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.623048 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.623121 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.623183 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.623241 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.623306 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.623365 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.623424 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.623488 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.623549 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.623619 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.623696 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.623760 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.623821 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.623886 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.623955 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624016 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.624082 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.624142 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624199 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.624272 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.624334 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624394 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.624458 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.624517 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624572 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.624653 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.624716 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624773 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.624845 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.624907 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.624976 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.625041 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.625100 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.625158 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.625223 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.625284 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.625342 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.625414 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.625476 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.625536 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.625601 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.625677 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.625735 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.625800 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.625860 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.625920 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.626008 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.626070 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.626129 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.626192 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.626250 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.626308 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.626391 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.626455 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.626517 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.626593 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.626674 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.626736 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.626805 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.626881 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.626964 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.627059 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.627127 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.627194 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.627276 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.627346 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.627412 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.627496 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.627561 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.627637 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.627707 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.627771 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.627832 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.627910 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.627986 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.628049 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.628117 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.628181 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.628252 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.628315 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.628375 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.628433 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.628504 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.628567 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.628638 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.628703 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.628764 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.628823 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.628888 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.628953 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629014 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.629088 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.629150 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629209 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.629273 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.629334 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629393 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.629458 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.629517 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629575 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.629663 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.629728 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629785 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.629849 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.629909 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.629977 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630043 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.630104 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.630161 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630235 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.630298 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.630356 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630419 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.630477 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.630536 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630603 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.630677 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.630734 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630806 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.630868 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.630926 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.630999 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.631058 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.631117 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.631182 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.631241 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.631299 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.631371 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.631435 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.631494 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.631557 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.631630 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.631691 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.631757 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.631827 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.631885 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.631965 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632030 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.632090 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.632154 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632213 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.632272 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.632337 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632397 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.632455 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.632528 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632590 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.632665 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.632731 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632791 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.632849 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.632914 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.632984 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633042 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.633107 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.633156 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633200 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.633247 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.633294 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633337 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.633392 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.633450 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633507 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.633598 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.633689 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633753 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.633824 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.633887 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.633958 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.634029 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.634093 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.634154 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.634232 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.634298 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.634359 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.634436 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.634496 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.634554 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.634634 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.634697 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.634755 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.634825 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.634887 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.634953 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635019 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.635080 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.635140 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635206 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.635265 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.635323 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635397 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.635459 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.635517 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635581 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.635657 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.635716 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635781 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.635841 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.635899 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.635978 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636043 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.636102 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.636167 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636227 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.636285 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.636351 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636411 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.636470 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.636541 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636603 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.636679 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.636744 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636803 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.636862 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.636926 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.636994 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.637052 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.637123 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.637186 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.637245 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.637309 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.637368 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.637422 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.637487 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.637548 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.637617 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.637692 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.637755 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.637813 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.637877 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.637944 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638004 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.638070 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.638132 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638190 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.638261 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.638324 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638384 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.638449 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.638509 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638566 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.638644 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.638707 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638765 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.638837 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.638898 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.638969 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639036 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.639096 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.639154 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639219 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.639280 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.639338 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639410 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.639471 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.639530 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639596 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.639671 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.639728 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639792 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.639852 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.639911 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.639990 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.640051 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.640110 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.640176 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.640236 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.640293 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.640357 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.640417 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.640476 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.640548 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.640620 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.640683 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.640753 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.640815 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.640873 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.640945 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641009 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.641071 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.641142 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641203 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.641262 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.641327 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641387 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.641445 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.641510 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641569 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.641640 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.641714 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641775 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.641833 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.641898 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.641966 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642026 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.642091 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.642149 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642208 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.642279 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.642340 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642399 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.642464 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.642525 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642583 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.642662 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.642722 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642780 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.642853 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.642914 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.642979 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.643044 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.643106 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.643164 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.643227 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.643285 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.643344 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.643417 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.643483 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.643541 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.643616 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.643681 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.643740 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.643873 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.643934 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.643994 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.644060 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.644119 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.644176 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.644239 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.644299 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.644357 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.644420 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.644479 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.644538 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.644624 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.644691 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.644750 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.644814 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.644876 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.644935 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645008 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.645067 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.645124 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645198 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.645261 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.645320 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645383 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.645444 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.645503 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645568 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.645642 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.645701 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645772 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.645835 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.645894 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.645964 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646025 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.646084 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.646151 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646211 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.646269 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.646340 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646404 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.646462 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.646526 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646586 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.646663 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.646730 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646790 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.646848 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.646919 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.646991 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.647052 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.647117 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.647177 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.647236 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.647301 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.647361 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.647420 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.647490 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.647552 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.647624 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.647692 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.647752 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.647809 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.647873 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.647933 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648008 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.648078 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.648140 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648200 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.648264 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.648323 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648380 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.648444 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.648506 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648564 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.648645 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.648708 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648768 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.648831 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.648891 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.648956 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.649020 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.649083 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.649153 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.649223 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.649283 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.649340 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.649403 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.649461 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.649534 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.649599 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.649679 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.649739 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.649810 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.649870 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.649927 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650001 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.650062 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.650119 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650182 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.650245 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.650305 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650376 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.650437 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.650494 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650559 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.650632 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.650693 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650757 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.650818 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.650877 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W1209 10:44:17.650957 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W1209 10:44:17.651021 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W1209 10:44:17.651080 139904075134848 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "W1209 10:44:18.549902 139904075134848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-12-09 10:44:19.866284: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-09 10:44:19.879349: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000134999 Hz\n",
            "2019-12-09 10:44:19.881392: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b02e140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-09 10:44:19.881434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-09 10:44:19.887192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-09 10:44:20.088247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:20.088901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b02e680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-09 10:44:20.088937: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-12-09 10:44:20.090037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:20.090534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-09 10:44:20.101453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 10:44:20.310657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-09 10:44:20.434308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-09 10:44:20.457141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-09 10:44:20.665835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-09 10:44:20.690466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-09 10:44:21.120409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 10:44:21.120593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:21.121232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:21.121716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-09 10:44:21.126152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 10:44:21.127402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-09 10:44:21.127429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-09 10:44:21.127439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-09 10:44:21.128884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:21.129451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 10:44:21.130028: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-09 10:44:21.130074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "I1209 10:44:25.026718 139904075134848 saver.py:1284] Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I1209 10:44:25.723047 139904075134848 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I1209 10:44:26.319046 139904075134848 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Starting Session.\n",
            "I1209 10:44:36.349931 139904075134848 learning.py:754] Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 10:44:36.804298 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Starting Queues.\n",
            "I1209 10:44:36.807279 139904075134848 learning.py:768] Starting Queues.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "I1209 10:44:53.051543 139900552390400 supervisor.py:1099] global_step/sec: 0\n",
            "2019-12-09 10:44:56.406188: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-09 10:44:57.260380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 10:44:58.761062: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-09 10:45:01.108176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "I1209 10:45:03.895749 139900543997696 supervisor.py:1050] Recording summary at step 0.\n",
            "2019-12-09 10:45:03.951315: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-09 10:45:04.897560: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "2019-12-09 10:45:05.790444: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 597196800 exceeds 10% of system memory.\n",
            "INFO:tensorflow:global step 1: loss = 11.9688 (31.353 sec/step)\n",
            "I1209 10:45:09.446938 139904075134848 learning.py:507] global step 1: loss = 11.9688 (31.353 sec/step)\n",
            "INFO:tensorflow:global step 2: loss = 11.0745 (3.009 sec/step)\n",
            "I1209 10:45:13.408336 139904075134848 learning.py:507] global step 2: loss = 11.0745 (3.009 sec/step)\n",
            "INFO:tensorflow:global step 3: loss = 10.9155 (2.701 sec/step)\n",
            "I1209 10:45:16.449939 139904075134848 learning.py:507] global step 3: loss = 10.9155 (2.701 sec/step)\n",
            "INFO:tensorflow:global step 4: loss = 9.8825 (0.766 sec/step)\n",
            "I1209 10:45:17.557307 139904075134848 learning.py:507] global step 4: loss = 9.8825 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 5: loss = 9.3760 (0.674 sec/step)\n",
            "I1209 10:45:18.415720 139904075134848 learning.py:507] global step 5: loss = 9.3760 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 6: loss = 8.7923 (2.869 sec/step)\n",
            "I1209 10:45:21.320401 139904075134848 learning.py:507] global step 6: loss = 8.7923 (2.869 sec/step)\n",
            "INFO:tensorflow:global step 7: loss = 8.7533 (0.690 sec/step)\n",
            "I1209 10:45:22.281981 139904075134848 learning.py:507] global step 7: loss = 8.7533 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 8: loss = 8.8223 (0.824 sec/step)\n",
            "I1209 10:45:23.325632 139904075134848 learning.py:507] global step 8: loss = 8.8223 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 9: loss = 8.4123 (1.402 sec/step)\n",
            "I1209 10:45:24.937875 139904075134848 learning.py:507] global step 9: loss = 8.4123 (1.402 sec/step)\n",
            "INFO:tensorflow:global step 10: loss = 8.4360 (0.748 sec/step)\n",
            "I1209 10:45:25.707783 139904075134848 learning.py:507] global step 10: loss = 8.4360 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 11: loss = 7.7934 (1.863 sec/step)\n",
            "I1209 10:45:27.872409 139904075134848 learning.py:507] global step 11: loss = 7.7934 (1.863 sec/step)\n",
            "INFO:tensorflow:global step 12: loss = 7.9184 (0.752 sec/step)\n",
            "I1209 10:45:28.626147 139904075134848 learning.py:507] global step 12: loss = 7.9184 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 13: loss = 8.1065 (2.281 sec/step)\n",
            "I1209 10:45:30.909190 139904075134848 learning.py:507] global step 13: loss = 8.1065 (2.281 sec/step)\n",
            "INFO:tensorflow:global step 14: loss = 7.7249 (0.878 sec/step)\n",
            "I1209 10:45:31.990854 139904075134848 learning.py:507] global step 14: loss = 7.7249 (0.878 sec/step)\n",
            "INFO:tensorflow:global step 15: loss = 8.0354 (0.721 sec/step)\n",
            "I1209 10:45:32.952474 139904075134848 learning.py:507] global step 15: loss = 8.0354 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 16: loss = 7.5510 (0.943 sec/step)\n",
            "I1209 10:45:34.195932 139904075134848 learning.py:507] global step 16: loss = 7.5510 (0.943 sec/step)\n",
            "INFO:tensorflow:global step 17: loss = 7.7596 (0.732 sec/step)\n",
            "I1209 10:45:35.180931 139904075134848 learning.py:507] global step 17: loss = 7.7596 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 18: loss = 7.9756 (0.669 sec/step)\n",
            "I1209 10:45:36.322106 139904075134848 learning.py:507] global step 18: loss = 7.9756 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 19: loss = 8.0057 (0.701 sec/step)\n",
            "I1209 10:45:37.232693 139904075134848 learning.py:507] global step 19: loss = 8.0057 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 20: loss = 7.2720 (0.587 sec/step)\n",
            "I1209 10:45:38.094250 139904075134848 learning.py:507] global step 20: loss = 7.2720 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 21: loss = 8.0129 (0.612 sec/step)\n",
            "I1209 10:45:38.708568 139904075134848 learning.py:507] global step 21: loss = 8.0129 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 22: loss = 7.2933 (3.015 sec/step)\n",
            "I1209 10:45:41.725147 139904075134848 learning.py:507] global step 22: loss = 7.2933 (3.015 sec/step)\n",
            "INFO:tensorflow:global step 23: loss = 7.6155 (0.699 sec/step)\n",
            "I1209 10:45:42.454270 139904075134848 learning.py:507] global step 23: loss = 7.6155 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 24: loss = 7.6951 (2.054 sec/step)\n",
            "I1209 10:45:44.694280 139904075134848 learning.py:507] global step 24: loss = 7.6951 (2.054 sec/step)\n",
            "INFO:tensorflow:global step 25: loss = 6.9320 (0.666 sec/step)\n",
            "I1209 10:45:45.530144 139904075134848 learning.py:507] global step 25: loss = 6.9320 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 26: loss = 7.2133 (1.459 sec/step)\n",
            "I1209 10:45:46.991273 139904075134848 learning.py:507] global step 26: loss = 7.2133 (1.459 sec/step)\n",
            "INFO:tensorflow:global step 27: loss = 6.7584 (0.796 sec/step)\n",
            "I1209 10:45:48.093445 139904075134848 learning.py:507] global step 27: loss = 6.7584 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 28: loss = 7.0303 (1.202 sec/step)\n",
            "I1209 10:45:49.352777 139904075134848 learning.py:507] global step 28: loss = 7.0303 (1.202 sec/step)\n",
            "INFO:tensorflow:global step 29: loss = 6.6501 (0.725 sec/step)\n",
            "I1209 10:45:50.271793 139904075134848 learning.py:507] global step 29: loss = 6.6501 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 30: loss = 6.6003 (0.773 sec/step)\n",
            "I1209 10:45:51.208570 139904075134848 learning.py:507] global step 30: loss = 6.6003 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 31: loss = 7.1566 (1.167 sec/step)\n",
            "I1209 10:45:52.420421 139904075134848 learning.py:507] global step 31: loss = 7.1566 (1.167 sec/step)\n",
            "INFO:tensorflow:global step 32: loss = 7.2916 (1.813 sec/step)\n",
            "I1209 10:45:54.450076 139904075134848 learning.py:507] global step 32: loss = 7.2916 (1.813 sec/step)\n",
            "INFO:tensorflow:global step 33: loss = 6.6569 (0.827 sec/step)\n",
            "I1209 10:45:55.433742 139904075134848 learning.py:507] global step 33: loss = 6.6569 (0.827 sec/step)\n",
            "INFO:tensorflow:global step 34: loss = 6.4706 (0.699 sec/step)\n",
            "I1209 10:45:56.338954 139904075134848 learning.py:507] global step 34: loss = 6.4706 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 35: loss = 6.9417 (1.884 sec/step)\n",
            "I1209 10:45:58.271224 139904075134848 learning.py:507] global step 35: loss = 6.9417 (1.884 sec/step)\n",
            "INFO:tensorflow:global step 36: loss = 6.4416 (0.800 sec/step)\n",
            "I1209 10:45:59.232520 139904075134848 learning.py:507] global step 36: loss = 6.4416 (0.800 sec/step)\n",
            "INFO:tensorflow:global step 37: loss = 7.0348 (0.703 sec/step)\n",
            "I1209 10:46:00.418166 139904075134848 learning.py:507] global step 37: loss = 7.0348 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 38: loss = 6.1732 (0.615 sec/step)\n",
            "I1209 10:46:01.202151 139904075134848 learning.py:507] global step 38: loss = 6.1732 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 39: loss = 6.5692 (1.296 sec/step)\n",
            "I1209 10:46:02.684418 139904075134848 learning.py:507] global step 39: loss = 6.5692 (1.296 sec/step)\n",
            "INFO:tensorflow:global step 40: loss = 6.5474 (0.665 sec/step)\n",
            "I1209 10:46:03.568434 139904075134848 learning.py:507] global step 40: loss = 6.5474 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 41: loss = 6.5426 (0.786 sec/step)\n",
            "I1209 10:46:04.646381 139904075134848 learning.py:507] global step 41: loss = 6.5426 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 42: loss = 5.6755 (0.798 sec/step)\n",
            "I1209 10:46:05.754843 139904075134848 learning.py:507] global step 42: loss = 5.6755 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 43: loss = 6.4993 (1.158 sec/step)\n",
            "I1209 10:46:06.921456 139904075134848 learning.py:507] global step 43: loss = 6.4993 (1.158 sec/step)\n",
            "INFO:tensorflow:global step 44: loss = 6.2698 (1.194 sec/step)\n",
            "I1209 10:46:08.116751 139904075134848 learning.py:507] global step 44: loss = 6.2698 (1.194 sec/step)\n",
            "INFO:tensorflow:global step 45: loss = 5.9634 (0.642 sec/step)\n",
            "I1209 10:46:09.047835 139904075134848 learning.py:507] global step 45: loss = 5.9634 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 46: loss = 6.0216 (1.730 sec/step)\n",
            "I1209 10:46:10.838893 139904075134848 learning.py:507] global step 46: loss = 6.0216 (1.730 sec/step)\n",
            "INFO:tensorflow:global step 47: loss = 5.8583 (0.637 sec/step)\n",
            "I1209 10:46:11.477059 139904075134848 learning.py:507] global step 47: loss = 5.8583 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 48: loss = 5.8183 (0.862 sec/step)\n",
            "I1209 10:46:12.664790 139904075134848 learning.py:507] global step 48: loss = 5.8183 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 49: loss = 6.1797 (1.536 sec/step)\n",
            "I1209 10:46:14.402413 139904075134848 learning.py:507] global step 49: loss = 6.1797 (1.536 sec/step)\n",
            "INFO:tensorflow:global step 50: loss = 6.1782 (0.569 sec/step)\n",
            "I1209 10:46:15.287013 139904075134848 learning.py:507] global step 50: loss = 6.1782 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 51: loss = 6.3823 (0.594 sec/step)\n",
            "I1209 10:46:16.269050 139904075134848 learning.py:507] global step 51: loss = 6.3823 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 52: loss = 5.8852 (0.732 sec/step)\n",
            "I1209 10:46:17.301571 139904075134848 learning.py:507] global step 52: loss = 5.8852 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 53: loss = 6.1414 (1.421 sec/step)\n",
            "I1209 10:46:18.724348 139904075134848 learning.py:507] global step 53: loss = 6.1414 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 54: loss = 6.5972 (0.679 sec/step)\n",
            "I1209 10:46:19.404878 139904075134848 learning.py:507] global step 54: loss = 6.5972 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 55: loss = 5.8672 (1.803 sec/step)\n",
            "I1209 10:46:21.227852 139904075134848 learning.py:507] global step 55: loss = 5.8672 (1.803 sec/step)\n",
            "INFO:tensorflow:global step 56: loss = 6.1781 (1.185 sec/step)\n",
            "I1209 10:46:22.415047 139904075134848 learning.py:507] global step 56: loss = 6.1781 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 57: loss = 5.5607 (0.758 sec/step)\n",
            "I1209 10:46:23.272677 139904075134848 learning.py:507] global step 57: loss = 5.5607 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 58: loss = 5.7866 (0.798 sec/step)\n",
            "I1209 10:46:24.285890 139904075134848 learning.py:507] global step 58: loss = 5.7866 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 59: loss = 6.1183 (1.314 sec/step)\n",
            "I1209 10:46:25.728530 139904075134848 learning.py:507] global step 59: loss = 6.1183 (1.314 sec/step)\n",
            "INFO:tensorflow:global step 60: loss = 6.0749 (1.120 sec/step)\n",
            "I1209 10:46:26.850400 139904075134848 learning.py:507] global step 60: loss = 6.0749 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 61: loss = 6.2483 (0.670 sec/step)\n",
            "I1209 10:46:27.546664 139904075134848 learning.py:507] global step 61: loss = 6.2483 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 62: loss = 5.6534 (1.492 sec/step)\n",
            "I1209 10:46:29.040436 139904075134848 learning.py:507] global step 62: loss = 5.6534 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 63: loss = 5.0377 (0.635 sec/step)\n",
            "I1209 10:46:29.953177 139904075134848 learning.py:507] global step 63: loss = 5.0377 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 64: loss = 5.2394 (1.140 sec/step)\n",
            "I1209 10:46:31.197251 139904075134848 learning.py:507] global step 64: loss = 5.2394 (1.140 sec/step)\n",
            "INFO:tensorflow:global step 65: loss = 5.5119 (0.649 sec/step)\n",
            "I1209 10:46:32.043452 139904075134848 learning.py:507] global step 65: loss = 5.5119 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 66: loss = 5.7394 (1.309 sec/step)\n",
            "I1209 10:46:33.568399 139904075134848 learning.py:507] global step 66: loss = 5.7394 (1.309 sec/step)\n",
            "INFO:tensorflow:global step 67: loss = 5.4077 (0.687 sec/step)\n",
            "I1209 10:46:34.257136 139904075134848 learning.py:507] global step 67: loss = 5.4077 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 68: loss = 5.0912 (2.061 sec/step)\n",
            "I1209 10:46:36.320236 139904075134848 learning.py:507] global step 68: loss = 5.0912 (2.061 sec/step)\n",
            "INFO:tensorflow:global step 69: loss = 5.5665 (2.123 sec/step)\n",
            "I1209 10:46:38.449412 139904075134848 learning.py:507] global step 69: loss = 5.5665 (2.123 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 69.\n",
            "I1209 10:46:38.839359 139900543997696 supervisor.py:1050] Recording summary at step 69.\n",
            "INFO:tensorflow:global step 70: loss = 5.4576 (1.247 sec/step)\n",
            "I1209 10:46:39.698375 139904075134848 learning.py:507] global step 70: loss = 5.4576 (1.247 sec/step)\n",
            "INFO:tensorflow:global step 71: loss = 5.4560 (1.287 sec/step)\n",
            "I1209 10:46:40.987527 139904075134848 learning.py:507] global step 71: loss = 5.4560 (1.287 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.655801\n",
            "I1209 10:46:41.318404 139900552390400 supervisor.py:1099] global_step/sec: 0.655801\n",
            "INFO:tensorflow:global step 72: loss = 5.1441 (0.726 sec/step)\n",
            "I1209 10:46:41.941352 139904075134848 learning.py:507] global step 72: loss = 5.1441 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 73: loss = 5.2956 (1.209 sec/step)\n",
            "I1209 10:46:43.215786 139904075134848 learning.py:507] global step 73: loss = 5.2956 (1.209 sec/step)\n",
            "INFO:tensorflow:global step 74: loss = 4.7389 (0.719 sec/step)\n",
            "I1209 10:46:43.937411 139904075134848 learning.py:507] global step 74: loss = 4.7389 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 75: loss = 5.1588 (0.834 sec/step)\n",
            "I1209 10:46:44.773767 139904075134848 learning.py:507] global step 75: loss = 5.1588 (0.834 sec/step)\n",
            "INFO:tensorflow:global step 76: loss = 5.6835 (3.652 sec/step)\n",
            "I1209 10:46:48.427180 139904075134848 learning.py:507] global step 76: loss = 5.6835 (3.652 sec/step)\n",
            "INFO:tensorflow:global step 77: loss = 4.9015 (0.797 sec/step)\n",
            "I1209 10:46:49.289804 139904075134848 learning.py:507] global step 77: loss = 4.9015 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 78: loss = 5.4297 (0.956 sec/step)\n",
            "I1209 10:46:50.564752 139904075134848 learning.py:507] global step 78: loss = 5.4297 (0.956 sec/step)\n",
            "INFO:tensorflow:global step 79: loss = 5.4316 (1.444 sec/step)\n",
            "I1209 10:46:52.010632 139904075134848 learning.py:507] global step 79: loss = 5.4316 (1.444 sec/step)\n",
            "INFO:tensorflow:global step 80: loss = 4.9001 (0.713 sec/step)\n",
            "I1209 10:46:52.996884 139904075134848 learning.py:507] global step 80: loss = 4.9001 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 81: loss = 4.9658 (0.867 sec/step)\n",
            "I1209 10:46:54.044913 139904075134848 learning.py:507] global step 81: loss = 4.9658 (0.867 sec/step)\n",
            "INFO:tensorflow:global step 82: loss = 4.8442 (1.489 sec/step)\n",
            "I1209 10:46:55.754980 139904075134848 learning.py:507] global step 82: loss = 4.8442 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 83: loss = 4.9895 (2.153 sec/step)\n",
            "I1209 10:46:58.259707 139904075134848 learning.py:507] global step 83: loss = 4.9895 (2.153 sec/step)\n",
            "INFO:tensorflow:global step 84: loss = 4.6554 (0.727 sec/step)\n",
            "I1209 10:46:58.988168 139904075134848 learning.py:507] global step 84: loss = 4.6554 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 85: loss = 4.4706 (1.909 sec/step)\n",
            "I1209 10:47:00.898420 139904075134848 learning.py:507] global step 85: loss = 4.4706 (1.909 sec/step)\n",
            "INFO:tensorflow:global step 86: loss = 4.9395 (0.733 sec/step)\n",
            "I1209 10:47:01.633389 139904075134848 learning.py:507] global step 86: loss = 4.9395 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 87: loss = 4.7518 (2.546 sec/step)\n",
            "I1209 10:47:04.205011 139904075134848 learning.py:507] global step 87: loss = 4.7518 (2.546 sec/step)\n",
            "INFO:tensorflow:global step 88: loss = 5.3459 (0.651 sec/step)\n",
            "I1209 10:47:04.871141 139904075134848 learning.py:507] global step 88: loss = 5.3459 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 89: loss = 4.5866 (2.499 sec/step)\n",
            "I1209 10:47:07.372141 139904075134848 learning.py:507] global step 89: loss = 4.5866 (2.499 sec/step)\n",
            "INFO:tensorflow:global step 90: loss = 4.7631 (0.579 sec/step)\n",
            "I1209 10:47:07.952593 139904075134848 learning.py:507] global step 90: loss = 4.7631 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 91: loss = 4.5683 (1.093 sec/step)\n",
            "I1209 10:47:09.279216 139904075134848 learning.py:507] global step 91: loss = 4.5683 (1.093 sec/step)\n",
            "INFO:tensorflow:global step 92: loss = 4.6030 (1.684 sec/step)\n",
            "I1209 10:47:11.169978 139904075134848 learning.py:507] global step 92: loss = 4.6030 (1.684 sec/step)\n",
            "INFO:tensorflow:global step 93: loss = 4.4406 (0.703 sec/step)\n",
            "I1209 10:47:12.004425 139904075134848 learning.py:507] global step 93: loss = 4.4406 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 94: loss = 4.5923 (1.952 sec/step)\n",
            "I1209 10:47:14.120987 139904075134848 learning.py:507] global step 94: loss = 4.5923 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 95: loss = 4.8878 (0.736 sec/step)\n",
            "I1209 10:47:15.066357 139904075134848 learning.py:507] global step 95: loss = 4.8878 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 96: loss = 5.5881 (1.263 sec/step)\n",
            "I1209 10:47:16.415987 139904075134848 learning.py:507] global step 96: loss = 5.5881 (1.263 sec/step)\n",
            "INFO:tensorflow:global step 97: loss = 4.4036 (0.672 sec/step)\n",
            "I1209 10:47:17.177412 139904075134848 learning.py:507] global step 97: loss = 4.4036 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 98: loss = 4.2166 (1.454 sec/step)\n",
            "I1209 10:47:18.805096 139904075134848 learning.py:507] global step 98: loss = 4.2166 (1.454 sec/step)\n",
            "INFO:tensorflow:global step 99: loss = 5.0932 (0.611 sec/step)\n",
            "I1209 10:47:19.763328 139904075134848 learning.py:507] global step 99: loss = 5.0932 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 100: loss = 4.2322 (1.352 sec/step)\n",
            "I1209 10:47:21.126942 139904075134848 learning.py:507] global step 100: loss = 4.2322 (1.352 sec/step)\n",
            "INFO:tensorflow:global step 101: loss = 4.3053 (0.681 sec/step)\n",
            "I1209 10:47:22.038566 139904075134848 learning.py:507] global step 101: loss = 4.3053 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 102: loss = 4.4013 (0.838 sec/step)\n",
            "I1209 10:47:23.138077 139904075134848 learning.py:507] global step 102: loss = 4.4013 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 103: loss = 5.0703 (1.371 sec/step)\n",
            "I1209 10:47:24.564501 139904075134848 learning.py:507] global step 103: loss = 5.0703 (1.371 sec/step)\n",
            "INFO:tensorflow:global step 104: loss = 5.1100 (0.777 sec/step)\n",
            "I1209 10:47:25.525115 139904075134848 learning.py:507] global step 104: loss = 5.1100 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 105: loss = 4.7977 (1.470 sec/step)\n",
            "I1209 10:47:27.087376 139904075134848 learning.py:507] global step 105: loss = 4.7977 (1.470 sec/step)\n",
            "INFO:tensorflow:global step 106: loss = 4.0876 (0.704 sec/step)\n",
            "I1209 10:47:27.999410 139904075134848 learning.py:507] global step 106: loss = 4.0876 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 107: loss = 4.6785 (0.969 sec/step)\n",
            "I1209 10:47:29.150396 139904075134848 learning.py:507] global step 107: loss = 4.6785 (0.969 sec/step)\n",
            "INFO:tensorflow:global step 108: loss = 4.9924 (1.380 sec/step)\n",
            "I1209 10:47:30.663732 139904075134848 learning.py:507] global step 108: loss = 4.9924 (1.380 sec/step)\n",
            "INFO:tensorflow:global step 109: loss = 4.5210 (0.665 sec/step)\n",
            "I1209 10:47:31.620309 139904075134848 learning.py:507] global step 109: loss = 4.5210 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 110: loss = 4.5765 (1.199 sec/step)\n",
            "I1209 10:47:32.821558 139904075134848 learning.py:507] global step 110: loss = 4.5765 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 111: loss = 4.4323 (0.703 sec/step)\n",
            "I1209 10:47:33.631551 139904075134848 learning.py:507] global step 111: loss = 4.4323 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 112: loss = 4.9664 (1.529 sec/step)\n",
            "I1209 10:47:35.162400 139904075134848 learning.py:507] global step 112: loss = 4.9664 (1.529 sec/step)\n",
            "INFO:tensorflow:global step 113: loss = 3.9831 (0.987 sec/step)\n",
            "I1209 10:47:36.150745 139904075134848 learning.py:507] global step 113: loss = 3.9831 (0.987 sec/step)\n",
            "INFO:tensorflow:global step 114: loss = 5.0841 (0.707 sec/step)\n",
            "I1209 10:47:37.063844 139904075134848 learning.py:507] global step 114: loss = 5.0841 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 115: loss = 5.0416 (1.222 sec/step)\n",
            "I1209 10:47:38.353516 139904075134848 learning.py:507] global step 115: loss = 5.0416 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 116: loss = 5.2735 (0.731 sec/step)\n",
            "I1209 10:47:39.300629 139904075134848 learning.py:507] global step 116: loss = 5.2735 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 117: loss = 4.2710 (1.097 sec/step)\n",
            "I1209 10:47:40.566092 139904075134848 learning.py:507] global step 117: loss = 4.2710 (1.097 sec/step)\n",
            "INFO:tensorflow:global step 118: loss = 4.8706 (1.124 sec/step)\n",
            "I1209 10:47:41.691203 139904075134848 learning.py:507] global step 118: loss = 4.8706 (1.124 sec/step)\n",
            "INFO:tensorflow:global step 119: loss = 3.9954 (0.697 sec/step)\n",
            "I1209 10:47:42.660542 139904075134848 learning.py:507] global step 119: loss = 3.9954 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 120: loss = 4.9625 (0.786 sec/step)\n",
            "I1209 10:47:43.519694 139904075134848 learning.py:507] global step 120: loss = 4.9625 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 121: loss = 5.0839 (1.837 sec/step)\n",
            "I1209 10:47:45.437630 139904075134848 learning.py:507] global step 121: loss = 5.0839 (1.837 sec/step)\n",
            "INFO:tensorflow:global step 122: loss = 4.4390 (0.839 sec/step)\n",
            "I1209 10:47:46.488696 139904075134848 learning.py:507] global step 122: loss = 4.4390 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 123: loss = 4.4081 (0.627 sec/step)\n",
            "I1209 10:47:47.461917 139904075134848 learning.py:507] global step 123: loss = 4.4081 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 124: loss = 4.1593 (0.689 sec/step)\n",
            "I1209 10:47:48.674030 139904075134848 learning.py:507] global step 124: loss = 4.1593 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 125: loss = 3.9424 (0.759 sec/step)\n",
            "I1209 10:47:49.582578 139904075134848 learning.py:507] global step 125: loss = 3.9424 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 126: loss = 4.0483 (2.317 sec/step)\n",
            "I1209 10:47:51.907273 139904075134848 learning.py:507] global step 126: loss = 4.0483 (2.317 sec/step)\n",
            "INFO:tensorflow:global step 127: loss = 4.2054 (0.743 sec/step)\n",
            "I1209 10:47:52.824935 139904075134848 learning.py:507] global step 127: loss = 4.2054 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 128: loss = 4.6173 (0.768 sec/step)\n",
            "I1209 10:47:53.872892 139904075134848 learning.py:507] global step 128: loss = 4.6173 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 129: loss = 4.6265 (1.157 sec/step)\n",
            "I1209 10:47:55.347017 139904075134848 learning.py:507] global step 129: loss = 4.6265 (1.157 sec/step)\n",
            "INFO:tensorflow:global step 130: loss = 5.0066 (0.741 sec/step)\n",
            "I1209 10:47:56.449261 139904075134848 learning.py:507] global step 130: loss = 5.0066 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 131: loss = 4.3764 (0.788 sec/step)\n",
            "I1209 10:47:57.548568 139904075134848 learning.py:507] global step 131: loss = 4.3764 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 132: loss = 4.2844 (0.692 sec/step)\n",
            "I1209 10:47:58.547656 139904075134848 learning.py:507] global step 132: loss = 4.2844 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 133: loss = 3.9602 (0.754 sec/step)\n",
            "I1209 10:47:59.817250 139904075134848 learning.py:507] global step 133: loss = 3.9602 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 134: loss = 3.4749 (1.242 sec/step)\n",
            "I1209 10:48:01.135476 139904075134848 learning.py:507] global step 134: loss = 3.4749 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 135: loss = 3.8544 (0.733 sec/step)\n",
            "I1209 10:48:02.041104 139904075134848 learning.py:507] global step 135: loss = 3.8544 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 136: loss = 3.4323 (1.266 sec/step)\n",
            "I1209 10:48:03.435204 139904075134848 learning.py:507] global step 136: loss = 3.4323 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 137: loss = 4.0417 (1.177 sec/step)\n",
            "I1209 10:48:04.613942 139904075134848 learning.py:507] global step 137: loss = 4.0417 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 138: loss = 4.0266 (0.753 sec/step)\n",
            "I1209 10:48:05.429214 139904075134848 learning.py:507] global step 138: loss = 4.0266 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 139: loss = 4.1684 (1.584 sec/step)\n",
            "I1209 10:48:07.015157 139904075134848 learning.py:507] global step 139: loss = 4.1684 (1.584 sec/step)\n",
            "INFO:tensorflow:global step 140: loss = 3.7010 (0.576 sec/step)\n",
            "I1209 10:48:07.593172 139904075134848 learning.py:507] global step 140: loss = 3.7010 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 141: loss = 4.6968 (1.736 sec/step)\n",
            "I1209 10:48:09.330330 139904075134848 learning.py:507] global step 141: loss = 4.6968 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 142: loss = 4.2434 (1.169 sec/step)\n",
            "I1209 10:48:10.501257 139904075134848 learning.py:507] global step 142: loss = 4.2434 (1.169 sec/step)\n",
            "INFO:tensorflow:global step 143: loss = 4.4187 (0.868 sec/step)\n",
            "I1209 10:48:11.588591 139904075134848 learning.py:507] global step 143: loss = 4.4187 (0.868 sec/step)\n",
            "INFO:tensorflow:global step 144: loss = 4.4439 (0.807 sec/step)\n",
            "I1209 10:48:12.494485 139904075134848 learning.py:507] global step 144: loss = 4.4439 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 145: loss = 4.1332 (1.239 sec/step)\n",
            "I1209 10:48:13.913785 139904075134848 learning.py:507] global step 145: loss = 4.1332 (1.239 sec/step)\n",
            "INFO:tensorflow:global step 146: loss = 4.3991 (0.680 sec/step)\n",
            "I1209 10:48:14.816684 139904075134848 learning.py:507] global step 146: loss = 4.3991 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 147: loss = 3.9272 (1.168 sec/step)\n",
            "I1209 10:48:16.161424 139904075134848 learning.py:507] global step 147: loss = 3.9272 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 148: loss = 4.2411 (0.730 sec/step)\n",
            "I1209 10:48:17.086139 139904075134848 learning.py:507] global step 148: loss = 4.2411 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 149: loss = 3.9292 (1.213 sec/step)\n",
            "I1209 10:48:18.412284 139904075134848 learning.py:507] global step 149: loss = 3.9292 (1.213 sec/step)\n",
            "INFO:tensorflow:global step 150: loss = 4.0612 (1.151 sec/step)\n",
            "I1209 10:48:19.564842 139904075134848 learning.py:507] global step 150: loss = 4.0612 (1.151 sec/step)\n",
            "INFO:tensorflow:global step 151: loss = 4.3229 (1.199 sec/step)\n",
            "I1209 10:48:20.765726 139904075134848 learning.py:507] global step 151: loss = 4.3229 (1.199 sec/step)\n",
            "INFO:tensorflow:global step 152: loss = 4.0638 (1.117 sec/step)\n",
            "I1209 10:48:21.884156 139904075134848 learning.py:507] global step 152: loss = 4.0638 (1.117 sec/step)\n",
            "INFO:tensorflow:global step 153: loss = 3.3772 (0.807 sec/step)\n",
            "I1209 10:48:22.700124 139904075134848 learning.py:507] global step 153: loss = 3.3772 (0.807 sec/step)\n",
            "INFO:tensorflow:global step 154: loss = 4.0422 (1.459 sec/step)\n",
            "I1209 10:48:24.194714 139904075134848 learning.py:507] global step 154: loss = 4.0422 (1.459 sec/step)\n",
            "INFO:tensorflow:global step 155: loss = 3.9155 (1.168 sec/step)\n",
            "I1209 10:48:25.365116 139904075134848 learning.py:507] global step 155: loss = 3.9155 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 156: loss = 3.7254 (0.696 sec/step)\n",
            "I1209 10:48:26.333672 139904075134848 learning.py:507] global step 156: loss = 3.7254 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 157: loss = 3.7661 (0.750 sec/step)\n",
            "I1209 10:48:27.468118 139904075134848 learning.py:507] global step 157: loss = 3.7661 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 158: loss = 3.9303 (1.418 sec/step)\n",
            "I1209 10:48:28.923269 139904075134848 learning.py:507] global step 158: loss = 3.9303 (1.418 sec/step)\n",
            "INFO:tensorflow:global step 159: loss = 3.5880 (0.641 sec/step)\n",
            "I1209 10:48:29.856086 139904075134848 learning.py:507] global step 159: loss = 3.5880 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 160: loss = 3.8472 (0.642 sec/step)\n",
            "I1209 10:48:30.648733 139904075134848 learning.py:507] global step 160: loss = 3.8472 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 161: loss = 4.4344 (0.881 sec/step)\n",
            "I1209 10:48:31.765411 139904075134848 learning.py:507] global step 161: loss = 4.4344 (0.881 sec/step)\n",
            "INFO:tensorflow:global step 162: loss = 3.9852 (1.656 sec/step)\n",
            "I1209 10:48:33.691061 139904075134848 learning.py:507] global step 162: loss = 3.9852 (1.656 sec/step)\n",
            "INFO:tensorflow:global step 163: loss = 3.8400 (0.767 sec/step)\n",
            "I1209 10:48:34.739217 139904075134848 learning.py:507] global step 163: loss = 3.8400 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 164: loss = 3.8129 (0.762 sec/step)\n",
            "I1209 10:48:35.857355 139904075134848 learning.py:507] global step 164: loss = 3.8129 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 165: loss = 3.9659 (0.704 sec/step)\n",
            "I1209 10:48:36.794051 139904075134848 learning.py:507] global step 165: loss = 3.9659 (0.704 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 165.\n",
            "I1209 10:48:39.128739 139900543997696 supervisor.py:1050] Recording summary at step 165.\n",
            "INFO:tensorflow:global step 166: loss = 4.0599 (2.422 sec/step)\n",
            "I1209 10:48:39.296812 139904075134848 learning.py:507] global step 166: loss = 4.0599 (2.422 sec/step)\n",
            "INFO:tensorflow:global step 167: loss = 3.4059 (0.743 sec/step)\n",
            "I1209 10:48:40.209508 139904075134848 learning.py:507] global step 167: loss = 3.4059 (0.743 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.808188\n",
            "I1209 10:48:41.337714 139900552390400 supervisor.py:1099] global_step/sec: 0.808188\n",
            "INFO:tensorflow:global step 168: loss = 4.8223 (0.861 sec/step)\n",
            "I1209 10:48:41.399154 139904075134848 learning.py:507] global step 168: loss = 4.8223 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 169: loss = 3.4912 (1.338 sec/step)\n",
            "I1209 10:48:42.763901 139904075134848 learning.py:507] global step 169: loss = 3.4912 (1.338 sec/step)\n",
            "INFO:tensorflow:global step 170: loss = 3.5906 (0.590 sec/step)\n",
            "I1209 10:48:43.503076 139904075134848 learning.py:507] global step 170: loss = 3.5906 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 171: loss = 4.6277 (1.100 sec/step)\n",
            "I1209 10:48:44.911031 139904075134848 learning.py:507] global step 171: loss = 4.6277 (1.100 sec/step)\n",
            "INFO:tensorflow:global step 172: loss = 3.9037 (0.842 sec/step)\n",
            "I1209 10:48:46.013873 139904075134848 learning.py:507] global step 172: loss = 3.9037 (0.842 sec/step)\n",
            "INFO:tensorflow:global step 173: loss = 4.1446 (0.815 sec/step)\n",
            "I1209 10:48:47.060548 139904075134848 learning.py:507] global step 173: loss = 4.1446 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 174: loss = 3.5771 (0.699 sec/step)\n",
            "I1209 10:48:48.050870 139904075134848 learning.py:507] global step 174: loss = 3.5771 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 175: loss = 3.9879 (1.300 sec/step)\n",
            "I1209 10:48:49.465084 139904075134848 learning.py:507] global step 175: loss = 3.9879 (1.300 sec/step)\n",
            "INFO:tensorflow:global step 176: loss = 4.0828 (1.145 sec/step)\n",
            "I1209 10:48:50.612299 139904075134848 learning.py:507] global step 176: loss = 4.0828 (1.145 sec/step)\n",
            "INFO:tensorflow:global step 177: loss = 3.8472 (0.665 sec/step)\n",
            "I1209 10:48:51.479083 139904075134848 learning.py:507] global step 177: loss = 3.8472 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 178: loss = 3.7502 (1.750 sec/step)\n",
            "I1209 10:48:53.358344 139904075134848 learning.py:507] global step 178: loss = 3.7502 (1.750 sec/step)\n",
            "INFO:tensorflow:global step 179: loss = 3.7426 (0.810 sec/step)\n",
            "I1209 10:48:54.315376 139904075134848 learning.py:507] global step 179: loss = 3.7426 (0.810 sec/step)\n",
            "INFO:tensorflow:global step 180: loss = 3.5231 (0.735 sec/step)\n",
            "I1209 10:48:55.529109 139904075134848 learning.py:507] global step 180: loss = 3.5231 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 181: loss = 3.7006 (3.214 sec/step)\n",
            "I1209 10:48:58.866660 139904075134848 learning.py:507] global step 181: loss = 3.7006 (3.214 sec/step)\n",
            "INFO:tensorflow:global step 182: loss = 4.4407 (0.925 sec/step)\n",
            "I1209 10:48:59.805448 139904075134848 learning.py:507] global step 182: loss = 4.4407 (0.925 sec/step)\n",
            "INFO:tensorflow:global step 183: loss = 3.1639 (0.805 sec/step)\n",
            "I1209 10:49:01.095014 139904075134848 learning.py:507] global step 183: loss = 3.1639 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 184: loss = 3.8557 (0.793 sec/step)\n",
            "I1209 10:49:02.224212 139904075134848 learning.py:507] global step 184: loss = 3.8557 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 185: loss = 3.7387 (1.193 sec/step)\n",
            "I1209 10:49:03.424211 139904075134848 learning.py:507] global step 185: loss = 3.7387 (1.193 sec/step)\n",
            "INFO:tensorflow:global step 186: loss = 3.6902 (0.770 sec/step)\n",
            "I1209 10:49:04.464457 139904075134848 learning.py:507] global step 186: loss = 3.6902 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 187: loss = 4.1178 (0.648 sec/step)\n",
            "I1209 10:49:05.364738 139904075134848 learning.py:507] global step 187: loss = 4.1178 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 188: loss = 3.6901 (0.793 sec/step)\n",
            "I1209 10:49:06.316996 139904075134848 learning.py:507] global step 188: loss = 3.6901 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 189: loss = 4.1588 (0.981 sec/step)\n",
            "I1209 10:49:07.553276 139904075134848 learning.py:507] global step 189: loss = 4.1588 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 190: loss = 3.4789 (1.734 sec/step)\n",
            "I1209 10:49:09.539950 139904075134848 learning.py:507] global step 190: loss = 3.4789 (1.734 sec/step)\n",
            "INFO:tensorflow:global step 191: loss = 3.3076 (0.832 sec/step)\n",
            "I1209 10:49:10.747526 139904075134848 learning.py:507] global step 191: loss = 3.3076 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 192: loss = 3.7065 (0.740 sec/step)\n",
            "I1209 10:49:11.851938 139904075134848 learning.py:507] global step 192: loss = 3.7065 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 193: loss = 4.0388 (1.755 sec/step)\n",
            "I1209 10:49:13.632879 139904075134848 learning.py:507] global step 193: loss = 4.0388 (1.755 sec/step)\n",
            "INFO:tensorflow:global step 194: loss = 3.6819 (0.711 sec/step)\n",
            "I1209 10:49:14.610109 139904075134848 learning.py:507] global step 194: loss = 3.6819 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 195: loss = 3.4276 (0.752 sec/step)\n",
            "I1209 10:49:15.750240 139904075134848 learning.py:507] global step 195: loss = 3.4276 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 196: loss = 4.2167 (0.892 sec/step)\n",
            "I1209 10:49:16.742238 139904075134848 learning.py:507] global step 196: loss = 4.2167 (0.892 sec/step)\n",
            "INFO:tensorflow:global step 197: loss = 3.2598 (1.319 sec/step)\n",
            "I1209 10:49:18.225391 139904075134848 learning.py:507] global step 197: loss = 3.2598 (1.319 sec/step)\n",
            "INFO:tensorflow:global step 198: loss = 3.7720 (0.906 sec/step)\n",
            "I1209 10:49:19.348419 139904075134848 learning.py:507] global step 198: loss = 3.7720 (0.906 sec/step)\n",
            "INFO:tensorflow:global step 199: loss = 3.7786 (0.594 sec/step)\n",
            "I1209 10:49:20.290188 139904075134848 learning.py:507] global step 199: loss = 3.7786 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 3.3395 (0.821 sec/step)\n",
            "I1209 10:49:21.515520 139904075134848 learning.py:507] global step 200: loss = 3.3395 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 201: loss = 3.6443 (1.238 sec/step)\n",
            "I1209 10:49:22.778039 139904075134848 learning.py:507] global step 201: loss = 3.6443 (1.238 sec/step)\n",
            "INFO:tensorflow:global step 202: loss = 3.5366 (0.634 sec/step)\n",
            "I1209 10:49:23.449165 139904075134848 learning.py:507] global step 202: loss = 3.5366 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 203: loss = 3.5578 (2.659 sec/step)\n",
            "I1209 10:49:26.332518 139904075134848 learning.py:507] global step 203: loss = 3.5578 (2.659 sec/step)\n",
            "INFO:tensorflow:global step 204: loss = 4.0887 (0.687 sec/step)\n",
            "I1209 10:49:27.149118 139904075134848 learning.py:507] global step 204: loss = 4.0887 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 205: loss = 3.5934 (0.887 sec/step)\n",
            "I1209 10:49:28.627930 139904075134848 learning.py:507] global step 205: loss = 3.5934 (0.887 sec/step)\n",
            "INFO:tensorflow:global step 206: loss = 3.6898 (0.731 sec/step)\n",
            "I1209 10:49:29.693375 139904075134848 learning.py:507] global step 206: loss = 3.6898 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 207: loss = 3.5710 (0.640 sec/step)\n",
            "I1209 10:49:30.579801 139904075134848 learning.py:507] global step 207: loss = 3.5710 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 208: loss = 4.2606 (0.642 sec/step)\n",
            "I1209 10:49:31.224217 139904075134848 learning.py:507] global step 208: loss = 4.2606 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 209: loss = 3.8888 (1.800 sec/step)\n",
            "I1209 10:49:33.139807 139904075134848 learning.py:507] global step 209: loss = 3.8888 (1.800 sec/step)\n",
            "INFO:tensorflow:global step 210: loss = 3.1832 (2.120 sec/step)\n",
            "I1209 10:49:35.533953 139904075134848 learning.py:507] global step 210: loss = 3.1832 (2.120 sec/step)\n",
            "INFO:tensorflow:global step 211: loss = 3.3481 (0.592 sec/step)\n",
            "I1209 10:49:36.127885 139904075134848 learning.py:507] global step 211: loss = 3.3481 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 212: loss = 4.1009 (1.691 sec/step)\n",
            "I1209 10:49:37.820890 139904075134848 learning.py:507] global step 212: loss = 4.1009 (1.691 sec/step)\n",
            "INFO:tensorflow:global step 213: loss = 3.4670 (0.672 sec/step)\n",
            "I1209 10:49:38.622936 139904075134848 learning.py:507] global step 213: loss = 3.4670 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 214: loss = 3.6890 (0.687 sec/step)\n",
            "I1209 10:49:39.537482 139904075134848 learning.py:507] global step 214: loss = 3.6890 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 215: loss = 3.3936 (1.222 sec/step)\n",
            "I1209 10:49:41.264805 139904075134848 learning.py:507] global step 215: loss = 3.3936 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 216: loss = 2.9485 (1.668 sec/step)\n",
            "I1209 10:49:43.263004 139904075134848 learning.py:507] global step 216: loss = 2.9485 (1.668 sec/step)\n",
            "INFO:tensorflow:global step 217: loss = 3.4842 (0.767 sec/step)\n",
            "I1209 10:49:44.304082 139904075134848 learning.py:507] global step 217: loss = 3.4842 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 218: loss = 3.5413 (0.802 sec/step)\n",
            "I1209 10:49:45.427188 139904075134848 learning.py:507] global step 218: loss = 3.5413 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 219: loss = 3.6666 (1.363 sec/step)\n",
            "I1209 10:49:46.810035 139904075134848 learning.py:507] global step 219: loss = 3.6666 (1.363 sec/step)\n",
            "INFO:tensorflow:global step 220: loss = 3.3304 (0.747 sec/step)\n",
            "I1209 10:49:47.768812 139904075134848 learning.py:507] global step 220: loss = 3.3304 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 221: loss = 3.6532 (1.326 sec/step)\n",
            "I1209 10:49:49.184638 139904075134848 learning.py:507] global step 221: loss = 3.6532 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 222: loss = 3.5716 (0.708 sec/step)\n",
            "I1209 10:49:50.008943 139904075134848 learning.py:507] global step 222: loss = 3.5716 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 223: loss = 4.6599 (1.102 sec/step)\n",
            "I1209 10:49:51.323400 139904075134848 learning.py:507] global step 223: loss = 4.6599 (1.102 sec/step)\n",
            "INFO:tensorflow:global step 224: loss = 3.5466 (0.649 sec/step)\n",
            "I1209 10:49:51.974370 139904075134848 learning.py:507] global step 224: loss = 3.5466 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 225: loss = 3.9112 (1.842 sec/step)\n",
            "I1209 10:49:53.818648 139904075134848 learning.py:507] global step 225: loss = 3.9112 (1.842 sec/step)\n",
            "INFO:tensorflow:global step 226: loss = 3.1804 (0.591 sec/step)\n",
            "I1209 10:49:54.411051 139904075134848 learning.py:507] global step 226: loss = 3.1804 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 227: loss = 2.8206 (1.730 sec/step)\n",
            "I1209 10:49:56.344736 139904075134848 learning.py:507] global step 227: loss = 2.8206 (1.730 sec/step)\n",
            "INFO:tensorflow:global step 228: loss = 3.5482 (1.305 sec/step)\n",
            "I1209 10:49:57.724511 139904075134848 learning.py:507] global step 228: loss = 3.5482 (1.305 sec/step)\n",
            "INFO:tensorflow:global step 229: loss = 3.6021 (0.756 sec/step)\n",
            "I1209 10:49:58.755836 139904075134848 learning.py:507] global step 229: loss = 3.6021 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 230: loss = 3.8061 (0.700 sec/step)\n",
            "I1209 10:49:59.575790 139904075134848 learning.py:507] global step 230: loss = 3.8061 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 231: loss = 3.4978 (1.342 sec/step)\n",
            "I1209 10:50:01.212011 139904075134848 learning.py:507] global step 231: loss = 3.4978 (1.342 sec/step)\n",
            "INFO:tensorflow:global step 232: loss = 3.3565 (0.593 sec/step)\n",
            "I1209 10:50:02.082018 139904075134848 learning.py:507] global step 232: loss = 3.3565 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 233: loss = 3.6294 (0.717 sec/step)\n",
            "I1209 10:50:03.179857 139904075134848 learning.py:507] global step 233: loss = 3.6294 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 234: loss = 3.9040 (2.003 sec/step)\n",
            "I1209 10:50:05.185087 139904075134848 learning.py:507] global step 234: loss = 3.9040 (2.003 sec/step)\n",
            "INFO:tensorflow:global step 235: loss = 3.6128 (1.155 sec/step)\n",
            "I1209 10:50:06.341251 139904075134848 learning.py:507] global step 235: loss = 3.6128 (1.155 sec/step)\n",
            "INFO:tensorflow:global step 236: loss = 3.4726 (0.692 sec/step)\n",
            "I1209 10:50:07.034491 139904075134848 learning.py:507] global step 236: loss = 3.4726 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 237: loss = 3.1821 (1.475 sec/step)\n",
            "I1209 10:50:08.694042 139904075134848 learning.py:507] global step 237: loss = 3.1821 (1.475 sec/step)\n",
            "INFO:tensorflow:global step 238: loss = 3.5244 (3.126 sec/step)\n",
            "I1209 10:50:12.042333 139904075134848 learning.py:507] global step 238: loss = 3.5244 (3.126 sec/step)\n",
            "INFO:tensorflow:global step 239: loss = 3.4708 (0.795 sec/step)\n",
            "I1209 10:50:13.109403 139904075134848 learning.py:507] global step 239: loss = 3.4708 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 240: loss = 3.0257 (0.601 sec/step)\n",
            "I1209 10:50:13.838997 139904075134848 learning.py:507] global step 240: loss = 3.0257 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 241: loss = 3.6936 (2.596 sec/step)\n",
            "I1209 10:50:16.437066 139904075134848 learning.py:507] global step 241: loss = 3.6936 (2.596 sec/step)\n",
            "INFO:tensorflow:global step 242: loss = 3.2394 (0.571 sec/step)\n",
            "I1209 10:50:17.009968 139904075134848 learning.py:507] global step 242: loss = 3.2394 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 243: loss = 3.7962 (2.029 sec/step)\n",
            "I1209 10:50:19.040767 139904075134848 learning.py:507] global step 243: loss = 3.7962 (2.029 sec/step)\n",
            "INFO:tensorflow:global step 244: loss = 3.5198 (0.708 sec/step)\n",
            "I1209 10:50:19.959043 139904075134848 learning.py:507] global step 244: loss = 3.5198 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 245: loss = 3.5657 (1.938 sec/step)\n",
            "I1209 10:50:22.039158 139904075134848 learning.py:507] global step 245: loss = 3.5657 (1.938 sec/step)\n",
            "INFO:tensorflow:global step 246: loss = 5.0244 (0.696 sec/step)\n",
            "I1209 10:50:22.737045 139904075134848 learning.py:507] global step 246: loss = 5.0244 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 247: loss = 3.2948 (2.047 sec/step)\n",
            "I1209 10:50:24.786391 139904075134848 learning.py:507] global step 247: loss = 3.2948 (2.047 sec/step)\n",
            "INFO:tensorflow:global step 248: loss = 3.3848 (0.769 sec/step)\n",
            "I1209 10:50:25.804396 139904075134848 learning.py:507] global step 248: loss = 3.3848 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 249: loss = 3.9749 (2.259 sec/step)\n",
            "I1209 10:50:28.100932 139904075134848 learning.py:507] global step 249: loss = 3.9749 (2.259 sec/step)\n",
            "INFO:tensorflow:global step 250: loss = 3.5838 (1.041 sec/step)\n",
            "I1209 10:50:29.284788 139904075134848 learning.py:507] global step 250: loss = 3.5838 (1.041 sec/step)\n",
            "INFO:tensorflow:global step 251: loss = 4.2358 (1.479 sec/step)\n",
            "I1209 10:50:30.896753 139904075134848 learning.py:507] global step 251: loss = 4.2358 (1.479 sec/step)\n",
            "INFO:tensorflow:global step 252: loss = 4.5391 (0.670 sec/step)\n",
            "I1209 10:50:31.568752 139904075134848 learning.py:507] global step 252: loss = 4.5391 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 253: loss = 3.4153 (1.931 sec/step)\n",
            "I1209 10:50:33.501285 139904075134848 learning.py:507] global step 253: loss = 3.4153 (1.931 sec/step)\n",
            "INFO:tensorflow:global step 254: loss = 3.8654 (0.761 sec/step)\n",
            "I1209 10:50:34.531685 139904075134848 learning.py:507] global step 254: loss = 3.8654 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 255: loss = 3.2069 (0.645 sec/step)\n",
            "I1209 10:50:35.592868 139904075134848 learning.py:507] global step 255: loss = 3.2069 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 256: loss = 3.7843 (0.695 sec/step)\n",
            "I1209 10:50:36.390942 139904075134848 learning.py:507] global step 256: loss = 3.7843 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 257: loss = 3.4440 (2.303 sec/step)\n",
            "I1209 10:50:38.854865 139904075134848 learning.py:507] global step 257: loss = 3.4440 (2.303 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 257.\n",
            "I1209 10:50:41.071928 139900543997696 supervisor.py:1050] Recording summary at step 257.\n",
            "INFO:tensorflow:global_step/sec: 0.741216\n",
            "I1209 10:50:41.410793 139900552390400 supervisor.py:1099] global_step/sec: 0.741216\n",
            "INFO:tensorflow:global step 258: loss = 3.7569 (2.646 sec/step)\n",
            "I1209 10:50:41.754280 139904075134848 learning.py:507] global step 258: loss = 3.7569 (2.646 sec/step)\n",
            "INFO:tensorflow:global step 259: loss = 3.3390 (0.737 sec/step)\n",
            "I1209 10:50:42.775333 139904075134848 learning.py:507] global step 259: loss = 3.3390 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 260: loss = 3.6875 (1.666 sec/step)\n",
            "I1209 10:50:44.567020 139904075134848 learning.py:507] global step 260: loss = 3.6875 (1.666 sec/step)\n",
            "INFO:tensorflow:global step 261: loss = 3.1053 (0.667 sec/step)\n",
            "I1209 10:50:45.235857 139904075134848 learning.py:507] global step 261: loss = 3.1053 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 262: loss = 3.6952 (1.054 sec/step)\n",
            "I1209 10:50:46.352532 139904075134848 learning.py:507] global step 262: loss = 3.6952 (1.054 sec/step)\n",
            "INFO:tensorflow:global step 263: loss = 3.6297 (2.041 sec/step)\n",
            "I1209 10:50:48.413872 139904075134848 learning.py:507] global step 263: loss = 3.6297 (2.041 sec/step)\n",
            "INFO:tensorflow:global step 264: loss = 3.8879 (0.883 sec/step)\n",
            "I1209 10:50:49.513963 139904075134848 learning.py:507] global step 264: loss = 3.8879 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 265: loss = 3.3558 (2.037 sec/step)\n",
            "I1209 10:50:51.621534 139904075134848 learning.py:507] global step 265: loss = 3.3558 (2.037 sec/step)\n",
            "INFO:tensorflow:global step 266: loss = 3.9416 (0.652 sec/step)\n",
            "I1209 10:50:52.469763 139904075134848 learning.py:507] global step 266: loss = 3.9416 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 267: loss = 3.0612 (0.619 sec/step)\n",
            "I1209 10:50:53.492007 139904075134848 learning.py:507] global step 267: loss = 3.0612 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 268: loss = 3.7145 (1.514 sec/step)\n",
            "I1209 10:50:55.338148 139904075134848 learning.py:507] global step 268: loss = 3.7145 (1.514 sec/step)\n",
            "INFO:tensorflow:global step 269: loss = 3.8905 (0.805 sec/step)\n",
            "I1209 10:50:56.592221 139904075134848 learning.py:507] global step 269: loss = 3.8905 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 270: loss = 3.5598 (0.858 sec/step)\n",
            "I1209 10:50:57.542137 139904075134848 learning.py:507] global step 270: loss = 3.5598 (0.858 sec/step)\n",
            "INFO:tensorflow:global step 271: loss = 3.7058 (1.708 sec/step)\n",
            "I1209 10:50:59.252466 139904075134848 learning.py:507] global step 271: loss = 3.7058 (1.708 sec/step)\n",
            "INFO:tensorflow:global step 272: loss = 3.5380 (0.777 sec/step)\n",
            "I1209 10:51:00.278182 139904075134848 learning.py:507] global step 272: loss = 3.5380 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 273: loss = 3.1738 (1.348 sec/step)\n",
            "I1209 10:51:01.699100 139904075134848 learning.py:507] global step 273: loss = 3.1738 (1.348 sec/step)\n",
            "INFO:tensorflow:global step 274: loss = 3.5322 (0.645 sec/step)\n",
            "I1209 10:51:02.345683 139904075134848 learning.py:507] global step 274: loss = 3.5322 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 275: loss = 3.2226 (1.907 sec/step)\n",
            "I1209 10:51:04.254625 139904075134848 learning.py:507] global step 275: loss = 3.2226 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 276: loss = 3.7988 (0.691 sec/step)\n",
            "I1209 10:51:04.947645 139904075134848 learning.py:507] global step 276: loss = 3.7988 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 277: loss = 3.5814 (1.777 sec/step)\n",
            "I1209 10:51:06.726934 139904075134848 learning.py:507] global step 277: loss = 3.5814 (1.777 sec/step)\n",
            "INFO:tensorflow:global step 278: loss = 3.7145 (1.196 sec/step)\n",
            "I1209 10:51:07.924941 139904075134848 learning.py:507] global step 278: loss = 3.7145 (1.196 sec/step)\n",
            "INFO:tensorflow:global step 279: loss = 3.4036 (0.727 sec/step)\n",
            "I1209 10:51:08.653443 139904075134848 learning.py:507] global step 279: loss = 3.4036 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 280: loss = 3.3106 (1.855 sec/step)\n",
            "I1209 10:51:10.510666 139904075134848 learning.py:507] global step 280: loss = 3.3106 (1.855 sec/step)\n",
            "INFO:tensorflow:global step 281: loss = 3.0623 (0.734 sec/step)\n",
            "I1209 10:51:11.498085 139904075134848 learning.py:507] global step 281: loss = 3.0623 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 282: loss = 3.2312 (0.687 sec/step)\n",
            "I1209 10:51:12.292566 139904075134848 learning.py:507] global step 282: loss = 3.2312 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 283: loss = 3.9196 (0.926 sec/step)\n",
            "I1209 10:51:13.374214 139904075134848 learning.py:507] global step 283: loss = 3.9196 (0.926 sec/step)\n",
            "INFO:tensorflow:global step 284: loss = 3.6100 (1.615 sec/step)\n",
            "I1209 10:51:15.393577 139904075134848 learning.py:507] global step 284: loss = 3.6100 (1.615 sec/step)\n",
            "INFO:tensorflow:global step 285: loss = 3.5045 (0.627 sec/step)\n",
            "I1209 10:51:16.238155 139904075134848 learning.py:507] global step 285: loss = 3.5045 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 286: loss = 3.1504 (0.860 sec/step)\n",
            "I1209 10:51:17.509375 139904075134848 learning.py:507] global step 286: loss = 3.1504 (0.860 sec/step)\n",
            "INFO:tensorflow:global step 287: loss = 2.8855 (1.304 sec/step)\n",
            "I1209 10:51:18.866112 139904075134848 learning.py:507] global step 287: loss = 2.8855 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 288: loss = 2.8437 (0.693 sec/step)\n",
            "I1209 10:51:19.867603 139904075134848 learning.py:507] global step 288: loss = 2.8437 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 289: loss = 3.2129 (1.297 sec/step)\n",
            "I1209 10:51:21.266969 139904075134848 learning.py:507] global step 289: loss = 3.2129 (1.297 sec/step)\n",
            "INFO:tensorflow:global step 290: loss = 3.2656 (0.698 sec/step)\n",
            "I1209 10:51:22.062162 139904075134848 learning.py:507] global step 290: loss = 3.2656 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 291: loss = 3.6556 (1.720 sec/step)\n",
            "I1209 10:51:24.027853 139904075134848 learning.py:507] global step 291: loss = 3.6556 (1.720 sec/step)\n",
            "INFO:tensorflow:global step 292: loss = 3.5408 (0.698 sec/step)\n",
            "I1209 10:51:24.939073 139904075134848 learning.py:507] global step 292: loss = 3.5408 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 293: loss = 3.4300 (1.351 sec/step)\n",
            "I1209 10:51:26.316476 139904075134848 learning.py:507] global step 293: loss = 3.4300 (1.351 sec/step)\n",
            "INFO:tensorflow:global step 294: loss = 3.6907 (0.710 sec/step)\n",
            "I1209 10:51:27.228909 139904075134848 learning.py:507] global step 294: loss = 3.6907 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 295: loss = 3.8617 (1.375 sec/step)\n",
            "I1209 10:51:28.726505 139904075134848 learning.py:507] global step 295: loss = 3.8617 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 296: loss = 2.6608 (0.779 sec/step)\n",
            "I1209 10:51:29.796093 139904075134848 learning.py:507] global step 296: loss = 2.6608 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 297: loss = 3.5500 (1.701 sec/step)\n",
            "I1209 10:51:31.517712 139904075134848 learning.py:507] global step 297: loss = 3.5500 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 298: loss = 3.4124 (0.670 sec/step)\n",
            "I1209 10:51:32.189540 139904075134848 learning.py:507] global step 298: loss = 3.4124 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 299: loss = 3.3011 (1.833 sec/step)\n",
            "I1209 10:51:34.024280 139904075134848 learning.py:507] global step 299: loss = 3.3011 (1.833 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 3.0718 (0.748 sec/step)\n",
            "I1209 10:51:34.899482 139904075134848 learning.py:507] global step 300: loss = 3.0718 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 301: loss = 3.5098 (1.471 sec/step)\n",
            "I1209 10:51:36.618762 139904075134848 learning.py:507] global step 301: loss = 3.5098 (1.471 sec/step)\n",
            "INFO:tensorflow:global step 302: loss = 3.1087 (0.739 sec/step)\n",
            "I1209 10:51:37.457250 139904075134848 learning.py:507] global step 302: loss = 3.1087 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 303: loss = 3.4983 (1.539 sec/step)\n",
            "I1209 10:51:39.067493 139904075134848 learning.py:507] global step 303: loss = 3.4983 (1.539 sec/step)\n",
            "INFO:tensorflow:global step 304: loss = 3.7117 (0.640 sec/step)\n",
            "I1209 10:51:39.976382 139904075134848 learning.py:507] global step 304: loss = 3.7117 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 305: loss = 3.4258 (1.281 sec/step)\n",
            "I1209 10:51:41.488270 139904075134848 learning.py:507] global step 305: loss = 3.4258 (1.281 sec/step)\n",
            "INFO:tensorflow:global step 306: loss = 3.9458 (0.739 sec/step)\n",
            "I1209 10:51:42.436276 139904075134848 learning.py:507] global step 306: loss = 3.9458 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 307: loss = 3.5133 (0.620 sec/step)\n",
            "I1209 10:51:43.268926 139904075134848 learning.py:507] global step 307: loss = 3.5133 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 308: loss = 3.3674 (1.287 sec/step)\n",
            "I1209 10:51:44.776189 139904075134848 learning.py:507] global step 308: loss = 3.3674 (1.287 sec/step)\n",
            "INFO:tensorflow:global step 309: loss = 3.8748 (0.735 sec/step)\n",
            "I1209 10:51:45.779129 139904075134848 learning.py:507] global step 309: loss = 3.8748 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 310: loss = 3.3174 (0.646 sec/step)\n",
            "I1209 10:51:46.781011 139904075134848 learning.py:507] global step 310: loss = 3.3174 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 311: loss = 2.9252 (0.795 sec/step)\n",
            "I1209 10:51:47.960274 139904075134848 learning.py:507] global step 311: loss = 2.9252 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 312: loss = 3.3859 (0.780 sec/step)\n",
            "I1209 10:51:48.897581 139904075134848 learning.py:507] global step 312: loss = 3.3859 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 313: loss = 3.6617 (0.606 sec/step)\n",
            "I1209 10:51:49.993101 139904075134848 learning.py:507] global step 313: loss = 3.6617 (0.606 sec/step)\n",
            "INFO:tensorflow:global step 314: loss = 3.4108 (0.630 sec/step)\n",
            "I1209 10:51:50.951402 139904075134848 learning.py:507] global step 314: loss = 3.4108 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 315: loss = 2.7910 (0.630 sec/step)\n",
            "I1209 10:51:51.647082 139904075134848 learning.py:507] global step 315: loss = 2.7910 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 316: loss = 3.4087 (2.441 sec/step)\n",
            "I1209 10:51:54.096761 139904075134848 learning.py:507] global step 316: loss = 3.4087 (2.441 sec/step)\n",
            "INFO:tensorflow:global step 317: loss = 3.2791 (0.704 sec/step)\n",
            "I1209 10:51:55.117126 139904075134848 learning.py:507] global step 317: loss = 3.2791 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 318: loss = 3.5839 (2.842 sec/step)\n",
            "I1209 10:51:58.054285 139904075134848 learning.py:507] global step 318: loss = 3.5839 (2.842 sec/step)\n",
            "INFO:tensorflow:global step 319: loss = 3.8334 (0.664 sec/step)\n",
            "I1209 10:51:59.044476 139904075134848 learning.py:507] global step 319: loss = 3.8334 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 320: loss = 3.4260 (1.183 sec/step)\n",
            "I1209 10:52:00.291984 139904075134848 learning.py:507] global step 320: loss = 3.4260 (1.183 sec/step)\n",
            "INFO:tensorflow:global step 321: loss = 3.4975 (0.769 sec/step)\n",
            "I1209 10:52:01.376107 139904075134848 learning.py:507] global step 321: loss = 3.4975 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 322: loss = 4.1941 (1.317 sec/step)\n",
            "I1209 10:52:02.698349 139904075134848 learning.py:507] global step 322: loss = 4.1941 (1.317 sec/step)\n",
            "INFO:tensorflow:global step 323: loss = 3.8819 (0.762 sec/step)\n",
            "I1209 10:52:03.719347 139904075134848 learning.py:507] global step 323: loss = 3.8819 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 324: loss = 3.4172 (0.727 sec/step)\n",
            "I1209 10:52:04.763604 139904075134848 learning.py:507] global step 324: loss = 3.4172 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 325: loss = 3.4376 (1.380 sec/step)\n",
            "I1209 10:52:06.347714 139904075134848 learning.py:507] global step 325: loss = 3.4376 (1.380 sec/step)\n",
            "INFO:tensorflow:global step 326: loss = 3.0106 (0.620 sec/step)\n",
            "I1209 10:52:07.267582 139904075134848 learning.py:507] global step 326: loss = 3.0106 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 327: loss = 3.7641 (1.343 sec/step)\n",
            "I1209 10:52:08.748345 139904075134848 learning.py:507] global step 327: loss = 3.7641 (1.343 sec/step)\n",
            "INFO:tensorflow:global step 328: loss = 3.4316 (0.751 sec/step)\n",
            "I1209 10:52:09.779262 139904075134848 learning.py:507] global step 328: loss = 3.4316 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 329: loss = 2.9524 (1.579 sec/step)\n",
            "I1209 10:52:11.637540 139904075134848 learning.py:507] global step 329: loss = 2.9524 (1.579 sec/step)\n",
            "INFO:tensorflow:global step 330: loss = 3.3390 (0.659 sec/step)\n",
            "I1209 10:52:12.298593 139904075134848 learning.py:507] global step 330: loss = 3.3390 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 331: loss = 3.3809 (1.836 sec/step)\n",
            "I1209 10:52:14.136558 139904075134848 learning.py:507] global step 331: loss = 3.3809 (1.836 sec/step)\n",
            "INFO:tensorflow:global step 332: loss = 2.9736 (0.749 sec/step)\n",
            "I1209 10:52:15.072526 139904075134848 learning.py:507] global step 332: loss = 2.9736 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 333: loss = 3.7513 (0.898 sec/step)\n",
            "I1209 10:52:16.545835 139904075134848 learning.py:507] global step 333: loss = 3.7513 (0.898 sec/step)\n",
            "INFO:tensorflow:global step 334: loss = 3.8713 (0.770 sec/step)\n",
            "I1209 10:52:17.534917 139904075134848 learning.py:507] global step 334: loss = 3.8713 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 335: loss = 3.6078 (0.781 sec/step)\n",
            "I1209 10:52:18.487312 139904075134848 learning.py:507] global step 335: loss = 3.6078 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 336: loss = 3.4404 (1.668 sec/step)\n",
            "I1209 10:52:20.292634 139904075134848 learning.py:507] global step 336: loss = 3.4404 (1.668 sec/step)\n",
            "INFO:tensorflow:global step 337: loss = 3.8883 (0.793 sec/step)\n",
            "I1209 10:52:21.551530 139904075134848 learning.py:507] global step 337: loss = 3.8883 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 338: loss = 3.1612 (0.676 sec/step)\n",
            "I1209 10:52:22.599679 139904075134848 learning.py:507] global step 338: loss = 3.1612 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 339: loss = 3.2163 (1.431 sec/step)\n",
            "I1209 10:52:24.057470 139904075134848 learning.py:507] global step 339: loss = 3.2163 (1.431 sec/step)\n",
            "INFO:tensorflow:global step 340: loss = 2.9960 (1.127 sec/step)\n",
            "I1209 10:52:25.191321 139904075134848 learning.py:507] global step 340: loss = 2.9960 (1.127 sec/step)\n",
            "INFO:tensorflow:global step 341: loss = 3.4860 (1.377 sec/step)\n",
            "I1209 10:52:26.580997 139904075134848 learning.py:507] global step 341: loss = 3.4860 (1.377 sec/step)\n",
            "INFO:tensorflow:global step 342: loss = 3.4641 (0.896 sec/step)\n",
            "I1209 10:52:27.618957 139904075134848 learning.py:507] global step 342: loss = 3.4641 (0.896 sec/step)\n",
            "INFO:tensorflow:global step 343: loss = 3.6527 (0.819 sec/step)\n",
            "I1209 10:52:28.598668 139904075134848 learning.py:507] global step 343: loss = 3.6527 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 344: loss = 3.4634 (1.292 sec/step)\n",
            "I1209 10:52:30.074507 139904075134848 learning.py:507] global step 344: loss = 3.4634 (1.292 sec/step)\n",
            "INFO:tensorflow:global step 345: loss = 3.4079 (0.861 sec/step)\n",
            "I1209 10:52:31.070423 139904075134848 learning.py:507] global step 345: loss = 3.4079 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 346: loss = 4.0193 (1.269 sec/step)\n",
            "I1209 10:52:32.343539 139904075134848 learning.py:507] global step 346: loss = 4.0193 (1.269 sec/step)\n",
            "INFO:tensorflow:global step 347: loss = 3.4723 (0.636 sec/step)\n",
            "I1209 10:52:32.981747 139904075134848 learning.py:507] global step 347: loss = 3.4723 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 348: loss = 3.4600 (1.713 sec/step)\n",
            "I1209 10:52:34.696470 139904075134848 learning.py:507] global step 348: loss = 3.4600 (1.713 sec/step)\n",
            "INFO:tensorflow:global step 349: loss = 3.4611 (0.662 sec/step)\n",
            "I1209 10:52:35.585025 139904075134848 learning.py:507] global step 349: loss = 3.4611 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 350: loss = 3.6297 (1.223 sec/step)\n",
            "I1209 10:52:36.987839 139904075134848 learning.py:507] global step 350: loss = 3.6297 (1.223 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 350.\n",
            "I1209 10:52:38.818019 139900543997696 supervisor.py:1050] Recording summary at step 350.\n",
            "INFO:tensorflow:global step 351: loss = 3.3681 (2.074 sec/step)\n",
            "I1209 10:52:39.063812 139904075134848 learning.py:507] global step 351: loss = 3.3681 (2.074 sec/step)\n",
            "INFO:tensorflow:global step 352: loss = 3.5757 (0.735 sec/step)\n",
            "I1209 10:52:40.050711 139904075134848 learning.py:507] global step 352: loss = 3.5757 (0.735 sec/step)\n",
            "INFO:tensorflow:global step 353: loss = 3.6877 (0.646 sec/step)\n",
            "I1209 10:52:40.721384 139904075134848 learning.py:507] global step 353: loss = 3.6877 (0.646 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.800592\n",
            "I1209 10:52:41.321851 139900552390400 supervisor.py:1099] global_step/sec: 0.800592\n",
            "INFO:tensorflow:global step 354: loss = 2.9912 (2.121 sec/step)\n",
            "I1209 10:52:43.084304 139904075134848 learning.py:507] global step 354: loss = 2.9912 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 355: loss = 2.6905 (2.099 sec/step)\n",
            "I1209 10:52:45.540702 139904075134848 learning.py:507] global step 355: loss = 2.6905 (2.099 sec/step)\n",
            "INFO:tensorflow:global step 356: loss = 3.4013 (0.799 sec/step)\n",
            "I1209 10:52:46.381457 139904075134848 learning.py:507] global step 356: loss = 3.4013 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 357: loss = 3.5879 (0.773 sec/step)\n",
            "I1209 10:52:47.617391 139904075134848 learning.py:507] global step 357: loss = 3.5879 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 358: loss = 3.1379 (2.000 sec/step)\n",
            "I1209 10:52:49.726191 139904075134848 learning.py:507] global step 358: loss = 3.1379 (2.000 sec/step)\n",
            "INFO:tensorflow:global step 359: loss = 2.9629 (0.691 sec/step)\n",
            "I1209 10:52:50.729939 139904075134848 learning.py:507] global step 359: loss = 2.9629 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 360: loss = 3.3058 (0.781 sec/step)\n",
            "I1209 10:52:51.863210 139904075134848 learning.py:507] global step 360: loss = 3.3058 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 361: loss = 3.5055 (0.736 sec/step)\n",
            "I1209 10:52:53.068432 139904075134848 learning.py:507] global step 361: loss = 3.5055 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 362: loss = 3.3571 (1.422 sec/step)\n",
            "I1209 10:52:54.497930 139904075134848 learning.py:507] global step 362: loss = 3.3571 (1.422 sec/step)\n",
            "INFO:tensorflow:global step 363: loss = 2.8719 (0.591 sec/step)\n",
            "I1209 10:52:55.222797 139904075134848 learning.py:507] global step 363: loss = 2.8719 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 364: loss = 3.8489 (1.868 sec/step)\n",
            "I1209 10:52:57.093114 139904075134848 learning.py:507] global step 364: loss = 3.8489 (1.868 sec/step)\n",
            "INFO:tensorflow:global step 365: loss = 3.4876 (0.883 sec/step)\n",
            "I1209 10:52:58.302271 139904075134848 learning.py:507] global step 365: loss = 3.4876 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 366: loss = 3.2658 (1.401 sec/step)\n",
            "I1209 10:52:59.718467 139904075134848 learning.py:507] global step 366: loss = 3.2658 (1.401 sec/step)\n",
            "INFO:tensorflow:global step 367: loss = 2.8816 (0.644 sec/step)\n",
            "I1209 10:53:00.364575 139904075134848 learning.py:507] global step 367: loss = 2.8816 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 368: loss = 3.3255 (1.861 sec/step)\n",
            "I1209 10:53:02.227338 139904075134848 learning.py:507] global step 368: loss = 3.3255 (1.861 sec/step)\n",
            "INFO:tensorflow:global step 369: loss = 3.3982 (0.749 sec/step)\n",
            "I1209 10:53:03.274114 139904075134848 learning.py:507] global step 369: loss = 3.3982 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 370: loss = 2.7016 (0.663 sec/step)\n",
            "I1209 10:53:04.130419 139904075134848 learning.py:507] global step 370: loss = 2.7016 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 371: loss = 2.7622 (0.841 sec/step)\n",
            "I1209 10:53:05.496872 139904075134848 learning.py:507] global step 371: loss = 2.7622 (0.841 sec/step)\n",
            "INFO:tensorflow:global step 372: loss = 3.8160 (1.442 sec/step)\n",
            "I1209 10:53:07.013340 139904075134848 learning.py:507] global step 372: loss = 3.8160 (1.442 sec/step)\n",
            "INFO:tensorflow:global step 373: loss = 3.5087 (0.833 sec/step)\n",
            "I1209 10:53:08.105836 139904075134848 learning.py:507] global step 373: loss = 3.5087 (0.833 sec/step)\n",
            "INFO:tensorflow:global step 374: loss = 3.1889 (1.713 sec/step)\n",
            "I1209 10:53:09.888490 139904075134848 learning.py:507] global step 374: loss = 3.1889 (1.713 sec/step)\n",
            "INFO:tensorflow:global step 375: loss = 2.7334 (0.865 sec/step)\n",
            "I1209 10:53:10.915083 139904075134848 learning.py:507] global step 375: loss = 2.7334 (0.865 sec/step)\n",
            "INFO:tensorflow:global step 376: loss = 3.4218 (1.432 sec/step)\n",
            "I1209 10:53:12.420646 139904075134848 learning.py:507] global step 376: loss = 3.4218 (1.432 sec/step)\n",
            "INFO:tensorflow:global step 377: loss = 3.0658 (0.924 sec/step)\n",
            "I1209 10:53:13.424838 139904075134848 learning.py:507] global step 377: loss = 3.0658 (0.924 sec/step)\n",
            "INFO:tensorflow:global step 378: loss = 3.1046 (1.214 sec/step)\n",
            "I1209 10:53:14.805784 139904075134848 learning.py:507] global step 378: loss = 3.1046 (1.214 sec/step)\n",
            "INFO:tensorflow:global step 379: loss = 3.1287 (0.819 sec/step)\n",
            "I1209 10:53:15.652592 139904075134848 learning.py:507] global step 379: loss = 3.1287 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 380: loss = 3.0895 (1.489 sec/step)\n",
            "I1209 10:53:17.150969 139904075134848 learning.py:507] global step 380: loss = 3.0895 (1.489 sec/step)\n",
            "INFO:tensorflow:global step 381: loss = 3.7657 (0.808 sec/step)\n",
            "I1209 10:53:18.158084 139904075134848 learning.py:507] global step 381: loss = 3.7657 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 382: loss = 3.2468 (1.326 sec/step)\n",
            "I1209 10:53:19.550108 139904075134848 learning.py:507] global step 382: loss = 3.2468 (1.326 sec/step)\n",
            "INFO:tensorflow:global step 383: loss = 3.1233 (0.744 sec/step)\n",
            "I1209 10:53:20.467911 139904075134848 learning.py:507] global step 383: loss = 3.1233 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 384: loss = 2.6254 (1.540 sec/step)\n",
            "I1209 10:53:22.024232 139904075134848 learning.py:507] global step 384: loss = 2.6254 (1.540 sec/step)\n",
            "INFO:tensorflow:global step 385: loss = 3.2313 (0.543 sec/step)\n",
            "I1209 10:53:22.855761 139904075134848 learning.py:507] global step 385: loss = 3.2313 (0.543 sec/step)\n",
            "INFO:tensorflow:global step 386: loss = 2.7236 (0.865 sec/step)\n",
            "I1209 10:53:23.917001 139904075134848 learning.py:507] global step 386: loss = 2.7236 (0.865 sec/step)\n",
            "INFO:tensorflow:global step 387: loss = 3.3319 (1.738 sec/step)\n",
            "I1209 10:53:25.656950 139904075134848 learning.py:507] global step 387: loss = 3.3319 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 388: loss = 3.1824 (0.722 sec/step)\n",
            "I1209 10:53:26.619181 139904075134848 learning.py:507] global step 388: loss = 3.1824 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 389: loss = 3.2328 (1.545 sec/step)\n",
            "I1209 10:53:28.252796 139904075134848 learning.py:507] global step 389: loss = 3.2328 (1.545 sec/step)\n",
            "INFO:tensorflow:global step 390: loss = 3.8068 (0.616 sec/step)\n",
            "I1209 10:53:28.870639 139904075134848 learning.py:507] global step 390: loss = 3.8068 (0.616 sec/step)\n",
            "INFO:tensorflow:global step 391: loss = 3.4819 (0.981 sec/step)\n",
            "I1209 10:53:29.961186 139904075134848 learning.py:507] global step 391: loss = 3.4819 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 392: loss = 3.5395 (1.710 sec/step)\n",
            "I1209 10:53:31.898252 139904075134848 learning.py:507] global step 392: loss = 3.5395 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 393: loss = 3.3875 (0.719 sec/step)\n",
            "I1209 10:53:32.873658 139904075134848 learning.py:507] global step 393: loss = 3.3875 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 394: loss = 2.9305 (0.780 sec/step)\n",
            "I1209 10:53:33.756802 139904075134848 learning.py:507] global step 394: loss = 2.9305 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 395: loss = 3.8050 (2.162 sec/step)\n",
            "I1209 10:53:35.933523 139904075134848 learning.py:507] global step 395: loss = 3.8050 (2.162 sec/step)\n",
            "INFO:tensorflow:global step 396: loss = 3.8872 (0.660 sec/step)\n",
            "I1209 10:53:36.865807 139904075134848 learning.py:507] global step 396: loss = 3.8872 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 397: loss = 3.7091 (0.721 sec/step)\n",
            "I1209 10:53:38.011182 139904075134848 learning.py:507] global step 397: loss = 3.7091 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 398: loss = 3.4700 (1.572 sec/step)\n",
            "I1209 10:53:39.590843 139904075134848 learning.py:507] global step 398: loss = 3.4700 (1.572 sec/step)\n",
            "INFO:tensorflow:global step 399: loss = 3.8730 (0.579 sec/step)\n",
            "I1209 10:53:40.528131 139904075134848 learning.py:507] global step 399: loss = 3.8730 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 2.9109 (1.204 sec/step)\n",
            "I1209 10:53:41.909632 139904075134848 learning.py:507] global step 400: loss = 2.9109 (1.204 sec/step)\n",
            "INFO:tensorflow:global step 401: loss = 3.8546 (0.548 sec/step)\n",
            "I1209 10:53:42.713993 139904075134848 learning.py:507] global step 401: loss = 3.8546 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 402: loss = 3.0285 (0.785 sec/step)\n",
            "I1209 10:53:43.880087 139904075134848 learning.py:507] global step 402: loss = 3.0285 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 403: loss = 3.5220 (0.806 sec/step)\n",
            "I1209 10:53:45.028455 139904075134848 learning.py:507] global step 403: loss = 3.5220 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 404: loss = 3.2853 (0.717 sec/step)\n",
            "I1209 10:53:46.127427 139904075134848 learning.py:507] global step 404: loss = 3.2853 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 405: loss = 3.6844 (0.747 sec/step)\n",
            "I1209 10:53:47.271413 139904075134848 learning.py:507] global step 405: loss = 3.6844 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 406: loss = 3.8573 (1.817 sec/step)\n",
            "I1209 10:53:49.194097 139904075134848 learning.py:507] global step 406: loss = 3.8573 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 407: loss = 2.7276 (0.728 sec/step)\n",
            "I1209 10:53:49.924471 139904075134848 learning.py:507] global step 407: loss = 2.7276 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 408: loss = 3.2511 (2.435 sec/step)\n",
            "I1209 10:53:52.361647 139904075134848 learning.py:507] global step 408: loss = 3.2511 (2.435 sec/step)\n",
            "INFO:tensorflow:global step 409: loss = 3.2378 (0.755 sec/step)\n",
            "I1209 10:53:53.296763 139904075134848 learning.py:507] global step 409: loss = 3.2378 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 410: loss = 3.0849 (0.810 sec/step)\n",
            "I1209 10:53:54.284265 139904075134848 learning.py:507] global step 410: loss = 3.0849 (0.810 sec/step)\n",
            "INFO:tensorflow:global step 411: loss = 2.8038 (1.554 sec/step)\n",
            "I1209 10:53:55.839735 139904075134848 learning.py:507] global step 411: loss = 2.8038 (1.554 sec/step)\n",
            "INFO:tensorflow:global step 412: loss = 3.6315 (0.633 sec/step)\n",
            "I1209 10:53:56.474282 139904075134848 learning.py:507] global step 412: loss = 3.6315 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 413: loss = 3.3464 (1.107 sec/step)\n",
            "I1209 10:53:57.754011 139904075134848 learning.py:507] global step 413: loss = 3.3464 (1.107 sec/step)\n",
            "INFO:tensorflow:global step 414: loss = 3.8018 (2.086 sec/step)\n",
            "I1209 10:54:00.126748 139904075134848 learning.py:507] global step 414: loss = 3.8018 (2.086 sec/step)\n",
            "INFO:tensorflow:global step 415: loss = 2.6786 (0.821 sec/step)\n",
            "I1209 10:54:01.127218 139904075134848 learning.py:507] global step 415: loss = 2.6786 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 416: loss = 2.9990 (1.459 sec/step)\n",
            "I1209 10:54:02.726243 139904075134848 learning.py:507] global step 416: loss = 2.9990 (1.459 sec/step)\n",
            "INFO:tensorflow:global step 417: loss = 3.0130 (0.702 sec/step)\n",
            "I1209 10:54:03.695473 139904075134848 learning.py:507] global step 417: loss = 3.0130 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 418: loss = 2.7517 (0.776 sec/step)\n",
            "I1209 10:54:04.795312 139904075134848 learning.py:507] global step 418: loss = 2.7517 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 419: loss = 3.7941 (0.817 sec/step)\n",
            "I1209 10:54:06.286198 139904075134848 learning.py:507] global step 419: loss = 3.7941 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 420: loss = 2.8520 (0.786 sec/step)\n",
            "I1209 10:54:07.395953 139904075134848 learning.py:507] global step 420: loss = 2.8520 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 421: loss = 3.1721 (2.014 sec/step)\n",
            "I1209 10:54:09.486497 139904075134848 learning.py:507] global step 421: loss = 3.1721 (2.014 sec/step)\n",
            "INFO:tensorflow:global step 422: loss = 2.9868 (0.724 sec/step)\n",
            "I1209 10:54:10.480076 139904075134848 learning.py:507] global step 422: loss = 2.9868 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 423: loss = 3.0184 (0.676 sec/step)\n",
            "I1209 10:54:11.541468 139904075134848 learning.py:507] global step 423: loss = 3.0184 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 424: loss = 3.1901 (0.887 sec/step)\n",
            "I1209 10:54:12.643667 139904075134848 learning.py:507] global step 424: loss = 3.1901 (0.887 sec/step)\n",
            "INFO:tensorflow:global step 425: loss = 3.1913 (2.439 sec/step)\n",
            "I1209 10:54:15.170846 139904075134848 learning.py:507] global step 425: loss = 3.1913 (2.439 sec/step)\n",
            "INFO:tensorflow:global step 426: loss = 3.3363 (0.597 sec/step)\n",
            "I1209 10:54:15.769728 139904075134848 learning.py:507] global step 426: loss = 3.3363 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 427: loss = 3.4036 (1.960 sec/step)\n",
            "I1209 10:54:17.731378 139904075134848 learning.py:507] global step 427: loss = 3.4036 (1.960 sec/step)\n",
            "INFO:tensorflow:global step 428: loss = 2.9775 (0.927 sec/step)\n",
            "I1209 10:54:18.672522 139904075134848 learning.py:507] global step 428: loss = 2.9775 (0.927 sec/step)\n",
            "INFO:tensorflow:global step 429: loss = 2.7425 (0.838 sec/step)\n",
            "I1209 10:54:19.833319 139904075134848 learning.py:507] global step 429: loss = 2.7425 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 430: loss = 3.4705 (1.992 sec/step)\n",
            "I1209 10:54:21.885055 139904075134848 learning.py:507] global step 430: loss = 3.4705 (1.992 sec/step)\n",
            "INFO:tensorflow:global step 431: loss = 4.0767 (0.658 sec/step)\n",
            "I1209 10:54:22.545029 139904075134848 learning.py:507] global step 431: loss = 4.0767 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 432: loss = 3.0142 (1.889 sec/step)\n",
            "I1209 10:54:24.437495 139904075134848 learning.py:507] global step 432: loss = 3.0142 (1.889 sec/step)\n",
            "INFO:tensorflow:global step 433: loss = 2.5440 (0.651 sec/step)\n",
            "I1209 10:54:25.090311 139904075134848 learning.py:507] global step 433: loss = 2.5440 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 434: loss = 3.4476 (1.911 sec/step)\n",
            "I1209 10:54:27.011655 139904075134848 learning.py:507] global step 434: loss = 3.4476 (1.911 sec/step)\n",
            "INFO:tensorflow:global step 435: loss = 3.4247 (0.556 sec/step)\n",
            "I1209 10:54:27.581904 139904075134848 learning.py:507] global step 435: loss = 3.4247 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 436: loss = 3.4262 (1.022 sec/step)\n",
            "I1209 10:54:28.830424 139904075134848 learning.py:507] global step 436: loss = 3.4262 (1.022 sec/step)\n",
            "INFO:tensorflow:global step 437: loss = 2.3771 (1.674 sec/step)\n",
            "I1209 10:54:30.814354 139904075134848 learning.py:507] global step 437: loss = 2.3771 (1.674 sec/step)\n",
            "INFO:tensorflow:global step 438: loss = 2.4895 (0.757 sec/step)\n",
            "I1209 10:54:31.690187 139904075134848 learning.py:507] global step 438: loss = 2.4895 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 439: loss = 3.6895 (2.095 sec/step)\n",
            "I1209 10:54:33.788485 139904075134848 learning.py:507] global step 439: loss = 3.6895 (2.095 sec/step)\n",
            "INFO:tensorflow:global step 440: loss = 2.9667 (0.628 sec/step)\n",
            "I1209 10:54:34.544491 139904075134848 learning.py:507] global step 440: loss = 2.9667 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 441: loss = 2.8460 (0.742 sec/step)\n",
            "I1209 10:54:35.652961 139904075134848 learning.py:507] global step 441: loss = 2.8460 (0.742 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 10:54:36.805155 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 442.\n",
            "I1209 10:54:38.601642 139900543997696 supervisor.py:1050] Recording summary at step 442.\n",
            "INFO:tensorflow:global step 442: loss = 4.0116 (2.951 sec/step)\n",
            "I1209 10:54:38.985535 139904075134848 learning.py:507] global step 442: loss = 4.0116 (2.951 sec/step)\n",
            "INFO:tensorflow:global step 443: loss = 3.5215 (1.219 sec/step)\n",
            "I1209 10:54:40.739797 139904075134848 learning.py:507] global step 443: loss = 3.5215 (1.219 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.748741\n",
            "I1209 10:54:41.523669 139900552390400 supervisor.py:1099] global_step/sec: 0.748741\n",
            "INFO:tensorflow:global step 444: loss = 3.0352 (1.120 sec/step)\n",
            "I1209 10:54:42.637584 139904075134848 learning.py:507] global step 444: loss = 3.0352 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 445: loss = 3.4774 (1.976 sec/step)\n",
            "I1209 10:54:44.615045 139904075134848 learning.py:507] global step 445: loss = 3.4774 (1.976 sec/step)\n",
            "INFO:tensorflow:global step 446: loss = 3.0285 (0.670 sec/step)\n",
            "I1209 10:54:45.471455 139904075134848 learning.py:507] global step 446: loss = 3.0285 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 447: loss = 3.5464 (0.753 sec/step)\n",
            "I1209 10:54:46.510158 139904075134848 learning.py:507] global step 447: loss = 3.5464 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 448: loss = 3.8695 (0.862 sec/step)\n",
            "I1209 10:54:47.748812 139904075134848 learning.py:507] global step 448: loss = 3.8695 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 449: loss = 2.8125 (0.785 sec/step)\n",
            "I1209 10:54:48.843744 139904075134848 learning.py:507] global step 449: loss = 2.8125 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 450: loss = 3.1865 (3.029 sec/step)\n",
            "I1209 10:54:52.028916 139904075134848 learning.py:507] global step 450: loss = 3.1865 (3.029 sec/step)\n",
            "INFO:tensorflow:global step 451: loss = 2.8698 (0.769 sec/step)\n",
            "I1209 10:54:52.953799 139904075134848 learning.py:507] global step 451: loss = 2.8698 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 452: loss = 3.0488 (0.651 sec/step)\n",
            "I1209 10:54:53.792524 139904075134848 learning.py:507] global step 452: loss = 3.0488 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 453: loss = 3.6037 (1.947 sec/step)\n",
            "I1209 10:54:55.803727 139904075134848 learning.py:507] global step 453: loss = 3.6037 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 454: loss = 3.3759 (0.799 sec/step)\n",
            "I1209 10:54:56.852891 139904075134848 learning.py:507] global step 454: loss = 3.3759 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 455: loss = 3.1197 (0.837 sec/step)\n",
            "I1209 10:54:57.886134 139904075134848 learning.py:507] global step 455: loss = 3.1197 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 456: loss = 3.5256 (2.389 sec/step)\n",
            "I1209 10:55:00.480514 139904075134848 learning.py:507] global step 456: loss = 3.5256 (2.389 sec/step)\n",
            "INFO:tensorflow:global step 457: loss = 3.1758 (0.747 sec/step)\n",
            "I1209 10:55:01.426800 139904075134848 learning.py:507] global step 457: loss = 3.1758 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 458: loss = 2.7277 (1.666 sec/step)\n",
            "I1209 10:55:03.119279 139904075134848 learning.py:507] global step 458: loss = 2.7277 (1.666 sec/step)\n",
            "INFO:tensorflow:global step 459: loss = 3.7127 (0.715 sec/step)\n",
            "I1209 10:55:03.836389 139904075134848 learning.py:507] global step 459: loss = 3.7127 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 460: loss = 2.7679 (0.984 sec/step)\n",
            "I1209 10:55:04.885690 139904075134848 learning.py:507] global step 460: loss = 2.7679 (0.984 sec/step)\n",
            "INFO:tensorflow:global step 461: loss = 2.4509 (2.051 sec/step)\n",
            "I1209 10:55:07.055067 139904075134848 learning.py:507] global step 461: loss = 2.4509 (2.051 sec/step)\n",
            "INFO:tensorflow:global step 462: loss = 2.9751 (0.818 sec/step)\n",
            "I1209 10:55:08.090821 139904075134848 learning.py:507] global step 462: loss = 2.9751 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 463: loss = 2.3633 (2.288 sec/step)\n",
            "I1209 10:55:10.391837 139904075134848 learning.py:507] global step 463: loss = 2.3633 (2.288 sec/step)\n",
            "INFO:tensorflow:global step 464: loss = 3.8046 (0.798 sec/step)\n",
            "I1209 10:55:11.360780 139904075134848 learning.py:507] global step 464: loss = 3.8046 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 465: loss = 3.3719 (2.017 sec/step)\n",
            "I1209 10:55:13.597799 139904075134848 learning.py:507] global step 465: loss = 3.3719 (2.017 sec/step)\n",
            "INFO:tensorflow:global step 466: loss = 3.4130 (0.717 sec/step)\n",
            "I1209 10:55:14.454864 139904075134848 learning.py:507] global step 466: loss = 3.4130 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 467: loss = 2.9582 (0.973 sec/step)\n",
            "I1209 10:55:15.691011 139904075134848 learning.py:507] global step 467: loss = 2.9582 (0.973 sec/step)\n",
            "INFO:tensorflow:global step 468: loss = 3.0547 (2.262 sec/step)\n",
            "I1209 10:55:17.954466 139904075134848 learning.py:507] global step 468: loss = 3.0547 (2.262 sec/step)\n",
            "INFO:tensorflow:global step 469: loss = 3.1069 (0.690 sec/step)\n",
            "I1209 10:55:18.871762 139904075134848 learning.py:507] global step 469: loss = 3.1069 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 470: loss = 3.0091 (2.397 sec/step)\n",
            "I1209 10:55:21.432999 139904075134848 learning.py:507] global step 470: loss = 3.0091 (2.397 sec/step)\n",
            "INFO:tensorflow:global step 471: loss = 3.7313 (0.777 sec/step)\n",
            "I1209 10:55:22.236417 139904075134848 learning.py:507] global step 471: loss = 3.7313 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 472: loss = 3.3407 (1.653 sec/step)\n",
            "I1209 10:55:23.960561 139904075134848 learning.py:507] global step 472: loss = 3.3407 (1.653 sec/step)\n",
            "INFO:tensorflow:global step 473: loss = 2.9303 (0.753 sec/step)\n",
            "I1209 10:55:24.913802 139904075134848 learning.py:507] global step 473: loss = 2.9303 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 474: loss = 3.1326 (0.576 sec/step)\n",
            "I1209 10:55:25.724761 139904075134848 learning.py:507] global step 474: loss = 3.1326 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 475: loss = 3.1484 (1.808 sec/step)\n",
            "I1209 10:55:27.534387 139904075134848 learning.py:507] global step 475: loss = 3.1484 (1.808 sec/step)\n",
            "INFO:tensorflow:global step 476: loss = 3.4263 (0.714 sec/step)\n",
            "I1209 10:55:28.250895 139904075134848 learning.py:507] global step 476: loss = 3.4263 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 477: loss = 2.7264 (1.085 sec/step)\n",
            "I1209 10:55:29.494620 139904075134848 learning.py:507] global step 477: loss = 2.7264 (1.085 sec/step)\n",
            "INFO:tensorflow:global step 478: loss = 3.3592 (1.716 sec/step)\n",
            "I1209 10:55:31.472872 139904075134848 learning.py:507] global step 478: loss = 3.3592 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 479: loss = 2.7072 (0.745 sec/step)\n",
            "I1209 10:55:32.501111 139904075134848 learning.py:507] global step 479: loss = 2.7072 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 480: loss = 3.0467 (2.364 sec/step)\n",
            "I1209 10:55:34.953764 139904075134848 learning.py:507] global step 480: loss = 3.0467 (2.364 sec/step)\n",
            "INFO:tensorflow:global step 481: loss = 3.6037 (0.755 sec/step)\n",
            "I1209 10:55:35.710847 139904075134848 learning.py:507] global step 481: loss = 3.6037 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 482: loss = 2.7823 (1.084 sec/step)\n",
            "I1209 10:55:37.013588 139904075134848 learning.py:507] global step 482: loss = 2.7823 (1.084 sec/step)\n",
            "INFO:tensorflow:global step 483: loss = 2.7013 (1.765 sec/step)\n",
            "I1209 10:55:39.047099 139904075134848 learning.py:507] global step 483: loss = 2.7013 (1.765 sec/step)\n",
            "INFO:tensorflow:global step 484: loss = 3.3365 (0.717 sec/step)\n",
            "I1209 10:55:39.992344 139904075134848 learning.py:507] global step 484: loss = 3.3365 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 485: loss = 3.2068 (2.381 sec/step)\n",
            "I1209 10:55:42.375295 139904075134848 learning.py:507] global step 485: loss = 3.2068 (2.381 sec/step)\n",
            "INFO:tensorflow:global step 486: loss = 3.2079 (0.642 sec/step)\n",
            "I1209 10:55:43.018464 139904075134848 learning.py:507] global step 486: loss = 3.2079 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 487: loss = 2.9518 (0.737 sec/step)\n",
            "I1209 10:55:43.891294 139904075134848 learning.py:507] global step 487: loss = 2.9518 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 488: loss = 3.2507 (2.443 sec/step)\n",
            "I1209 10:55:46.338925 139904075134848 learning.py:507] global step 488: loss = 3.2507 (2.443 sec/step)\n",
            "INFO:tensorflow:global step 489: loss = 3.2976 (0.839 sec/step)\n",
            "I1209 10:55:47.250356 139904075134848 learning.py:507] global step 489: loss = 3.2976 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 490: loss = 3.1230 (0.836 sec/step)\n",
            "I1209 10:55:48.475789 139904075134848 learning.py:507] global step 490: loss = 3.1230 (0.836 sec/step)\n",
            "INFO:tensorflow:global step 491: loss = 3.1527 (0.707 sec/step)\n",
            "I1209 10:55:49.510844 139904075134848 learning.py:507] global step 491: loss = 3.1527 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 492: loss = 2.9884 (0.763 sec/step)\n",
            "I1209 10:55:50.675035 139904075134848 learning.py:507] global step 492: loss = 2.9884 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 493: loss = 3.0684 (0.639 sec/step)\n",
            "I1209 10:55:51.377434 139904075134848 learning.py:507] global step 493: loss = 3.0684 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 494: loss = 2.3285 (0.650 sec/step)\n",
            "I1209 10:55:52.029513 139904075134848 learning.py:507] global step 494: loss = 2.3285 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 495: loss = 3.4557 (2.036 sec/step)\n",
            "I1209 10:55:54.312813 139904075134848 learning.py:507] global step 495: loss = 3.4557 (2.036 sec/step)\n",
            "INFO:tensorflow:global step 496: loss = 2.9963 (1.517 sec/step)\n",
            "I1209 10:55:55.870378 139904075134848 learning.py:507] global step 496: loss = 2.9963 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 497: loss = 3.0550 (0.818 sec/step)\n",
            "I1209 10:55:56.944669 139904075134848 learning.py:507] global step 497: loss = 3.0550 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 498: loss = 2.1144 (2.225 sec/step)\n",
            "I1209 10:55:59.253894 139904075134848 learning.py:507] global step 498: loss = 2.1144 (2.225 sec/step)\n",
            "INFO:tensorflow:global step 499: loss = 3.0913 (0.784 sec/step)\n",
            "I1209 10:56:00.395904 139904075134848 learning.py:507] global step 499: loss = 3.0913 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 500: loss = 3.5616 (1.575 sec/step)\n",
            "I1209 10:56:02.013208 139904075134848 learning.py:507] global step 500: loss = 3.5616 (1.575 sec/step)\n",
            "INFO:tensorflow:global step 501: loss = 2.8611 (0.710 sec/step)\n",
            "I1209 10:56:02.724827 139904075134848 learning.py:507] global step 501: loss = 2.8611 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 502: loss = 3.3299 (1.822 sec/step)\n",
            "I1209 10:56:04.548688 139904075134848 learning.py:507] global step 502: loss = 3.3299 (1.822 sec/step)\n",
            "INFO:tensorflow:global step 503: loss = 2.3555 (0.705 sec/step)\n",
            "I1209 10:56:05.467438 139904075134848 learning.py:507] global step 503: loss = 2.3555 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 504: loss = 3.4617 (1.717 sec/step)\n",
            "I1209 10:56:07.205954 139904075134848 learning.py:507] global step 504: loss = 3.4617 (1.717 sec/step)\n",
            "INFO:tensorflow:global step 505: loss = 3.5795 (0.642 sec/step)\n",
            "I1209 10:56:08.021229 139904075134848 learning.py:507] global step 505: loss = 3.5795 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 506: loss = 3.1132 (1.955 sec/step)\n",
            "I1209 10:56:10.122048 139904075134848 learning.py:507] global step 506: loss = 3.1132 (1.955 sec/step)\n",
            "INFO:tensorflow:global step 507: loss = 2.9612 (0.786 sec/step)\n",
            "I1209 10:56:11.182084 139904075134848 learning.py:507] global step 507: loss = 2.9612 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 508: loss = 3.0351 (0.937 sec/step)\n",
            "I1209 10:56:12.483728 139904075134848 learning.py:507] global step 508: loss = 3.0351 (0.937 sec/step)\n",
            "INFO:tensorflow:global step 509: loss = 3.1664 (0.631 sec/step)\n",
            "I1209 10:56:13.515866 139904075134848 learning.py:507] global step 509: loss = 3.1664 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 510: loss = 2.9515 (1.445 sec/step)\n",
            "I1209 10:56:15.157382 139904075134848 learning.py:507] global step 510: loss = 2.9515 (1.445 sec/step)\n",
            "INFO:tensorflow:global step 511: loss = 3.3122 (0.818 sec/step)\n",
            "I1209 10:56:16.175217 139904075134848 learning.py:507] global step 511: loss = 3.3122 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 512: loss = 2.8233 (1.525 sec/step)\n",
            "I1209 10:56:17.793719 139904075134848 learning.py:507] global step 512: loss = 2.8233 (1.525 sec/step)\n",
            "INFO:tensorflow:global step 513: loss = 2.9156 (0.673 sec/step)\n",
            "I1209 10:56:18.619386 139904075134848 learning.py:507] global step 513: loss = 2.9156 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 514: loss = 2.6445 (1.517 sec/step)\n",
            "I1209 10:56:20.354665 139904075134848 learning.py:507] global step 514: loss = 2.6445 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 515: loss = 2.9731 (0.705 sec/step)\n",
            "I1209 10:56:21.112458 139904075134848 learning.py:507] global step 515: loss = 2.9731 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 516: loss = 2.9188 (1.428 sec/step)\n",
            "I1209 10:56:22.782782 139904075134848 learning.py:507] global step 516: loss = 2.9188 (1.428 sec/step)\n",
            "INFO:tensorflow:global step 517: loss = 3.7486 (0.780 sec/step)\n",
            "I1209 10:56:23.773696 139904075134848 learning.py:507] global step 517: loss = 3.7486 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 518: loss = 2.9182 (1.828 sec/step)\n",
            "I1209 10:56:25.642642 139904075134848 learning.py:507] global step 518: loss = 2.9182 (1.828 sec/step)\n",
            "INFO:tensorflow:global step 519: loss = 2.7339 (0.717 sec/step)\n",
            "I1209 10:56:26.421177 139904075134848 learning.py:507] global step 519: loss = 2.7339 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 520: loss = 2.8285 (1.560 sec/step)\n",
            "I1209 10:56:27.983239 139904075134848 learning.py:507] global step 520: loss = 2.8285 (1.560 sec/step)\n",
            "INFO:tensorflow:global step 521: loss = 3.2146 (0.612 sec/step)\n",
            "I1209 10:56:28.954037 139904075134848 learning.py:507] global step 521: loss = 3.2146 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 522: loss = 2.8573 (0.754 sec/step)\n",
            "I1209 10:56:30.154834 139904075134848 learning.py:507] global step 522: loss = 2.8573 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 523: loss = 3.3607 (0.809 sec/step)\n",
            "I1209 10:56:31.084725 139904075134848 learning.py:507] global step 523: loss = 3.3607 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 524: loss = 3.2165 (1.867 sec/step)\n",
            "I1209 10:56:32.953822 139904075134848 learning.py:507] global step 524: loss = 3.2165 (1.867 sec/step)\n",
            "INFO:tensorflow:global step 525: loss = 3.8860 (0.851 sec/step)\n",
            "I1209 10:56:33.988669 139904075134848 learning.py:507] global step 525: loss = 3.8860 (0.851 sec/step)\n",
            "INFO:tensorflow:global step 526: loss = 2.8932 (0.884 sec/step)\n",
            "I1209 10:56:35.149031 139904075134848 learning.py:507] global step 526: loss = 2.8932 (0.884 sec/step)\n",
            "INFO:tensorflow:global step 527: loss = 2.7826 (0.672 sec/step)\n",
            "I1209 10:56:36.122276 139904075134848 learning.py:507] global step 527: loss = 2.7826 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 528: loss = 3.0890 (2.199 sec/step)\n",
            "I1209 10:56:38.566387 139904075134848 learning.py:507] global step 528: loss = 3.0890 (2.199 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 528.\n",
            "I1209 10:56:40.252254 139900543997696 supervisor.py:1050] Recording summary at step 528.\n",
            "INFO:tensorflow:global step 529: loss = 3.4178 (1.782 sec/step)\n",
            "I1209 10:56:40.535237 139904075134848 learning.py:507] global step 529: loss = 3.4178 (1.782 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.71778\n",
            "I1209 10:56:41.337578 139900552390400 supervisor.py:1099] global_step/sec: 0.71778\n",
            "INFO:tensorflow:global step 530: loss = 3.7891 (0.930 sec/step)\n",
            "I1209 10:56:41.651589 139904075134848 learning.py:507] global step 530: loss = 3.7891 (0.930 sec/step)\n",
            "INFO:tensorflow:global step 531: loss = 2.8508 (0.682 sec/step)\n",
            "I1209 10:56:42.379043 139904075134848 learning.py:507] global step 531: loss = 2.8508 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 532: loss = 3.6453 (1.956 sec/step)\n",
            "I1209 10:56:44.336469 139904075134848 learning.py:507] global step 532: loss = 3.6453 (1.956 sec/step)\n",
            "INFO:tensorflow:global step 533: loss = 2.5219 (0.784 sec/step)\n",
            "I1209 10:56:45.170783 139904075134848 learning.py:507] global step 533: loss = 2.5219 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 534: loss = 2.5338 (0.782 sec/step)\n",
            "I1209 10:56:46.140886 139904075134848 learning.py:507] global step 534: loss = 2.5338 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 535: loss = 3.0832 (1.750 sec/step)\n",
            "I1209 10:56:48.050301 139904075134848 learning.py:507] global step 535: loss = 3.0832 (1.750 sec/step)\n",
            "INFO:tensorflow:global step 536: loss = 3.4601 (0.825 sec/step)\n",
            "I1209 10:56:48.978761 139904075134848 learning.py:507] global step 536: loss = 3.4601 (0.825 sec/step)\n",
            "INFO:tensorflow:global step 537: loss = 2.7645 (0.738 sec/step)\n",
            "I1209 10:56:49.848312 139904075134848 learning.py:507] global step 537: loss = 2.7645 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 538: loss = 2.6648 (1.120 sec/step)\n",
            "I1209 10:56:51.019348 139904075134848 learning.py:507] global step 538: loss = 2.6648 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 539: loss = 3.4378 (1.673 sec/step)\n",
            "I1209 10:56:52.782372 139904075134848 learning.py:507] global step 539: loss = 3.4378 (1.673 sec/step)\n",
            "INFO:tensorflow:global step 540: loss = 3.1219 (0.680 sec/step)\n",
            "I1209 10:56:53.650840 139904075134848 learning.py:507] global step 540: loss = 3.1219 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 541: loss = 2.3845 (0.818 sec/step)\n",
            "I1209 10:56:54.829853 139904075134848 learning.py:507] global step 541: loss = 2.3845 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 542: loss = 3.4894 (1.794 sec/step)\n",
            "I1209 10:56:56.680586 139904075134848 learning.py:507] global step 542: loss = 3.4894 (1.794 sec/step)\n",
            "INFO:tensorflow:global step 543: loss = 2.8294 (0.688 sec/step)\n",
            "I1209 10:56:57.731060 139904075134848 learning.py:507] global step 543: loss = 2.8294 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 544: loss = 2.5416 (0.578 sec/step)\n",
            "I1209 10:56:58.504776 139904075134848 learning.py:507] global step 544: loss = 2.5416 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 545: loss = 2.9447 (0.979 sec/step)\n",
            "I1209 10:56:59.726968 139904075134848 learning.py:507] global step 545: loss = 2.9447 (0.979 sec/step)\n",
            "INFO:tensorflow:global step 546: loss = 2.4879 (1.886 sec/step)\n",
            "I1209 10:57:01.842903 139904075134848 learning.py:507] global step 546: loss = 2.4879 (1.886 sec/step)\n",
            "INFO:tensorflow:global step 547: loss = 3.0043 (0.708 sec/step)\n",
            "I1209 10:57:02.552353 139904075134848 learning.py:507] global step 547: loss = 3.0043 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 548: loss = 2.9395 (0.714 sec/step)\n",
            "I1209 10:57:03.268065 139904075134848 learning.py:507] global step 548: loss = 2.9395 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 549: loss = 2.6287 (1.875 sec/step)\n",
            "I1209 10:57:05.416670 139904075134848 learning.py:507] global step 549: loss = 2.6287 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 550: loss = 3.3057 (2.745 sec/step)\n",
            "I1209 10:57:08.348746 139904075134848 learning.py:507] global step 550: loss = 3.3057 (2.745 sec/step)\n",
            "INFO:tensorflow:global step 551: loss = 2.7151 (0.846 sec/step)\n",
            "I1209 10:57:09.210938 139904075134848 learning.py:507] global step 551: loss = 2.7151 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 552: loss = 2.6519 (1.709 sec/step)\n",
            "I1209 10:57:11.048858 139904075134848 learning.py:507] global step 552: loss = 2.6519 (1.709 sec/step)\n",
            "INFO:tensorflow:global step 553: loss = 2.3284 (0.713 sec/step)\n",
            "I1209 10:57:11.963688 139904075134848 learning.py:507] global step 553: loss = 2.3284 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 554: loss = 3.4185 (1.615 sec/step)\n",
            "I1209 10:57:13.746863 139904075134848 learning.py:507] global step 554: loss = 3.4185 (1.615 sec/step)\n",
            "INFO:tensorflow:global step 555: loss = 3.1216 (0.712 sec/step)\n",
            "I1209 10:57:14.911931 139904075134848 learning.py:507] global step 555: loss = 3.1216 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 556: loss = 3.5950 (0.631 sec/step)\n",
            "I1209 10:57:15.600360 139904075134848 learning.py:507] global step 556: loss = 3.5950 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 557: loss = 2.4639 (2.293 sec/step)\n",
            "I1209 10:57:17.895198 139904075134848 learning.py:507] global step 557: loss = 2.4639 (2.293 sec/step)\n",
            "INFO:tensorflow:global step 558: loss = 2.7721 (0.858 sec/step)\n",
            "I1209 10:57:19.067430 139904075134848 learning.py:507] global step 558: loss = 2.7721 (0.858 sec/step)\n",
            "INFO:tensorflow:global step 559: loss = 2.6851 (0.718 sec/step)\n",
            "I1209 10:57:19.953998 139904075134848 learning.py:507] global step 559: loss = 2.6851 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 560: loss = 2.6224 (2.161 sec/step)\n",
            "I1209 10:57:22.117416 139904075134848 learning.py:507] global step 560: loss = 2.6224 (2.161 sec/step)\n",
            "INFO:tensorflow:global step 561: loss = 2.7172 (0.702 sec/step)\n",
            "I1209 10:57:22.821172 139904075134848 learning.py:507] global step 561: loss = 2.7172 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 562: loss = 2.5729 (2.295 sec/step)\n",
            "I1209 10:57:25.117530 139904075134848 learning.py:507] global step 562: loss = 2.5729 (2.295 sec/step)\n",
            "INFO:tensorflow:global step 563: loss = 3.4391 (0.773 sec/step)\n",
            "I1209 10:57:25.892622 139904075134848 learning.py:507] global step 563: loss = 3.4391 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 564: loss = 3.2681 (2.036 sec/step)\n",
            "I1209 10:57:27.931003 139904075134848 learning.py:507] global step 564: loss = 3.2681 (2.036 sec/step)\n",
            "INFO:tensorflow:global step 565: loss = 2.9901 (0.675 sec/step)\n",
            "I1209 10:57:28.825775 139904075134848 learning.py:507] global step 565: loss = 2.9901 (0.675 sec/step)\n",
            "INFO:tensorflow:global step 566: loss = 2.7211 (3.015 sec/step)\n",
            "I1209 10:57:32.206629 139904075134848 learning.py:507] global step 566: loss = 2.7211 (3.015 sec/step)\n",
            "INFO:tensorflow:global step 567: loss = 2.5578 (0.600 sec/step)\n",
            "I1209 10:57:32.809497 139904075134848 learning.py:507] global step 567: loss = 2.5578 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 568: loss = 2.5892 (1.883 sec/step)\n",
            "I1209 10:57:34.694787 139904075134848 learning.py:507] global step 568: loss = 2.5892 (1.883 sec/step)\n",
            "INFO:tensorflow:global step 569: loss = 2.8676 (0.688 sec/step)\n",
            "I1209 10:57:35.384522 139904075134848 learning.py:507] global step 569: loss = 2.8676 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 570: loss = 2.0946 (2.554 sec/step)\n",
            "I1209 10:57:37.940259 139904075134848 learning.py:507] global step 570: loss = 2.0946 (2.554 sec/step)\n",
            "INFO:tensorflow:global step 571: loss = 3.1939 (0.745 sec/step)\n",
            "I1209 10:57:38.687139 139904075134848 learning.py:507] global step 571: loss = 3.1939 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 572: loss = 2.6766 (1.880 sec/step)\n",
            "I1209 10:57:40.568936 139904075134848 learning.py:507] global step 572: loss = 2.6766 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 573: loss = 3.3217 (0.840 sec/step)\n",
            "I1209 10:57:41.746547 139904075134848 learning.py:507] global step 573: loss = 3.3217 (0.840 sec/step)\n",
            "INFO:tensorflow:global step 574: loss = 3.5386 (2.039 sec/step)\n",
            "I1209 10:57:43.814058 139904075134848 learning.py:507] global step 574: loss = 3.5386 (2.039 sec/step)\n",
            "INFO:tensorflow:global step 575: loss = 2.9261 (0.717 sec/step)\n",
            "I1209 10:57:44.532663 139904075134848 learning.py:507] global step 575: loss = 2.9261 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 576: loss = 3.2285 (1.320 sec/step)\n",
            "I1209 10:57:45.994105 139904075134848 learning.py:507] global step 576: loss = 3.2285 (1.320 sec/step)\n",
            "INFO:tensorflow:global step 577: loss = 3.1302 (1.716 sec/step)\n",
            "I1209 10:57:47.726240 139904075134848 learning.py:507] global step 577: loss = 3.1302 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 578: loss = 2.4746 (0.811 sec/step)\n",
            "I1209 10:57:48.895209 139904075134848 learning.py:507] global step 578: loss = 2.4746 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 579: loss = 2.6426 (0.835 sec/step)\n",
            "I1209 10:57:50.153484 139904075134848 learning.py:507] global step 579: loss = 2.6426 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 580: loss = 3.0755 (1.528 sec/step)\n",
            "I1209 10:57:51.964815 139904075134848 learning.py:507] global step 580: loss = 3.0755 (1.528 sec/step)\n",
            "INFO:tensorflow:global step 581: loss = 2.4668 (0.671 sec/step)\n",
            "I1209 10:57:52.637603 139904075134848 learning.py:507] global step 581: loss = 2.4668 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 582: loss = 2.7218 (1.817 sec/step)\n",
            "I1209 10:57:54.455820 139904075134848 learning.py:507] global step 582: loss = 2.7218 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 583: loss = 2.6324 (0.786 sec/step)\n",
            "I1209 10:57:55.372134 139904075134848 learning.py:507] global step 583: loss = 2.6324 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 584: loss = 3.4996 (2.354 sec/step)\n",
            "I1209 10:57:57.916702 139904075134848 learning.py:507] global step 584: loss = 3.4996 (2.354 sec/step)\n",
            "INFO:tensorflow:global step 585: loss = 2.5539 (0.779 sec/step)\n",
            "I1209 10:57:58.697158 139904075134848 learning.py:507] global step 585: loss = 2.5539 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 586: loss = 2.5938 (2.109 sec/step)\n",
            "I1209 10:58:00.807967 139904075134848 learning.py:507] global step 586: loss = 2.5938 (2.109 sec/step)\n",
            "INFO:tensorflow:global step 587: loss = 2.9820 (0.839 sec/step)\n",
            "I1209 10:58:01.781729 139904075134848 learning.py:507] global step 587: loss = 2.9820 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 588: loss = 3.0717 (2.054 sec/step)\n",
            "I1209 10:58:03.960286 139904075134848 learning.py:507] global step 588: loss = 3.0717 (2.054 sec/step)\n",
            "INFO:tensorflow:global step 589: loss = 3.1756 (0.810 sec/step)\n",
            "I1209 10:58:05.070846 139904075134848 learning.py:507] global step 589: loss = 3.1756 (0.810 sec/step)\n",
            "INFO:tensorflow:global step 590: loss = 2.6833 (0.611 sec/step)\n",
            "I1209 10:58:05.710685 139904075134848 learning.py:507] global step 590: loss = 2.6833 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 591: loss = 2.9536 (2.032 sec/step)\n",
            "I1209 10:58:07.744376 139904075134848 learning.py:507] global step 591: loss = 2.9536 (2.032 sec/step)\n",
            "INFO:tensorflow:global step 592: loss = 2.4522 (0.774 sec/step)\n",
            "I1209 10:58:08.714550 139904075134848 learning.py:507] global step 592: loss = 2.4522 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 593: loss = 2.4884 (0.883 sec/step)\n",
            "I1209 10:58:10.129693 139904075134848 learning.py:507] global step 593: loss = 2.4884 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 594: loss = 2.4845 (2.228 sec/step)\n",
            "I1209 10:58:12.407102 139904075134848 learning.py:507] global step 594: loss = 2.4845 (2.228 sec/step)\n",
            "INFO:tensorflow:global step 595: loss = 2.9013 (0.727 sec/step)\n",
            "I1209 10:58:13.135562 139904075134848 learning.py:507] global step 595: loss = 2.9013 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 596: loss = 3.2410 (1.827 sec/step)\n",
            "I1209 10:58:14.964006 139904075134848 learning.py:507] global step 596: loss = 3.2410 (1.827 sec/step)\n",
            "INFO:tensorflow:global step 597: loss = 2.6912 (0.662 sec/step)\n",
            "I1209 10:58:15.627273 139904075134848 learning.py:507] global step 597: loss = 2.6912 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 598: loss = 2.9370 (2.010 sec/step)\n",
            "I1209 10:58:17.639212 139904075134848 learning.py:507] global step 598: loss = 2.9370 (2.010 sec/step)\n",
            "INFO:tensorflow:global step 599: loss = 2.6051 (0.751 sec/step)\n",
            "I1209 10:58:18.547598 139904075134848 learning.py:507] global step 599: loss = 2.6051 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 2.8585 (0.696 sec/step)\n",
            "I1209 10:58:19.875745 139904075134848 learning.py:507] global step 600: loss = 2.8585 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 601: loss = 3.2586 (0.801 sec/step)\n",
            "I1209 10:58:21.008248 139904075134848 learning.py:507] global step 601: loss = 3.2586 (0.801 sec/step)\n",
            "INFO:tensorflow:global step 602: loss = 3.0626 (1.262 sec/step)\n",
            "I1209 10:58:22.316494 139904075134848 learning.py:507] global step 602: loss = 3.0626 (1.262 sec/step)\n",
            "INFO:tensorflow:global step 603: loss = 3.1443 (0.769 sec/step)\n",
            "I1209 10:58:23.314048 139904075134848 learning.py:507] global step 603: loss = 3.1443 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 604: loss = 3.1521 (0.683 sec/step)\n",
            "I1209 10:58:24.340722 139904075134848 learning.py:507] global step 604: loss = 3.1521 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 605: loss = 3.0442 (0.683 sec/step)\n",
            "I1209 10:58:25.289962 139904075134848 learning.py:507] global step 605: loss = 3.0442 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 606: loss = 2.7689 (1.115 sec/step)\n",
            "I1209 10:58:26.420926 139904075134848 learning.py:507] global step 606: loss = 2.7689 (1.115 sec/step)\n",
            "INFO:tensorflow:global step 607: loss = 2.8376 (2.166 sec/step)\n",
            "I1209 10:58:28.633335 139904075134848 learning.py:507] global step 607: loss = 2.8376 (2.166 sec/step)\n",
            "INFO:tensorflow:global step 608: loss = 2.9018 (0.691 sec/step)\n",
            "I1209 10:58:29.326086 139904075134848 learning.py:507] global step 608: loss = 2.9018 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 609: loss = 3.1340 (1.837 sec/step)\n",
            "I1209 10:58:31.164904 139904075134848 learning.py:507] global step 609: loss = 3.1340 (1.837 sec/step)\n",
            "INFO:tensorflow:global step 610: loss = 2.6369 (0.750 sec/step)\n",
            "I1209 10:58:32.251470 139904075134848 learning.py:507] global step 610: loss = 2.6369 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 611: loss = 2.6615 (0.740 sec/step)\n",
            "I1209 10:58:33.056114 139904075134848 learning.py:507] global step 611: loss = 2.6615 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 612: loss = 3.3428 (1.847 sec/step)\n",
            "I1209 10:58:34.905360 139904075134848 learning.py:507] global step 612: loss = 3.3428 (1.847 sec/step)\n",
            "INFO:tensorflow:global step 613: loss = 2.7949 (0.659 sec/step)\n",
            "I1209 10:58:35.849718 139904075134848 learning.py:507] global step 613: loss = 2.7949 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 614: loss = 3.6211 (0.622 sec/step)\n",
            "I1209 10:58:36.798065 139904075134848 learning.py:507] global step 614: loss = 3.6211 (0.622 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 614.\n",
            "I1209 10:58:39.136329 139900543997696 supervisor.py:1050] Recording summary at step 614.\n",
            "INFO:tensorflow:global step 615: loss = 2.6531 (2.427 sec/step)\n",
            "I1209 10:58:39.374376 139904075134848 learning.py:507] global step 615: loss = 2.6531 (2.427 sec/step)\n",
            "INFO:tensorflow:global step 616: loss = 2.9613 (0.558 sec/step)\n",
            "I1209 10:58:40.225792 139904075134848 learning.py:507] global step 616: loss = 2.9613 (0.558 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.720143\n",
            "I1209 10:58:42.147819 139900552390400 supervisor.py:1099] global_step/sec: 0.720143\n",
            "INFO:tensorflow:global step 617: loss = 2.1253 (2.780 sec/step)\n",
            "I1209 10:58:43.187254 139904075134848 learning.py:507] global step 617: loss = 2.1253 (2.780 sec/step)\n",
            "INFO:tensorflow:global step 618: loss = 2.8171 (0.871 sec/step)\n",
            "I1209 10:58:44.250897 139904075134848 learning.py:507] global step 618: loss = 2.8171 (0.871 sec/step)\n",
            "INFO:tensorflow:global step 619: loss = 2.5638 (1.830 sec/step)\n",
            "I1209 10:58:46.101350 139904075134848 learning.py:507] global step 619: loss = 2.5638 (1.830 sec/step)\n",
            "INFO:tensorflow:global step 620: loss = 3.0054 (0.766 sec/step)\n",
            "I1209 10:58:47.120482 139904075134848 learning.py:507] global step 620: loss = 3.0054 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 621: loss = 2.6836 (0.705 sec/step)\n",
            "I1209 10:58:48.187749 139904075134848 learning.py:507] global step 621: loss = 2.6836 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 622: loss = 3.4657 (0.703 sec/step)\n",
            "I1209 10:58:49.173353 139904075134848 learning.py:507] global step 622: loss = 3.4657 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 623: loss = 3.2746 (0.914 sec/step)\n",
            "I1209 10:58:50.301400 139904075134848 learning.py:507] global step 623: loss = 3.2746 (0.914 sec/step)\n",
            "INFO:tensorflow:global step 624: loss = 2.9124 (1.910 sec/step)\n",
            "I1209 10:58:52.353215 139904075134848 learning.py:507] global step 624: loss = 2.9124 (1.910 sec/step)\n",
            "INFO:tensorflow:global step 625: loss = 2.4960 (0.765 sec/step)\n",
            "I1209 10:58:53.162379 139904075134848 learning.py:507] global step 625: loss = 2.4960 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 626: loss = 3.0561 (0.592 sec/step)\n",
            "I1209 10:58:54.265682 139904075134848 learning.py:507] global step 626: loss = 3.0561 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 627: loss = 2.9639 (0.709 sec/step)\n",
            "I1209 10:58:55.269160 139904075134848 learning.py:507] global step 627: loss = 2.9639 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 628: loss = 3.1919 (0.739 sec/step)\n",
            "I1209 10:58:56.539986 139904075134848 learning.py:507] global step 628: loss = 3.1919 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 629: loss = 3.8655 (2.447 sec/step)\n",
            "I1209 10:58:59.093266 139904075134848 learning.py:507] global step 629: loss = 3.8655 (2.447 sec/step)\n",
            "INFO:tensorflow:global step 630: loss = 3.3116 (0.656 sec/step)\n",
            "I1209 10:59:00.062805 139904075134848 learning.py:507] global step 630: loss = 3.3116 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 631: loss = 3.0664 (0.796 sec/step)\n",
            "I1209 10:59:01.250391 139904075134848 learning.py:507] global step 631: loss = 3.0664 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 632: loss = 2.6191 (1.617 sec/step)\n",
            "I1209 10:59:03.086292 139904075134848 learning.py:507] global step 632: loss = 2.6191 (1.617 sec/step)\n",
            "INFO:tensorflow:global step 633: loss = 3.0846 (0.832 sec/step)\n",
            "I1209 10:59:04.105975 139904075134848 learning.py:507] global step 633: loss = 3.0846 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 634: loss = 3.9121 (0.759 sec/step)\n",
            "I1209 10:59:05.145067 139904075134848 learning.py:507] global step 634: loss = 3.9121 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 635: loss = 2.2220 (1.706 sec/step)\n",
            "I1209 10:59:07.131596 139904075134848 learning.py:507] global step 635: loss = 2.2220 (1.706 sec/step)\n",
            "INFO:tensorflow:global step 636: loss = 2.3822 (0.633 sec/step)\n",
            "I1209 10:59:07.766340 139904075134848 learning.py:507] global step 636: loss = 2.3822 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 637: loss = 2.3346 (0.612 sec/step)\n",
            "I1209 10:59:08.380373 139904075134848 learning.py:507] global step 637: loss = 2.3346 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 638: loss = 2.6450 (3.558 sec/step)\n",
            "I1209 10:59:11.939539 139904075134848 learning.py:507] global step 638: loss = 2.6450 (3.558 sec/step)\n",
            "INFO:tensorflow:global step 639: loss = 2.9003 (0.679 sec/step)\n",
            "I1209 10:59:12.620642 139904075134848 learning.py:507] global step 639: loss = 2.9003 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 640: loss = 3.2869 (2.223 sec/step)\n",
            "I1209 10:59:14.846658 139904075134848 learning.py:507] global step 640: loss = 3.2869 (2.223 sec/step)\n",
            "INFO:tensorflow:global step 641: loss = 2.9341 (0.706 sec/step)\n",
            "I1209 10:59:15.719483 139904075134848 learning.py:507] global step 641: loss = 2.9341 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 642: loss = 2.6076 (0.817 sec/step)\n",
            "I1209 10:59:17.183645 139904075134848 learning.py:507] global step 642: loss = 2.6076 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 643: loss = 2.6241 (2.448 sec/step)\n",
            "I1209 10:59:19.743675 139904075134848 learning.py:507] global step 643: loss = 2.6241 (2.448 sec/step)\n",
            "INFO:tensorflow:global step 644: loss = 3.3533 (0.646 sec/step)\n",
            "I1209 10:59:20.392225 139904075134848 learning.py:507] global step 644: loss = 3.3533 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 645: loss = 3.1887 (2.109 sec/step)\n",
            "I1209 10:59:22.502885 139904075134848 learning.py:507] global step 645: loss = 3.1887 (2.109 sec/step)\n",
            "INFO:tensorflow:global step 646: loss = 2.3181 (0.673 sec/step)\n",
            "I1209 10:59:23.177706 139904075134848 learning.py:507] global step 646: loss = 2.3181 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 647: loss = 2.7083 (1.171 sec/step)\n",
            "I1209 10:59:24.641627 139904075134848 learning.py:507] global step 647: loss = 2.7083 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 648: loss = 3.4359 (0.929 sec/step)\n",
            "I1209 10:59:25.948212 139904075134848 learning.py:507] global step 648: loss = 3.4359 (0.929 sec/step)\n",
            "INFO:tensorflow:global step 649: loss = 2.8882 (0.771 sec/step)\n",
            "I1209 10:59:27.107461 139904075134848 learning.py:507] global step 649: loss = 2.8882 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 650: loss = 3.1574 (1.990 sec/step)\n",
            "I1209 10:59:29.194907 139904075134848 learning.py:507] global step 650: loss = 3.1574 (1.990 sec/step)\n",
            "INFO:tensorflow:global step 651: loss = 4.1708 (0.723 sec/step)\n",
            "I1209 10:59:30.284486 139904075134848 learning.py:507] global step 651: loss = 4.1708 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 652: loss = 3.2390 (0.780 sec/step)\n",
            "I1209 10:59:31.387592 139904075134848 learning.py:507] global step 652: loss = 3.2390 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 653: loss = 2.6359 (1.648 sec/step)\n",
            "I1209 10:59:33.244425 139904075134848 learning.py:507] global step 653: loss = 2.6359 (1.648 sec/step)\n",
            "INFO:tensorflow:global step 654: loss = 2.7900 (0.605 sec/step)\n",
            "I1209 10:59:33.851471 139904075134848 learning.py:507] global step 654: loss = 2.7900 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 655: loss = 2.7754 (1.907 sec/step)\n",
            "I1209 10:59:35.759798 139904075134848 learning.py:507] global step 655: loss = 2.7754 (1.907 sec/step)\n",
            "INFO:tensorflow:global step 656: loss = 3.5178 (0.660 sec/step)\n",
            "I1209 10:59:36.421575 139904075134848 learning.py:507] global step 656: loss = 3.5178 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 657: loss = 2.7495 (1.264 sec/step)\n",
            "I1209 10:59:37.963083 139904075134848 learning.py:507] global step 657: loss = 2.7495 (1.264 sec/step)\n",
            "INFO:tensorflow:global step 658: loss = 3.5912 (1.517 sec/step)\n",
            "I1209 10:59:39.667484 139904075134848 learning.py:507] global step 658: loss = 3.5912 (1.517 sec/step)\n",
            "INFO:tensorflow:global step 659: loss = 3.2115 (0.828 sec/step)\n",
            "I1209 10:59:40.664142 139904075134848 learning.py:507] global step 659: loss = 3.2115 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 660: loss = 3.2517 (0.859 sec/step)\n",
            "I1209 10:59:41.664599 139904075134848 learning.py:507] global step 660: loss = 3.2517 (0.859 sec/step)\n",
            "INFO:tensorflow:global step 661: loss = 2.5657 (1.915 sec/step)\n",
            "I1209 10:59:43.582154 139904075134848 learning.py:507] global step 661: loss = 2.5657 (1.915 sec/step)\n",
            "INFO:tensorflow:global step 662: loss = 2.5806 (0.855 sec/step)\n",
            "I1209 10:59:44.450049 139904075134848 learning.py:507] global step 662: loss = 2.5806 (0.855 sec/step)\n",
            "INFO:tensorflow:global step 663: loss = 2.3359 (1.450 sec/step)\n",
            "I1209 10:59:46.111924 139904075134848 learning.py:507] global step 663: loss = 2.3359 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 664: loss = 2.6190 (0.678 sec/step)\n",
            "I1209 10:59:47.114155 139904075134848 learning.py:507] global step 664: loss = 2.6190 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 665: loss = 2.9920 (0.652 sec/step)\n",
            "I1209 10:59:47.959965 139904075134848 learning.py:507] global step 665: loss = 2.9920 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 666: loss = 2.5418 (1.813 sec/step)\n",
            "I1209 10:59:49.775247 139904075134848 learning.py:507] global step 666: loss = 2.5418 (1.813 sec/step)\n",
            "INFO:tensorflow:global step 667: loss = 3.1881 (0.630 sec/step)\n",
            "I1209 10:59:50.658413 139904075134848 learning.py:507] global step 667: loss = 3.1881 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 668: loss = 3.2573 (0.613 sec/step)\n",
            "I1209 10:59:51.541112 139904075134848 learning.py:507] global step 668: loss = 3.2573 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 669: loss = 2.9722 (1.019 sec/step)\n",
            "I1209 10:59:52.775661 139904075134848 learning.py:507] global step 669: loss = 2.9722 (1.019 sec/step)\n",
            "INFO:tensorflow:global step 670: loss = 2.7384 (1.836 sec/step)\n",
            "I1209 10:59:54.736978 139904075134848 learning.py:507] global step 670: loss = 2.7384 (1.836 sec/step)\n",
            "INFO:tensorflow:global step 671: loss = 2.2583 (0.661 sec/step)\n",
            "I1209 10:59:55.740261 139904075134848 learning.py:507] global step 671: loss = 2.2583 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 672: loss = 2.7209 (0.661 sec/step)\n",
            "I1209 10:59:56.705765 139904075134848 learning.py:507] global step 672: loss = 2.7209 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 673: loss = 2.6146 (0.539 sec/step)\n",
            "I1209 10:59:57.431703 139904075134848 learning.py:507] global step 673: loss = 2.6146 (0.539 sec/step)\n",
            "INFO:tensorflow:global step 674: loss = 3.1707 (1.282 sec/step)\n",
            "I1209 10:59:58.882558 139904075134848 learning.py:507] global step 674: loss = 3.1707 (1.282 sec/step)\n",
            "INFO:tensorflow:global step 675: loss = 2.9973 (0.886 sec/step)\n",
            "I1209 10:59:59.988412 139904075134848 learning.py:507] global step 675: loss = 2.9973 (0.886 sec/step)\n",
            "INFO:tensorflow:global step 676: loss = 3.0355 (2.038 sec/step)\n",
            "I1209 11:00:02.313921 139904075134848 learning.py:507] global step 676: loss = 3.0355 (2.038 sec/step)\n",
            "INFO:tensorflow:global step 677: loss = 2.3885 (0.652 sec/step)\n",
            "I1209 11:00:03.206965 139904075134848 learning.py:507] global step 677: loss = 2.3885 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 678: loss = 2.6470 (1.501 sec/step)\n",
            "I1209 11:00:04.748924 139904075134848 learning.py:507] global step 678: loss = 2.6470 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 679: loss = 2.9028 (0.800 sec/step)\n",
            "I1209 11:00:05.574032 139904075134848 learning.py:507] global step 679: loss = 2.9028 (0.800 sec/step)\n",
            "INFO:tensorflow:global step 680: loss = 3.2080 (2.020 sec/step)\n",
            "I1209 11:00:07.613702 139904075134848 learning.py:507] global step 680: loss = 3.2080 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 681: loss = 3.2394 (0.692 sec/step)\n",
            "I1209 11:00:08.398458 139904075134848 learning.py:507] global step 681: loss = 3.2394 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 682: loss = 2.9028 (1.572 sec/step)\n",
            "I1209 11:00:10.146358 139904075134848 learning.py:507] global step 682: loss = 2.9028 (1.572 sec/step)\n",
            "INFO:tensorflow:global step 683: loss = 3.4438 (0.799 sec/step)\n",
            "I1209 11:00:11.163325 139904075134848 learning.py:507] global step 683: loss = 3.4438 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 684: loss = 3.1205 (1.562 sec/step)\n",
            "I1209 11:00:12.876656 139904075134848 learning.py:507] global step 684: loss = 3.1205 (1.562 sec/step)\n",
            "INFO:tensorflow:global step 685: loss = 2.6869 (0.803 sec/step)\n",
            "I1209 11:00:14.058259 139904075134848 learning.py:507] global step 685: loss = 2.6869 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 686: loss = 2.8966 (0.762 sec/step)\n",
            "I1209 11:00:14.822747 139904075134848 learning.py:507] global step 686: loss = 2.8966 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 687: loss = 2.9325 (2.003 sec/step)\n",
            "I1209 11:00:16.827596 139904075134848 learning.py:507] global step 687: loss = 2.9325 (2.003 sec/step)\n",
            "INFO:tensorflow:global step 688: loss = 3.9416 (0.633 sec/step)\n",
            "I1209 11:00:17.462548 139904075134848 learning.py:507] global step 688: loss = 3.9416 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 689: loss = 3.4696 (1.986 sec/step)\n",
            "I1209 11:00:19.450265 139904075134848 learning.py:507] global step 689: loss = 3.4696 (1.986 sec/step)\n",
            "INFO:tensorflow:global step 690: loss = 2.6228 (0.752 sec/step)\n",
            "I1209 11:00:20.507471 139904075134848 learning.py:507] global step 690: loss = 2.6228 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 691: loss = 3.3343 (0.762 sec/step)\n",
            "I1209 11:00:21.515847 139904075134848 learning.py:507] global step 691: loss = 3.3343 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 692: loss = 3.2101 (1.864 sec/step)\n",
            "I1209 11:00:23.495598 139904075134848 learning.py:507] global step 692: loss = 3.2101 (1.864 sec/step)\n",
            "INFO:tensorflow:global step 693: loss = 3.5518 (0.705 sec/step)\n",
            "I1209 11:00:24.378965 139904075134848 learning.py:507] global step 693: loss = 3.5518 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 694: loss = 2.9872 (1.915 sec/step)\n",
            "I1209 11:00:26.484688 139904075134848 learning.py:507] global step 694: loss = 2.9872 (1.915 sec/step)\n",
            "INFO:tensorflow:global step 695: loss = 3.5550 (0.661 sec/step)\n",
            "I1209 11:00:27.147895 139904075134848 learning.py:507] global step 695: loss = 3.5550 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 696: loss = 2.6809 (1.329 sec/step)\n",
            "I1209 11:00:28.748559 139904075134848 learning.py:507] global step 696: loss = 2.6809 (1.329 sec/step)\n",
            "INFO:tensorflow:global step 697: loss = 2.8246 (0.771 sec/step)\n",
            "I1209 11:00:29.666257 139904075134848 learning.py:507] global step 697: loss = 2.8246 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 698: loss = 2.7487 (0.782 sec/step)\n",
            "I1209 11:00:31.006842 139904075134848 learning.py:507] global step 698: loss = 2.7487 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 699: loss = 2.9871 (1.948 sec/step)\n",
            "I1209 11:00:32.960072 139904075134848 learning.py:507] global step 699: loss = 2.9871 (1.948 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 3.2423 (0.567 sec/step)\n",
            "I1209 11:00:33.529008 139904075134848 learning.py:507] global step 700: loss = 3.2423 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 701: loss = 2.9160 (0.989 sec/step)\n",
            "I1209 11:00:34.940273 139904075134848 learning.py:507] global step 701: loss = 2.9160 (0.989 sec/step)\n",
            "INFO:tensorflow:global step 702: loss = 3.1315 (1.777 sec/step)\n",
            "I1209 11:00:37.787591 139904075134848 learning.py:507] global step 702: loss = 3.1315 (1.777 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 702.\n",
            "I1209 11:00:39.334819 139900543997696 supervisor.py:1050] Recording summary at step 702.\n",
            "INFO:tensorflow:global step 703: loss = 2.7684 (1.757 sec/step)\n",
            "I1209 11:00:39.633847 139904075134848 learning.py:507] global step 703: loss = 2.7684 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 704: loss = 2.9743 (0.858 sec/step)\n",
            "I1209 11:00:40.703058 139904075134848 learning.py:507] global step 704: loss = 2.9743 (0.858 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.738145\n",
            "I1209 11:00:41.364886 139900552390400 supervisor.py:1099] global_step/sec: 0.738145\n",
            "INFO:tensorflow:global step 705: loss = 2.8706 (0.698 sec/step)\n",
            "I1209 11:00:41.784706 139904075134848 learning.py:507] global step 705: loss = 2.8706 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 706: loss = 2.7957 (1.433 sec/step)\n",
            "I1209 11:00:43.328758 139904075134848 learning.py:507] global step 706: loss = 2.7957 (1.433 sec/step)\n",
            "INFO:tensorflow:global step 707: loss = 3.1907 (0.743 sec/step)\n",
            "I1209 11:00:44.446146 139904075134848 learning.py:507] global step 707: loss = 3.1907 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 708: loss = 2.9308 (0.819 sec/step)\n",
            "I1209 11:00:45.701571 139904075134848 learning.py:507] global step 708: loss = 2.9308 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 709: loss = 2.4642 (0.714 sec/step)\n",
            "I1209 11:00:46.488335 139904075134848 learning.py:507] global step 709: loss = 2.4642 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 710: loss = 2.8490 (1.521 sec/step)\n",
            "I1209 11:00:48.116640 139904075134848 learning.py:507] global step 710: loss = 2.8490 (1.521 sec/step)\n",
            "INFO:tensorflow:global step 711: loss = 3.7241 (1.701 sec/step)\n",
            "I1209 11:00:49.837545 139904075134848 learning.py:507] global step 711: loss = 3.7241 (1.701 sec/step)\n",
            "INFO:tensorflow:global step 712: loss = 2.5152 (0.813 sec/step)\n",
            "I1209 11:00:50.903971 139904075134848 learning.py:507] global step 712: loss = 2.5152 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 713: loss = 2.8504 (0.785 sec/step)\n",
            "I1209 11:00:52.127157 139904075134848 learning.py:507] global step 713: loss = 2.8504 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 714: loss = 3.1740 (1.284 sec/step)\n",
            "I1209 11:00:53.644560 139904075134848 learning.py:507] global step 714: loss = 3.1740 (1.284 sec/step)\n",
            "INFO:tensorflow:global step 715: loss = 2.5334 (1.223 sec/step)\n",
            "I1209 11:00:54.868912 139904075134848 learning.py:507] global step 715: loss = 2.5334 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 716: loss = 2.6497 (0.594 sec/step)\n",
            "I1209 11:00:55.464756 139904075134848 learning.py:507] global step 716: loss = 2.6497 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 717: loss = 3.5518 (0.762 sec/step)\n",
            "I1209 11:00:56.819030 139904075134848 learning.py:507] global step 717: loss = 3.5518 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 718: loss = 3.1020 (0.949 sec/step)\n",
            "I1209 11:00:58.244394 139904075134848 learning.py:507] global step 718: loss = 3.1020 (0.949 sec/step)\n",
            "INFO:tensorflow:global step 719: loss = 2.5420 (1.900 sec/step)\n",
            "I1209 11:01:00.415654 139904075134848 learning.py:507] global step 719: loss = 2.5420 (1.900 sec/step)\n",
            "INFO:tensorflow:global step 720: loss = 3.6141 (0.954 sec/step)\n",
            "I1209 11:01:01.683669 139904075134848 learning.py:507] global step 720: loss = 3.6141 (0.954 sec/step)\n",
            "INFO:tensorflow:global step 721: loss = 2.9083 (0.867 sec/step)\n",
            "I1209 11:01:03.011106 139904075134848 learning.py:507] global step 721: loss = 2.9083 (0.867 sec/step)\n",
            "INFO:tensorflow:global step 722: loss = 2.8673 (1.799 sec/step)\n",
            "I1209 11:01:04.958724 139904075134848 learning.py:507] global step 722: loss = 2.8673 (1.799 sec/step)\n",
            "INFO:tensorflow:global step 723: loss = 3.1360 (0.702 sec/step)\n",
            "I1209 11:01:05.940762 139904075134848 learning.py:507] global step 723: loss = 3.1360 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 724: loss = 3.0323 (1.880 sec/step)\n",
            "I1209 11:01:08.023760 139904075134848 learning.py:507] global step 724: loss = 3.0323 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 725: loss = 3.3248 (0.710 sec/step)\n",
            "I1209 11:01:09.100797 139904075134848 learning.py:507] global step 725: loss = 3.3248 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 726: loss = 2.8558 (0.789 sec/step)\n",
            "I1209 11:01:10.283858 139904075134848 learning.py:507] global step 726: loss = 2.8558 (0.789 sec/step)\n",
            "INFO:tensorflow:global step 727: loss = 3.1450 (2.146 sec/step)\n",
            "I1209 11:01:12.588411 139904075134848 learning.py:507] global step 727: loss = 3.1450 (2.146 sec/step)\n",
            "INFO:tensorflow:global step 728: loss = 3.2439 (0.779 sec/step)\n",
            "I1209 11:01:13.668557 139904075134848 learning.py:507] global step 728: loss = 3.2439 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 729: loss = 3.3408 (1.938 sec/step)\n",
            "I1209 11:01:15.608854 139904075134848 learning.py:507] global step 729: loss = 3.3408 (1.938 sec/step)\n",
            "INFO:tensorflow:global step 730: loss = 2.9982 (0.680 sec/step)\n",
            "I1209 11:01:16.290971 139904075134848 learning.py:507] global step 730: loss = 2.9982 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 731: loss = 2.5633 (1.716 sec/step)\n",
            "I1209 11:01:18.023071 139904075134848 learning.py:507] global step 731: loss = 2.5633 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 732: loss = 2.7191 (0.839 sec/step)\n",
            "I1209 11:01:19.055560 139904075134848 learning.py:507] global step 732: loss = 2.7191 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 733: loss = 2.1659 (1.936 sec/step)\n",
            "I1209 11:01:21.118547 139904075134848 learning.py:507] global step 733: loss = 2.1659 (1.936 sec/step)\n",
            "INFO:tensorflow:global step 734: loss = 2.2058 (0.774 sec/step)\n",
            "I1209 11:01:21.912773 139904075134848 learning.py:507] global step 734: loss = 2.2058 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 735: loss = 2.1547 (1.512 sec/step)\n",
            "I1209 11:01:23.553046 139904075134848 learning.py:507] global step 735: loss = 2.1547 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 736: loss = 3.2753 (0.909 sec/step)\n",
            "I1209 11:01:24.592280 139904075134848 learning.py:507] global step 736: loss = 3.2753 (0.909 sec/step)\n",
            "INFO:tensorflow:global step 737: loss = 2.9358 (0.782 sec/step)\n",
            "I1209 11:01:25.550948 139904075134848 learning.py:507] global step 737: loss = 2.9358 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 738: loss = 2.4170 (2.165 sec/step)\n",
            "I1209 11:01:27.717814 139904075134848 learning.py:507] global step 738: loss = 2.4170 (2.165 sec/step)\n",
            "INFO:tensorflow:global step 739: loss = 2.3081 (0.824 sec/step)\n",
            "I1209 11:01:28.675971 139904075134848 learning.py:507] global step 739: loss = 2.3081 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 740: loss = 3.0201 (2.070 sec/step)\n",
            "I1209 11:01:30.790840 139904075134848 learning.py:507] global step 740: loss = 3.0201 (2.070 sec/step)\n",
            "INFO:tensorflow:global step 741: loss = 3.1801 (0.764 sec/step)\n",
            "I1209 11:01:31.840817 139904075134848 learning.py:507] global step 741: loss = 3.1801 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 742: loss = 3.1338 (0.786 sec/step)\n",
            "I1209 11:01:32.866784 139904075134848 learning.py:507] global step 742: loss = 3.1338 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 743: loss = 3.1223 (1.979 sec/step)\n",
            "I1209 11:01:34.848205 139904075134848 learning.py:507] global step 743: loss = 3.1223 (1.979 sec/step)\n",
            "INFO:tensorflow:global step 744: loss = 3.1322 (0.678 sec/step)\n",
            "I1209 11:01:35.528319 139904075134848 learning.py:507] global step 744: loss = 3.1322 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 745: loss = 2.8101 (1.398 sec/step)\n",
            "I1209 11:01:37.095439 139904075134848 learning.py:507] global step 745: loss = 2.8101 (1.398 sec/step)\n",
            "INFO:tensorflow:global step 746: loss = 2.3689 (2.390 sec/step)\n",
            "I1209 11:01:39.826600 139904075134848 learning.py:507] global step 746: loss = 2.3689 (2.390 sec/step)\n",
            "INFO:tensorflow:global step 747: loss = 2.6220 (0.722 sec/step)\n",
            "I1209 11:01:40.743934 139904075134848 learning.py:507] global step 747: loss = 2.6220 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 748: loss = 1.9272 (0.761 sec/step)\n",
            "I1209 11:01:41.767141 139904075134848 learning.py:507] global step 748: loss = 1.9272 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 749: loss = 3.2624 (0.917 sec/step)\n",
            "I1209 11:01:43.071113 139904075134848 learning.py:507] global step 749: loss = 3.2624 (0.917 sec/step)\n",
            "INFO:tensorflow:global step 750: loss = 2.6224 (1.523 sec/step)\n",
            "I1209 11:01:44.782241 139904075134848 learning.py:507] global step 750: loss = 2.6224 (1.523 sec/step)\n",
            "INFO:tensorflow:global step 751: loss = 2.6403 (0.795 sec/step)\n",
            "I1209 11:01:45.883495 139904075134848 learning.py:507] global step 751: loss = 2.6403 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 752: loss = 2.9871 (1.388 sec/step)\n",
            "I1209 11:01:47.398036 139904075134848 learning.py:507] global step 752: loss = 2.9871 (1.388 sec/step)\n",
            "INFO:tensorflow:global step 753: loss = 2.6614 (0.728 sec/step)\n",
            "I1209 11:01:48.128302 139904075134848 learning.py:507] global step 753: loss = 2.6614 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 754: loss = 2.7472 (1.781 sec/step)\n",
            "I1209 11:01:49.911463 139904075134848 learning.py:507] global step 754: loss = 2.7472 (1.781 sec/step)\n",
            "INFO:tensorflow:global step 755: loss = 2.6544 (0.728 sec/step)\n",
            "I1209 11:01:51.087417 139904075134848 learning.py:507] global step 755: loss = 2.6544 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 756: loss = 3.0827 (0.815 sec/step)\n",
            "I1209 11:01:52.411719 139904075134848 learning.py:507] global step 756: loss = 3.0827 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 757: loss = 3.2135 (1.659 sec/step)\n",
            "I1209 11:01:54.102594 139904075134848 learning.py:507] global step 757: loss = 3.2135 (1.659 sec/step)\n",
            "INFO:tensorflow:global step 758: loss = 3.0714 (0.806 sec/step)\n",
            "I1209 11:01:55.191273 139904075134848 learning.py:507] global step 758: loss = 3.0714 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 759: loss = 2.9315 (0.816 sec/step)\n",
            "I1209 11:01:56.009751 139904075134848 learning.py:507] global step 759: loss = 2.9315 (0.816 sec/step)\n",
            "INFO:tensorflow:global step 760: loss = 2.7285 (1.578 sec/step)\n",
            "I1209 11:01:57.724564 139904075134848 learning.py:507] global step 760: loss = 2.7285 (1.578 sec/step)\n",
            "INFO:tensorflow:global step 761: loss = 2.2274 (1.757 sec/step)\n",
            "I1209 11:01:59.503378 139904075134848 learning.py:507] global step 761: loss = 2.2274 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 762: loss = 3.0833 (0.671 sec/step)\n",
            "I1209 11:02:00.176409 139904075134848 learning.py:507] global step 762: loss = 3.0833 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 763: loss = 3.1588 (1.016 sec/step)\n",
            "I1209 11:02:01.563185 139904075134848 learning.py:507] global step 763: loss = 3.1588 (1.016 sec/step)\n",
            "INFO:tensorflow:global step 764: loss = 2.5310 (0.808 sec/step)\n",
            "I1209 11:02:02.881059 139904075134848 learning.py:507] global step 764: loss = 2.5310 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 765: loss = 3.5466 (1.767 sec/step)\n",
            "I1209 11:02:04.650245 139904075134848 learning.py:507] global step 765: loss = 3.5466 (1.767 sec/step)\n",
            "INFO:tensorflow:global step 766: loss = 2.8695 (0.722 sec/step)\n",
            "I1209 11:02:05.649583 139904075134848 learning.py:507] global step 766: loss = 2.8695 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 767: loss = 2.7432 (0.775 sec/step)\n",
            "I1209 11:02:06.546679 139904075134848 learning.py:507] global step 767: loss = 2.7432 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 768: loss = 2.7433 (2.447 sec/step)\n",
            "I1209 11:02:08.995307 139904075134848 learning.py:507] global step 768: loss = 2.7433 (2.447 sec/step)\n",
            "INFO:tensorflow:global step 769: loss = 2.7954 (0.726 sec/step)\n",
            "I1209 11:02:09.936803 139904075134848 learning.py:507] global step 769: loss = 2.7954 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 770: loss = 3.7281 (0.903 sec/step)\n",
            "I1209 11:02:11.164758 139904075134848 learning.py:507] global step 770: loss = 3.7281 (0.903 sec/step)\n",
            "INFO:tensorflow:global step 771: loss = 3.0227 (0.795 sec/step)\n",
            "I1209 11:02:12.181570 139904075134848 learning.py:507] global step 771: loss = 3.0227 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 772: loss = 2.9795 (1.403 sec/step)\n",
            "I1209 11:02:13.722185 139904075134848 learning.py:507] global step 772: loss = 2.9795 (1.403 sec/step)\n",
            "INFO:tensorflow:global step 773: loss = 2.4984 (0.842 sec/step)\n",
            "I1209 11:02:14.611544 139904075134848 learning.py:507] global step 773: loss = 2.4984 (0.842 sec/step)\n",
            "INFO:tensorflow:global step 774: loss = 3.1627 (2.395 sec/step)\n",
            "I1209 11:02:17.023553 139904075134848 learning.py:507] global step 774: loss = 3.1627 (2.395 sec/step)\n",
            "INFO:tensorflow:global step 775: loss = 2.7001 (0.622 sec/step)\n",
            "I1209 11:02:17.647565 139904075134848 learning.py:507] global step 775: loss = 2.7001 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 776: loss = 2.6470 (2.395 sec/step)\n",
            "I1209 11:02:20.044660 139904075134848 learning.py:507] global step 776: loss = 2.6470 (2.395 sec/step)\n",
            "INFO:tensorflow:global step 777: loss = 2.4837 (0.683 sec/step)\n",
            "I1209 11:02:20.729290 139904075134848 learning.py:507] global step 777: loss = 2.4837 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 778: loss = 3.4043 (1.998 sec/step)\n",
            "I1209 11:02:22.729068 139904075134848 learning.py:507] global step 778: loss = 3.4043 (1.998 sec/step)\n",
            "INFO:tensorflow:global step 779: loss = 3.0175 (0.732 sec/step)\n",
            "I1209 11:02:23.738334 139904075134848 learning.py:507] global step 779: loss = 3.0175 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 780: loss = 3.5544 (0.695 sec/step)\n",
            "I1209 11:02:24.596279 139904075134848 learning.py:507] global step 780: loss = 3.5544 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 781: loss = 2.5685 (1.839 sec/step)\n",
            "I1209 11:02:26.771880 139904075134848 learning.py:507] global step 781: loss = 2.5685 (1.839 sec/step)\n",
            "INFO:tensorflow:global step 782: loss = 2.5048 (2.413 sec/step)\n",
            "I1209 11:02:29.463361 139904075134848 learning.py:507] global step 782: loss = 2.5048 (2.413 sec/step)\n",
            "INFO:tensorflow:global step 783: loss = 3.1446 (0.526 sec/step)\n",
            "I1209 11:02:29.991467 139904075134848 learning.py:507] global step 783: loss = 3.1446 (0.526 sec/step)\n",
            "INFO:tensorflow:global step 784: loss = 3.0930 (1.066 sec/step)\n",
            "I1209 11:02:31.178092 139904075134848 learning.py:507] global step 784: loss = 3.0930 (1.066 sec/step)\n",
            "INFO:tensorflow:global step 785: loss = 2.2482 (2.879 sec/step)\n",
            "I1209 11:02:34.075965 139904075134848 learning.py:507] global step 785: loss = 2.2482 (2.879 sec/step)\n",
            "INFO:tensorflow:global step 786: loss = 2.7679 (0.555 sec/step)\n",
            "I1209 11:02:34.633323 139904075134848 learning.py:507] global step 786: loss = 2.7679 (0.555 sec/step)\n",
            "INFO:tensorflow:global step 787: loss = 3.7235 (2.407 sec/step)\n",
            "I1209 11:02:37.043495 139904075134848 learning.py:507] global step 787: loss = 3.7235 (2.407 sec/step)\n",
            "INFO:tensorflow:global step 788: loss = 3.7777 (0.601 sec/step)\n",
            "I1209 11:02:37.648008 139904075134848 learning.py:507] global step 788: loss = 3.7777 (0.601 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 788.\n",
            "I1209 11:02:38.544454 139900543997696 supervisor.py:1050] Recording summary at step 788.\n",
            "INFO:tensorflow:global step 789: loss = 3.0986 (2.526 sec/step)\n",
            "I1209 11:02:40.437808 139904075134848 learning.py:507] global step 789: loss = 3.0986 (2.526 sec/step)\n",
            "INFO:tensorflow:global step 790: loss = 3.7154 (0.753 sec/step)\n",
            "I1209 11:02:41.197884 139904075134848 learning.py:507] global step 790: loss = 3.7154 (0.753 sec/step)\n",
            "INFO:tensorflow:global_step/sec: 0.710542\n",
            "I1209 11:02:42.399035 139900552390400 supervisor.py:1099] global_step/sec: 0.710542\n",
            "INFO:tensorflow:global step 791: loss = 3.4786 (2.798 sec/step)\n",
            "I1209 11:02:43.997504 139904075134848 learning.py:507] global step 791: loss = 3.4786 (2.798 sec/step)\n",
            "INFO:tensorflow:global step 792: loss = 2.8336 (1.090 sec/step)\n",
            "I1209 11:02:45.389892 139904075134848 learning.py:507] global step 792: loss = 2.8336 (1.090 sec/step)\n",
            "INFO:tensorflow:global step 793: loss = 3.0738 (1.829 sec/step)\n",
            "I1209 11:02:47.220487 139904075134848 learning.py:507] global step 793: loss = 3.0738 (1.829 sec/step)\n",
            "INFO:tensorflow:global step 794: loss = 2.7130 (0.685 sec/step)\n",
            "I1209 11:02:47.906875 139904075134848 learning.py:507] global step 794: loss = 2.7130 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 795: loss = 2.8376 (1.872 sec/step)\n",
            "I1209 11:02:49.782166 139904075134848 learning.py:507] global step 795: loss = 2.8376 (1.872 sec/step)\n",
            "INFO:tensorflow:global step 796: loss = 3.0458 (0.725 sec/step)\n",
            "I1209 11:02:50.509392 139904075134848 learning.py:507] global step 796: loss = 3.0458 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 797: loss = 2.4055 (2.129 sec/step)\n",
            "I1209 11:02:52.640075 139904075134848 learning.py:507] global step 797: loss = 2.4055 (2.129 sec/step)\n",
            "INFO:tensorflow:global step 798: loss = 3.1818 (0.762 sec/step)\n",
            "I1209 11:02:53.664048 139904075134848 learning.py:507] global step 798: loss = 3.1818 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 799: loss = 2.7938 (0.793 sec/step)\n",
            "I1209 11:02:54.833768 139904075134848 learning.py:507] global step 799: loss = 2.7938 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 3.4891 (0.957 sec/step)\n",
            "I1209 11:02:55.920367 139904075134848 learning.py:507] global step 800: loss = 3.4891 (0.957 sec/step)\n",
            "INFO:tensorflow:global step 801: loss = 2.8260 (2.359 sec/step)\n",
            "I1209 11:02:58.294553 139904075134848 learning.py:507] global step 801: loss = 2.8260 (2.359 sec/step)\n",
            "INFO:tensorflow:global step 802: loss = 3.3401 (0.582 sec/step)\n",
            "I1209 11:02:59.135233 139904075134848 learning.py:507] global step 802: loss = 3.3401 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 803: loss = 2.8388 (1.864 sec/step)\n",
            "I1209 11:03:01.043091 139904075134848 learning.py:507] global step 803: loss = 2.8388 (1.864 sec/step)\n",
            "INFO:tensorflow:global step 804: loss = 2.7981 (0.748 sec/step)\n",
            "I1209 11:03:01.793293 139904075134848 learning.py:507] global step 804: loss = 2.7981 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 805: loss = 3.4866 (2.486 sec/step)\n",
            "I1209 11:03:04.281427 139904075134848 learning.py:507] global step 805: loss = 3.4866 (2.486 sec/step)\n",
            "INFO:tensorflow:global step 806: loss = 3.4241 (0.890 sec/step)\n",
            "I1209 11:03:05.231730 139904075134848 learning.py:507] global step 806: loss = 3.4241 (0.890 sec/step)\n",
            "INFO:tensorflow:global step 807: loss = 2.6667 (1.002 sec/step)\n",
            "I1209 11:03:06.580253 139904075134848 learning.py:507] global step 807: loss = 2.6667 (1.002 sec/step)\n",
            "INFO:tensorflow:global step 808: loss = 2.9093 (0.755 sec/step)\n",
            "I1209 11:03:07.421489 139904075134848 learning.py:507] global step 808: loss = 2.9093 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 809: loss = 3.4011 (1.629 sec/step)\n",
            "I1209 11:03:09.072185 139904075134848 learning.py:507] global step 809: loss = 3.4011 (1.629 sec/step)\n",
            "INFO:tensorflow:global step 810: loss = 2.7602 (1.492 sec/step)\n",
            "I1209 11:03:10.570778 139904075134848 learning.py:507] global step 810: loss = 2.7602 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 811: loss = 3.1381 (0.665 sec/step)\n",
            "I1209 11:03:11.280689 139904075134848 learning.py:507] global step 811: loss = 3.1381 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 812: loss = 2.9786 (1.252 sec/step)\n",
            "I1209 11:03:12.851097 139904075134848 learning.py:507] global step 812: loss = 2.9786 (1.252 sec/step)\n",
            "INFO:tensorflow:global step 813: loss = 3.0719 (0.746 sec/step)\n",
            "I1209 11:03:13.936269 139904075134848 learning.py:507] global step 813: loss = 3.0719 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 814: loss = 2.5758 (0.707 sec/step)\n",
            "I1209 11:03:15.062012 139904075134848 learning.py:507] global step 814: loss = 2.5758 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 815: loss = 2.4501 (0.591 sec/step)\n",
            "I1209 11:03:15.770979 139904075134848 learning.py:507] global step 815: loss = 2.4501 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 816: loss = 2.8513 (1.241 sec/step)\n",
            "I1209 11:03:17.323592 139904075134848 learning.py:507] global step 816: loss = 2.8513 (1.241 sec/step)\n",
            "INFO:tensorflow:global step 817: loss = 3.1168 (0.824 sec/step)\n",
            "I1209 11:03:18.213182 139904075134848 learning.py:507] global step 817: loss = 3.1168 (0.824 sec/step)\n",
            "INFO:tensorflow:global step 818: loss = 3.2381 (2.159 sec/step)\n",
            "I1209 11:03:20.374627 139904075134848 learning.py:507] global step 818: loss = 3.2381 (2.159 sec/step)\n",
            "INFO:tensorflow:global step 819: loss = 2.8573 (0.723 sec/step)\n",
            "I1209 11:03:21.100243 139904075134848 learning.py:507] global step 819: loss = 2.8573 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 820: loss = 2.6027 (1.124 sec/step)\n",
            "I1209 11:03:22.397080 139904075134848 learning.py:507] global step 820: loss = 2.6027 (1.124 sec/step)\n",
            "INFO:tensorflow:global step 821: loss = 2.7230 (1.929 sec/step)\n",
            "I1209 11:03:24.405830 139904075134848 learning.py:507] global step 821: loss = 2.7230 (1.929 sec/step)\n",
            "INFO:tensorflow:global step 822: loss = 3.0556 (0.782 sec/step)\n",
            "I1209 11:03:25.391016 139904075134848 learning.py:507] global step 822: loss = 3.0556 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 823: loss = 2.7199 (0.666 sec/step)\n",
            "I1209 11:03:26.325191 139904075134848 learning.py:507] global step 823: loss = 2.7199 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 824: loss = 3.0139 (0.838 sec/step)\n",
            "I1209 11:03:27.464420 139904075134848 learning.py:507] global step 824: loss = 3.0139 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 825: loss = 3.8674 (2.351 sec/step)\n",
            "I1209 11:03:30.009570 139904075134848 learning.py:507] global step 825: loss = 3.8674 (2.351 sec/step)\n",
            "INFO:tensorflow:global step 826: loss = 2.6294 (0.559 sec/step)\n",
            "I1209 11:03:30.570756 139904075134848 learning.py:507] global step 826: loss = 2.6294 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 827: loss = 3.2064 (2.427 sec/step)\n",
            "I1209 11:03:32.999994 139904075134848 learning.py:507] global step 827: loss = 3.2064 (2.427 sec/step)\n",
            "INFO:tensorflow:global step 828: loss = 2.5804 (0.743 sec/step)\n",
            "I1209 11:03:34.035815 139904075134848 learning.py:507] global step 828: loss = 2.5804 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 829: loss = 2.3023 (0.642 sec/step)\n",
            "I1209 11:03:34.897032 139904075134848 learning.py:507] global step 829: loss = 2.3023 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 830: loss = 2.6640 (0.648 sec/step)\n",
            "I1209 11:03:35.547119 139904075134848 learning.py:507] global step 830: loss = 2.6640 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 831: loss = 2.6915 (1.364 sec/step)\n",
            "I1209 11:03:37.249038 139904075134848 learning.py:507] global step 831: loss = 2.6915 (1.364 sec/step)\n",
            "INFO:tensorflow:global step 832: loss = 2.7808 (2.308 sec/step)\n",
            "I1209 11:03:39.893283 139904075134848 learning.py:507] global step 832: loss = 2.7808 (2.308 sec/step)\n",
            "INFO:tensorflow:global step 833: loss = 2.8152 (0.685 sec/step)\n",
            "I1209 11:03:40.580244 139904075134848 learning.py:507] global step 833: loss = 2.8152 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 834: loss = 3.2702 (1.935 sec/step)\n",
            "I1209 11:03:42.517060 139904075134848 learning.py:507] global step 834: loss = 3.2702 (1.935 sec/step)\n",
            "INFO:tensorflow:global step 835: loss = 2.4880 (0.664 sec/step)\n",
            "I1209 11:03:43.477734 139904075134848 learning.py:507] global step 835: loss = 2.4880 (0.664 sec/step)\n",
            "INFO:tensorflow:global step 836: loss = 2.8295 (3.161 sec/step)\n",
            "I1209 11:03:46.784168 139904075134848 learning.py:507] global step 836: loss = 2.8295 (3.161 sec/step)\n",
            "INFO:tensorflow:global step 837: loss = 2.7312 (1.047 sec/step)\n",
            "I1209 11:03:47.962878 139904075134848 learning.py:507] global step 837: loss = 2.7312 (1.047 sec/step)\n",
            "INFO:tensorflow:global step 838: loss = 2.1566 (0.586 sec/step)\n",
            "I1209 11:03:48.676094 139904075134848 learning.py:507] global step 838: loss = 2.1566 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 839: loss = 2.9665 (0.750 sec/step)\n",
            "I1209 11:03:49.428224 139904075134848 learning.py:507] global step 839: loss = 2.9665 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 840: loss = 2.4825 (2.735 sec/step)\n",
            "I1209 11:03:52.169902 139904075134848 learning.py:507] global step 840: loss = 2.4825 (2.735 sec/step)\n",
            "INFO:tensorflow:global step 841: loss = 2.8207 (1.915 sec/step)\n",
            "I1209 11:03:54.086178 139904075134848 learning.py:507] global step 841: loss = 2.8207 (1.915 sec/step)\n",
            "INFO:tensorflow:global step 842: loss = 2.6248 (0.590 sec/step)\n",
            "I1209 11:03:54.678699 139904075134848 learning.py:507] global step 842: loss = 2.6248 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 843: loss = 2.8929 (2.125 sec/step)\n",
            "I1209 11:03:56.805696 139904075134848 learning.py:507] global step 843: loss = 2.8929 (2.125 sec/step)\n",
            "INFO:tensorflow:global step 844: loss = 2.7922 (0.730 sec/step)\n",
            "I1209 11:03:57.882687 139904075134848 learning.py:507] global step 844: loss = 2.7922 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 845: loss = 3.2440 (0.767 sec/step)\n",
            "I1209 11:03:58.732756 139904075134848 learning.py:507] global step 845: loss = 3.2440 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 846: loss = 2.6677 (1.156 sec/step)\n",
            "I1209 11:04:00.322848 139904075134848 learning.py:507] global step 846: loss = 2.6677 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 847: loss = 2.8378 (1.831 sec/step)\n",
            "I1209 11:04:02.471536 139904075134848 learning.py:507] global step 847: loss = 2.8378 (1.831 sec/step)\n",
            "INFO:tensorflow:global step 848: loss = 2.0581 (0.699 sec/step)\n",
            "I1209 11:04:03.172301 139904075134848 learning.py:507] global step 848: loss = 2.0581 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 849: loss = 3.2191 (0.705 sec/step)\n",
            "I1209 11:04:03.878741 139904075134848 learning.py:507] global step 849: loss = 3.2191 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 850: loss = 2.4565 (1.253 sec/step)\n",
            "I1209 11:04:05.257658 139904075134848 learning.py:507] global step 850: loss = 2.4565 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 851: loss = 2.3587 (2.263 sec/step)\n",
            "I1209 11:04:07.835506 139904075134848 learning.py:507] global step 851: loss = 2.3587 (2.263 sec/step)\n",
            "INFO:tensorflow:global step 852: loss = 2.9283 (0.725 sec/step)\n",
            "I1209 11:04:08.725147 139904075134848 learning.py:507] global step 852: loss = 2.9283 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 853: loss = 3.4781 (0.613 sec/step)\n",
            "I1209 11:04:09.579918 139904075134848 learning.py:507] global step 853: loss = 3.4781 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 854: loss = 3.0032 (1.791 sec/step)\n",
            "I1209 11:04:11.372796 139904075134848 learning.py:507] global step 854: loss = 3.0032 (1.791 sec/step)\n",
            "INFO:tensorflow:global step 855: loss = 3.2813 (0.732 sec/step)\n",
            "I1209 11:04:12.116771 139904075134848 learning.py:507] global step 855: loss = 3.2813 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 856: loss = 3.3828 (0.718 sec/step)\n",
            "I1209 11:04:13.202764 139904075134848 learning.py:507] global step 856: loss = 3.3828 (0.718 sec/step)\n",
            "INFO:tensorflow:global step 857: loss = 2.9412 (0.947 sec/step)\n",
            "I1209 11:04:14.437650 139904075134848 learning.py:507] global step 857: loss = 2.9412 (0.947 sec/step)\n",
            "INFO:tensorflow:global step 858: loss = 3.6644 (0.630 sec/step)\n",
            "I1209 11:04:15.709974 139904075134848 learning.py:507] global step 858: loss = 3.6644 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 859: loss = 2.4628 (0.635 sec/step)\n",
            "I1209 11:04:16.475935 139904075134848 learning.py:507] global step 859: loss = 2.4628 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 860: loss = 2.4193 (1.511 sec/step)\n",
            "I1209 11:04:18.411104 139904075134848 learning.py:507] global step 860: loss = 2.4193 (1.511 sec/step)\n",
            "INFO:tensorflow:global step 861: loss = 2.9118 (2.280 sec/step)\n",
            "I1209 11:04:20.979834 139904075134848 learning.py:507] global step 861: loss = 2.9118 (2.280 sec/step)\n",
            "INFO:tensorflow:global step 862: loss = 2.4884 (0.705 sec/step)\n",
            "I1209 11:04:21.686285 139904075134848 learning.py:507] global step 862: loss = 2.4884 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 863: loss = 2.7848 (2.090 sec/step)\n",
            "I1209 11:04:23.777841 139904075134848 learning.py:507] global step 863: loss = 2.7848 (2.090 sec/step)\n",
            "INFO:tensorflow:global step 864: loss = 2.4032 (0.673 sec/step)\n",
            "I1209 11:04:24.666815 139904075134848 learning.py:507] global step 864: loss = 2.4032 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 865: loss = 3.3301 (0.589 sec/step)\n",
            "I1209 11:04:25.533475 139904075134848 learning.py:507] global step 865: loss = 3.3301 (0.589 sec/step)\n",
            "INFO:tensorflow:global step 866: loss = 3.2035 (0.700 sec/step)\n",
            "I1209 11:04:26.235908 139904075134848 learning.py:507] global step 866: loss = 3.2035 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 867: loss = 3.4467 (2.732 sec/step)\n",
            "I1209 11:04:28.969544 139904075134848 learning.py:507] global step 867: loss = 3.4467 (2.732 sec/step)\n",
            "INFO:tensorflow:global step 868: loss = 2.8890 (0.738 sec/step)\n",
            "I1209 11:04:29.983323 139904075134848 learning.py:507] global step 868: loss = 2.8890 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 869: loss = 2.6927 (0.776 sec/step)\n",
            "I1209 11:04:31.224320 139904075134848 learning.py:507] global step 869: loss = 2.6927 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 870: loss = 2.7766 (0.637 sec/step)\n",
            "I1209 11:04:31.888514 139904075134848 learning.py:507] global step 870: loss = 2.7766 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 871: loss = 2.6947 (1.075 sec/step)\n",
            "I1209 11:04:33.209215 139904075134848 learning.py:507] global step 871: loss = 2.6947 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 872: loss = 2.3619 (1.633 sec/step)\n",
            "I1209 11:04:35.149750 139904075134848 learning.py:507] global step 872: loss = 2.3619 (1.633 sec/step)\n",
            "INFO:tensorflow:global step 873: loss = 2.0961 (0.805 sec/step)\n",
            "I1209 11:04:36.171124 139904075134848 learning.py:507] global step 873: loss = 2.0961 (0.805 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 11:04:36.804708 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 874: loss = 2.7782 (1.146 sec/step)\n",
            "I1209 11:04:38.607281 139904075134848 learning.py:507] global step 874: loss = 2.7782 (1.146 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 874.\n",
            "I1209 11:04:39.341554 139900543997696 supervisor.py:1050] Recording summary at step 874.\n",
            "INFO:tensorflow:global step 875: loss = 2.7624 (2.383 sec/step)\n",
            "I1209 11:04:41.823273 139904075134848 learning.py:507] global step 875: loss = 2.7624 (2.383 sec/step)\n",
            "INFO:tensorflow:global step 876: loss = 2.4677 (1.311 sec/step)\n",
            "I1209 11:04:43.510175 139904075134848 learning.py:507] global step 876: loss = 2.4677 (1.311 sec/step)\n",
            "INFO:tensorflow:global step 877: loss = 3.3512 (0.822 sec/step)\n",
            "I1209 11:04:44.623039 139904075134848 learning.py:507] global step 877: loss = 3.3512 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 878: loss = 2.4212 (3.071 sec/step)\n",
            "I1209 11:04:47.695890 139904075134848 learning.py:507] global step 878: loss = 2.4212 (3.071 sec/step)\n",
            "INFO:tensorflow:global step 879: loss = 2.7303 (0.940 sec/step)\n",
            "I1209 11:04:48.789719 139904075134848 learning.py:507] global step 879: loss = 2.7303 (0.940 sec/step)\n",
            "INFO:tensorflow:global step 880: loss = 2.1432 (0.760 sec/step)\n",
            "I1209 11:04:49.796110 139904075134848 learning.py:507] global step 880: loss = 2.1432 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 881: loss = 2.5237 (1.692 sec/step)\n",
            "I1209 11:04:51.721719 139904075134848 learning.py:507] global step 881: loss = 2.5237 (1.692 sec/step)\n",
            "INFO:tensorflow:global step 882: loss = 2.0698 (1.964 sec/step)\n",
            "I1209 11:04:53.746273 139904075134848 learning.py:507] global step 882: loss = 2.0698 (1.964 sec/step)\n",
            "INFO:tensorflow:global step 883: loss = 2.2947 (0.801 sec/step)\n",
            "I1209 11:04:54.942903 139904075134848 learning.py:507] global step 883: loss = 2.2947 (0.801 sec/step)\n",
            "INFO:tensorflow:global step 884: loss = 2.7836 (0.719 sec/step)\n",
            "I1209 11:04:55.804650 139904075134848 learning.py:507] global step 884: loss = 2.7836 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 885: loss = 2.6504 (2.071 sec/step)\n",
            "I1209 11:04:57.877412 139904075134848 learning.py:507] global step 885: loss = 2.6504 (2.071 sec/step)\n",
            "INFO:tensorflow:global step 886: loss = 2.7211 (0.570 sec/step)\n",
            "I1209 11:04:58.449155 139904075134848 learning.py:507] global step 886: loss = 2.7211 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 887: loss = 3.4681 (2.605 sec/step)\n",
            "I1209 11:05:01.056231 139904075134848 learning.py:507] global step 887: loss = 3.4681 (2.605 sec/step)\n",
            "INFO:tensorflow:global step 888: loss = 2.6897 (0.711 sec/step)\n",
            "I1209 11:05:01.882482 139904075134848 learning.py:507] global step 888: loss = 2.6897 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 889: loss = 2.3326 (1.856 sec/step)\n",
            "I1209 11:05:04.108383 139904075134848 learning.py:507] global step 889: loss = 2.3326 (1.856 sec/step)\n",
            "INFO:tensorflow:global step 890: loss = 2.9720 (0.769 sec/step)\n",
            "I1209 11:05:04.882126 139904075134848 learning.py:507] global step 890: loss = 2.9720 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 891: loss = 2.5183 (2.314 sec/step)\n",
            "I1209 11:05:07.296855 139904075134848 learning.py:507] global step 891: loss = 2.5183 (2.314 sec/step)\n",
            "INFO:tensorflow:global step 892: loss = 3.1526 (0.845 sec/step)\n",
            "I1209 11:05:08.186778 139904075134848 learning.py:507] global step 892: loss = 3.1526 (0.845 sec/step)\n",
            "INFO:tensorflow:global step 893: loss = 2.9439 (1.566 sec/step)\n",
            "I1209 11:05:09.875375 139904075134848 learning.py:507] global step 893: loss = 2.9439 (1.566 sec/step)\n",
            "INFO:tensorflow:global step 894: loss = 2.6912 (0.650 sec/step)\n",
            "I1209 11:05:10.762762 139904075134848 learning.py:507] global step 894: loss = 2.6912 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 895: loss = 2.7570 (2.369 sec/step)\n",
            "I1209 11:05:13.302391 139904075134848 learning.py:507] global step 895: loss = 2.7570 (2.369 sec/step)\n",
            "INFO:tensorflow:global step 896: loss = 3.1192 (1.328 sec/step)\n",
            "I1209 11:05:14.632418 139904075134848 learning.py:507] global step 896: loss = 3.1192 (1.328 sec/step)\n",
            "INFO:tensorflow:global step 897: loss = 2.9529 (0.709 sec/step)\n",
            "I1209 11:05:15.585657 139904075134848 learning.py:507] global step 897: loss = 2.9529 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 898: loss = 2.2234 (0.724 sec/step)\n",
            "I1209 11:05:16.565218 139904075134848 learning.py:507] global step 898: loss = 2.2234 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 899: loss = 3.2097 (0.774 sec/step)\n",
            "I1209 11:05:18.527837 139904075134848 learning.py:507] global step 899: loss = 3.2097 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 2.6417 (2.920 sec/step)\n",
            "I1209 11:05:21.449392 139904075134848 learning.py:507] global step 900: loss = 2.6417 (2.920 sec/step)\n",
            "INFO:tensorflow:global step 901: loss = 2.9769 (0.688 sec/step)\n",
            "I1209 11:05:22.167211 139904075134848 learning.py:507] global step 901: loss = 2.9769 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 902: loss = 3.1218 (1.922 sec/step)\n",
            "I1209 11:05:24.199703 139904075134848 learning.py:507] global step 902: loss = 3.1218 (1.922 sec/step)\n",
            "INFO:tensorflow:global step 903: loss = 2.9547 (0.709 sec/step)\n",
            "I1209 11:05:25.194297 139904075134848 learning.py:507] global step 903: loss = 2.9547 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 904: loss = 2.7008 (2.555 sec/step)\n",
            "I1209 11:05:27.801939 139904075134848 learning.py:507] global step 904: loss = 2.7008 (2.555 sec/step)\n",
            "INFO:tensorflow:global step 905: loss = 2.4596 (0.646 sec/step)\n",
            "I1209 11:05:28.449728 139904075134848 learning.py:507] global step 905: loss = 2.4596 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 906: loss = 2.6349 (1.381 sec/step)\n",
            "I1209 11:05:29.887380 139904075134848 learning.py:507] global step 906: loss = 2.6349 (1.381 sec/step)\n",
            "INFO:tensorflow:global step 907: loss = 2.9489 (2.481 sec/step)\n",
            "I1209 11:05:32.582995 139904075134848 learning.py:507] global step 907: loss = 2.9489 (2.481 sec/step)\n",
            "INFO:tensorflow:global step 908: loss = 2.4954 (0.678 sec/step)\n",
            "I1209 11:05:33.262564 139904075134848 learning.py:507] global step 908: loss = 2.4954 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 909: loss = 2.8791 (2.136 sec/step)\n",
            "I1209 11:05:35.400062 139904075134848 learning.py:507] global step 909: loss = 2.8791 (2.136 sec/step)\n",
            "INFO:tensorflow:global step 910: loss = 3.0966 (0.663 sec/step)\n",
            "I1209 11:05:36.065233 139904075134848 learning.py:507] global step 910: loss = 3.0966 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 911: loss = 3.6637 (0.969 sec/step)\n",
            "I1209 11:05:37.303300 139904075134848 learning.py:507] global step 911: loss = 3.6637 (0.969 sec/step)\n",
            "INFO:tensorflow:global step 912: loss = 2.8187 (0.835 sec/step)\n",
            "I1209 11:05:38.441462 139904075134848 learning.py:507] global step 912: loss = 2.8187 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 913: loss = 2.7364 (2.244 sec/step)\n",
            "I1209 11:05:40.949494 139904075134848 learning.py:507] global step 913: loss = 2.7364 (2.244 sec/step)\n",
            "INFO:tensorflow:global step 914: loss = 3.0483 (0.599 sec/step)\n",
            "I1209 11:05:41.550296 139904075134848 learning.py:507] global step 914: loss = 3.0483 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 915: loss = 2.7349 (1.843 sec/step)\n",
            "I1209 11:05:43.395066 139904075134848 learning.py:507] global step 915: loss = 2.7349 (1.843 sec/step)\n",
            "INFO:tensorflow:global step 916: loss = 2.7876 (0.701 sec/step)\n",
            "I1209 11:05:44.097954 139904075134848 learning.py:507] global step 916: loss = 2.7876 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 917: loss = 2.2883 (1.289 sec/step)\n",
            "I1209 11:05:45.678425 139904075134848 learning.py:507] global step 917: loss = 2.2883 (1.289 sec/step)\n",
            "INFO:tensorflow:global step 918: loss = 3.0569 (2.726 sec/step)\n",
            "I1209 11:05:48.563839 139904075134848 learning.py:507] global step 918: loss = 3.0569 (2.726 sec/step)\n",
            "INFO:tensorflow:global step 919: loss = 2.7702 (0.690 sec/step)\n",
            "I1209 11:05:49.256375 139904075134848 learning.py:507] global step 919: loss = 2.7702 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 920: loss = 2.6852 (2.143 sec/step)\n",
            "I1209 11:05:51.401484 139904075134848 learning.py:507] global step 920: loss = 2.6852 (2.143 sec/step)\n",
            "INFO:tensorflow:global step 921: loss = 3.4969 (0.829 sec/step)\n",
            "I1209 11:05:52.234642 139904075134848 learning.py:507] global step 921: loss = 3.4969 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 922: loss = 2.8059 (1.423 sec/step)\n",
            "I1209 11:05:53.791597 139904075134848 learning.py:507] global step 922: loss = 2.8059 (1.423 sec/step)\n",
            "INFO:tensorflow:global step 923: loss = 3.4774 (0.782 sec/step)\n",
            "I1209 11:05:54.862095 139904075134848 learning.py:507] global step 923: loss = 3.4774 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 924: loss = 3.0172 (0.600 sec/step)\n",
            "I1209 11:05:55.601817 139904075134848 learning.py:507] global step 924: loss = 3.0172 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 925: loss = 3.3609 (2.549 sec/step)\n",
            "I1209 11:05:58.152661 139904075134848 learning.py:507] global step 925: loss = 3.3609 (2.549 sec/step)\n",
            "INFO:tensorflow:global step 926: loss = 2.6575 (0.716 sec/step)\n",
            "I1209 11:05:58.870256 139904075134848 learning.py:507] global step 926: loss = 2.6575 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 927: loss = 2.5190 (1.418 sec/step)\n",
            "I1209 11:06:00.641103 139904075134848 learning.py:507] global step 927: loss = 2.5190 (1.418 sec/step)\n",
            "INFO:tensorflow:global step 928: loss = 2.4781 (2.525 sec/step)\n",
            "I1209 11:06:03.289355 139904075134848 learning.py:507] global step 928: loss = 2.4781 (2.525 sec/step)\n",
            "INFO:tensorflow:global step 929: loss = 2.9302 (0.698 sec/step)\n",
            "I1209 11:06:04.157793 139904075134848 learning.py:507] global step 929: loss = 2.9302 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 930: loss = 2.7157 (2.593 sec/step)\n",
            "I1209 11:06:07.056360 139904075134848 learning.py:507] global step 930: loss = 2.7157 (2.593 sec/step)\n",
            "INFO:tensorflow:global step 931: loss = 3.1173 (0.793 sec/step)\n",
            "I1209 11:06:08.054259 139904075134848 learning.py:507] global step 931: loss = 3.1173 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 932: loss = 3.8387 (0.694 sec/step)\n",
            "I1209 11:06:08.916900 139904075134848 learning.py:507] global step 932: loss = 3.8387 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 933: loss = 2.8984 (1.661 sec/step)\n",
            "I1209 11:06:10.616939 139904075134848 learning.py:507] global step 933: loss = 2.8984 (1.661 sec/step)\n",
            "INFO:tensorflow:global step 934: loss = 3.2432 (0.823 sec/step)\n",
            "I1209 11:06:11.940836 139904075134848 learning.py:507] global step 934: loss = 3.2432 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 935: loss = 2.5948 (2.859 sec/step)\n",
            "I1209 11:06:14.902105 139904075134848 learning.py:507] global step 935: loss = 2.5948 (2.859 sec/step)\n",
            "INFO:tensorflow:global step 936: loss = 2.6397 (0.639 sec/step)\n",
            "I1209 11:06:15.543417 139904075134848 learning.py:507] global step 936: loss = 2.6397 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 937: loss = 2.3334 (0.997 sec/step)\n",
            "I1209 11:06:16.811241 139904075134848 learning.py:507] global step 937: loss = 2.3334 (0.997 sec/step)\n",
            "INFO:tensorflow:global step 938: loss = 2.2059 (2.182 sec/step)\n",
            "I1209 11:06:19.339865 139904075134848 learning.py:507] global step 938: loss = 2.2059 (2.182 sec/step)\n",
            "INFO:tensorflow:global step 939: loss = 3.0949 (0.698 sec/step)\n",
            "I1209 11:06:20.039585 139904075134848 learning.py:507] global step 939: loss = 3.0949 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 940: loss = 2.2962 (0.909 sec/step)\n",
            "I1209 11:06:21.128563 139904075134848 learning.py:507] global step 940: loss = 2.2962 (0.909 sec/step)\n",
            "INFO:tensorflow:global step 941: loss = 2.9991 (1.679 sec/step)\n",
            "I1209 11:06:23.156464 139904075134848 learning.py:507] global step 941: loss = 2.9991 (1.679 sec/step)\n",
            "INFO:tensorflow:global step 942: loss = 2.6872 (0.630 sec/step)\n",
            "I1209 11:06:23.787837 139904075134848 learning.py:507] global step 942: loss = 2.6872 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 943: loss = 2.3831 (1.594 sec/step)\n",
            "I1209 11:06:25.407011 139904075134848 learning.py:507] global step 943: loss = 2.3831 (1.594 sec/step)\n",
            "INFO:tensorflow:global step 944: loss = 2.3427 (1.976 sec/step)\n",
            "I1209 11:06:27.385067 139904075134848 learning.py:507] global step 944: loss = 2.3427 (1.976 sec/step)\n",
            "INFO:tensorflow:global step 945: loss = 2.6644 (0.748 sec/step)\n",
            "I1209 11:06:28.311682 139904075134848 learning.py:507] global step 945: loss = 2.6644 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 946: loss = 2.2700 (1.538 sec/step)\n",
            "I1209 11:06:30.097336 139904075134848 learning.py:507] global step 946: loss = 2.2700 (1.538 sec/step)\n",
            "INFO:tensorflow:global step 947: loss = 1.9675 (0.767 sec/step)\n",
            "I1209 11:06:31.214804 139904075134848 learning.py:507] global step 947: loss = 1.9675 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 948: loss = 2.8242 (0.749 sec/step)\n",
            "I1209 11:06:32.082051 139904075134848 learning.py:507] global step 948: loss = 2.8242 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 949: loss = 3.0968 (1.950 sec/step)\n",
            "I1209 11:06:34.033739 139904075134848 learning.py:507] global step 949: loss = 3.0968 (1.950 sec/step)\n",
            "INFO:tensorflow:global step 950: loss = 2.4392 (0.565 sec/step)\n",
            "I1209 11:06:34.600647 139904075134848 learning.py:507] global step 950: loss = 2.4392 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 951: loss = 2.7644 (1.044 sec/step)\n",
            "I1209 11:06:35.936802 139904075134848 learning.py:507] global step 951: loss = 2.7644 (1.044 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 952.\n",
            "I1209 11:06:39.251517 139900543997696 supervisor.py:1050] Recording summary at step 952.\n",
            "INFO:tensorflow:global step 952: loss = 2.3108 (3.068 sec/step)\n",
            "I1209 11:06:39.264199 139904075134848 learning.py:507] global step 952: loss = 2.3108 (3.068 sec/step)\n",
            "INFO:tensorflow:global step 953: loss = 2.2292 (0.587 sec/step)\n",
            "I1209 11:06:39.853151 139904075134848 learning.py:507] global step 953: loss = 2.2292 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 954: loss = 2.6844 (1.082 sec/step)\n",
            "I1209 11:06:41.185152 139904075134848 learning.py:507] global step 954: loss = 2.6844 (1.082 sec/step)\n",
            "INFO:tensorflow:global step 955: loss = 2.6069 (1.738 sec/step)\n",
            "I1209 11:06:43.228028 139904075134848 learning.py:507] global step 955: loss = 2.6069 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 956: loss = 2.8745 (0.829 sec/step)\n",
            "I1209 11:06:44.220188 139904075134848 learning.py:507] global step 956: loss = 2.8745 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 957: loss = 2.5291 (0.736 sec/step)\n",
            "I1209 11:06:45.100373 139904075134848 learning.py:507] global step 957: loss = 2.5291 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 958: loss = 2.2883 (0.907 sec/step)\n",
            "I1209 11:06:46.409816 139904075134848 learning.py:507] global step 958: loss = 2.2883 (0.907 sec/step)\n",
            "INFO:tensorflow:global step 959: loss = 2.8006 (1.784 sec/step)\n",
            "I1209 11:06:48.225471 139904075134848 learning.py:507] global step 959: loss = 2.8006 (1.784 sec/step)\n",
            "INFO:tensorflow:global step 960: loss = 2.9479 (0.684 sec/step)\n",
            "I1209 11:06:49.251039 139904075134848 learning.py:507] global step 960: loss = 2.9479 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 961: loss = 2.3392 (0.799 sec/step)\n",
            "I1209 11:06:50.447884 139904075134848 learning.py:507] global step 961: loss = 2.3392 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 962: loss = 2.6185 (1.400 sec/step)\n",
            "I1209 11:06:51.903120 139904075134848 learning.py:507] global step 962: loss = 2.6185 (1.400 sec/step)\n",
            "INFO:tensorflow:global step 963: loss = 2.5183 (0.704 sec/step)\n",
            "I1209 11:06:52.625557 139904075134848 learning.py:507] global step 963: loss = 2.5183 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 964: loss = 2.5308 (1.253 sec/step)\n",
            "I1209 11:06:54.202932 139904075134848 learning.py:507] global step 964: loss = 2.5308 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 965: loss = 2.8720 (0.640 sec/step)\n",
            "I1209 11:06:55.181636 139904075134848 learning.py:507] global step 965: loss = 2.8720 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 966: loss = 3.2645 (2.138 sec/step)\n",
            "I1209 11:06:57.378807 139904075134848 learning.py:507] global step 966: loss = 3.2645 (2.138 sec/step)\n",
            "INFO:tensorflow:global step 967: loss = 2.9880 (0.744 sec/step)\n",
            "I1209 11:06:58.125031 139904075134848 learning.py:507] global step 967: loss = 2.9880 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 968: loss = 2.2878 (1.255 sec/step)\n",
            "I1209 11:06:59.643464 139904075134848 learning.py:507] global step 968: loss = 2.2878 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 969: loss = 3.3119 (2.132 sec/step)\n",
            "I1209 11:07:01.862903 139904075134848 learning.py:507] global step 969: loss = 3.3119 (2.132 sec/step)\n",
            "INFO:tensorflow:global step 970: loss = 2.5410 (0.653 sec/step)\n",
            "I1209 11:07:02.518427 139904075134848 learning.py:507] global step 970: loss = 2.5410 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 971: loss = 3.3349 (1.222 sec/step)\n",
            "I1209 11:07:03.991808 139904075134848 learning.py:507] global step 971: loss = 3.3349 (1.222 sec/step)\n",
            "INFO:tensorflow:global step 972: loss = 2.2741 (1.813 sec/step)\n",
            "I1209 11:07:05.820426 139904075134848 learning.py:507] global step 972: loss = 2.2741 (1.813 sec/step)\n",
            "INFO:tensorflow:global step 973: loss = 2.9955 (0.716 sec/step)\n",
            "I1209 11:07:06.857831 139904075134848 learning.py:507] global step 973: loss = 2.9955 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 974: loss = 2.6054 (1.476 sec/step)\n",
            "I1209 11:07:08.444528 139904075134848 learning.py:507] global step 974: loss = 2.6054 (1.476 sec/step)\n",
            "INFO:tensorflow:global step 975: loss = 2.6387 (0.932 sec/step)\n",
            "I1209 11:07:09.618945 139904075134848 learning.py:507] global step 975: loss = 2.6387 (0.932 sec/step)\n",
            "INFO:tensorflow:global step 976: loss = 2.7785 (1.437 sec/step)\n",
            "I1209 11:07:11.095530 139904075134848 learning.py:507] global step 976: loss = 2.7785 (1.437 sec/step)\n",
            "INFO:tensorflow:global step 977: loss = 3.1796 (0.590 sec/step)\n",
            "I1209 11:07:11.687754 139904075134848 learning.py:507] global step 977: loss = 3.1796 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 978: loss = 2.8712 (1.001 sec/step)\n",
            "I1209 11:07:12.995806 139904075134848 learning.py:507] global step 978: loss = 2.8712 (1.001 sec/step)\n",
            "INFO:tensorflow:global step 979: loss = 2.8345 (2.003 sec/step)\n",
            "I1209 11:07:15.120763 139904075134848 learning.py:507] global step 979: loss = 2.8345 (2.003 sec/step)\n",
            "INFO:tensorflow:global step 980: loss = 2.7368 (0.778 sec/step)\n",
            "I1209 11:07:15.949303 139904075134848 learning.py:507] global step 980: loss = 2.7368 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 981: loss = 3.0786 (1.494 sec/step)\n",
            "I1209 11:07:17.786686 139904075134848 learning.py:507] global step 981: loss = 3.0786 (1.494 sec/step)\n",
            "INFO:tensorflow:global step 982: loss = 2.8215 (0.732 sec/step)\n",
            "I1209 11:07:18.682260 139904075134848 learning.py:507] global step 982: loss = 2.8215 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 983: loss = 2.7503 (2.737 sec/step)\n",
            "I1209 11:07:21.650074 139904075134848 learning.py:507] global step 983: loss = 2.7503 (2.737 sec/step)\n",
            "INFO:tensorflow:global step 984: loss = 2.2432 (0.669 sec/step)\n",
            "I1209 11:07:22.320600 139904075134848 learning.py:507] global step 984: loss = 2.2432 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 985: loss = 2.7789 (0.996 sec/step)\n",
            "I1209 11:07:23.611405 139904075134848 learning.py:507] global step 985: loss = 2.7789 (0.996 sec/step)\n",
            "INFO:tensorflow:global step 986: loss = 2.2792 (1.678 sec/step)\n",
            "I1209 11:07:25.459857 139904075134848 learning.py:507] global step 986: loss = 2.2792 (1.678 sec/step)\n",
            "INFO:tensorflow:global step 987: loss = 2.6001 (0.825 sec/step)\n",
            "I1209 11:07:26.434729 139904075134848 learning.py:507] global step 987: loss = 2.6001 (0.825 sec/step)\n",
            "INFO:tensorflow:global step 988: loss = 2.2798 (0.803 sec/step)\n",
            "I1209 11:07:27.685232 139904075134848 learning.py:507] global step 988: loss = 2.2798 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 989: loss = 2.5265 (2.117 sec/step)\n",
            "I1209 11:07:29.807792 139904075134848 learning.py:507] global step 989: loss = 2.5265 (2.117 sec/step)\n",
            "INFO:tensorflow:global step 990: loss = 2.4819 (0.859 sec/step)\n",
            "I1209 11:07:30.756588 139904075134848 learning.py:507] global step 990: loss = 2.4819 (0.859 sec/step)\n",
            "INFO:tensorflow:global step 991: loss = 2.8869 (2.467 sec/step)\n",
            "I1209 11:07:33.371851 139904075134848 learning.py:507] global step 991: loss = 2.8869 (2.467 sec/step)\n",
            "INFO:tensorflow:global step 992: loss = 2.7250 (0.785 sec/step)\n",
            "I1209 11:07:34.351078 139904075134848 learning.py:507] global step 992: loss = 2.7250 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 993: loss = 2.4921 (0.776 sec/step)\n",
            "I1209 11:07:35.240010 139904075134848 learning.py:507] global step 993: loss = 2.4921 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 994: loss = 2.6964 (1.318 sec/step)\n",
            "I1209 11:07:36.762854 139904075134848 learning.py:507] global step 994: loss = 2.6964 (1.318 sec/step)\n",
            "INFO:tensorflow:global step 995: loss = 2.5434 (1.642 sec/step)\n",
            "I1209 11:07:38.619248 139904075134848 learning.py:507] global step 995: loss = 2.5434 (1.642 sec/step)\n",
            "INFO:tensorflow:global step 996: loss = 2.6138 (0.722 sec/step)\n",
            "I1209 11:07:39.594627 139904075134848 learning.py:507] global step 996: loss = 2.6138 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 997: loss = 2.9089 (0.654 sec/step)\n",
            "I1209 11:07:40.472742 139904075134848 learning.py:507] global step 997: loss = 2.9089 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 998: loss = 2.1366 (2.001 sec/step)\n",
            "I1209 11:07:42.475228 139904075134848 learning.py:507] global step 998: loss = 2.1366 (2.001 sec/step)\n",
            "INFO:tensorflow:global step 999: loss = 2.3096 (0.584 sec/step)\n",
            "I1209 11:07:43.061011 139904075134848 learning.py:507] global step 999: loss = 2.3096 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 2.6628 (0.706 sec/step)\n",
            "I1209 11:07:43.902031 139904075134848 learning.py:507] global step 1000: loss = 2.6628 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1001: loss = 2.9904 (2.676 sec/step)\n",
            "I1209 11:07:46.600477 139904075134848 learning.py:507] global step 1001: loss = 2.9904 (2.676 sec/step)\n",
            "INFO:tensorflow:global step 1002: loss = 2.2242 (0.837 sec/step)\n",
            "I1209 11:07:47.683665 139904075134848 learning.py:507] global step 1002: loss = 2.2242 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 1003: loss = 1.7756 (1.677 sec/step)\n",
            "I1209 11:07:49.565785 139904075134848 learning.py:507] global step 1003: loss = 1.7756 (1.677 sec/step)\n",
            "INFO:tensorflow:global step 1004: loss = 2.6924 (0.705 sec/step)\n",
            "I1209 11:07:50.273167 139904075134848 learning.py:507] global step 1004: loss = 2.6924 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 1005: loss = 2.3739 (1.797 sec/step)\n",
            "I1209 11:07:52.072252 139904075134848 learning.py:507] global step 1005: loss = 2.3739 (1.797 sec/step)\n",
            "INFO:tensorflow:global step 1006: loss = 2.8147 (0.811 sec/step)\n",
            "I1209 11:07:52.950305 139904075134848 learning.py:507] global step 1006: loss = 2.8147 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 1007: loss = 3.0789 (1.686 sec/step)\n",
            "I1209 11:07:54.821546 139904075134848 learning.py:507] global step 1007: loss = 3.0789 (1.686 sec/step)\n",
            "INFO:tensorflow:global step 1008: loss = 3.2134 (0.645 sec/step)\n",
            "I1209 11:07:55.716660 139904075134848 learning.py:507] global step 1008: loss = 3.2134 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 1009: loss = 2.9810 (1.919 sec/step)\n",
            "I1209 11:07:57.924853 139904075134848 learning.py:507] global step 1009: loss = 2.9810 (1.919 sec/step)\n",
            "INFO:tensorflow:global step 1010: loss = 3.1153 (0.829 sec/step)\n",
            "I1209 11:07:58.779976 139904075134848 learning.py:507] global step 1010: loss = 3.1153 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1011: loss = 2.5238 (0.759 sec/step)\n",
            "I1209 11:07:59.877138 139904075134848 learning.py:507] global step 1011: loss = 2.5238 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 1012: loss = 3.0762 (1.947 sec/step)\n",
            "I1209 11:08:01.831831 139904075134848 learning.py:507] global step 1012: loss = 3.0762 (1.947 sec/step)\n",
            "INFO:tensorflow:global step 1013: loss = 2.9883 (1.941 sec/step)\n",
            "I1209 11:08:03.774463 139904075134848 learning.py:507] global step 1013: loss = 2.9883 (1.941 sec/step)\n",
            "INFO:tensorflow:global step 1014: loss = 2.7995 (0.891 sec/step)\n",
            "I1209 11:08:04.842425 139904075134848 learning.py:507] global step 1014: loss = 2.7995 (0.891 sec/step)\n",
            "INFO:tensorflow:global step 1015: loss = 2.2471 (0.810 sec/step)\n",
            "I1209 11:08:05.831479 139904075134848 learning.py:507] global step 1015: loss = 2.2471 (0.810 sec/step)\n",
            "INFO:tensorflow:global step 1016: loss = 3.2360 (1.044 sec/step)\n",
            "I1209 11:08:06.970655 139904075134848 learning.py:507] global step 1016: loss = 3.2360 (1.044 sec/step)\n",
            "INFO:tensorflow:global step 1017: loss = 2.6611 (2.255 sec/step)\n",
            "I1209 11:08:09.227255 139904075134848 learning.py:507] global step 1017: loss = 2.6611 (2.255 sec/step)\n",
            "INFO:tensorflow:global step 1018: loss = 2.6476 (0.659 sec/step)\n",
            "I1209 11:08:09.887805 139904075134848 learning.py:507] global step 1018: loss = 2.6476 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1019: loss = 2.1397 (0.889 sec/step)\n",
            "I1209 11:08:10.967157 139904075134848 learning.py:507] global step 1019: loss = 2.1397 (0.889 sec/step)\n",
            "INFO:tensorflow:global step 1020: loss = 2.7998 (1.920 sec/step)\n",
            "I1209 11:08:13.072118 139904075134848 learning.py:507] global step 1020: loss = 2.7998 (1.920 sec/step)\n",
            "INFO:tensorflow:global step 1021: loss = 2.5925 (0.725 sec/step)\n",
            "I1209 11:08:13.799246 139904075134848 learning.py:507] global step 1021: loss = 2.5925 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1022: loss = 2.4632 (1.089 sec/step)\n",
            "I1209 11:08:15.023493 139904075134848 learning.py:507] global step 1022: loss = 2.4632 (1.089 sec/step)\n",
            "INFO:tensorflow:global step 1023: loss = 2.4055 (2.193 sec/step)\n",
            "I1209 11:08:17.251454 139904075134848 learning.py:507] global step 1023: loss = 2.4055 (2.193 sec/step)\n",
            "INFO:tensorflow:global step 1024: loss = 2.4541 (0.704 sec/step)\n",
            "I1209 11:08:17.957494 139904075134848 learning.py:507] global step 1024: loss = 2.4541 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1025: loss = 2.6547 (1.883 sec/step)\n",
            "I1209 11:08:19.842313 139904075134848 learning.py:507] global step 1025: loss = 2.6547 (1.883 sec/step)\n",
            "INFO:tensorflow:global step 1026: loss = 2.3412 (0.670 sec/step)\n",
            "I1209 11:08:20.772127 139904075134848 learning.py:507] global step 1026: loss = 2.3412 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1027: loss = 2.2281 (2.085 sec/step)\n",
            "I1209 11:08:22.941408 139904075134848 learning.py:507] global step 1027: loss = 2.2281 (2.085 sec/step)\n",
            "INFO:tensorflow:global step 1028: loss = 2.5067 (0.661 sec/step)\n",
            "I1209 11:08:23.604347 139904075134848 learning.py:507] global step 1028: loss = 2.5067 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1029: loss = 3.4580 (1.107 sec/step)\n",
            "I1209 11:08:24.743030 139904075134848 learning.py:507] global step 1029: loss = 3.4580 (1.107 sec/step)\n",
            "INFO:tensorflow:global step 1030: loss = 2.7161 (2.008 sec/step)\n",
            "I1209 11:08:27.100906 139904075134848 learning.py:507] global step 1030: loss = 2.7161 (2.008 sec/step)\n",
            "INFO:tensorflow:global step 1031: loss = 2.1516 (0.656 sec/step)\n",
            "I1209 11:08:27.928720 139904075134848 learning.py:507] global step 1031: loss = 2.1516 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1032: loss = 2.7058 (0.583 sec/step)\n",
            "I1209 11:08:29.137878 139904075134848 learning.py:507] global step 1032: loss = 2.7058 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 1033: loss = 2.8459 (0.713 sec/step)\n",
            "I1209 11:08:29.981215 139904075134848 learning.py:507] global step 1033: loss = 2.8459 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1034: loss = 2.1947 (0.730 sec/step)\n",
            "I1209 11:08:30.767984 139904075134848 learning.py:507] global step 1034: loss = 2.1947 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 1035: loss = 2.4991 (2.803 sec/step)\n",
            "I1209 11:08:33.580421 139904075134848 learning.py:507] global step 1035: loss = 2.4991 (2.803 sec/step)\n",
            "INFO:tensorflow:global step 1036: loss = 1.9548 (0.750 sec/step)\n",
            "I1209 11:08:34.531159 139904075134848 learning.py:507] global step 1036: loss = 1.9548 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 1037: loss = 2.8870 (1.739 sec/step)\n",
            "I1209 11:08:36.445796 139904075134848 learning.py:507] global step 1037: loss = 2.8870 (1.739 sec/step)\n",
            "INFO:tensorflow:global step 1038: loss = 2.2927 (2.082 sec/step)\n",
            "I1209 11:08:38.761566 139904075134848 learning.py:507] global step 1038: loss = 2.2927 (2.082 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1038.\n",
            "I1209 11:08:39.241332 139900543997696 supervisor.py:1050] Recording summary at step 1038.\n",
            "INFO:tensorflow:global step 1039: loss = 2.3770 (1.223 sec/step)\n",
            "I1209 11:08:40.521242 139904075134848 learning.py:507] global step 1039: loss = 2.3770 (1.223 sec/step)\n",
            "INFO:tensorflow:global step 1040: loss = 2.3035 (0.662 sec/step)\n",
            "I1209 11:08:41.265357 139904075134848 learning.py:507] global step 1040: loss = 2.3035 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1041: loss = 2.9451 (1.020 sec/step)\n",
            "I1209 11:08:42.615191 139904075134848 learning.py:507] global step 1041: loss = 2.9451 (1.020 sec/step)\n",
            "INFO:tensorflow:global step 1042: loss = 3.2280 (2.075 sec/step)\n",
            "I1209 11:08:44.933118 139904075134848 learning.py:507] global step 1042: loss = 3.2280 (2.075 sec/step)\n",
            "INFO:tensorflow:global step 1043: loss = 2.8383 (0.579 sec/step)\n",
            "I1209 11:08:45.833750 139904075134848 learning.py:507] global step 1043: loss = 2.8383 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 1044: loss = 2.9225 (1.749 sec/step)\n",
            "I1209 11:08:47.713448 139904075134848 learning.py:507] global step 1044: loss = 2.9225 (1.749 sec/step)\n",
            "INFO:tensorflow:global step 1045: loss = 2.0073 (0.784 sec/step)\n",
            "I1209 11:08:48.723856 139904075134848 learning.py:507] global step 1045: loss = 2.0073 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 1046: loss = 3.0499 (0.699 sec/step)\n",
            "I1209 11:08:50.001429 139904075134848 learning.py:507] global step 1046: loss = 3.0499 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1047: loss = 2.7352 (0.774 sec/step)\n",
            "I1209 11:08:51.259110 139904075134848 learning.py:507] global step 1047: loss = 2.7352 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 1048: loss = 2.9147 (1.790 sec/step)\n",
            "I1209 11:08:53.198943 139904075134848 learning.py:507] global step 1048: loss = 2.9147 (1.790 sec/step)\n",
            "INFO:tensorflow:global step 1049: loss = 2.3248 (0.659 sec/step)\n",
            "I1209 11:08:53.859570 139904075134848 learning.py:507] global step 1049: loss = 2.3248 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1050: loss = 3.1890 (0.588 sec/step)\n",
            "I1209 11:08:54.449861 139904075134848 learning.py:507] global step 1050: loss = 3.1890 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 1051: loss = 2.4648 (2.895 sec/step)\n",
            "I1209 11:08:57.347216 139904075134848 learning.py:507] global step 1051: loss = 2.4648 (2.895 sec/step)\n",
            "INFO:tensorflow:global step 1052: loss = 3.0258 (0.795 sec/step)\n",
            "I1209 11:08:58.311939 139904075134848 learning.py:507] global step 1052: loss = 3.0258 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 1053: loss = 2.5816 (0.860 sec/step)\n",
            "I1209 11:08:59.418332 139904075134848 learning.py:507] global step 1053: loss = 2.5816 (0.860 sec/step)\n",
            "INFO:tensorflow:global step 1054: loss = 2.5652 (2.049 sec/step)\n",
            "I1209 11:09:01.475249 139904075134848 learning.py:507] global step 1054: loss = 2.5652 (2.049 sec/step)\n",
            "INFO:tensorflow:global step 1055: loss = 2.9086 (0.690 sec/step)\n",
            "I1209 11:09:02.166588 139904075134848 learning.py:507] global step 1055: loss = 2.9086 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1056: loss = 2.6552 (1.898 sec/step)\n",
            "I1209 11:09:04.066210 139904075134848 learning.py:507] global step 1056: loss = 2.6552 (1.898 sec/step)\n",
            "INFO:tensorflow:global step 1057: loss = 2.8528 (0.612 sec/step)\n",
            "I1209 11:09:05.041291 139904075134848 learning.py:507] global step 1057: loss = 2.8528 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1058: loss = 4.2525 (2.635 sec/step)\n",
            "I1209 11:09:07.837911 139904075134848 learning.py:507] global step 1058: loss = 4.2525 (2.635 sec/step)\n",
            "INFO:tensorflow:global step 1059: loss = 2.7038 (0.838 sec/step)\n",
            "I1209 11:09:08.974078 139904075134848 learning.py:507] global step 1059: loss = 2.7038 (0.838 sec/step)\n",
            "INFO:tensorflow:global step 1060: loss = 2.5181 (1.267 sec/step)\n",
            "I1209 11:09:10.559771 139904075134848 learning.py:507] global step 1060: loss = 2.5181 (1.267 sec/step)\n",
            "INFO:tensorflow:global step 1061: loss = 2.4458 (0.672 sec/step)\n",
            "I1209 11:09:11.434585 139904075134848 learning.py:507] global step 1061: loss = 2.4458 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1062: loss = 2.2885 (1.926 sec/step)\n",
            "I1209 11:09:13.362052 139904075134848 learning.py:507] global step 1062: loss = 2.2885 (1.926 sec/step)\n",
            "INFO:tensorflow:global step 1063: loss = 2.8095 (0.688 sec/step)\n",
            "I1209 11:09:14.052333 139904075134848 learning.py:507] global step 1063: loss = 2.8095 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1064: loss = 2.3117 (2.026 sec/step)\n",
            "I1209 11:09:16.081195 139904075134848 learning.py:507] global step 1064: loss = 2.3117 (2.026 sec/step)\n",
            "INFO:tensorflow:global step 1065: loss = 2.6208 (0.777 sec/step)\n",
            "I1209 11:09:16.860010 139904075134848 learning.py:507] global step 1065: loss = 2.6208 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 1066: loss = 2.4317 (2.097 sec/step)\n",
            "I1209 11:09:18.959800 139904075134848 learning.py:507] global step 1066: loss = 2.4317 (2.097 sec/step)\n",
            "INFO:tensorflow:global step 1067: loss = 2.0841 (0.669 sec/step)\n",
            "I1209 11:09:19.930231 139904075134848 learning.py:507] global step 1067: loss = 2.0841 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1068: loss = 2.7080 (2.205 sec/step)\n",
            "I1209 11:09:22.267803 139904075134848 learning.py:507] global step 1068: loss = 2.7080 (2.205 sec/step)\n",
            "INFO:tensorflow:global step 1069: loss = 2.6809 (0.777 sec/step)\n",
            "I1209 11:09:23.054794 139904075134848 learning.py:507] global step 1069: loss = 2.6809 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 1070: loss = 2.0285 (2.398 sec/step)\n",
            "I1209 11:09:25.454723 139904075134848 learning.py:507] global step 1070: loss = 2.0285 (2.398 sec/step)\n",
            "INFO:tensorflow:global step 1071: loss = 3.0560 (0.647 sec/step)\n",
            "I1209 11:09:26.404077 139904075134848 learning.py:507] global step 1071: loss = 3.0560 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1072: loss = 2.1809 (2.082 sec/step)\n",
            "I1209 11:09:28.487781 139904075134848 learning.py:507] global step 1072: loss = 2.1809 (2.082 sec/step)\n",
            "INFO:tensorflow:global step 1073: loss = 2.4594 (0.593 sec/step)\n",
            "I1209 11:09:29.082363 139904075134848 learning.py:507] global step 1073: loss = 2.4594 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 1074: loss = 2.9777 (1.865 sec/step)\n",
            "I1209 11:09:30.949594 139904075134848 learning.py:507] global step 1074: loss = 2.9777 (1.865 sec/step)\n",
            "INFO:tensorflow:global step 1075: loss = 2.4905 (0.646 sec/step)\n",
            "I1209 11:09:31.716686 139904075134848 learning.py:507] global step 1075: loss = 2.4905 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1076: loss = 2.4589 (1.421 sec/step)\n",
            "I1209 11:09:33.283603 139904075134848 learning.py:507] global step 1076: loss = 2.4589 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 1077: loss = 2.8747 (0.651 sec/step)\n",
            "I1209 11:09:33.936521 139904075134848 learning.py:507] global step 1077: loss = 2.8747 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1078: loss = 2.2356 (1.089 sec/step)\n",
            "I1209 11:09:35.350122 139904075134848 learning.py:507] global step 1078: loss = 2.2356 (1.089 sec/step)\n",
            "INFO:tensorflow:global step 1079: loss = 2.3207 (1.880 sec/step)\n",
            "I1209 11:09:37.414847 139904075134848 learning.py:507] global step 1079: loss = 2.3207 (1.880 sec/step)\n",
            "INFO:tensorflow:global step 1080: loss = 2.2443 (0.745 sec/step)\n",
            "I1209 11:09:38.424719 139904075134848 learning.py:507] global step 1080: loss = 2.2443 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 1081: loss = 2.9722 (1.552 sec/step)\n",
            "I1209 11:09:40.112987 139904075134848 learning.py:507] global step 1081: loss = 2.9722 (1.552 sec/step)\n",
            "INFO:tensorflow:global step 1082: loss = 3.2519 (1.192 sec/step)\n",
            "I1209 11:09:41.307060 139904075134848 learning.py:507] global step 1082: loss = 3.2519 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1083: loss = 2.5880 (0.612 sec/step)\n",
            "I1209 11:09:41.920961 139904075134848 learning.py:507] global step 1083: loss = 2.5880 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 1084: loss = 2.8078 (1.107 sec/step)\n",
            "I1209 11:09:43.284286 139904075134848 learning.py:507] global step 1084: loss = 2.8078 (1.107 sec/step)\n",
            "INFO:tensorflow:global step 1085: loss = 2.4221 (1.787 sec/step)\n",
            "I1209 11:09:45.268219 139904075134848 learning.py:507] global step 1085: loss = 2.4221 (1.787 sec/step)\n",
            "INFO:tensorflow:global step 1086: loss = 2.7518 (0.713 sec/step)\n",
            "I1209 11:09:45.982826 139904075134848 learning.py:507] global step 1086: loss = 2.7518 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1087: loss = 3.2990 (1.951 sec/step)\n",
            "I1209 11:09:47.935745 139904075134848 learning.py:507] global step 1087: loss = 3.2990 (1.951 sec/step)\n",
            "INFO:tensorflow:global step 1088: loss = 2.2245 (0.610 sec/step)\n",
            "I1209 11:09:48.547239 139904075134848 learning.py:507] global step 1088: loss = 2.2245 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1089: loss = 2.8570 (1.141 sec/step)\n",
            "I1209 11:09:49.864830 139904075134848 learning.py:507] global step 1089: loss = 2.8570 (1.141 sec/step)\n",
            "INFO:tensorflow:global step 1090: loss = 3.4520 (2.407 sec/step)\n",
            "I1209 11:09:52.400715 139904075134848 learning.py:507] global step 1090: loss = 3.4520 (2.407 sec/step)\n",
            "INFO:tensorflow:global step 1091: loss = 2.4303 (0.971 sec/step)\n",
            "I1209 11:09:53.501776 139904075134848 learning.py:507] global step 1091: loss = 2.4303 (0.971 sec/step)\n",
            "INFO:tensorflow:global step 1092: loss = 2.2524 (1.491 sec/step)\n",
            "I1209 11:09:55.172158 139904075134848 learning.py:507] global step 1092: loss = 2.2524 (1.491 sec/step)\n",
            "INFO:tensorflow:global step 1093: loss = 2.6530 (0.728 sec/step)\n",
            "I1209 11:09:55.908422 139904075134848 learning.py:507] global step 1093: loss = 2.6530 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 1094: loss = 2.5792 (2.212 sec/step)\n",
            "I1209 11:09:58.148302 139904075134848 learning.py:507] global step 1094: loss = 2.5792 (2.212 sec/step)\n",
            "INFO:tensorflow:global step 1095: loss = 2.3795 (0.812 sec/step)\n",
            "I1209 11:09:59.280221 139904075134848 learning.py:507] global step 1095: loss = 2.3795 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 1096: loss = 2.9973 (0.875 sec/step)\n",
            "I1209 11:10:00.240416 139904075134848 learning.py:507] global step 1096: loss = 2.9973 (0.875 sec/step)\n",
            "INFO:tensorflow:global step 1097: loss = 2.7234 (0.713 sec/step)\n",
            "I1209 11:10:01.129299 139904075134848 learning.py:507] global step 1097: loss = 2.7234 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1098: loss = 2.6790 (0.627 sec/step)\n",
            "I1209 11:10:01.758765 139904075134848 learning.py:507] global step 1098: loss = 2.6790 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1099: loss = 2.3767 (2.313 sec/step)\n",
            "I1209 11:10:04.308641 139904075134848 learning.py:507] global step 1099: loss = 2.3767 (2.313 sec/step)\n",
            "INFO:tensorflow:global step 1100: loss = 2.4287 (1.794 sec/step)\n",
            "I1209 11:10:06.192431 139904075134848 learning.py:507] global step 1100: loss = 2.4287 (1.794 sec/step)\n",
            "INFO:tensorflow:global step 1101: loss = 3.2414 (0.711 sec/step)\n",
            "I1209 11:10:07.171244 139904075134848 learning.py:507] global step 1101: loss = 3.2414 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 1102: loss = 2.1968 (2.053 sec/step)\n",
            "I1209 11:10:09.363133 139904075134848 learning.py:507] global step 1102: loss = 2.1968 (2.053 sec/step)\n",
            "INFO:tensorflow:global step 1103: loss = 3.5365 (0.815 sec/step)\n",
            "I1209 11:10:10.387695 139904075134848 learning.py:507] global step 1103: loss = 3.5365 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1104: loss = 2.7550 (1.852 sec/step)\n",
            "I1209 11:10:12.435230 139904075134848 learning.py:507] global step 1104: loss = 2.7550 (1.852 sec/step)\n",
            "INFO:tensorflow:global step 1105: loss = 3.1277 (0.598 sec/step)\n",
            "I1209 11:10:13.035237 139904075134848 learning.py:507] global step 1105: loss = 3.1277 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 1106: loss = 2.2074 (2.123 sec/step)\n",
            "I1209 11:10:15.160344 139904075134848 learning.py:507] global step 1106: loss = 2.2074 (2.123 sec/step)\n",
            "INFO:tensorflow:global step 1107: loss = 2.6032 (0.635 sec/step)\n",
            "I1209 11:10:15.797420 139904075134848 learning.py:507] global step 1107: loss = 2.6032 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1108: loss = 3.3227 (0.902 sec/step)\n",
            "I1209 11:10:16.731747 139904075134848 learning.py:507] global step 1108: loss = 3.3227 (0.902 sec/step)\n",
            "INFO:tensorflow:global step 1109: loss = 2.9255 (2.472 sec/step)\n",
            "I1209 11:10:19.242773 139904075134848 learning.py:507] global step 1109: loss = 2.9255 (2.472 sec/step)\n",
            "INFO:tensorflow:global step 1110: loss = 2.9057 (0.681 sec/step)\n",
            "I1209 11:10:20.410767 139904075134848 learning.py:507] global step 1110: loss = 2.9057 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1111: loss = 3.1048 (1.674 sec/step)\n",
            "I1209 11:10:22.342919 139904075134848 learning.py:507] global step 1111: loss = 3.1048 (1.674 sec/step)\n",
            "INFO:tensorflow:global step 1112: loss = 3.1069 (0.856 sec/step)\n",
            "I1209 11:10:23.434492 139904075134848 learning.py:507] global step 1112: loss = 3.1069 (0.856 sec/step)\n",
            "INFO:tensorflow:global step 1113: loss = 2.9175 (1.948 sec/step)\n",
            "I1209 11:10:25.529286 139904075134848 learning.py:507] global step 1113: loss = 2.9175 (1.948 sec/step)\n",
            "INFO:tensorflow:global step 1114: loss = 2.6248 (0.676 sec/step)\n",
            "I1209 11:10:26.207462 139904075134848 learning.py:507] global step 1114: loss = 2.6248 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1115: loss = 2.3258 (1.958 sec/step)\n",
            "I1209 11:10:28.166904 139904075134848 learning.py:507] global step 1115: loss = 2.3258 (1.958 sec/step)\n",
            "INFO:tensorflow:global step 1116: loss = 3.1135 (0.732 sec/step)\n",
            "I1209 11:10:29.229572 139904075134848 learning.py:507] global step 1116: loss = 3.1135 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1117: loss = 2.5470 (1.930 sec/step)\n",
            "I1209 11:10:31.213897 139904075134848 learning.py:507] global step 1117: loss = 2.5470 (1.930 sec/step)\n",
            "INFO:tensorflow:global step 1118: loss = 2.1550 (0.756 sec/step)\n",
            "I1209 11:10:32.274275 139904075134848 learning.py:507] global step 1118: loss = 2.1550 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 1119: loss = 2.6946 (0.798 sec/step)\n",
            "I1209 11:10:33.569516 139904075134848 learning.py:507] global step 1119: loss = 2.6946 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 1120: loss = 2.4496 (0.829 sec/step)\n",
            "I1209 11:10:34.518238 139904075134848 learning.py:507] global step 1120: loss = 2.4496 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1121: loss = 2.2806 (2.747 sec/step)\n",
            "I1209 11:10:37.972056 139904075134848 learning.py:507] global step 1121: loss = 2.2806 (2.747 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1121.\n",
            "I1209 11:10:39.631252 139900543997696 supervisor.py:1050] Recording summary at step 1121.\n",
            "INFO:tensorflow:global step 1122: loss = 2.4789 (1.887 sec/step)\n",
            "I1209 11:10:39.996940 139904075134848 learning.py:507] global step 1122: loss = 2.4789 (1.887 sec/step)\n",
            "INFO:tensorflow:global step 1123: loss = 2.4285 (0.683 sec/step)\n",
            "I1209 11:10:40.846704 139904075134848 learning.py:507] global step 1123: loss = 2.4285 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 1124: loss = 2.9503 (2.014 sec/step)\n",
            "I1209 11:10:43.003830 139904075134848 learning.py:507] global step 1124: loss = 2.9503 (2.014 sec/step)\n",
            "INFO:tensorflow:global step 1125: loss = 2.5580 (0.727 sec/step)\n",
            "I1209 11:10:43.733387 139904075134848 learning.py:507] global step 1125: loss = 2.5580 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1126: loss = 3.3718 (0.971 sec/step)\n",
            "I1209 11:10:44.961439 139904075134848 learning.py:507] global step 1126: loss = 3.3718 (0.971 sec/step)\n",
            "INFO:tensorflow:global step 1127: loss = 2.5984 (1.620 sec/step)\n",
            "I1209 11:10:47.011342 139904075134848 learning.py:507] global step 1127: loss = 2.5984 (1.620 sec/step)\n",
            "INFO:tensorflow:global step 1128: loss = 2.6596 (0.633 sec/step)\n",
            "I1209 11:10:47.860414 139904075134848 learning.py:507] global step 1128: loss = 2.6596 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1129: loss = 2.3799 (2.284 sec/step)\n",
            "I1209 11:10:50.393510 139904075134848 learning.py:507] global step 1129: loss = 2.3799 (2.284 sec/step)\n",
            "INFO:tensorflow:global step 1130: loss = 2.1272 (0.689 sec/step)\n",
            "I1209 11:10:51.084765 139904075134848 learning.py:507] global step 1130: loss = 2.1272 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1131: loss = 3.6067 (1.943 sec/step)\n",
            "I1209 11:10:53.030063 139904075134848 learning.py:507] global step 1131: loss = 3.6067 (1.943 sec/step)\n",
            "INFO:tensorflow:global step 1132: loss = 2.2281 (0.619 sec/step)\n",
            "I1209 11:10:53.650767 139904075134848 learning.py:507] global step 1132: loss = 2.2281 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1133: loss = 2.4075 (1.168 sec/step)\n",
            "I1209 11:10:55.087596 139904075134848 learning.py:507] global step 1133: loss = 2.4075 (1.168 sec/step)\n",
            "INFO:tensorflow:global step 1134: loss = 2.8972 (1.833 sec/step)\n",
            "I1209 11:10:57.094783 139904075134848 learning.py:507] global step 1134: loss = 2.8972 (1.833 sec/step)\n",
            "INFO:tensorflow:global step 1135: loss = 2.3917 (0.779 sec/step)\n",
            "I1209 11:10:58.172007 139904075134848 learning.py:507] global step 1135: loss = 2.3917 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 1136: loss = 2.9066 (0.800 sec/step)\n",
            "I1209 11:10:59.176156 139904075134848 learning.py:507] global step 1136: loss = 2.9066 (0.800 sec/step)\n",
            "INFO:tensorflow:global step 1137: loss = 2.4807 (1.382 sec/step)\n",
            "I1209 11:11:00.559915 139904075134848 learning.py:507] global step 1137: loss = 2.4807 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 1138: loss = 3.2060 (0.693 sec/step)\n",
            "I1209 11:11:01.526709 139904075134848 learning.py:507] global step 1138: loss = 3.2060 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1139: loss = 3.0620 (0.652 sec/step)\n",
            "I1209 11:11:02.249128 139904075134848 learning.py:507] global step 1139: loss = 3.0620 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1140: loss = 3.1038 (1.816 sec/step)\n",
            "I1209 11:11:04.066665 139904075134848 learning.py:507] global step 1140: loss = 3.1038 (1.816 sec/step)\n",
            "INFO:tensorflow:global step 1141: loss = 2.9064 (0.829 sec/step)\n",
            "I1209 11:11:05.086181 139904075134848 learning.py:507] global step 1141: loss = 2.9064 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1142: loss = 2.4875 (0.623 sec/step)\n",
            "I1209 11:11:05.751191 139904075134848 learning.py:507] global step 1142: loss = 2.4875 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1143: loss = 2.4117 (0.649 sec/step)\n",
            "I1209 11:11:06.402328 139904075134848 learning.py:507] global step 1143: loss = 2.4117 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1144: loss = 3.3878 (2.734 sec/step)\n",
            "I1209 11:11:09.138025 139904075134848 learning.py:507] global step 1144: loss = 3.3878 (2.734 sec/step)\n",
            "INFO:tensorflow:global step 1145: loss = 2.7600 (0.709 sec/step)\n",
            "I1209 11:11:10.133651 139904075134848 learning.py:507] global step 1145: loss = 2.7600 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 1146: loss = 2.7116 (0.900 sec/step)\n",
            "I1209 11:11:11.344324 139904075134848 learning.py:507] global step 1146: loss = 2.7116 (0.900 sec/step)\n",
            "INFO:tensorflow:global step 1147: loss = 2.6169 (0.676 sec/step)\n",
            "I1209 11:11:12.037659 139904075134848 learning.py:507] global step 1147: loss = 2.6169 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1148: loss = 2.8687 (1.075 sec/step)\n",
            "I1209 11:11:13.160552 139904075134848 learning.py:507] global step 1148: loss = 2.8687 (1.075 sec/step)\n",
            "INFO:tensorflow:global step 1149: loss = 2.1472 (1.724 sec/step)\n",
            "I1209 11:11:15.189370 139904075134848 learning.py:507] global step 1149: loss = 2.1472 (1.724 sec/step)\n",
            "INFO:tensorflow:global step 1150: loss = 2.4183 (0.680 sec/step)\n",
            "I1209 11:11:16.051125 139904075134848 learning.py:507] global step 1150: loss = 2.4183 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1151: loss = 2.6318 (1.882 sec/step)\n",
            "I1209 11:11:18.137685 139904075134848 learning.py:507] global step 1151: loss = 2.6318 (1.882 sec/step)\n",
            "INFO:tensorflow:global step 1152: loss = 2.9847 (0.780 sec/step)\n",
            "I1209 11:11:19.122459 139904075134848 learning.py:507] global step 1152: loss = 2.9847 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 1153: loss = 2.0054 (0.668 sec/step)\n",
            "I1209 11:11:20.298712 139904075134848 learning.py:507] global step 1153: loss = 2.0054 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1154: loss = 2.5241 (0.828 sec/step)\n",
            "I1209 11:11:21.409228 139904075134848 learning.py:507] global step 1154: loss = 2.5241 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 1155: loss = 2.4687 (0.655 sec/step)\n",
            "I1209 11:11:22.565066 139904075134848 learning.py:507] global step 1155: loss = 2.4687 (0.655 sec/step)\n",
            "INFO:tensorflow:global step 1156: loss = 2.1289 (2.157 sec/step)\n",
            "I1209 11:11:24.809970 139904075134848 learning.py:507] global step 1156: loss = 2.1289 (2.157 sec/step)\n",
            "INFO:tensorflow:global step 1157: loss = 2.3438 (0.679 sec/step)\n",
            "I1209 11:11:25.693999 139904075134848 learning.py:507] global step 1157: loss = 2.3438 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1158: loss = 2.3784 (0.592 sec/step)\n",
            "I1209 11:11:26.719537 139904075134848 learning.py:507] global step 1158: loss = 2.3784 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 1159: loss = 2.3419 (1.676 sec/step)\n",
            "I1209 11:11:28.526129 139904075134848 learning.py:507] global step 1159: loss = 2.3419 (1.676 sec/step)\n",
            "INFO:tensorflow:global step 1160: loss = 2.6146 (0.623 sec/step)\n",
            "I1209 11:11:29.151296 139904075134848 learning.py:507] global step 1160: loss = 2.6146 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 1161: loss = 2.7568 (1.908 sec/step)\n",
            "I1209 11:11:31.061456 139904075134848 learning.py:507] global step 1161: loss = 2.7568 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 1162: loss = 2.6216 (0.643 sec/step)\n",
            "I1209 11:11:31.706132 139904075134848 learning.py:507] global step 1162: loss = 2.6216 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1163: loss = 2.5706 (1.123 sec/step)\n",
            "I1209 11:11:32.968531 139904075134848 learning.py:507] global step 1163: loss = 2.5706 (1.123 sec/step)\n",
            "INFO:tensorflow:global step 1164: loss = 2.9477 (1.768 sec/step)\n",
            "I1209 11:11:35.056679 139904075134848 learning.py:507] global step 1164: loss = 2.9477 (1.768 sec/step)\n",
            "INFO:tensorflow:global step 1165: loss = 1.9968 (0.828 sec/step)\n",
            "I1209 11:11:36.009840 139904075134848 learning.py:507] global step 1165: loss = 1.9968 (0.828 sec/step)\n",
            "INFO:tensorflow:global step 1166: loss = 2.1432 (3.010 sec/step)\n",
            "I1209 11:11:39.153542 139904075134848 learning.py:507] global step 1166: loss = 2.1432 (3.010 sec/step)\n",
            "INFO:tensorflow:global step 1167: loss = 2.2444 (0.723 sec/step)\n",
            "I1209 11:11:40.249279 139904075134848 learning.py:507] global step 1167: loss = 2.2444 (0.723 sec/step)\n",
            "INFO:tensorflow:global step 1168: loss = 2.5894 (0.823 sec/step)\n",
            "I1209 11:11:41.468785 139904075134848 learning.py:507] global step 1168: loss = 2.5894 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 1169: loss = 2.6243 (1.405 sec/step)\n",
            "I1209 11:11:42.934657 139904075134848 learning.py:507] global step 1169: loss = 2.6243 (1.405 sec/step)\n",
            "INFO:tensorflow:global step 1170: loss = 2.4959 (0.756 sec/step)\n",
            "I1209 11:11:43.884752 139904075134848 learning.py:507] global step 1170: loss = 2.4959 (0.756 sec/step)\n",
            "INFO:tensorflow:global step 1171: loss = 2.5411 (0.728 sec/step)\n",
            "I1209 11:11:44.781972 139904075134848 learning.py:507] global step 1171: loss = 2.5411 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 1172: loss = 2.3622 (0.684 sec/step)\n",
            "I1209 11:11:45.468038 139904075134848 learning.py:507] global step 1172: loss = 2.3622 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1173: loss = 2.8956 (2.032 sec/step)\n",
            "I1209 11:11:47.719754 139904075134848 learning.py:507] global step 1173: loss = 2.8956 (2.032 sec/step)\n",
            "INFO:tensorflow:global step 1174: loss = 2.7275 (0.799 sec/step)\n",
            "I1209 11:11:48.822410 139904075134848 learning.py:507] global step 1174: loss = 2.7275 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 1175: loss = 1.9119 (2.437 sec/step)\n",
            "I1209 11:11:51.261561 139904075134848 learning.py:507] global step 1175: loss = 1.9119 (2.437 sec/step)\n",
            "INFO:tensorflow:global step 1176: loss = 1.9666 (0.749 sec/step)\n",
            "I1209 11:11:52.100344 139904075134848 learning.py:507] global step 1176: loss = 1.9666 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1177: loss = 1.9071 (1.692 sec/step)\n",
            "I1209 11:11:54.095851 139904075134848 learning.py:507] global step 1177: loss = 1.9071 (1.692 sec/step)\n",
            "INFO:tensorflow:global step 1178: loss = 2.6001 (0.643 sec/step)\n",
            "I1209 11:11:54.992082 139904075134848 learning.py:507] global step 1178: loss = 2.6001 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1179: loss = 2.3246 (1.870 sec/step)\n",
            "I1209 11:11:57.084163 139904075134848 learning.py:507] global step 1179: loss = 2.3246 (1.870 sec/step)\n",
            "INFO:tensorflow:global step 1180: loss = 2.3876 (0.643 sec/step)\n",
            "I1209 11:11:57.728691 139904075134848 learning.py:507] global step 1180: loss = 2.3876 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1181: loss = 2.2076 (1.668 sec/step)\n",
            "I1209 11:11:59.634629 139904075134848 learning.py:507] global step 1181: loss = 2.2076 (1.668 sec/step)\n",
            "INFO:tensorflow:global step 1182: loss = 2.1747 (0.717 sec/step)\n",
            "I1209 11:12:00.651825 139904075134848 learning.py:507] global step 1182: loss = 2.1747 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1183: loss = 2.4567 (0.822 sec/step)\n",
            "I1209 11:12:01.872371 139904075134848 learning.py:507] global step 1183: loss = 2.4567 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 1184: loss = 2.4108 (0.821 sec/step)\n",
            "I1209 11:12:03.032275 139904075134848 learning.py:507] global step 1184: loss = 2.4108 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 1185: loss = 2.3823 (0.747 sec/step)\n",
            "I1209 11:12:03.982666 139904075134848 learning.py:507] global step 1185: loss = 2.3823 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1186: loss = 2.5389 (0.695 sec/step)\n",
            "I1209 11:12:04.828987 139904075134848 learning.py:507] global step 1186: loss = 2.5389 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1187: loss = 2.7756 (0.980 sec/step)\n",
            "I1209 11:12:06.020727 139904075134848 learning.py:507] global step 1187: loss = 2.7756 (0.980 sec/step)\n",
            "INFO:tensorflow:global step 1188: loss = 2.7965 (3.294 sec/step)\n",
            "I1209 11:12:09.643712 139904075134848 learning.py:507] global step 1188: loss = 2.7965 (3.294 sec/step)\n",
            "INFO:tensorflow:global step 1189: loss = 2.7259 (0.680 sec/step)\n",
            "I1209 11:12:10.502989 139904075134848 learning.py:507] global step 1189: loss = 2.7259 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1190: loss = 2.5203 (1.811 sec/step)\n",
            "I1209 11:12:12.554359 139904075134848 learning.py:507] global step 1190: loss = 2.5203 (1.811 sec/step)\n",
            "INFO:tensorflow:global step 1191: loss = 2.1325 (0.722 sec/step)\n",
            "I1209 11:12:13.352818 139904075134848 learning.py:507] global step 1191: loss = 2.1325 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1192: loss = 2.9050 (1.927 sec/step)\n",
            "I1209 11:12:15.521423 139904075134848 learning.py:507] global step 1192: loss = 2.9050 (1.927 sec/step)\n",
            "INFO:tensorflow:global step 1193: loss = 2.4606 (1.399 sec/step)\n",
            "I1209 11:12:16.922114 139904075134848 learning.py:507] global step 1193: loss = 2.4606 (1.399 sec/step)\n",
            "INFO:tensorflow:global step 1194: loss = 3.0131 (0.631 sec/step)\n",
            "I1209 11:12:17.554815 139904075134848 learning.py:507] global step 1194: loss = 3.0131 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1195: loss = 2.0704 (2.187 sec/step)\n",
            "I1209 11:12:19.743807 139904075134848 learning.py:507] global step 1195: loss = 2.0704 (2.187 sec/step)\n",
            "INFO:tensorflow:global step 1196: loss = 2.5589 (0.703 sec/step)\n",
            "I1209 11:12:20.449021 139904075134848 learning.py:507] global step 1196: loss = 2.5589 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 1197: loss = 2.1227 (2.020 sec/step)\n",
            "I1209 11:12:22.470368 139904075134848 learning.py:507] global step 1197: loss = 2.1227 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 1198: loss = 2.3492 (0.715 sec/step)\n",
            "I1209 11:12:23.428661 139904075134848 learning.py:507] global step 1198: loss = 2.3492 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 1199: loss = 2.7117 (0.734 sec/step)\n",
            "I1209 11:12:24.328920 139904075134848 learning.py:507] global step 1199: loss = 2.7117 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1200: loss = 2.6339 (0.633 sec/step)\n",
            "I1209 11:12:24.963570 139904075134848 learning.py:507] global step 1200: loss = 2.6339 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1201: loss = 2.3817 (1.418 sec/step)\n",
            "I1209 11:12:26.550281 139904075134848 learning.py:507] global step 1201: loss = 2.3817 (1.418 sec/step)\n",
            "INFO:tensorflow:global step 1202: loss = 2.8935 (2.636 sec/step)\n",
            "I1209 11:12:29.477247 139904075134848 learning.py:507] global step 1202: loss = 2.8935 (2.636 sec/step)\n",
            "INFO:tensorflow:global step 1203: loss = 2.4410 (0.625 sec/step)\n",
            "I1209 11:12:30.103938 139904075134848 learning.py:507] global step 1203: loss = 2.4410 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1204: loss = 2.7960 (0.695 sec/step)\n",
            "I1209 11:12:30.800936 139904075134848 learning.py:507] global step 1204: loss = 2.7960 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1205: loss = 2.3840 (2.706 sec/step)\n",
            "I1209 11:12:33.508573 139904075134848 learning.py:507] global step 1205: loss = 2.3840 (2.706 sec/step)\n",
            "INFO:tensorflow:global step 1206: loss = 2.6940 (0.846 sec/step)\n",
            "I1209 11:12:34.599132 139904075134848 learning.py:507] global step 1206: loss = 2.6940 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 1207: loss = 2.0731 (3.929 sec/step)\n",
            "I1209 11:12:38.736646 139904075134848 learning.py:507] global step 1207: loss = 2.0731 (3.929 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1207.\n",
            "I1209 11:12:39.125707 139900543997696 supervisor.py:1050] Recording summary at step 1207.\n",
            "INFO:tensorflow:global step 1208: loss = 2.1660 (0.754 sec/step)\n",
            "I1209 11:12:39.493340 139904075134848 learning.py:507] global step 1208: loss = 2.1660 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 1209: loss = 2.8284 (1.908 sec/step)\n",
            "I1209 11:12:41.403465 139904075134848 learning.py:507] global step 1209: loss = 2.8284 (1.908 sec/step)\n",
            "INFO:tensorflow:global step 1210: loss = 2.5987 (0.700 sec/step)\n",
            "I1209 11:12:42.104940 139904075134848 learning.py:507] global step 1210: loss = 2.5987 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1211: loss = 2.7181 (2.334 sec/step)\n",
            "I1209 11:12:44.636409 139904075134848 learning.py:507] global step 1211: loss = 2.7181 (2.334 sec/step)\n",
            "INFO:tensorflow:global step 1212: loss = 3.0857 (0.770 sec/step)\n",
            "I1209 11:12:45.575188 139904075134848 learning.py:507] global step 1212: loss = 3.0857 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 1213: loss = 2.5298 (0.802 sec/step)\n",
            "I1209 11:12:46.728713 139904075134848 learning.py:507] global step 1213: loss = 2.5298 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 1214: loss = 2.4981 (0.736 sec/step)\n",
            "I1209 11:12:47.788857 139904075134848 learning.py:507] global step 1214: loss = 2.4981 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 1215: loss = 2.6741 (0.883 sec/step)\n",
            "I1209 11:12:48.964801 139904075134848 learning.py:507] global step 1215: loss = 2.6741 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 1216: loss = 2.7763 (1.951 sec/step)\n",
            "I1209 11:12:51.053437 139904075134848 learning.py:507] global step 1216: loss = 2.7763 (1.951 sec/step)\n",
            "INFO:tensorflow:global step 1217: loss = 2.8104 (0.688 sec/step)\n",
            "I1209 11:12:51.743266 139904075134848 learning.py:507] global step 1217: loss = 2.8104 (0.688 sec/step)\n",
            "INFO:tensorflow:global step 1218: loss = 2.8921 (1.716 sec/step)\n",
            "I1209 11:12:53.672113 139904075134848 learning.py:507] global step 1218: loss = 2.8921 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 1219: loss = 2.4725 (0.720 sec/step)\n",
            "I1209 11:12:54.734229 139904075134848 learning.py:507] global step 1219: loss = 2.4725 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 1220: loss = 2.5140 (1.543 sec/step)\n",
            "I1209 11:12:56.489237 139904075134848 learning.py:507] global step 1220: loss = 2.5140 (1.543 sec/step)\n",
            "INFO:tensorflow:global step 1221: loss = 2.5884 (0.799 sec/step)\n",
            "I1209 11:12:57.578815 139904075134848 learning.py:507] global step 1221: loss = 2.5884 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 1222: loss = 2.1838 (0.669 sec/step)\n",
            "I1209 11:12:58.328648 139904075134848 learning.py:507] global step 1222: loss = 2.1838 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 1223: loss = 2.9218 (1.255 sec/step)\n",
            "I1209 11:12:59.780880 139904075134848 learning.py:507] global step 1223: loss = 2.9218 (1.255 sec/step)\n",
            "INFO:tensorflow:global step 1224: loss = 2.4897 (1.846 sec/step)\n",
            "I1209 11:13:01.900445 139904075134848 learning.py:507] global step 1224: loss = 2.4897 (1.846 sec/step)\n",
            "INFO:tensorflow:global step 1225: loss = 2.5183 (0.758 sec/step)\n",
            "I1209 11:13:02.895553 139904075134848 learning.py:507] global step 1225: loss = 2.5183 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1226: loss = 3.2054 (0.753 sec/step)\n",
            "I1209 11:13:04.082898 139904075134848 learning.py:507] global step 1226: loss = 3.2054 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 1227: loss = 2.5217 (2.658 sec/step)\n",
            "I1209 11:13:06.834871 139904075134848 learning.py:507] global step 1227: loss = 2.5217 (2.658 sec/step)\n",
            "INFO:tensorflow:global step 1228: loss = 2.3833 (0.659 sec/step)\n",
            "I1209 11:13:07.666396 139904075134848 learning.py:507] global step 1228: loss = 2.3833 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1229: loss = 2.9067 (0.765 sec/step)\n",
            "I1209 11:13:08.884128 139904075134848 learning.py:507] global step 1229: loss = 2.9067 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 1230: loss = 2.9352 (2.303 sec/step)\n",
            "I1209 11:13:11.374312 139904075134848 learning.py:507] global step 1230: loss = 2.9352 (2.303 sec/step)\n",
            "INFO:tensorflow:global step 1231: loss = 2.4283 (0.818 sec/step)\n",
            "I1209 11:13:12.209707 139904075134848 learning.py:507] global step 1231: loss = 2.4283 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1232: loss = 2.3350 (1.814 sec/step)\n",
            "I1209 11:13:14.025012 139904075134848 learning.py:507] global step 1232: loss = 2.3350 (1.814 sec/step)\n",
            "INFO:tensorflow:global step 1233: loss = 2.1855 (0.531 sec/step)\n",
            "I1209 11:13:14.557960 139904075134848 learning.py:507] global step 1233: loss = 2.1855 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 1234: loss = 2.4079 (1.080 sec/step)\n",
            "I1209 11:13:15.858534 139904075134848 learning.py:507] global step 1234: loss = 2.4079 (1.080 sec/step)\n",
            "INFO:tensorflow:global step 1235: loss = 2.3888 (1.787 sec/step)\n",
            "I1209 11:13:17.972986 139904075134848 learning.py:507] global step 1235: loss = 2.3888 (1.787 sec/step)\n",
            "INFO:tensorflow:global step 1236: loss = 2.6546 (0.652 sec/step)\n",
            "I1209 11:13:18.938421 139904075134848 learning.py:507] global step 1236: loss = 2.6546 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 1237: loss = 2.5388 (0.825 sec/step)\n",
            "I1209 11:13:20.271693 139904075134848 learning.py:507] global step 1237: loss = 2.5388 (0.825 sec/step)\n",
            "INFO:tensorflow:global step 1238: loss = 2.2112 (1.721 sec/step)\n",
            "I1209 11:13:22.176276 139904075134848 learning.py:507] global step 1238: loss = 2.2112 (1.721 sec/step)\n",
            "INFO:tensorflow:global step 1239: loss = 3.2126 (0.634 sec/step)\n",
            "I1209 11:13:22.811457 139904075134848 learning.py:507] global step 1239: loss = 3.2126 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 1240: loss = 2.7908 (1.733 sec/step)\n",
            "I1209 11:13:24.568183 139904075134848 learning.py:507] global step 1240: loss = 2.7908 (1.733 sec/step)\n",
            "INFO:tensorflow:global step 1241: loss = 2.6114 (0.685 sec/step)\n",
            "I1209 11:13:25.255435 139904075134848 learning.py:507] global step 1241: loss = 2.6114 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 1242: loss = 2.4260 (0.928 sec/step)\n",
            "I1209 11:13:26.471926 139904075134848 learning.py:507] global step 1242: loss = 2.4260 (0.928 sec/step)\n",
            "INFO:tensorflow:global step 1243: loss = 2.6728 (1.754 sec/step)\n",
            "I1209 11:13:28.305818 139904075134848 learning.py:507] global step 1243: loss = 2.6728 (1.754 sec/step)\n",
            "INFO:tensorflow:global step 1244: loss = 2.3947 (0.651 sec/step)\n",
            "I1209 11:13:29.129806 139904075134848 learning.py:507] global step 1244: loss = 2.3947 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 1245: loss = 1.8456 (0.729 sec/step)\n",
            "I1209 11:13:30.161502 139904075134848 learning.py:507] global step 1245: loss = 1.8456 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1246: loss = 2.2892 (2.193 sec/step)\n",
            "I1209 11:13:32.356933 139904075134848 learning.py:507] global step 1246: loss = 2.2892 (2.193 sec/step)\n",
            "INFO:tensorflow:global step 1247: loss = 1.9281 (0.682 sec/step)\n",
            "I1209 11:13:33.259170 139904075134848 learning.py:507] global step 1247: loss = 1.9281 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 1248: loss = 2.1818 (0.725 sec/step)\n",
            "I1209 11:13:34.151020 139904075134848 learning.py:507] global step 1248: loss = 2.1818 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1249: loss = 2.5754 (1.838 sec/step)\n",
            "I1209 11:13:35.991439 139904075134848 learning.py:507] global step 1249: loss = 2.5754 (1.838 sec/step)\n",
            "INFO:tensorflow:global step 1250: loss = 2.5428 (0.627 sec/step)\n",
            "I1209 11:13:36.845505 139904075134848 learning.py:507] global step 1250: loss = 2.5428 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 1251: loss = 1.8605 (0.774 sec/step)\n",
            "I1209 11:13:37.842681 139904075134848 learning.py:507] global step 1251: loss = 1.8605 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 1252: loss = 2.5892 (1.952 sec/step)\n",
            "I1209 11:13:39.796096 139904075134848 learning.py:507] global step 1252: loss = 2.5892 (1.952 sec/step)\n",
            "INFO:tensorflow:global step 1253: loss = 2.6292 (0.661 sec/step)\n",
            "I1209 11:13:40.898980 139904075134848 learning.py:507] global step 1253: loss = 2.6292 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1254: loss = 2.1719 (0.726 sec/step)\n",
            "I1209 11:13:41.644452 139904075134848 learning.py:507] global step 1254: loss = 2.1719 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 1255: loss = 2.1795 (1.714 sec/step)\n",
            "I1209 11:13:43.458431 139904075134848 learning.py:507] global step 1255: loss = 2.1795 (1.714 sec/step)\n",
            "INFO:tensorflow:global step 1256: loss = 2.6193 (0.794 sec/step)\n",
            "I1209 11:13:44.533262 139904075134848 learning.py:507] global step 1256: loss = 2.6193 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 1257: loss = 2.0616 (0.818 sec/step)\n",
            "I1209 11:13:45.736720 139904075134848 learning.py:507] global step 1257: loss = 2.0616 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1258: loss = 2.1869 (0.656 sec/step)\n",
            "I1209 11:13:46.489033 139904075134848 learning.py:507] global step 1258: loss = 2.1869 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1259: loss = 2.0391 (1.995 sec/step)\n",
            "I1209 11:13:48.677047 139904075134848 learning.py:507] global step 1259: loss = 2.0391 (1.995 sec/step)\n",
            "INFO:tensorflow:global step 1260: loss = 3.1723 (0.697 sec/step)\n",
            "I1209 11:13:49.669110 139904075134848 learning.py:507] global step 1260: loss = 3.1723 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1261: loss = 2.8816 (2.830 sec/step)\n",
            "I1209 11:13:52.675775 139904075134848 learning.py:507] global step 1261: loss = 2.8816 (2.830 sec/step)\n",
            "INFO:tensorflow:global step 1262: loss = 2.8877 (0.895 sec/step)\n",
            "I1209 11:13:53.839778 139904075134848 learning.py:507] global step 1262: loss = 2.8877 (0.895 sec/step)\n",
            "INFO:tensorflow:global step 1263: loss = 2.1908 (1.344 sec/step)\n",
            "I1209 11:13:55.189766 139904075134848 learning.py:507] global step 1263: loss = 2.1908 (1.344 sec/step)\n",
            "INFO:tensorflow:global step 1264: loss = 2.5522 (0.632 sec/step)\n",
            "I1209 11:13:55.823247 139904075134848 learning.py:507] global step 1264: loss = 2.5522 (0.632 sec/step)\n",
            "INFO:tensorflow:global step 1265: loss = 2.4621 (2.117 sec/step)\n",
            "I1209 11:13:57.942597 139904075134848 learning.py:507] global step 1265: loss = 2.4621 (2.117 sec/step)\n",
            "INFO:tensorflow:global step 1266: loss = 3.0474 (0.747 sec/step)\n",
            "I1209 11:13:58.987857 139904075134848 learning.py:507] global step 1266: loss = 3.0474 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 1267: loss = 2.7694 (2.215 sec/step)\n",
            "I1209 11:14:01.354223 139904075134848 learning.py:507] global step 1267: loss = 2.7694 (2.215 sec/step)\n",
            "INFO:tensorflow:global step 1268: loss = 2.2749 (0.910 sec/step)\n",
            "I1209 11:14:02.573990 139904075134848 learning.py:507] global step 1268: loss = 2.2749 (0.910 sec/step)\n",
            "INFO:tensorflow:global step 1269: loss = 2.9288 (0.660 sec/step)\n",
            "I1209 11:14:03.246030 139904075134848 learning.py:507] global step 1269: loss = 2.9288 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 1270: loss = 2.1586 (2.283 sec/step)\n",
            "I1209 11:14:05.530471 139904075134848 learning.py:507] global step 1270: loss = 2.1586 (2.283 sec/step)\n",
            "INFO:tensorflow:global step 1271: loss = 2.0813 (0.867 sec/step)\n",
            "I1209 11:14:06.759100 139904075134848 learning.py:507] global step 1271: loss = 2.0813 (0.867 sec/step)\n",
            "INFO:tensorflow:global step 1272: loss = 1.9887 (0.812 sec/step)\n",
            "I1209 11:14:07.903862 139904075134848 learning.py:507] global step 1272: loss = 1.9887 (0.812 sec/step)\n",
            "INFO:tensorflow:global step 1273: loss = 2.5415 (1.867 sec/step)\n",
            "I1209 11:14:09.951182 139904075134848 learning.py:507] global step 1273: loss = 2.5415 (1.867 sec/step)\n",
            "INFO:tensorflow:global step 1274: loss = 2.4825 (0.596 sec/step)\n",
            "I1209 11:14:10.548700 139904075134848 learning.py:507] global step 1274: loss = 2.4825 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1275: loss = 2.9529 (1.978 sec/step)\n",
            "I1209 11:14:12.528828 139904075134848 learning.py:507] global step 1275: loss = 2.9529 (1.978 sec/step)\n",
            "INFO:tensorflow:global step 1276: loss = 2.1999 (0.776 sec/step)\n",
            "I1209 11:14:13.582096 139904075134848 learning.py:507] global step 1276: loss = 2.1999 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1277: loss = 2.9479 (0.737 sec/step)\n",
            "I1209 11:14:14.380508 139904075134848 learning.py:507] global step 1277: loss = 2.9479 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1278: loss = 2.2447 (1.503 sec/step)\n",
            "I1209 11:14:16.050776 139904075134848 learning.py:507] global step 1278: loss = 2.2447 (1.503 sec/step)\n",
            "INFO:tensorflow:global step 1279: loss = 2.3859 (2.204 sec/step)\n",
            "I1209 11:14:18.581140 139904075134848 learning.py:507] global step 1279: loss = 2.3859 (2.204 sec/step)\n",
            "INFO:tensorflow:global step 1280: loss = 2.3070 (0.720 sec/step)\n",
            "I1209 11:14:19.629729 139904075134848 learning.py:507] global step 1280: loss = 2.3070 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 1281: loss = 2.3295 (0.700 sec/step)\n",
            "I1209 11:14:20.497278 139904075134848 learning.py:507] global step 1281: loss = 2.3295 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1282: loss = 2.3635 (2.035 sec/step)\n",
            "I1209 11:14:22.534368 139904075134848 learning.py:507] global step 1282: loss = 2.3635 (2.035 sec/step)\n",
            "INFO:tensorflow:global step 1283: loss = 2.9620 (0.869 sec/step)\n",
            "I1209 11:14:23.491788 139904075134848 learning.py:507] global step 1283: loss = 2.9620 (0.869 sec/step)\n",
            "INFO:tensorflow:global step 1284: loss = 2.0436 (0.721 sec/step)\n",
            "I1209 11:14:24.404750 139904075134848 learning.py:507] global step 1284: loss = 2.0436 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1285: loss = 2.5116 (1.026 sec/step)\n",
            "I1209 11:14:25.467581 139904075134848 learning.py:507] global step 1285: loss = 2.5116 (1.026 sec/step)\n",
            "INFO:tensorflow:global step 1286: loss = 3.0579 (2.511 sec/step)\n",
            "I1209 11:14:27.980134 139904075134848 learning.py:507] global step 1286: loss = 3.0579 (2.511 sec/step)\n",
            "INFO:tensorflow:global step 1287: loss = 2.3374 (0.611 sec/step)\n",
            "I1209 11:14:28.593468 139904075134848 learning.py:507] global step 1287: loss = 2.3374 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1288: loss = 2.9509 (1.156 sec/step)\n",
            "I1209 11:14:30.004728 139904075134848 learning.py:507] global step 1288: loss = 2.9509 (1.156 sec/step)\n",
            "INFO:tensorflow:global step 1289: loss = 2.8653 (2.263 sec/step)\n",
            "I1209 11:14:32.664198 139904075134848 learning.py:507] global step 1289: loss = 2.8653 (2.263 sec/step)\n",
            "INFO:tensorflow:global step 1290: loss = 2.8488 (0.855 sec/step)\n",
            "I1209 11:14:33.756820 139904075134848 learning.py:507] global step 1290: loss = 2.8488 (0.855 sec/step)\n",
            "INFO:tensorflow:global step 1291: loss = 2.2081 (0.683 sec/step)\n",
            "I1209 11:14:34.880007 139904075134848 learning.py:507] global step 1291: loss = 2.2081 (0.683 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 11:14:36.805212 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 1291.\n",
            "I1209 11:14:38.321228 139900543997696 supervisor.py:1050] Recording summary at step 1291.\n",
            "INFO:tensorflow:global step 1292: loss = 2.1864 (3.776 sec/step)\n",
            "I1209 11:14:38.790768 139904075134848 learning.py:507] global step 1292: loss = 2.1864 (3.776 sec/step)\n",
            "INFO:tensorflow:global step 1293: loss = 2.7466 (1.126 sec/step)\n",
            "I1209 11:14:40.284002 139904075134848 learning.py:507] global step 1293: loss = 2.7466 (1.126 sec/step)\n",
            "INFO:tensorflow:global step 1294: loss = 2.7446 (1.606 sec/step)\n",
            "I1209 11:14:42.662506 139904075134848 learning.py:507] global step 1294: loss = 2.7446 (1.606 sec/step)\n",
            "INFO:tensorflow:global step 1295: loss = 2.5473 (0.768 sec/step)\n",
            "I1209 11:14:43.619941 139904075134848 learning.py:507] global step 1295: loss = 2.5473 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 1296: loss = 2.7570 (0.559 sec/step)\n",
            "I1209 11:14:44.712557 139904075134848 learning.py:507] global step 1296: loss = 2.7570 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 1297: loss = 2.3735 (0.848 sec/step)\n",
            "I1209 11:14:45.640978 139904075134848 learning.py:507] global step 1297: loss = 2.3735 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 1298: loss = 2.7585 (2.072 sec/step)\n",
            "I1209 11:14:47.714366 139904075134848 learning.py:507] global step 1298: loss = 2.7585 (2.072 sec/step)\n",
            "INFO:tensorflow:global step 1299: loss = 2.8881 (0.708 sec/step)\n",
            "I1209 11:14:48.424749 139904075134848 learning.py:507] global step 1299: loss = 2.8881 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 1300: loss = 2.4677 (2.163 sec/step)\n",
            "I1209 11:14:50.594504 139904075134848 learning.py:507] global step 1300: loss = 2.4677 (2.163 sec/step)\n",
            "INFO:tensorflow:global step 1301: loss = 1.8800 (0.689 sec/step)\n",
            "I1209 11:14:51.697808 139904075134848 learning.py:507] global step 1301: loss = 1.8800 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1302: loss = 2.4398 (2.135 sec/step)\n",
            "I1209 11:14:53.900852 139904075134848 learning.py:507] global step 1302: loss = 2.4398 (2.135 sec/step)\n",
            "INFO:tensorflow:global step 1303: loss = 2.5613 (0.633 sec/step)\n",
            "I1209 11:14:54.751066 139904075134848 learning.py:507] global step 1303: loss = 2.5613 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 1304: loss = 2.0274 (0.814 sec/step)\n",
            "I1209 11:14:56.164564 139904075134848 learning.py:507] global step 1304: loss = 2.0274 (0.814 sec/step)\n",
            "INFO:tensorflow:global step 1305: loss = 2.6124 (0.743 sec/step)\n",
            "I1209 11:14:57.194856 139904075134848 learning.py:507] global step 1305: loss = 2.6124 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1306: loss = 2.4204 (0.835 sec/step)\n",
            "I1209 11:14:58.529626 139904075134848 learning.py:507] global step 1306: loss = 2.4204 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 1307: loss = 2.5547 (0.727 sec/step)\n",
            "I1209 11:14:59.297377 139904075134848 learning.py:507] global step 1307: loss = 2.5547 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1308: loss = 2.5015 (1.321 sec/step)\n",
            "I1209 11:15:00.665441 139904075134848 learning.py:507] global step 1308: loss = 2.5015 (1.321 sec/step)\n",
            "INFO:tensorflow:global step 1309: loss = 2.4273 (1.881 sec/step)\n",
            "I1209 11:15:02.803490 139904075134848 learning.py:507] global step 1309: loss = 2.4273 (1.881 sec/step)\n",
            "INFO:tensorflow:global step 1310: loss = 2.5079 (0.724 sec/step)\n",
            "I1209 11:15:03.529572 139904075134848 learning.py:507] global step 1310: loss = 2.5079 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1311: loss = 2.3952 (2.190 sec/step)\n",
            "I1209 11:15:05.721576 139904075134848 learning.py:507] global step 1311: loss = 2.3952 (2.190 sec/step)\n",
            "INFO:tensorflow:global step 1312: loss = 2.2577 (0.725 sec/step)\n",
            "I1209 11:15:06.558722 139904075134848 learning.py:507] global step 1312: loss = 2.2577 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1313: loss = 2.3631 (0.658 sec/step)\n",
            "I1209 11:15:07.501065 139904075134848 learning.py:507] global step 1313: loss = 2.3631 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 1314: loss = 2.4437 (1.069 sec/step)\n",
            "I1209 11:15:08.591782 139904075134848 learning.py:507] global step 1314: loss = 2.4437 (1.069 sec/step)\n",
            "INFO:tensorflow:global step 1315: loss = 2.5473 (2.447 sec/step)\n",
            "I1209 11:15:11.059010 139904075134848 learning.py:507] global step 1315: loss = 2.5473 (2.447 sec/step)\n",
            "INFO:tensorflow:global step 1316: loss = 2.5531 (0.690 sec/step)\n",
            "I1209 11:15:11.750638 139904075134848 learning.py:507] global step 1316: loss = 2.5531 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 1317: loss = 2.1715 (1.961 sec/step)\n",
            "I1209 11:15:13.713567 139904075134848 learning.py:507] global step 1317: loss = 2.1715 (1.961 sec/step)\n",
            "INFO:tensorflow:global step 1318: loss = 3.0624 (0.848 sec/step)\n",
            "I1209 11:15:14.817701 139904075134848 learning.py:507] global step 1318: loss = 3.0624 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 1319: loss = 2.4807 (1.937 sec/step)\n",
            "I1209 11:15:16.757337 139904075134848 learning.py:507] global step 1319: loss = 2.4807 (1.937 sec/step)\n",
            "INFO:tensorflow:global step 1320: loss = 2.4962 (0.694 sec/step)\n",
            "I1209 11:15:17.454080 139904075134848 learning.py:507] global step 1320: loss = 2.4962 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1321: loss = 2.3358 (1.974 sec/step)\n",
            "I1209 11:15:19.429674 139904075134848 learning.py:507] global step 1321: loss = 2.3358 (1.974 sec/step)\n",
            "INFO:tensorflow:global step 1322: loss = 2.4094 (0.695 sec/step)\n",
            "I1209 11:15:20.389764 139904075134848 learning.py:507] global step 1322: loss = 2.4094 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1323: loss = 2.4733 (2.188 sec/step)\n",
            "I1209 11:15:22.626944 139904075134848 learning.py:507] global step 1323: loss = 2.4733 (2.188 sec/step)\n",
            "INFO:tensorflow:global step 1324: loss = 2.3938 (0.649 sec/step)\n",
            "I1209 11:15:23.277214 139904075134848 learning.py:507] global step 1324: loss = 2.3938 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1325: loss = 2.2436 (0.848 sec/step)\n",
            "I1209 11:15:24.474734 139904075134848 learning.py:507] global step 1325: loss = 2.2436 (0.848 sec/step)\n",
            "INFO:tensorflow:global step 1326: loss = 2.0749 (1.917 sec/step)\n",
            "I1209 11:15:26.555964 139904075134848 learning.py:507] global step 1326: loss = 2.0749 (1.917 sec/step)\n",
            "INFO:tensorflow:global step 1327: loss = 2.7377 (0.640 sec/step)\n",
            "I1209 11:15:27.197297 139904075134848 learning.py:507] global step 1327: loss = 2.7377 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1328: loss = 2.5412 (1.873 sec/step)\n",
            "I1209 11:15:29.071677 139904075134848 learning.py:507] global step 1328: loss = 2.5412 (1.873 sec/step)\n",
            "INFO:tensorflow:global step 1329: loss = 2.8841 (0.619 sec/step)\n",
            "I1209 11:15:29.973286 139904075134848 learning.py:507] global step 1329: loss = 2.8841 (0.619 sec/step)\n",
            "INFO:tensorflow:global step 1330: loss = 2.7763 (2.277 sec/step)\n",
            "I1209 11:15:32.407633 139904075134848 learning.py:507] global step 1330: loss = 2.7763 (2.277 sec/step)\n",
            "INFO:tensorflow:global step 1331: loss = 2.1863 (0.708 sec/step)\n",
            "I1209 11:15:33.386363 139904075134848 learning.py:507] global step 1331: loss = 2.1863 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 1332: loss = 2.9269 (1.644 sec/step)\n",
            "I1209 11:15:35.257791 139904075134848 learning.py:507] global step 1332: loss = 2.9269 (1.644 sec/step)\n",
            "INFO:tensorflow:global step 1333: loss = 2.4235 (0.719 sec/step)\n",
            "I1209 11:15:36.277632 139904075134848 learning.py:507] global step 1333: loss = 2.4235 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 1334: loss = 2.4389 (0.789 sec/step)\n",
            "I1209 11:15:37.502343 139904075134848 learning.py:507] global step 1334: loss = 2.4389 (0.789 sec/step)\n",
            "INFO:tensorflow:global step 1335: loss = 2.9334 (0.716 sec/step)\n",
            "I1209 11:15:38.364771 139904075134848 learning.py:507] global step 1335: loss = 2.9334 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1336: loss = 1.9285 (1.097 sec/step)\n",
            "I1209 11:15:39.692958 139904075134848 learning.py:507] global step 1336: loss = 1.9285 (1.097 sec/step)\n",
            "INFO:tensorflow:global step 1337: loss = 2.6211 (1.891 sec/step)\n",
            "I1209 11:15:41.611024 139904075134848 learning.py:507] global step 1337: loss = 2.6211 (1.891 sec/step)\n",
            "INFO:tensorflow:global step 1338: loss = 2.3147 (0.738 sec/step)\n",
            "I1209 11:15:42.350179 139904075134848 learning.py:507] global step 1338: loss = 2.3147 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 1339: loss = 2.5538 (1.889 sec/step)\n",
            "I1209 11:15:44.240329 139904075134848 learning.py:507] global step 1339: loss = 2.5538 (1.889 sec/step)\n",
            "INFO:tensorflow:global step 1340: loss = 2.6026 (0.745 sec/step)\n",
            "I1209 11:15:45.231567 139904075134848 learning.py:507] global step 1340: loss = 2.6026 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 1341: loss = 2.1500 (0.694 sec/step)\n",
            "I1209 11:15:46.040602 139904075134848 learning.py:507] global step 1341: loss = 2.1500 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1342: loss = 2.8986 (0.993 sec/step)\n",
            "I1209 11:15:47.146446 139904075134848 learning.py:507] global step 1342: loss = 2.8986 (0.993 sec/step)\n",
            "INFO:tensorflow:global step 1343: loss = 2.0851 (2.417 sec/step)\n",
            "I1209 11:15:49.702139 139904075134848 learning.py:507] global step 1343: loss = 2.0851 (2.417 sec/step)\n",
            "INFO:tensorflow:global step 1344: loss = 2.1878 (0.648 sec/step)\n",
            "I1209 11:15:50.476644 139904075134848 learning.py:507] global step 1344: loss = 2.1878 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 1345: loss = 2.7600 (1.801 sec/step)\n",
            "I1209 11:15:52.432069 139904075134848 learning.py:507] global step 1345: loss = 2.7600 (1.801 sec/step)\n",
            "INFO:tensorflow:global step 1346: loss = 2.7325 (0.746 sec/step)\n",
            "I1209 11:15:53.205085 139904075134848 learning.py:507] global step 1346: loss = 2.7325 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1347: loss = 2.4113 (1.580 sec/step)\n",
            "I1209 11:15:55.153545 139904075134848 learning.py:507] global step 1347: loss = 2.4113 (1.580 sec/step)\n",
            "INFO:tensorflow:global step 1348: loss = 2.4476 (0.646 sec/step)\n",
            "I1209 11:15:56.152642 139904075134848 learning.py:507] global step 1348: loss = 2.4476 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1349: loss = 2.0733 (2.866 sec/step)\n",
            "I1209 11:15:59.106117 139904075134848 learning.py:507] global step 1349: loss = 2.0733 (2.866 sec/step)\n",
            "INFO:tensorflow:global step 1350: loss = 2.4244 (0.695 sec/step)\n",
            "I1209 11:16:00.126566 139904075134848 learning.py:507] global step 1350: loss = 2.4244 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1351: loss = 2.6065 (0.928 sec/step)\n",
            "I1209 11:16:01.374640 139904075134848 learning.py:507] global step 1351: loss = 2.6065 (0.928 sec/step)\n",
            "INFO:tensorflow:global step 1352: loss = 2.5568 (0.789 sec/step)\n",
            "I1209 11:16:02.552156 139904075134848 learning.py:507] global step 1352: loss = 2.5568 (0.789 sec/step)\n",
            "INFO:tensorflow:global step 1353: loss = 2.3757 (0.630 sec/step)\n",
            "I1209 11:16:03.761140 139904075134848 learning.py:507] global step 1353: loss = 2.3757 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1354: loss = 2.8769 (0.758 sec/step)\n",
            "I1209 11:16:04.973551 139904075134848 learning.py:507] global step 1354: loss = 2.8769 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 1355: loss = 2.1753 (1.507 sec/step)\n",
            "I1209 11:16:06.697585 139904075134848 learning.py:507] global step 1355: loss = 2.1753 (1.507 sec/step)\n",
            "INFO:tensorflow:global step 1356: loss = 2.3763 (0.785 sec/step)\n",
            "I1209 11:16:07.804540 139904075134848 learning.py:507] global step 1356: loss = 2.3763 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 1357: loss = 2.0841 (0.729 sec/step)\n",
            "I1209 11:16:08.604319 139904075134848 learning.py:507] global step 1357: loss = 2.0841 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1358: loss = 2.2083 (0.781 sec/step)\n",
            "I1209 11:16:09.387933 139904075134848 learning.py:507] global step 1358: loss = 2.2083 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 1359: loss = 2.7308 (3.561 sec/step)\n",
            "I1209 11:16:12.988806 139904075134848 learning.py:507] global step 1359: loss = 2.7308 (3.561 sec/step)\n",
            "INFO:tensorflow:global step 1360: loss = 2.1799 (0.887 sec/step)\n",
            "I1209 11:16:14.203801 139904075134848 learning.py:507] global step 1360: loss = 2.1799 (0.887 sec/step)\n",
            "INFO:tensorflow:global step 1361: loss = 2.2428 (1.733 sec/step)\n",
            "I1209 11:16:15.960756 139904075134848 learning.py:507] global step 1361: loss = 2.2428 (1.733 sec/step)\n",
            "INFO:tensorflow:global step 1362: loss = 2.3039 (0.753 sec/step)\n",
            "I1209 11:16:16.998298 139904075134848 learning.py:507] global step 1362: loss = 2.3039 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 1363: loss = 2.2923 (1.621 sec/step)\n",
            "I1209 11:16:18.790192 139904075134848 learning.py:507] global step 1363: loss = 2.2923 (1.621 sec/step)\n",
            "INFO:tensorflow:global step 1364: loss = 2.0909 (0.716 sec/step)\n",
            "I1209 11:16:19.508136 139904075134848 learning.py:507] global step 1364: loss = 2.0909 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1365: loss = 2.1972 (1.780 sec/step)\n",
            "I1209 11:16:21.322870 139904075134848 learning.py:507] global step 1365: loss = 2.1972 (1.780 sec/step)\n",
            "INFO:tensorflow:global step 1366: loss = 2.2437 (1.574 sec/step)\n",
            "I1209 11:16:22.898395 139904075134848 learning.py:507] global step 1366: loss = 2.2437 (1.574 sec/step)\n",
            "INFO:tensorflow:global step 1367: loss = 2.9767 (0.968 sec/step)\n",
            "I1209 11:16:24.194163 139904075134848 learning.py:507] global step 1367: loss = 2.9767 (0.968 sec/step)\n",
            "INFO:tensorflow:global step 1368: loss = 2.3650 (1.589 sec/step)\n",
            "I1209 11:16:25.785325 139904075134848 learning.py:507] global step 1368: loss = 2.3650 (1.589 sec/step)\n",
            "INFO:tensorflow:global step 1369: loss = 2.6189 (1.364 sec/step)\n",
            "I1209 11:16:27.150403 139904075134848 learning.py:507] global step 1369: loss = 2.6189 (1.364 sec/step)\n",
            "INFO:tensorflow:global step 1370: loss = 2.1658 (0.706 sec/step)\n",
            "I1209 11:16:27.858295 139904075134848 learning.py:507] global step 1370: loss = 2.1658 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1371: loss = 3.3380 (1.298 sec/step)\n",
            "I1209 11:16:29.390692 139904075134848 learning.py:507] global step 1371: loss = 3.3380 (1.298 sec/step)\n",
            "INFO:tensorflow:global step 1372: loss = 1.9648 (2.077 sec/step)\n",
            "I1209 11:16:31.674672 139904075134848 learning.py:507] global step 1372: loss = 1.9648 (2.077 sec/step)\n",
            "INFO:tensorflow:global step 1373: loss = 3.1797 (1.212 sec/step)\n",
            "I1209 11:16:32.918502 139904075134848 learning.py:507] global step 1373: loss = 3.1797 (1.212 sec/step)\n",
            "INFO:tensorflow:global step 1374: loss = 2.4287 (0.736 sec/step)\n",
            "I1209 11:16:33.664131 139904075134848 learning.py:507] global step 1374: loss = 2.4287 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 1375: loss = 4.1150 (0.954 sec/step)\n",
            "I1209 11:16:34.862428 139904075134848 learning.py:507] global step 1375: loss = 4.1150 (0.954 sec/step)\n",
            "INFO:tensorflow:global step 1376: loss = 3.6035 (1.623 sec/step)\n",
            "I1209 11:16:37.026729 139904075134848 learning.py:507] global step 1376: loss = 3.6035 (1.623 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1376.\n",
            "I1209 11:16:39.649305 139900543997696 supervisor.py:1050] Recording summary at step 1376.\n",
            "INFO:tensorflow:global step 1377: loss = 2.7160 (2.831 sec/step)\n",
            "I1209 11:16:39.858877 139904075134848 learning.py:507] global step 1377: loss = 2.7160 (2.831 sec/step)\n",
            "INFO:tensorflow:global step 1378: loss = 2.0627 (0.659 sec/step)\n",
            "I1209 11:16:40.519583 139904075134848 learning.py:507] global step 1378: loss = 2.0627 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 1379: loss = 2.1119 (2.038 sec/step)\n",
            "I1209 11:16:42.559624 139904075134848 learning.py:507] global step 1379: loss = 2.1119 (2.038 sec/step)\n",
            "INFO:tensorflow:global step 1380: loss = 2.3466 (0.784 sec/step)\n",
            "I1209 11:16:43.574811 139904075134848 learning.py:507] global step 1380: loss = 2.3466 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 1381: loss = 2.3367 (0.673 sec/step)\n",
            "I1209 11:16:44.546087 139904075134848 learning.py:507] global step 1381: loss = 2.3367 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1382: loss = 2.7798 (1.277 sec/step)\n",
            "I1209 11:16:45.889787 139904075134848 learning.py:507] global step 1382: loss = 2.7798 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 1383: loss = 2.3814 (2.378 sec/step)\n",
            "I1209 11:16:48.576915 139904075134848 learning.py:507] global step 1383: loss = 2.3814 (2.378 sec/step)\n",
            "INFO:tensorflow:global step 1384: loss = 2.2477 (0.700 sec/step)\n",
            "I1209 11:16:49.554621 139904075134848 learning.py:507] global step 1384: loss = 2.2477 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1385: loss = 2.5975 (1.980 sec/step)\n",
            "I1209 11:16:51.671238 139904075134848 learning.py:507] global step 1385: loss = 2.5975 (1.980 sec/step)\n",
            "INFO:tensorflow:global step 1386: loss = 2.1082 (0.728 sec/step)\n",
            "I1209 11:16:52.401142 139904075134848 learning.py:507] global step 1386: loss = 2.1082 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 1387: loss = 2.4001 (1.978 sec/step)\n",
            "I1209 11:16:54.381297 139904075134848 learning.py:507] global step 1387: loss = 2.4001 (1.978 sec/step)\n",
            "INFO:tensorflow:global step 1388: loss = 2.4623 (0.743 sec/step)\n",
            "I1209 11:16:55.466004 139904075134848 learning.py:507] global step 1388: loss = 2.4623 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1389: loss = 2.5541 (0.991 sec/step)\n",
            "I1209 11:16:56.495006 139904075134848 learning.py:507] global step 1389: loss = 2.5541 (0.991 sec/step)\n",
            "INFO:tensorflow:global step 1390: loss = 2.3491 (2.442 sec/step)\n",
            "I1209 11:16:58.972567 139904075134848 learning.py:507] global step 1390: loss = 2.3491 (2.442 sec/step)\n",
            "INFO:tensorflow:global step 1391: loss = 2.2094 (1.301 sec/step)\n",
            "I1209 11:17:00.275689 139904075134848 learning.py:507] global step 1391: loss = 2.2094 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 1392: loss = 2.5830 (0.689 sec/step)\n",
            "I1209 11:17:00.966061 139904075134848 learning.py:507] global step 1392: loss = 2.5830 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1393: loss = 3.0061 (1.088 sec/step)\n",
            "I1209 11:17:02.274576 139904075134848 learning.py:507] global step 1393: loss = 3.0061 (1.088 sec/step)\n",
            "INFO:tensorflow:global step 1394: loss = 2.2722 (2.956 sec/step)\n",
            "I1209 11:17:05.280685 139904075134848 learning.py:507] global step 1394: loss = 2.2722 (2.956 sec/step)\n",
            "INFO:tensorflow:global step 1395: loss = 2.0896 (0.740 sec/step)\n",
            "I1209 11:17:06.022828 139904075134848 learning.py:507] global step 1395: loss = 2.0896 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 1396: loss = 2.4114 (0.918 sec/step)\n",
            "I1209 11:17:07.199756 139904075134848 learning.py:507] global step 1396: loss = 2.4114 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 1397: loss = 2.1909 (1.858 sec/step)\n",
            "I1209 11:17:09.449119 139904075134848 learning.py:507] global step 1397: loss = 2.1909 (1.858 sec/step)\n",
            "INFO:tensorflow:global step 1398: loss = 2.7569 (0.822 sec/step)\n",
            "I1209 11:17:10.661911 139904075134848 learning.py:507] global step 1398: loss = 2.7569 (0.822 sec/step)\n",
            "INFO:tensorflow:global step 1399: loss = 2.3481 (0.767 sec/step)\n",
            "I1209 11:17:11.589662 139904075134848 learning.py:507] global step 1399: loss = 2.3481 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 1400: loss = 2.1368 (2.263 sec/step)\n",
            "I1209 11:17:13.854769 139904075134848 learning.py:507] global step 1400: loss = 2.1368 (2.263 sec/step)\n",
            "INFO:tensorflow:global step 1401: loss = 2.5795 (0.717 sec/step)\n",
            "I1209 11:17:15.004081 139904075134848 learning.py:507] global step 1401: loss = 2.5795 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 1402: loss = 2.5463 (0.815 sec/step)\n",
            "I1209 11:17:16.300093 139904075134848 learning.py:507] global step 1402: loss = 2.5463 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1403: loss = 2.0836 (0.803 sec/step)\n",
            "I1209 11:17:17.257770 139904075134848 learning.py:507] global step 1403: loss = 2.0836 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 1404: loss = 2.3670 (0.884 sec/step)\n",
            "I1209 11:17:18.373561 139904075134848 learning.py:507] global step 1404: loss = 2.3670 (0.884 sec/step)\n",
            "INFO:tensorflow:global step 1405: loss = 2.0971 (1.024 sec/step)\n",
            "I1209 11:17:19.654812 139904075134848 learning.py:507] global step 1405: loss = 2.0971 (1.024 sec/step)\n",
            "INFO:tensorflow:global step 1406: loss = 2.5725 (1.819 sec/step)\n",
            "I1209 11:17:21.512436 139904075134848 learning.py:507] global step 1406: loss = 2.5725 (1.819 sec/step)\n",
            "INFO:tensorflow:global step 1407: loss = 2.4341 (0.846 sec/step)\n",
            "I1209 11:17:22.682339 139904075134848 learning.py:507] global step 1407: loss = 2.4341 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 1408: loss = 1.9398 (0.677 sec/step)\n",
            "I1209 11:17:23.361936 139904075134848 learning.py:507] global step 1408: loss = 1.9398 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1409: loss = 2.9599 (1.120 sec/step)\n",
            "I1209 11:17:24.735747 139904075134848 learning.py:507] global step 1409: loss = 2.9599 (1.120 sec/step)\n",
            "INFO:tensorflow:global step 1410: loss = 2.2192 (1.775 sec/step)\n",
            "I1209 11:17:26.809959 139904075134848 learning.py:507] global step 1410: loss = 2.2192 (1.775 sec/step)\n",
            "INFO:tensorflow:global step 1411: loss = 2.4368 (0.825 sec/step)\n",
            "I1209 11:17:27.813916 139904075134848 learning.py:507] global step 1411: loss = 2.4368 (0.825 sec/step)\n",
            "INFO:tensorflow:global step 1412: loss = 2.1795 (0.640 sec/step)\n",
            "I1209 11:17:28.665495 139904075134848 learning.py:507] global step 1412: loss = 2.1795 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 1413: loss = 2.3840 (1.143 sec/step)\n",
            "I1209 11:17:29.971398 139904075134848 learning.py:507] global step 1413: loss = 2.3840 (1.143 sec/step)\n",
            "INFO:tensorflow:global step 1414: loss = 2.4916 (1.975 sec/step)\n",
            "I1209 11:17:32.240898 139904075134848 learning.py:507] global step 1414: loss = 2.4916 (1.975 sec/step)\n",
            "INFO:tensorflow:global step 1415: loss = 2.9312 (0.686 sec/step)\n",
            "I1209 11:17:32.928949 139904075134848 learning.py:507] global step 1415: loss = 2.9312 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1416: loss = 3.3620 (1.850 sec/step)\n",
            "I1209 11:17:34.780732 139904075134848 learning.py:507] global step 1416: loss = 3.3620 (1.850 sec/step)\n",
            "INFO:tensorflow:global step 1417: loss = 1.9694 (0.775 sec/step)\n",
            "I1209 11:17:35.790532 139904075134848 learning.py:507] global step 1417: loss = 1.9694 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 1418: loss = 2.0840 (0.584 sec/step)\n",
            "I1209 11:17:36.502838 139904075134848 learning.py:507] global step 1418: loss = 2.0840 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 1419: loss = 1.9579 (1.500 sec/step)\n",
            "I1209 11:17:38.431913 139904075134848 learning.py:507] global step 1419: loss = 1.9579 (1.500 sec/step)\n",
            "INFO:tensorflow:global step 1420: loss = 2.2687 (1.931 sec/step)\n",
            "I1209 11:17:40.645986 139904075134848 learning.py:507] global step 1420: loss = 2.2687 (1.931 sec/step)\n",
            "INFO:tensorflow:global step 1421: loss = 2.2625 (0.760 sec/step)\n",
            "I1209 11:17:41.408494 139904075134848 learning.py:507] global step 1421: loss = 2.2625 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 1422: loss = 1.9935 (2.285 sec/step)\n",
            "I1209 11:17:43.695752 139904075134848 learning.py:507] global step 1422: loss = 1.9935 (2.285 sec/step)\n",
            "INFO:tensorflow:global step 1423: loss = 2.5903 (0.790 sec/step)\n",
            "I1209 11:17:44.502500 139904075134848 learning.py:507] global step 1423: loss = 2.5903 (0.790 sec/step)\n",
            "INFO:tensorflow:global step 1424: loss = 1.7163 (2.049 sec/step)\n",
            "I1209 11:17:46.835154 139904075134848 learning.py:507] global step 1424: loss = 1.7163 (2.049 sec/step)\n",
            "INFO:tensorflow:global step 1425: loss = 2.0651 (0.734 sec/step)\n",
            "I1209 11:17:47.757957 139904075134848 learning.py:507] global step 1425: loss = 2.0651 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1426: loss = 2.3929 (0.620 sec/step)\n",
            "I1209 11:17:48.584151 139904075134848 learning.py:507] global step 1426: loss = 2.3929 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1427: loss = 2.6726 (0.647 sec/step)\n",
            "I1209 11:17:49.233276 139904075134848 learning.py:507] global step 1427: loss = 2.6726 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 1428: loss = 2.3800 (2.964 sec/step)\n",
            "I1209 11:17:52.198484 139904075134848 learning.py:507] global step 1428: loss = 2.3800 (2.964 sec/step)\n",
            "INFO:tensorflow:global step 1429: loss = 3.0902 (0.770 sec/step)\n",
            "I1209 11:17:52.970082 139904075134848 learning.py:507] global step 1429: loss = 3.0902 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 1430: loss = 2.3180 (1.024 sec/step)\n",
            "I1209 11:17:54.341212 139904075134848 learning.py:507] global step 1430: loss = 2.3180 (1.024 sec/step)\n",
            "INFO:tensorflow:global step 1431: loss = 2.2029 (1.931 sec/step)\n",
            "I1209 11:17:56.586102 139904075134848 learning.py:507] global step 1431: loss = 2.2029 (1.931 sec/step)\n",
            "INFO:tensorflow:global step 1432: loss = 3.3218 (0.696 sec/step)\n",
            "I1209 11:17:57.684743 139904075134848 learning.py:507] global step 1432: loss = 3.3218 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 1433: loss = 2.3267 (1.855 sec/step)\n",
            "I1209 11:17:59.572341 139904075134848 learning.py:507] global step 1433: loss = 2.3267 (1.855 sec/step)\n",
            "INFO:tensorflow:global step 1434: loss = 2.4393 (0.610 sec/step)\n",
            "I1209 11:18:00.184325 139904075134848 learning.py:507] global step 1434: loss = 2.4393 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1435: loss = 2.4358 (2.015 sec/step)\n",
            "I1209 11:18:02.201422 139904075134848 learning.py:507] global step 1435: loss = 2.4358 (2.015 sec/step)\n",
            "INFO:tensorflow:global step 1436: loss = 2.8158 (0.793 sec/step)\n",
            "I1209 11:18:03.173654 139904075134848 learning.py:507] global step 1436: loss = 2.8158 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 1437: loss = 2.7045 (1.654 sec/step)\n",
            "I1209 11:18:04.841842 139904075134848 learning.py:507] global step 1437: loss = 2.7045 (1.654 sec/step)\n",
            "INFO:tensorflow:global step 1438: loss = 2.6414 (0.705 sec/step)\n",
            "I1209 11:18:05.820062 139904075134848 learning.py:507] global step 1438: loss = 2.6414 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 1439: loss = 2.4616 (1.437 sec/step)\n",
            "I1209 11:18:07.422521 139904075134848 learning.py:507] global step 1439: loss = 2.4616 (1.437 sec/step)\n",
            "INFO:tensorflow:global step 1440: loss = 2.8589 (0.677 sec/step)\n",
            "I1209 11:18:08.101056 139904075134848 learning.py:507] global step 1440: loss = 2.8589 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1441: loss = 1.7287 (1.203 sec/step)\n",
            "I1209 11:18:09.333132 139904075134848 learning.py:507] global step 1441: loss = 1.7287 (1.203 sec/step)\n",
            "INFO:tensorflow:global step 1442: loss = 2.2465 (3.067 sec/step)\n",
            "I1209 11:18:12.577303 139904075134848 learning.py:507] global step 1442: loss = 2.2465 (3.067 sec/step)\n",
            "INFO:tensorflow:global step 1443: loss = 2.8728 (0.871 sec/step)\n",
            "I1209 11:18:13.706881 139904075134848 learning.py:507] global step 1443: loss = 2.8728 (0.871 sec/step)\n",
            "INFO:tensorflow:global step 1444: loss = 2.4433 (0.992 sec/step)\n",
            "I1209 11:18:14.995937 139904075134848 learning.py:507] global step 1444: loss = 2.4433 (0.992 sec/step)\n",
            "INFO:tensorflow:global step 1445: loss = 2.0193 (0.957 sec/step)\n",
            "I1209 11:18:16.371671 139904075134848 learning.py:507] global step 1445: loss = 2.0193 (0.957 sec/step)\n",
            "INFO:tensorflow:global step 1446: loss = 1.9992 (0.805 sec/step)\n",
            "I1209 11:18:17.315576 139904075134848 learning.py:507] global step 1446: loss = 1.9992 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 1447: loss = 2.8917 (1.760 sec/step)\n",
            "I1209 11:18:19.301504 139904075134848 learning.py:507] global step 1447: loss = 2.8917 (1.760 sec/step)\n",
            "INFO:tensorflow:global step 1448: loss = 2.2442 (0.731 sec/step)\n",
            "I1209 11:18:20.034487 139904075134848 learning.py:507] global step 1448: loss = 2.2442 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 1449: loss = 1.7593 (1.378 sec/step)\n",
            "I1209 11:18:21.426023 139904075134848 learning.py:507] global step 1449: loss = 1.7593 (1.378 sec/step)\n",
            "INFO:tensorflow:global step 1450: loss = 3.0330 (2.641 sec/step)\n",
            "I1209 11:18:24.315566 139904075134848 learning.py:507] global step 1450: loss = 3.0330 (2.641 sec/step)\n",
            "INFO:tensorflow:global step 1451: loss = 2.6731 (0.748 sec/step)\n",
            "I1209 11:18:25.113657 139904075134848 learning.py:507] global step 1451: loss = 2.6731 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 1452: loss = 2.7947 (1.756 sec/step)\n",
            "I1209 11:18:26.951299 139904075134848 learning.py:507] global step 1452: loss = 2.7947 (1.756 sec/step)\n",
            "INFO:tensorflow:global step 1453: loss = 2.3281 (0.672 sec/step)\n",
            "I1209 11:18:27.857075 139904075134848 learning.py:507] global step 1453: loss = 2.3281 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1454: loss = 2.3053 (0.678 sec/step)\n",
            "I1209 11:18:28.704839 139904075134848 learning.py:507] global step 1454: loss = 2.3053 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 1455: loss = 2.6586 (1.105 sec/step)\n",
            "I1209 11:18:29.967776 139904075134848 learning.py:507] global step 1455: loss = 2.6586 (1.105 sec/step)\n",
            "INFO:tensorflow:global step 1456: loss = 3.6075 (1.862 sec/step)\n",
            "I1209 11:18:32.142785 139904075134848 learning.py:507] global step 1456: loss = 3.6075 (1.862 sec/step)\n",
            "INFO:tensorflow:global step 1457: loss = 2.2576 (0.693 sec/step)\n",
            "I1209 11:18:32.885805 139904075134848 learning.py:507] global step 1457: loss = 2.2576 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 1458: loss = 2.2018 (1.542 sec/step)\n",
            "I1209 11:18:34.646466 139904075134848 learning.py:507] global step 1458: loss = 2.2018 (1.542 sec/step)\n",
            "INFO:tensorflow:global step 1459: loss = 2.7585 (0.796 sec/step)\n",
            "I1209 11:18:35.455255 139904075134848 learning.py:507] global step 1459: loss = 2.7585 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 1460: loss = 2.6563 (0.822 sec/step)\n",
            "I1209 11:18:36.605671 139904075134848 learning.py:507] global step 1460: loss = 2.6563 (0.822 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1460.\n",
            "I1209 11:18:39.866378 139900543997696 supervisor.py:1050] Recording summary at step 1460.\n",
            "INFO:tensorflow:global step 1461: loss = 2.3357 (3.243 sec/step)\n",
            "I1209 11:18:40.104423 139904075134848 learning.py:507] global step 1461: loss = 2.3357 (3.243 sec/step)\n",
            "INFO:tensorflow:global step 1462: loss = 2.1219 (0.680 sec/step)\n",
            "I1209 11:18:40.786819 139904075134848 learning.py:507] global step 1462: loss = 2.1219 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1463: loss = 3.4456 (1.978 sec/step)\n",
            "I1209 11:18:43.100652 139904075134848 learning.py:507] global step 1463: loss = 3.4456 (1.978 sec/step)\n",
            "INFO:tensorflow:global step 1464: loss = 2.7414 (0.716 sec/step)\n",
            "I1209 11:18:44.273629 139904075134848 learning.py:507] global step 1464: loss = 2.7414 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1465: loss = 2.9129 (0.697 sec/step)\n",
            "I1209 11:18:45.553228 139904075134848 learning.py:507] global step 1465: loss = 2.9129 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1466: loss = 2.0506 (0.638 sec/step)\n",
            "I1209 11:18:46.398386 139904075134848 learning.py:507] global step 1466: loss = 2.0506 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 1467: loss = 2.3063 (1.933 sec/step)\n",
            "I1209 11:18:48.333544 139904075134848 learning.py:507] global step 1467: loss = 2.3063 (1.933 sec/step)\n",
            "INFO:tensorflow:global step 1468: loss = 2.2869 (0.725 sec/step)\n",
            "I1209 11:18:49.324319 139904075134848 learning.py:507] global step 1468: loss = 2.2869 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1469: loss = 1.9250 (1.765 sec/step)\n",
            "I1209 11:18:51.236837 139904075134848 learning.py:507] global step 1469: loss = 1.9250 (1.765 sec/step)\n",
            "INFO:tensorflow:global step 1470: loss = 2.2682 (0.815 sec/step)\n",
            "I1209 11:18:52.097595 139904075134848 learning.py:507] global step 1470: loss = 2.2682 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1471: loss = 1.6967 (1.977 sec/step)\n",
            "I1209 11:18:54.196943 139904075134848 learning.py:507] global step 1471: loss = 1.6967 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 1472: loss = 3.2738 (0.656 sec/step)\n",
            "I1209 11:18:54.854730 139904075134848 learning.py:507] global step 1472: loss = 3.2738 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1473: loss = 2.3374 (1.095 sec/step)\n",
            "I1209 11:18:56.027160 139904075134848 learning.py:507] global step 1473: loss = 2.3374 (1.095 sec/step)\n",
            "INFO:tensorflow:global step 1474: loss = 2.1756 (1.976 sec/step)\n",
            "I1209 11:18:58.328819 139904075134848 learning.py:507] global step 1474: loss = 2.1756 (1.976 sec/step)\n",
            "INFO:tensorflow:global step 1475: loss = 2.6676 (0.715 sec/step)\n",
            "I1209 11:18:59.425389 139904075134848 learning.py:507] global step 1475: loss = 2.6676 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 1476: loss = 2.3624 (0.769 sec/step)\n",
            "I1209 11:19:00.507494 139904075134848 learning.py:507] global step 1476: loss = 2.3624 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 1477: loss = 3.2659 (0.698 sec/step)\n",
            "I1209 11:19:01.515776 139904075134848 learning.py:507] global step 1477: loss = 3.2659 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1478: loss = 2.6215 (2.423 sec/step)\n",
            "I1209 11:19:03.949985 139904075134848 learning.py:507] global step 1478: loss = 2.6215 (2.423 sec/step)\n",
            "INFO:tensorflow:global step 1479: loss = 2.3182 (0.722 sec/step)\n",
            "I1209 11:19:04.946446 139904075134848 learning.py:507] global step 1479: loss = 2.3182 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1480: loss = 2.2188 (0.621 sec/step)\n",
            "I1209 11:19:06.078565 139904075134848 learning.py:507] global step 1480: loss = 2.2188 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 1481: loss = 2.0476 (0.768 sec/step)\n",
            "I1209 11:19:07.214926 139904075134848 learning.py:507] global step 1481: loss = 2.0476 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 1482: loss = 2.2991 (1.352 sec/step)\n",
            "I1209 11:19:08.580559 139904075134848 learning.py:507] global step 1482: loss = 2.2991 (1.352 sec/step)\n",
            "INFO:tensorflow:global step 1483: loss = 1.7550 (0.749 sec/step)\n",
            "I1209 11:19:09.542959 139904075134848 learning.py:507] global step 1483: loss = 1.7550 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1484: loss = 2.2779 (1.993 sec/step)\n",
            "I1209 11:19:11.603720 139904075134848 learning.py:507] global step 1484: loss = 2.2779 (1.993 sec/step)\n",
            "INFO:tensorflow:global step 1485: loss = 2.5619 (0.893 sec/step)\n",
            "I1209 11:19:12.727339 139904075134848 learning.py:507] global step 1485: loss = 2.5619 (0.893 sec/step)\n",
            "INFO:tensorflow:global step 1486: loss = 2.7232 (0.681 sec/step)\n",
            "I1209 11:19:13.572787 139904075134848 learning.py:507] global step 1486: loss = 2.7232 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1487: loss = 2.4614 (1.957 sec/step)\n",
            "I1209 11:19:15.746324 139904075134848 learning.py:507] global step 1487: loss = 2.4614 (1.957 sec/step)\n",
            "INFO:tensorflow:global step 1488: loss = 2.1482 (0.649 sec/step)\n",
            "I1209 11:19:16.647815 139904075134848 learning.py:507] global step 1488: loss = 2.1482 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1489: loss = 2.7694 (1.661 sec/step)\n",
            "I1209 11:19:18.451860 139904075134848 learning.py:507] global step 1489: loss = 2.7694 (1.661 sec/step)\n",
            "INFO:tensorflow:global step 1490: loss = 2.4871 (0.639 sec/step)\n",
            "I1209 11:19:19.092916 139904075134848 learning.py:507] global step 1490: loss = 2.4871 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 1491: loss = 2.5632 (0.705 sec/step)\n",
            "I1209 11:19:19.800028 139904075134848 learning.py:507] global step 1491: loss = 2.5632 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 1492: loss = 1.8382 (0.906 sec/step)\n",
            "I1209 11:19:20.918961 139904075134848 learning.py:507] global step 1492: loss = 1.8382 (0.906 sec/step)\n",
            "INFO:tensorflow:global step 1493: loss = 2.1828 (2.676 sec/step)\n",
            "I1209 11:19:24.474181 139904075134848 learning.py:507] global step 1493: loss = 2.1828 (2.676 sec/step)\n",
            "INFO:tensorflow:global step 1494: loss = 2.5670 (0.732 sec/step)\n",
            "I1209 11:19:25.446523 139904075134848 learning.py:507] global step 1494: loss = 2.5670 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1495: loss = 2.3975 (0.746 sec/step)\n",
            "I1209 11:19:26.763573 139904075134848 learning.py:507] global step 1495: loss = 2.3975 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1496: loss = 1.8737 (1.586 sec/step)\n",
            "I1209 11:19:28.384464 139904075134848 learning.py:507] global step 1496: loss = 1.8737 (1.586 sec/step)\n",
            "INFO:tensorflow:global step 1497: loss = 2.5228 (0.714 sec/step)\n",
            "I1209 11:19:29.402882 139904075134848 learning.py:507] global step 1497: loss = 2.5228 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 1498: loss = 2.5088 (0.713 sec/step)\n",
            "I1209 11:19:30.250772 139904075134848 learning.py:507] global step 1498: loss = 2.5088 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1499: loss = 2.7396 (0.653 sec/step)\n",
            "I1209 11:19:30.905477 139904075134848 learning.py:507] global step 1499: loss = 2.7396 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1500: loss = 2.0078 (3.239 sec/step)\n",
            "I1209 11:19:34.146328 139904075134848 learning.py:507] global step 1500: loss = 2.0078 (3.239 sec/step)\n",
            "INFO:tensorflow:global step 1501: loss = 2.2610 (0.673 sec/step)\n",
            "I1209 11:19:34.821028 139904075134848 learning.py:507] global step 1501: loss = 2.2610 (0.673 sec/step)\n",
            "INFO:tensorflow:global step 1502: loss = 2.0896 (2.475 sec/step)\n",
            "I1209 11:19:37.297417 139904075134848 learning.py:507] global step 1502: loss = 2.0896 (2.475 sec/step)\n",
            "INFO:tensorflow:global step 1503: loss = 2.5749 (0.667 sec/step)\n",
            "I1209 11:19:37.966377 139904075134848 learning.py:507] global step 1503: loss = 2.5749 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 1504: loss = 2.8506 (2.034 sec/step)\n",
            "I1209 11:19:40.002404 139904075134848 learning.py:507] global step 1504: loss = 2.8506 (2.034 sec/step)\n",
            "INFO:tensorflow:global step 1505: loss = 2.4473 (0.558 sec/step)\n",
            "I1209 11:19:40.562448 139904075134848 learning.py:507] global step 1505: loss = 2.4473 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 1506: loss = 2.5226 (1.857 sec/step)\n",
            "I1209 11:19:42.420924 139904075134848 learning.py:507] global step 1506: loss = 2.5226 (1.857 sec/step)\n",
            "INFO:tensorflow:global step 1507: loss = 2.0509 (0.916 sec/step)\n",
            "I1209 11:19:43.359646 139904075134848 learning.py:507] global step 1507: loss = 2.0509 (0.916 sec/step)\n",
            "INFO:tensorflow:global step 1508: loss = 2.0345 (0.759 sec/step)\n",
            "I1209 11:19:44.664821 139904075134848 learning.py:507] global step 1508: loss = 2.0345 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 1509: loss = 2.3715 (0.650 sec/step)\n",
            "I1209 11:19:45.605081 139904075134848 learning.py:507] global step 1509: loss = 2.3715 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1510: loss = 2.8386 (1.980 sec/step)\n",
            "I1209 11:19:47.587568 139904075134848 learning.py:507] global step 1510: loss = 2.8386 (1.980 sec/step)\n",
            "INFO:tensorflow:global step 1511: loss = 2.7552 (0.776 sec/step)\n",
            "I1209 11:19:48.524501 139904075134848 learning.py:507] global step 1511: loss = 2.7552 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1512: loss = 2.8737 (2.555 sec/step)\n",
            "I1209 11:19:51.097055 139904075134848 learning.py:507] global step 1512: loss = 2.8737 (2.555 sec/step)\n",
            "INFO:tensorflow:global step 1513: loss = 2.4361 (0.565 sec/step)\n",
            "I1209 11:19:51.664309 139904075134848 learning.py:507] global step 1513: loss = 2.4361 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 1514: loss = 2.7185 (2.266 sec/step)\n",
            "I1209 11:19:53.932171 139904075134848 learning.py:507] global step 1514: loss = 2.7185 (2.266 sec/step)\n",
            "INFO:tensorflow:global step 1515: loss = 2.7425 (0.641 sec/step)\n",
            "I1209 11:19:54.574419 139904075134848 learning.py:507] global step 1515: loss = 2.7425 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1516: loss = 2.3003 (1.085 sec/step)\n",
            "I1209 11:19:55.730443 139904075134848 learning.py:507] global step 1516: loss = 2.3003 (1.085 sec/step)\n",
            "INFO:tensorflow:global step 1517: loss = 2.6000 (2.115 sec/step)\n",
            "I1209 11:19:57.948226 139904075134848 learning.py:507] global step 1517: loss = 2.6000 (2.115 sec/step)\n",
            "INFO:tensorflow:global step 1518: loss = 1.7962 (1.450 sec/step)\n",
            "I1209 11:19:59.401097 139904075134848 learning.py:507] global step 1518: loss = 1.7962 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 1519: loss = 2.1428 (0.923 sec/step)\n",
            "I1209 11:20:00.555953 139904075134848 learning.py:507] global step 1519: loss = 2.1428 (0.923 sec/step)\n",
            "INFO:tensorflow:global step 1520: loss = 2.3750 (0.819 sec/step)\n",
            "I1209 11:20:01.701336 139904075134848 learning.py:507] global step 1520: loss = 2.3750 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1521: loss = 2.5900 (1.341 sec/step)\n",
            "I1209 11:20:03.209700 139904075134848 learning.py:507] global step 1521: loss = 2.5900 (1.341 sec/step)\n",
            "INFO:tensorflow:global step 1522: loss = 2.2484 (0.798 sec/step)\n",
            "I1209 11:20:04.289409 139904075134848 learning.py:507] global step 1522: loss = 2.2484 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 1523: loss = 1.7085 (0.746 sec/step)\n",
            "I1209 11:20:05.284947 139904075134848 learning.py:507] global step 1523: loss = 1.7085 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1524: loss = 1.9724 (2.020 sec/step)\n",
            "I1209 11:20:07.551252 139904075134848 learning.py:507] global step 1524: loss = 1.9724 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 1525: loss = 2.7719 (0.818 sec/step)\n",
            "I1209 11:20:08.647174 139904075134848 learning.py:507] global step 1525: loss = 2.7719 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1526: loss = 2.9153 (0.809 sec/step)\n",
            "I1209 11:20:09.826349 139904075134848 learning.py:507] global step 1526: loss = 2.9153 (0.809 sec/step)\n",
            "INFO:tensorflow:global step 1527: loss = 2.9284 (0.846 sec/step)\n",
            "I1209 11:20:11.280256 139904075134848 learning.py:507] global step 1527: loss = 2.9284 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 1528: loss = 2.4675 (0.912 sec/step)\n",
            "I1209 11:20:12.560503 139904075134848 learning.py:507] global step 1528: loss = 2.4675 (0.912 sec/step)\n",
            "INFO:tensorflow:global step 1529: loss = 2.9375 (2.032 sec/step)\n",
            "I1209 11:20:14.647660 139904075134848 learning.py:507] global step 1529: loss = 2.9375 (2.032 sec/step)\n",
            "INFO:tensorflow:global step 1530: loss = 2.7542 (0.796 sec/step)\n",
            "I1209 11:20:15.654222 139904075134848 learning.py:507] global step 1530: loss = 2.7542 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 1531: loss = 2.6210 (0.711 sec/step)\n",
            "I1209 11:20:16.581024 139904075134848 learning.py:507] global step 1531: loss = 2.6210 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 1532: loss = 2.6858 (2.683 sec/step)\n",
            "I1209 11:20:19.265332 139904075134848 learning.py:507] global step 1532: loss = 2.6858 (2.683 sec/step)\n",
            "INFO:tensorflow:global step 1533: loss = 2.7077 (0.797 sec/step)\n",
            "I1209 11:20:20.253082 139904075134848 learning.py:507] global step 1533: loss = 2.7077 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 1534: loss = 2.7964 (0.732 sec/step)\n",
            "I1209 11:20:21.361501 139904075134848 learning.py:507] global step 1534: loss = 2.7964 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1535: loss = 2.8158 (0.697 sec/step)\n",
            "I1209 11:20:22.371559 139904075134848 learning.py:507] global step 1535: loss = 2.8158 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1536: loss = 3.0634 (2.115 sec/step)\n",
            "I1209 11:20:24.489120 139904075134848 learning.py:507] global step 1536: loss = 3.0634 (2.115 sec/step)\n",
            "INFO:tensorflow:global step 1537: loss = 2.6761 (0.653 sec/step)\n",
            "I1209 11:20:25.143396 139904075134848 learning.py:507] global step 1537: loss = 2.6761 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1538: loss = 2.6602 (2.002 sec/step)\n",
            "I1209 11:20:27.146665 139904075134848 learning.py:507] global step 1538: loss = 2.6602 (2.002 sec/step)\n",
            "INFO:tensorflow:global step 1539: loss = 2.7895 (0.596 sec/step)\n",
            "I1209 11:20:27.965337 139904075134848 learning.py:507] global step 1539: loss = 2.7895 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 1540: loss = 3.3176 (1.481 sec/step)\n",
            "I1209 11:20:29.572373 139904075134848 learning.py:507] global step 1540: loss = 3.3176 (1.481 sec/step)\n",
            "INFO:tensorflow:global step 1541: loss = 2.0489 (0.746 sec/step)\n",
            "I1209 11:20:30.630197 139904075134848 learning.py:507] global step 1541: loss = 2.0489 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1542: loss = 2.2226 (0.681 sec/step)\n",
            "I1209 11:20:31.511676 139904075134848 learning.py:507] global step 1542: loss = 2.2226 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1543: loss = 2.4514 (1.692 sec/step)\n",
            "I1209 11:20:33.205412 139904075134848 learning.py:507] global step 1543: loss = 2.4514 (1.692 sec/step)\n",
            "INFO:tensorflow:global step 1544: loss = 2.8409 (0.798 sec/step)\n",
            "I1209 11:20:34.143827 139904075134848 learning.py:507] global step 1544: loss = 2.8409 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 1545: loss = 2.2531 (0.677 sec/step)\n",
            "I1209 11:20:35.000728 139904075134848 learning.py:507] global step 1545: loss = 2.2531 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1546: loss = 1.9295 (0.630 sec/step)\n",
            "I1209 11:20:35.632585 139904075134848 learning.py:507] global step 1546: loss = 1.9295 (0.630 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1547.\n",
            "I1209 11:20:40.394802 139900543997696 supervisor.py:1050] Recording summary at step 1547.\n",
            "INFO:tensorflow:global step 1547: loss = 3.6455 (4.693 sec/step)\n",
            "I1209 11:20:40.600629 139904075134848 learning.py:507] global step 1547: loss = 3.6455 (4.693 sec/step)\n",
            "INFO:tensorflow:global step 1548: loss = 2.3739 (0.676 sec/step)\n",
            "I1209 11:20:41.282237 139904075134848 learning.py:507] global step 1548: loss = 2.3739 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1549: loss = 3.2345 (2.894 sec/step)\n",
            "I1209 11:20:44.178181 139904075134848 learning.py:507] global step 1549: loss = 3.2345 (2.894 sec/step)\n",
            "INFO:tensorflow:global step 1550: loss = 2.0012 (0.883 sec/step)\n",
            "I1209 11:20:45.100886 139904075134848 learning.py:507] global step 1550: loss = 2.0012 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 1551: loss = 2.3573 (2.720 sec/step)\n",
            "I1209 11:20:47.946691 139904075134848 learning.py:507] global step 1551: loss = 2.3573 (2.720 sec/step)\n",
            "INFO:tensorflow:global step 1552: loss = 2.2492 (0.817 sec/step)\n",
            "I1209 11:20:48.983871 139904075134848 learning.py:507] global step 1552: loss = 2.2492 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 1553: loss = 2.3882 (1.795 sec/step)\n",
            "I1209 11:20:50.869167 139904075134848 learning.py:507] global step 1553: loss = 2.3882 (1.795 sec/step)\n",
            "INFO:tensorflow:global step 1554: loss = 2.0586 (0.716 sec/step)\n",
            "I1209 11:20:51.586633 139904075134848 learning.py:507] global step 1554: loss = 2.0586 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1555: loss = 3.2093 (1.330 sec/step)\n",
            "I1209 11:20:53.145829 139904075134848 learning.py:507] global step 1555: loss = 3.2093 (1.330 sec/step)\n",
            "INFO:tensorflow:global step 1556: loss = 3.1409 (1.854 sec/step)\n",
            "I1209 11:20:55.215001 139904075134848 learning.py:507] global step 1556: loss = 3.1409 (1.854 sec/step)\n",
            "INFO:tensorflow:global step 1557: loss = 2.7443 (0.790 sec/step)\n",
            "I1209 11:20:56.146351 139904075134848 learning.py:507] global step 1557: loss = 2.7443 (0.790 sec/step)\n",
            "INFO:tensorflow:global step 1558: loss = 2.4292 (0.677 sec/step)\n",
            "I1209 11:20:57.210394 139904075134848 learning.py:507] global step 1558: loss = 2.4292 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 1559: loss = 2.2730 (1.121 sec/step)\n",
            "I1209 11:20:58.696085 139904075134848 learning.py:507] global step 1559: loss = 2.2730 (1.121 sec/step)\n",
            "INFO:tensorflow:global step 1560: loss = 2.3302 (0.702 sec/step)\n",
            "I1209 11:20:59.738099 139904075134848 learning.py:507] global step 1560: loss = 2.3302 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 1561: loss = 2.0512 (2.249 sec/step)\n",
            "I1209 11:21:02.126208 139904075134848 learning.py:507] global step 1561: loss = 2.0512 (2.249 sec/step)\n",
            "INFO:tensorflow:global step 1562: loss = 2.6582 (0.763 sec/step)\n",
            "I1209 11:21:03.167879 139904075134848 learning.py:507] global step 1562: loss = 2.6582 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 1563: loss = 2.7075 (0.646 sec/step)\n",
            "I1209 11:21:03.981518 139904075134848 learning.py:507] global step 1563: loss = 2.7075 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1564: loss = 2.7831 (1.245 sec/step)\n",
            "I1209 11:21:05.471906 139904075134848 learning.py:507] global step 1564: loss = 2.7831 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1565: loss = 2.6660 (1.719 sec/step)\n",
            "I1209 11:21:07.496820 139904075134848 learning.py:507] global step 1565: loss = 2.6660 (1.719 sec/step)\n",
            "INFO:tensorflow:global step 1566: loss = 2.3194 (0.799 sec/step)\n",
            "I1209 11:21:08.311466 139904075134848 learning.py:507] global step 1566: loss = 2.3194 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 1567: loss = 1.7633 (2.373 sec/step)\n",
            "I1209 11:21:10.686148 139904075134848 learning.py:507] global step 1567: loss = 1.7633 (2.373 sec/step)\n",
            "INFO:tensorflow:global step 1568: loss = 2.2655 (0.704 sec/step)\n",
            "I1209 11:21:11.391790 139904075134848 learning.py:507] global step 1568: loss = 2.2655 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1569: loss = 2.6267 (1.578 sec/step)\n",
            "I1209 11:21:13.072528 139904075134848 learning.py:507] global step 1569: loss = 2.6267 (1.578 sec/step)\n",
            "INFO:tensorflow:global step 1570: loss = 2.2153 (0.722 sec/step)\n",
            "I1209 11:21:13.812344 139904075134848 learning.py:507] global step 1570: loss = 2.2153 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1571: loss = 2.1530 (1.966 sec/step)\n",
            "I1209 11:21:15.780538 139904075134848 learning.py:507] global step 1571: loss = 2.1530 (1.966 sec/step)\n",
            "INFO:tensorflow:global step 1572: loss = 2.7057 (0.834 sec/step)\n",
            "I1209 11:21:16.629132 139904075134848 learning.py:507] global step 1572: loss = 2.7057 (0.834 sec/step)\n",
            "INFO:tensorflow:global step 1573: loss = 2.5604 (2.196 sec/step)\n",
            "I1209 11:21:19.088952 139904075134848 learning.py:507] global step 1573: loss = 2.5604 (2.196 sec/step)\n",
            "INFO:tensorflow:global step 1574: loss = 2.9916 (0.905 sec/step)\n",
            "I1209 11:21:20.131029 139904075134848 learning.py:507] global step 1574: loss = 2.9916 (0.905 sec/step)\n",
            "INFO:tensorflow:global step 1575: loss = 2.5007 (0.685 sec/step)\n",
            "I1209 11:21:20.987428 139904075134848 learning.py:507] global step 1575: loss = 2.5007 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 1576: loss = 1.6882 (3.275 sec/step)\n",
            "I1209 11:21:24.265116 139904075134848 learning.py:507] global step 1576: loss = 1.6882 (3.275 sec/step)\n",
            "INFO:tensorflow:global step 1577: loss = 2.0499 (0.694 sec/step)\n",
            "I1209 11:21:24.961572 139904075134848 learning.py:507] global step 1577: loss = 2.0499 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1578: loss = 2.0822 (2.060 sec/step)\n",
            "I1209 11:21:27.023426 139904075134848 learning.py:507] global step 1578: loss = 2.0822 (2.060 sec/step)\n",
            "INFO:tensorflow:global step 1579: loss = 2.5134 (0.832 sec/step)\n",
            "I1209 11:21:28.109710 139904075134848 learning.py:507] global step 1579: loss = 2.5134 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 1580: loss = 2.2637 (1.842 sec/step)\n",
            "I1209 11:21:30.078161 139904075134848 learning.py:507] global step 1580: loss = 2.2637 (1.842 sec/step)\n",
            "INFO:tensorflow:global step 1581: loss = 2.2524 (0.649 sec/step)\n",
            "I1209 11:21:30.729212 139904075134848 learning.py:507] global step 1581: loss = 2.2524 (0.649 sec/step)\n",
            "INFO:tensorflow:global step 1582: loss = 2.1461 (1.026 sec/step)\n",
            "I1209 11:21:31.904078 139904075134848 learning.py:507] global step 1582: loss = 2.1461 (1.026 sec/step)\n",
            "INFO:tensorflow:global step 1583: loss = 1.7589 (1.989 sec/step)\n",
            "I1209 11:21:34.100763 139904075134848 learning.py:507] global step 1583: loss = 1.7589 (1.989 sec/step)\n",
            "INFO:tensorflow:global step 1584: loss = 2.0093 (0.592 sec/step)\n",
            "I1209 11:21:34.694322 139904075134848 learning.py:507] global step 1584: loss = 2.0093 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 1585: loss = 2.6549 (2.043 sec/step)\n",
            "I1209 11:21:36.740597 139904075134848 learning.py:507] global step 1585: loss = 2.6549 (2.043 sec/step)\n",
            "INFO:tensorflow:global step 1586: loss = 2.1029 (0.774 sec/step)\n",
            "I1209 11:21:37.781146 139904075134848 learning.py:507] global step 1586: loss = 2.1029 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 1587: loss = 2.8366 (0.755 sec/step)\n",
            "I1209 11:21:38.945064 139904075134848 learning.py:507] global step 1587: loss = 2.8366 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 1588: loss = 2.7489 (2.590 sec/step)\n",
            "I1209 11:21:41.839142 139904075134848 learning.py:507] global step 1588: loss = 2.7489 (2.590 sec/step)\n",
            "INFO:tensorflow:global step 1589: loss = 2.2991 (0.701 sec/step)\n",
            "I1209 11:21:42.541617 139904075134848 learning.py:507] global step 1589: loss = 2.2991 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1590: loss = 1.9447 (2.673 sec/step)\n",
            "I1209 11:21:45.216721 139904075134848 learning.py:507] global step 1590: loss = 1.9447 (2.673 sec/step)\n",
            "INFO:tensorflow:global step 1591: loss = 2.9701 (0.618 sec/step)\n",
            "I1209 11:21:45.914963 139904075134848 learning.py:507] global step 1591: loss = 2.9701 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 1592: loss = 1.8927 (2.097 sec/step)\n",
            "I1209 11:21:48.287994 139904075134848 learning.py:507] global step 1592: loss = 1.8927 (2.097 sec/step)\n",
            "INFO:tensorflow:global step 1593: loss = 1.9384 (0.715 sec/step)\n",
            "I1209 11:21:49.165198 139904075134848 learning.py:507] global step 1593: loss = 1.9384 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 1594: loss = 2.2427 (1.682 sec/step)\n",
            "I1209 11:21:50.849912 139904075134848 learning.py:507] global step 1594: loss = 2.2427 (1.682 sec/step)\n",
            "INFO:tensorflow:global step 1595: loss = 1.9705 (0.811 sec/step)\n",
            "I1209 11:21:51.679091 139904075134848 learning.py:507] global step 1595: loss = 1.9705 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 1596: loss = 2.2582 (1.623 sec/step)\n",
            "I1209 11:21:53.339654 139904075134848 learning.py:507] global step 1596: loss = 2.2582 (1.623 sec/step)\n",
            "INFO:tensorflow:global step 1597: loss = 2.5356 (0.741 sec/step)\n",
            "I1209 11:21:54.412980 139904075134848 learning.py:507] global step 1597: loss = 2.5356 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 1598: loss = 2.9369 (0.823 sec/step)\n",
            "I1209 11:21:55.330619 139904075134848 learning.py:507] global step 1598: loss = 2.9369 (0.823 sec/step)\n",
            "INFO:tensorflow:global step 1599: loss = 2.8681 (1.984 sec/step)\n",
            "I1209 11:21:57.594960 139904075134848 learning.py:507] global step 1599: loss = 2.8681 (1.984 sec/step)\n",
            "INFO:tensorflow:global step 1600: loss = 2.6461 (0.771 sec/step)\n",
            "I1209 11:21:58.914793 139904075134848 learning.py:507] global step 1600: loss = 2.6461 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 1601: loss = 2.3850 (2.514 sec/step)\n",
            "I1209 11:22:01.570885 139904075134848 learning.py:507] global step 1601: loss = 2.3850 (2.514 sec/step)\n",
            "INFO:tensorflow:global step 1602: loss = 2.3334 (0.835 sec/step)\n",
            "I1209 11:22:02.615228 139904075134848 learning.py:507] global step 1602: loss = 2.3334 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 1603: loss = 2.3234 (0.876 sec/step)\n",
            "I1209 11:22:03.651459 139904075134848 learning.py:507] global step 1603: loss = 2.3234 (0.876 sec/step)\n",
            "INFO:tensorflow:global step 1604: loss = 1.9358 (1.894 sec/step)\n",
            "I1209 11:22:05.567753 139904075134848 learning.py:507] global step 1604: loss = 1.9358 (1.894 sec/step)\n",
            "INFO:tensorflow:global step 1605: loss = 2.5556 (0.722 sec/step)\n",
            "I1209 11:22:06.520694 139904075134848 learning.py:507] global step 1605: loss = 2.5556 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1606: loss = 2.5342 (0.697 sec/step)\n",
            "I1209 11:22:07.422047 139904075134848 learning.py:507] global step 1606: loss = 2.5342 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 1607: loss = 2.2833 (1.192 sec/step)\n",
            "I1209 11:22:08.719913 139904075134848 learning.py:507] global step 1607: loss = 2.2833 (1.192 sec/step)\n",
            "INFO:tensorflow:global step 1608: loss = 2.1429 (2.255 sec/step)\n",
            "I1209 11:22:11.166238 139904075134848 learning.py:507] global step 1608: loss = 2.1429 (2.255 sec/step)\n",
            "INFO:tensorflow:global step 1609: loss = 2.4418 (0.849 sec/step)\n",
            "I1209 11:22:12.143635 139904075134848 learning.py:507] global step 1609: loss = 2.4418 (0.849 sec/step)\n",
            "INFO:tensorflow:global step 1610: loss = 2.1001 (1.795 sec/step)\n",
            "I1209 11:22:14.331576 139904075134848 learning.py:507] global step 1610: loss = 2.1001 (1.795 sec/step)\n",
            "INFO:tensorflow:global step 1611: loss = 2.2997 (0.941 sec/step)\n",
            "I1209 11:22:15.525558 139904075134848 learning.py:507] global step 1611: loss = 2.2997 (0.941 sec/step)\n",
            "INFO:tensorflow:global step 1612: loss = 2.3888 (1.621 sec/step)\n",
            "I1209 11:22:17.242977 139904075134848 learning.py:507] global step 1612: loss = 2.3888 (1.621 sec/step)\n",
            "INFO:tensorflow:global step 1613: loss = 2.2811 (0.642 sec/step)\n",
            "I1209 11:22:17.886438 139904075134848 learning.py:507] global step 1613: loss = 2.2811 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1614: loss = 2.3882 (1.677 sec/step)\n",
            "I1209 11:22:19.855321 139904075134848 learning.py:507] global step 1614: loss = 2.3882 (1.677 sec/step)\n",
            "INFO:tensorflow:global step 1615: loss = 1.7088 (1.568 sec/step)\n",
            "I1209 11:22:21.648867 139904075134848 learning.py:507] global step 1615: loss = 1.7088 (1.568 sec/step)\n",
            "INFO:tensorflow:global step 1616: loss = 2.5896 (0.689 sec/step)\n",
            "I1209 11:22:22.339929 139904075134848 learning.py:507] global step 1616: loss = 2.5896 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1617: loss = 2.3210 (1.245 sec/step)\n",
            "I1209 11:22:23.664199 139904075134848 learning.py:507] global step 1617: loss = 2.3210 (1.245 sec/step)\n",
            "INFO:tensorflow:global step 1618: loss = 2.0299 (2.389 sec/step)\n",
            "I1209 11:22:26.267565 139904075134848 learning.py:507] global step 1618: loss = 2.0299 (2.389 sec/step)\n",
            "INFO:tensorflow:global step 1619: loss = 2.5156 (0.629 sec/step)\n",
            "I1209 11:22:26.898307 139904075134848 learning.py:507] global step 1619: loss = 2.5156 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 1620: loss = 2.8102 (2.395 sec/step)\n",
            "I1209 11:22:29.295361 139904075134848 learning.py:507] global step 1620: loss = 2.8102 (2.395 sec/step)\n",
            "INFO:tensorflow:global step 1621: loss = 2.3953 (0.785 sec/step)\n",
            "I1209 11:22:30.215981 139904075134848 learning.py:507] global step 1621: loss = 2.3953 (0.785 sec/step)\n",
            "INFO:tensorflow:global step 1622: loss = 2.4558 (0.721 sec/step)\n",
            "I1209 11:22:31.175099 139904075134848 learning.py:507] global step 1622: loss = 2.4558 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1623: loss = 3.0241 (0.908 sec/step)\n",
            "I1209 11:22:32.336215 139904075134848 learning.py:507] global step 1623: loss = 3.0241 (0.908 sec/step)\n",
            "INFO:tensorflow:global step 1624: loss = 3.0499 (0.951 sec/step)\n",
            "I1209 11:22:33.978599 139904075134848 learning.py:507] global step 1624: loss = 3.0499 (0.951 sec/step)\n",
            "INFO:tensorflow:global step 1625: loss = 2.4413 (2.420 sec/step)\n",
            "I1209 11:22:36.581114 139904075134848 learning.py:507] global step 1625: loss = 2.4413 (2.420 sec/step)\n",
            "INFO:tensorflow:global step 1626: loss = 1.9075 (2.397 sec/step)\n",
            "I1209 11:22:39.227468 139904075134848 learning.py:507] global step 1626: loss = 1.9075 (2.397 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1626.\n",
            "I1209 11:22:40.891044 139900543997696 supervisor.py:1050] Recording summary at step 1626.\n",
            "INFO:tensorflow:global step 1627: loss = 2.5487 (1.779 sec/step)\n",
            "I1209 11:22:41.195744 139904075134848 learning.py:507] global step 1627: loss = 2.5487 (1.779 sec/step)\n",
            "INFO:tensorflow:global step 1628: loss = 2.1900 (0.696 sec/step)\n",
            "I1209 11:22:41.893500 139904075134848 learning.py:507] global step 1628: loss = 2.1900 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 1629: loss = 2.6966 (2.206 sec/step)\n",
            "I1209 11:22:44.101377 139904075134848 learning.py:507] global step 1629: loss = 2.6966 (2.206 sec/step)\n",
            "INFO:tensorflow:global step 1630: loss = 2.6283 (0.575 sec/step)\n",
            "I1209 11:22:44.678266 139904075134848 learning.py:507] global step 1630: loss = 2.6283 (0.575 sec/step)\n",
            "INFO:tensorflow:global step 1631: loss = 2.4426 (1.526 sec/step)\n",
            "I1209 11:22:46.382236 139904075134848 learning.py:507] global step 1631: loss = 2.4426 (1.526 sec/step)\n",
            "INFO:tensorflow:global step 1632: loss = 2.0598 (1.827 sec/step)\n",
            "I1209 11:22:48.617780 139904075134848 learning.py:507] global step 1632: loss = 2.0598 (1.827 sec/step)\n",
            "INFO:tensorflow:global step 1633: loss = 2.8186 (0.739 sec/step)\n",
            "I1209 11:22:49.358401 139904075134848 learning.py:507] global step 1633: loss = 2.8186 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 1634: loss = 2.4566 (0.996 sec/step)\n",
            "I1209 11:22:50.620852 139904075134848 learning.py:507] global step 1634: loss = 2.4566 (0.996 sec/step)\n",
            "INFO:tensorflow:global step 1635: loss = 3.2279 (2.609 sec/step)\n",
            "I1209 11:22:53.458544 139904075134848 learning.py:507] global step 1635: loss = 3.2279 (2.609 sec/step)\n",
            "INFO:tensorflow:global step 1636: loss = 2.1636 (0.700 sec/step)\n",
            "I1209 11:22:54.161462 139904075134848 learning.py:507] global step 1636: loss = 2.1636 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 1637: loss = 2.0624 (1.957 sec/step)\n",
            "I1209 11:22:56.120722 139904075134848 learning.py:507] global step 1637: loss = 2.0624 (1.957 sec/step)\n",
            "INFO:tensorflow:global step 1638: loss = 3.0722 (0.729 sec/step)\n",
            "I1209 11:22:56.851434 139904075134848 learning.py:507] global step 1638: loss = 3.0722 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1639: loss = 3.0543 (1.741 sec/step)\n",
            "I1209 11:22:58.810776 139904075134848 learning.py:507] global step 1639: loss = 3.0543 (1.741 sec/step)\n",
            "INFO:tensorflow:global step 1640: loss = 3.0929 (2.026 sec/step)\n",
            "I1209 11:23:01.165107 139904075134848 learning.py:507] global step 1640: loss = 3.0929 (2.026 sec/step)\n",
            "INFO:tensorflow:global step 1641: loss = 2.9631 (0.776 sec/step)\n",
            "I1209 11:23:02.156303 139904075134848 learning.py:507] global step 1641: loss = 2.9631 (0.776 sec/step)\n",
            "INFO:tensorflow:global step 1642: loss = 2.2970 (1.816 sec/step)\n",
            "I1209 11:23:04.126006 139904075134848 learning.py:507] global step 1642: loss = 2.2970 (1.816 sec/step)\n",
            "INFO:tensorflow:global step 1643: loss = 2.3049 (0.709 sec/step)\n",
            "I1209 11:23:04.837169 139904075134848 learning.py:507] global step 1643: loss = 2.3049 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 1644: loss = 2.8824 (2.087 sec/step)\n",
            "I1209 11:23:06.926209 139904075134848 learning.py:507] global step 1644: loss = 2.8824 (2.087 sec/step)\n",
            "INFO:tensorflow:global step 1645: loss = 2.2963 (0.766 sec/step)\n",
            "I1209 11:23:07.941768 139904075134848 learning.py:507] global step 1645: loss = 2.2963 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 1646: loss = 2.5372 (0.706 sec/step)\n",
            "I1209 11:23:08.837779 139904075134848 learning.py:507] global step 1646: loss = 2.5372 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 1647: loss = 2.7176 (2.267 sec/step)\n",
            "I1209 11:23:11.106492 139904075134848 learning.py:507] global step 1647: loss = 2.7176 (2.267 sec/step)\n",
            "INFO:tensorflow:global step 1648: loss = 1.9607 (0.744 sec/step)\n",
            "I1209 11:23:11.852078 139904075134848 learning.py:507] global step 1648: loss = 1.9607 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 1649: loss = 2.5881 (1.266 sec/step)\n",
            "I1209 11:23:13.228862 139904075134848 learning.py:507] global step 1649: loss = 2.5881 (1.266 sec/step)\n",
            "INFO:tensorflow:global step 1650: loss = 2.3240 (2.167 sec/step)\n",
            "I1209 11:23:15.894477 139904075134848 learning.py:507] global step 1650: loss = 2.3240 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 1651: loss = 2.7438 (0.695 sec/step)\n",
            "I1209 11:23:16.591494 139904075134848 learning.py:507] global step 1651: loss = 2.7438 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1652: loss = 2.2897 (1.736 sec/step)\n",
            "I1209 11:23:18.552232 139904075134848 learning.py:507] global step 1652: loss = 2.2897 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 1653: loss = 2.4352 (2.137 sec/step)\n",
            "I1209 11:23:20.712486 139904075134848 learning.py:507] global step 1653: loss = 2.4352 (2.137 sec/step)\n",
            "INFO:tensorflow:global step 1654: loss = 2.3197 (0.896 sec/step)\n",
            "I1209 11:23:21.848344 139904075134848 learning.py:507] global step 1654: loss = 2.3197 (0.896 sec/step)\n",
            "INFO:tensorflow:global step 1655: loss = 2.0846 (0.695 sec/step)\n",
            "I1209 11:23:22.586998 139904075134848 learning.py:507] global step 1655: loss = 2.0846 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1656: loss = 2.6917 (1.260 sec/step)\n",
            "I1209 11:23:24.011691 139904075134848 learning.py:507] global step 1656: loss = 2.6917 (1.260 sec/step)\n",
            "INFO:tensorflow:global step 1657: loss = 2.6101 (1.971 sec/step)\n",
            "I1209 11:23:26.476746 139904075134848 learning.py:507] global step 1657: loss = 2.6101 (1.971 sec/step)\n",
            "INFO:tensorflow:global step 1658: loss = 2.6262 (0.732 sec/step)\n",
            "I1209 11:23:27.211056 139904075134848 learning.py:507] global step 1658: loss = 2.6262 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1659: loss = 2.7827 (2.109 sec/step)\n",
            "I1209 11:23:29.322053 139904075134848 learning.py:507] global step 1659: loss = 2.7827 (2.109 sec/step)\n",
            "INFO:tensorflow:global step 1660: loss = 1.7506 (0.594 sec/step)\n",
            "I1209 11:23:29.918093 139904075134848 learning.py:507] global step 1660: loss = 1.7506 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1661: loss = 2.1864 (2.151 sec/step)\n",
            "I1209 11:23:32.070965 139904075134848 learning.py:507] global step 1661: loss = 2.1864 (2.151 sec/step)\n",
            "INFO:tensorflow:global step 1662: loss = 2.1916 (1.108 sec/step)\n",
            "I1209 11:23:33.190073 139904075134848 learning.py:507] global step 1662: loss = 2.1916 (1.108 sec/step)\n",
            "INFO:tensorflow:global step 1663: loss = 3.6030 (1.844 sec/step)\n",
            "I1209 11:23:35.071563 139904075134848 learning.py:507] global step 1663: loss = 3.6030 (1.844 sec/step)\n",
            "INFO:tensorflow:global step 1664: loss = 2.1049 (0.635 sec/step)\n",
            "I1209 11:23:35.708780 139904075134848 learning.py:507] global step 1664: loss = 2.1049 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1665: loss = 2.1956 (2.552 sec/step)\n",
            "I1209 11:23:38.262447 139904075134848 learning.py:507] global step 1665: loss = 2.1956 (2.552 sec/step)\n",
            "INFO:tensorflow:global step 1666: loss = 2.4605 (0.761 sec/step)\n",
            "I1209 11:23:39.026117 139904075134848 learning.py:507] global step 1666: loss = 2.4605 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 1667: loss = 2.6507 (2.330 sec/step)\n",
            "I1209 11:23:41.357752 139904075134848 learning.py:507] global step 1667: loss = 2.6507 (2.330 sec/step)\n",
            "INFO:tensorflow:global step 1668: loss = 2.4396 (0.906 sec/step)\n",
            "I1209 11:23:42.612937 139904075134848 learning.py:507] global step 1668: loss = 2.4396 (0.906 sec/step)\n",
            "INFO:tensorflow:global step 1669: loss = 2.0510 (0.770 sec/step)\n",
            "I1209 11:23:43.667876 139904075134848 learning.py:507] global step 1669: loss = 2.0510 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 1670: loss = 1.7620 (0.846 sec/step)\n",
            "I1209 11:23:44.942882 139904075134848 learning.py:507] global step 1670: loss = 1.7620 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 1671: loss = 1.8058 (2.610 sec/step)\n",
            "I1209 11:23:47.554530 139904075134848 learning.py:507] global step 1671: loss = 1.8058 (2.610 sec/step)\n",
            "INFO:tensorflow:global step 1672: loss = 2.3699 (0.707 sec/step)\n",
            "I1209 11:23:48.263831 139904075134848 learning.py:507] global step 1672: loss = 2.3699 (0.707 sec/step)\n",
            "INFO:tensorflow:global step 1673: loss = 1.9324 (0.701 sec/step)\n",
            "I1209 11:23:49.067893 139904075134848 learning.py:507] global step 1673: loss = 1.9324 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1674: loss = 2.0092 (2.876 sec/step)\n",
            "I1209 11:23:51.945867 139904075134848 learning.py:507] global step 1674: loss = 2.0092 (2.876 sec/step)\n",
            "INFO:tensorflow:global step 1675: loss = 2.0048 (0.729 sec/step)\n",
            "I1209 11:23:52.676989 139904075134848 learning.py:507] global step 1675: loss = 2.0048 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1676: loss = 2.4541 (0.836 sec/step)\n",
            "I1209 11:23:53.916565 139904075134848 learning.py:507] global step 1676: loss = 2.4541 (0.836 sec/step)\n",
            "INFO:tensorflow:global step 1677: loss = 2.3709 (2.085 sec/step)\n",
            "I1209 11:23:56.435308 139904075134848 learning.py:507] global step 1677: loss = 2.3709 (2.085 sec/step)\n",
            "INFO:tensorflow:global step 1678: loss = 1.8999 (0.772 sec/step)\n",
            "I1209 11:23:57.252181 139904075134848 learning.py:507] global step 1678: loss = 1.8999 (0.772 sec/step)\n",
            "INFO:tensorflow:global step 1679: loss = 2.1476 (2.566 sec/step)\n",
            "I1209 11:23:59.854063 139904075134848 learning.py:507] global step 1679: loss = 2.1476 (2.566 sec/step)\n",
            "INFO:tensorflow:global step 1680: loss = 1.6074 (0.797 sec/step)\n",
            "I1209 11:24:00.738899 139904075134848 learning.py:507] global step 1680: loss = 1.6074 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 1681: loss = 2.0399 (1.830 sec/step)\n",
            "I1209 11:24:02.595934 139904075134848 learning.py:507] global step 1681: loss = 2.0399 (1.830 sec/step)\n",
            "INFO:tensorflow:global step 1682: loss = 1.8907 (0.719 sec/step)\n",
            "I1209 11:24:03.667321 139904075134848 learning.py:507] global step 1682: loss = 1.8907 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 1683: loss = 2.1095 (2.280 sec/step)\n",
            "I1209 11:24:05.957087 139904075134848 learning.py:507] global step 1683: loss = 2.1095 (2.280 sec/step)\n",
            "INFO:tensorflow:global step 1684: loss = 2.7484 (0.746 sec/step)\n",
            "I1209 11:24:07.108054 139904075134848 learning.py:507] global step 1684: loss = 2.7484 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 1685: loss = 2.0149 (2.106 sec/step)\n",
            "I1209 11:24:09.257285 139904075134848 learning.py:507] global step 1685: loss = 2.0149 (2.106 sec/step)\n",
            "INFO:tensorflow:global step 1686: loss = 2.4219 (0.705 sec/step)\n",
            "I1209 11:24:09.964343 139904075134848 learning.py:507] global step 1686: loss = 2.4219 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 1687: loss = 2.1816 (0.974 sec/step)\n",
            "I1209 11:24:11.225074 139904075134848 learning.py:507] global step 1687: loss = 2.1816 (0.974 sec/step)\n",
            "INFO:tensorflow:global step 1688: loss = 2.0115 (1.762 sec/step)\n",
            "I1209 11:24:13.136137 139904075134848 learning.py:507] global step 1688: loss = 2.0115 (1.762 sec/step)\n",
            "INFO:tensorflow:global step 1689: loss = 2.5198 (0.866 sec/step)\n",
            "I1209 11:24:14.010787 139904075134848 learning.py:507] global step 1689: loss = 2.5198 (0.866 sec/step)\n",
            "INFO:tensorflow:global step 1690: loss = 1.9050 (2.172 sec/step)\n",
            "I1209 11:24:16.184243 139904075134848 learning.py:507] global step 1690: loss = 1.9050 (2.172 sec/step)\n",
            "INFO:tensorflow:global step 1691: loss = 2.2240 (0.622 sec/step)\n",
            "I1209 11:24:16.938990 139904075134848 learning.py:507] global step 1691: loss = 2.2240 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 1692: loss = 2.2423 (2.348 sec/step)\n",
            "I1209 11:24:19.289390 139904075134848 learning.py:507] global step 1692: loss = 2.2423 (2.348 sec/step)\n",
            "INFO:tensorflow:global step 1693: loss = 2.0575 (0.584 sec/step)\n",
            "I1209 11:24:19.875449 139904075134848 learning.py:507] global step 1693: loss = 2.0575 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 1694: loss = 1.5123 (2.335 sec/step)\n",
            "I1209 11:24:22.211587 139904075134848 learning.py:507] global step 1694: loss = 1.5123 (2.335 sec/step)\n",
            "INFO:tensorflow:global step 1695: loss = 1.8659 (0.607 sec/step)\n",
            "I1209 11:24:22.820785 139904075134848 learning.py:507] global step 1695: loss = 1.8659 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 1696: loss = 2.3107 (1.813 sec/step)\n",
            "I1209 11:24:24.635844 139904075134848 learning.py:507] global step 1696: loss = 2.3107 (1.813 sec/step)\n",
            "INFO:tensorflow:global step 1697: loss = 2.0289 (0.788 sec/step)\n",
            "I1209 11:24:25.780173 139904075134848 learning.py:507] global step 1697: loss = 2.0289 (0.788 sec/step)\n",
            "INFO:tensorflow:global step 1698: loss = 2.1053 (0.679 sec/step)\n",
            "I1209 11:24:26.576464 139904075134848 learning.py:507] global step 1698: loss = 2.1053 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 1699: loss = 1.7453 (0.642 sec/step)\n",
            "I1209 11:24:27.220370 139904075134848 learning.py:507] global step 1699: loss = 1.7453 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 1700: loss = 1.9909 (1.421 sec/step)\n",
            "I1209 11:24:28.782003 139904075134848 learning.py:507] global step 1700: loss = 1.9909 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 1701: loss = 1.9136 (3.372 sec/step)\n",
            "I1209 11:24:32.445024 139904075134848 learning.py:507] global step 1701: loss = 1.9136 (3.372 sec/step)\n",
            "INFO:tensorflow:global step 1702: loss = 2.2670 (0.572 sec/step)\n",
            "I1209 11:24:33.018815 139904075134848 learning.py:507] global step 1702: loss = 2.2670 (0.572 sec/step)\n",
            "INFO:tensorflow:global step 1703: loss = 2.1717 (1.945 sec/step)\n",
            "I1209 11:24:34.965708 139904075134848 learning.py:507] global step 1703: loss = 2.1717 (1.945 sec/step)\n",
            "INFO:tensorflow:global step 1704: loss = 2.3964 (0.544 sec/step)\n",
            "I1209 11:24:35.511795 139904075134848 learning.py:507] global step 1704: loss = 2.3964 (0.544 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 11:24:36.818641 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:Recording summary at step 1704.\n",
            "I1209 11:24:38.847399 139900543997696 supervisor.py:1050] Recording summary at step 1704.\n",
            "INFO:tensorflow:global step 1705: loss = 2.3870 (3.858 sec/step)\n",
            "I1209 11:24:39.690920 139904075134848 learning.py:507] global step 1705: loss = 2.3870 (3.858 sec/step)\n",
            "INFO:tensorflow:global step 1706: loss = 1.7677 (0.906 sec/step)\n",
            "I1209 11:24:40.662362 139904075134848 learning.py:507] global step 1706: loss = 1.7677 (0.906 sec/step)\n",
            "INFO:tensorflow:global step 1707: loss = 2.0131 (0.853 sec/step)\n",
            "I1209 11:24:42.397036 139904075134848 learning.py:507] global step 1707: loss = 2.0131 (0.853 sec/step)\n",
            "INFO:tensorflow:global step 1708: loss = 3.0385 (2.222 sec/step)\n",
            "I1209 11:24:45.083849 139904075134848 learning.py:507] global step 1708: loss = 3.0385 (2.222 sec/step)\n",
            "INFO:tensorflow:global step 1709: loss = 2.6749 (0.774 sec/step)\n",
            "I1209 11:24:45.859538 139904075134848 learning.py:507] global step 1709: loss = 2.6749 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 1710: loss = 2.1995 (2.224 sec/step)\n",
            "I1209 11:24:48.101547 139904075134848 learning.py:507] global step 1710: loss = 2.1995 (2.224 sec/step)\n",
            "INFO:tensorflow:global step 1711: loss = 2.9519 (0.829 sec/step)\n",
            "I1209 11:24:48.945783 139904075134848 learning.py:507] global step 1711: loss = 2.9519 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1712: loss = 3.0101 (1.887 sec/step)\n",
            "I1209 11:24:51.135786 139904075134848 learning.py:507] global step 1712: loss = 3.0101 (1.887 sec/step)\n",
            "INFO:tensorflow:global step 1713: loss = 2.2189 (0.763 sec/step)\n",
            "I1209 11:24:52.210155 139904075134848 learning.py:507] global step 1713: loss = 2.2189 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 1714: loss = 2.2564 (1.719 sec/step)\n",
            "I1209 11:24:54.135914 139904075134848 learning.py:507] global step 1714: loss = 2.2564 (1.719 sec/step)\n",
            "INFO:tensorflow:global step 1715: loss = 2.1098 (0.646 sec/step)\n",
            "I1209 11:24:54.972850 139904075134848 learning.py:507] global step 1715: loss = 2.1098 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1716: loss = 1.8817 (0.737 sec/step)\n",
            "I1209 11:24:56.073535 139904075134848 learning.py:507] global step 1716: loss = 1.8817 (0.737 sec/step)\n",
            "INFO:tensorflow:global step 1717: loss = 2.0069 (0.863 sec/step)\n",
            "I1209 11:24:57.323215 139904075134848 learning.py:507] global step 1717: loss = 2.0069 (0.863 sec/step)\n",
            "INFO:tensorflow:global step 1718: loss = 2.2033 (2.261 sec/step)\n",
            "I1209 11:24:59.741287 139904075134848 learning.py:507] global step 1718: loss = 2.2033 (2.261 sec/step)\n",
            "INFO:tensorflow:global step 1719: loss = 2.0246 (0.689 sec/step)\n",
            "I1209 11:25:00.432290 139904075134848 learning.py:507] global step 1719: loss = 2.0246 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 1720: loss = 2.5916 (2.026 sec/step)\n",
            "I1209 11:25:02.459914 139904075134848 learning.py:507] global step 1720: loss = 2.5916 (2.026 sec/step)\n",
            "INFO:tensorflow:global step 1721: loss = 3.1083 (0.699 sec/step)\n",
            "I1209 11:25:03.160590 139904075134848 learning.py:507] global step 1721: loss = 3.1083 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 1722: loss = 1.6918 (2.031 sec/step)\n",
            "I1209 11:25:05.193850 139904075134848 learning.py:507] global step 1722: loss = 1.6918 (2.031 sec/step)\n",
            "INFO:tensorflow:global step 1723: loss = 2.3068 (0.853 sec/step)\n",
            "I1209 11:25:06.167085 139904075134848 learning.py:507] global step 1723: loss = 2.3068 (0.853 sec/step)\n",
            "INFO:tensorflow:global step 1724: loss = 1.9053 (2.363 sec/step)\n",
            "I1209 11:25:08.786526 139904075134848 learning.py:507] global step 1724: loss = 1.9053 (2.363 sec/step)\n",
            "INFO:tensorflow:global step 1725: loss = 1.8638 (0.635 sec/step)\n",
            "I1209 11:25:09.423544 139904075134848 learning.py:507] global step 1725: loss = 1.8638 (0.635 sec/step)\n",
            "INFO:tensorflow:global step 1726: loss = 1.7291 (1.738 sec/step)\n",
            "I1209 11:25:11.187300 139904075134848 learning.py:507] global step 1726: loss = 1.7291 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 1727: loss = 2.7003 (0.751 sec/step)\n",
            "I1209 11:25:12.128365 139904075134848 learning.py:507] global step 1727: loss = 2.7003 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 1728: loss = 2.4698 (2.250 sec/step)\n",
            "I1209 11:25:14.383391 139904075134848 learning.py:507] global step 1728: loss = 2.4698 (2.250 sec/step)\n",
            "INFO:tensorflow:global step 1729: loss = 2.2740 (0.839 sec/step)\n",
            "I1209 11:25:15.224695 139904075134848 learning.py:507] global step 1729: loss = 2.2740 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 1730: loss = 2.3339 (0.874 sec/step)\n",
            "I1209 11:25:16.430722 139904075134848 learning.py:507] global step 1730: loss = 2.3339 (0.874 sec/step)\n",
            "INFO:tensorflow:global step 1731: loss = 2.4071 (1.895 sec/step)\n",
            "I1209 11:25:18.497447 139904075134848 learning.py:507] global step 1731: loss = 2.4071 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 1732: loss = 1.9556 (0.692 sec/step)\n",
            "I1209 11:25:19.544074 139904075134848 learning.py:507] global step 1732: loss = 1.9556 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1733: loss = 1.9098 (0.701 sec/step)\n",
            "I1209 11:25:20.469915 139904075134848 learning.py:507] global step 1733: loss = 1.9098 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1734: loss = 2.4194 (1.736 sec/step)\n",
            "I1209 11:25:22.247658 139904075134848 learning.py:507] global step 1734: loss = 2.4194 (1.736 sec/step)\n",
            "INFO:tensorflow:global step 1735: loss = 2.7861 (0.749 sec/step)\n",
            "I1209 11:25:23.108073 139904075134848 learning.py:507] global step 1735: loss = 2.7861 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 1736: loss = 2.3204 (0.767 sec/step)\n",
            "I1209 11:25:24.131861 139904075134848 learning.py:507] global step 1736: loss = 2.3204 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 1737: loss = 2.4472 (1.961 sec/step)\n",
            "I1209 11:25:26.094420 139904075134848 learning.py:507] global step 1737: loss = 2.4472 (1.961 sec/step)\n",
            "INFO:tensorflow:global step 1738: loss = 2.2287 (0.745 sec/step)\n",
            "I1209 11:25:26.872573 139904075134848 learning.py:507] global step 1738: loss = 2.2287 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 1739: loss = 2.3509 (2.355 sec/step)\n",
            "I1209 11:25:29.379453 139904075134848 learning.py:507] global step 1739: loss = 2.3509 (2.355 sec/step)\n",
            "INFO:tensorflow:global step 1740: loss = 2.1033 (0.818 sec/step)\n",
            "I1209 11:25:30.483745 139904075134848 learning.py:507] global step 1740: loss = 2.1033 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1741: loss = 1.8868 (0.795 sec/step)\n",
            "I1209 11:25:31.350343 139904075134848 learning.py:507] global step 1741: loss = 1.8868 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 1742: loss = 2.0421 (2.211 sec/step)\n",
            "I1209 11:25:33.563866 139904075134848 learning.py:507] global step 1742: loss = 2.0421 (2.211 sec/step)\n",
            "INFO:tensorflow:global step 1743: loss = 1.8785 (0.843 sec/step)\n",
            "I1209 11:25:34.425277 139904075134848 learning.py:507] global step 1743: loss = 1.8785 (0.843 sec/step)\n",
            "INFO:tensorflow:global step 1744: loss = 2.0888 (1.559 sec/step)\n",
            "I1209 11:25:36.180312 139904075134848 learning.py:507] global step 1744: loss = 2.0888 (1.559 sec/step)\n",
            "INFO:tensorflow:global step 1745: loss = 2.2998 (0.626 sec/step)\n",
            "I1209 11:25:37.052786 139904075134848 learning.py:507] global step 1745: loss = 2.2998 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 1746: loss = 2.5801 (0.781 sec/step)\n",
            "I1209 11:25:38.340334 139904075134848 learning.py:507] global step 1746: loss = 2.5801 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 1747: loss = 2.2480 (0.625 sec/step)\n",
            "I1209 11:25:39.140403 139904075134848 learning.py:507] global step 1747: loss = 2.2480 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1748: loss = 2.6870 (0.637 sec/step)\n",
            "I1209 11:25:40.223127 139904075134848 learning.py:507] global step 1748: loss = 2.6870 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 1749: loss = 2.4352 (2.587 sec/step)\n",
            "I1209 11:25:43.064507 139904075134848 learning.py:507] global step 1749: loss = 2.4352 (2.587 sec/step)\n",
            "INFO:tensorflow:global step 1750: loss = 2.5322 (0.743 sec/step)\n",
            "I1209 11:25:44.128406 139904075134848 learning.py:507] global step 1750: loss = 2.5322 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1751: loss = 2.6228 (0.724 sec/step)\n",
            "I1209 11:25:45.422691 139904075134848 learning.py:507] global step 1751: loss = 2.6228 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1752: loss = 2.4135 (0.725 sec/step)\n",
            "I1209 11:25:46.452042 139904075134848 learning.py:507] global step 1752: loss = 2.4135 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1753: loss = 2.3845 (2.073 sec/step)\n",
            "I1209 11:25:48.526402 139904075134848 learning.py:507] global step 1753: loss = 2.3845 (2.073 sec/step)\n",
            "INFO:tensorflow:global step 1754: loss = 2.5350 (0.753 sec/step)\n",
            "I1209 11:25:49.338744 139904075134848 learning.py:507] global step 1754: loss = 2.5350 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 1755: loss = 1.9093 (1.927 sec/step)\n",
            "I1209 11:25:51.339433 139904075134848 learning.py:507] global step 1755: loss = 1.9093 (1.927 sec/step)\n",
            "INFO:tensorflow:global step 1756: loss = 3.1237 (0.716 sec/step)\n",
            "I1209 11:25:52.426806 139904075134848 learning.py:507] global step 1756: loss = 3.1237 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1757: loss = 2.7890 (0.712 sec/step)\n",
            "I1209 11:25:53.278817 139904075134848 learning.py:507] global step 1757: loss = 2.7890 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 1758: loss = 2.6797 (1.024 sec/step)\n",
            "I1209 11:25:54.320768 139904075134848 learning.py:507] global step 1758: loss = 2.6797 (1.024 sec/step)\n",
            "INFO:tensorflow:global step 1759: loss = 1.9532 (2.911 sec/step)\n",
            "I1209 11:25:57.279796 139904075134848 learning.py:507] global step 1759: loss = 1.9532 (2.911 sec/step)\n",
            "INFO:tensorflow:global step 1760: loss = 2.6843 (0.813 sec/step)\n",
            "I1209 11:25:58.222652 139904075134848 learning.py:507] global step 1760: loss = 2.6843 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 1761: loss = 2.6652 (1.790 sec/step)\n",
            "I1209 11:26:00.326514 139904075134848 learning.py:507] global step 1761: loss = 2.6652 (1.790 sec/step)\n",
            "INFO:tensorflow:global step 1762: loss = 2.0701 (0.861 sec/step)\n",
            "I1209 11:26:01.222719 139904075134848 learning.py:507] global step 1762: loss = 2.0701 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 1763: loss = 2.3810 (1.527 sec/step)\n",
            "I1209 11:26:02.908037 139904075134848 learning.py:507] global step 1763: loss = 2.3810 (1.527 sec/step)\n",
            "INFO:tensorflow:global step 1764: loss = 2.0922 (0.841 sec/step)\n",
            "I1209 11:26:03.941020 139904075134848 learning.py:507] global step 1764: loss = 2.0922 (0.841 sec/step)\n",
            "INFO:tensorflow:global step 1765: loss = 2.2302 (0.641 sec/step)\n",
            "I1209 11:26:04.766865 139904075134848 learning.py:507] global step 1765: loss = 2.2302 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 1766: loss = 2.3408 (1.048 sec/step)\n",
            "I1209 11:26:06.048288 139904075134848 learning.py:507] global step 1766: loss = 2.3408 (1.048 sec/step)\n",
            "INFO:tensorflow:global step 1767: loss = 2.5951 (2.611 sec/step)\n",
            "I1209 11:26:08.796777 139904075134848 learning.py:507] global step 1767: loss = 2.5951 (2.611 sec/step)\n",
            "INFO:tensorflow:global step 1768: loss = 2.7081 (0.670 sec/step)\n",
            "I1209 11:26:09.687259 139904075134848 learning.py:507] global step 1768: loss = 2.7081 (0.670 sec/step)\n",
            "INFO:tensorflow:global step 1769: loss = 2.4828 (0.656 sec/step)\n",
            "I1209 11:26:10.684228 139904075134848 learning.py:507] global step 1769: loss = 2.4828 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 1770: loss = 2.1579 (1.087 sec/step)\n",
            "I1209 11:26:12.036767 139904075134848 learning.py:507] global step 1770: loss = 2.1579 (1.087 sec/step)\n",
            "INFO:tensorflow:global step 1771: loss = 2.3731 (0.620 sec/step)\n",
            "I1209 11:26:13.244562 139904075134848 learning.py:507] global step 1771: loss = 2.3731 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 1772: loss = 1.9579 (0.999 sec/step)\n",
            "I1209 11:26:14.509802 139904075134848 learning.py:507] global step 1772: loss = 1.9579 (0.999 sec/step)\n",
            "INFO:tensorflow:global step 1773: loss = 1.8215 (1.910 sec/step)\n",
            "I1209 11:26:16.505700 139904075134848 learning.py:507] global step 1773: loss = 1.8215 (1.910 sec/step)\n",
            "INFO:tensorflow:global step 1774: loss = 2.5690 (0.727 sec/step)\n",
            "I1209 11:26:17.234646 139904075134848 learning.py:507] global step 1774: loss = 2.5690 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1775: loss = 1.8752 (2.214 sec/step)\n",
            "I1209 11:26:19.450424 139904075134848 learning.py:507] global step 1775: loss = 1.8752 (2.214 sec/step)\n",
            "INFO:tensorflow:global step 1776: loss = 2.5213 (0.681 sec/step)\n",
            "I1209 11:26:20.347062 139904075134848 learning.py:507] global step 1776: loss = 2.5213 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 1777: loss = 2.2970 (0.698 sec/step)\n",
            "I1209 11:26:21.287090 139904075134848 learning.py:507] global step 1777: loss = 2.2970 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1778: loss = 1.7844 (1.929 sec/step)\n",
            "I1209 11:26:23.415620 139904075134848 learning.py:507] global step 1778: loss = 1.7844 (1.929 sec/step)\n",
            "INFO:tensorflow:global step 1779: loss = 2.1468 (2.020 sec/step)\n",
            "I1209 11:26:25.824223 139904075134848 learning.py:507] global step 1779: loss = 2.1468 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 1780: loss = 2.3461 (0.643 sec/step)\n",
            "I1209 11:26:26.469269 139904075134848 learning.py:507] global step 1780: loss = 2.3461 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1781: loss = 2.8303 (2.029 sec/step)\n",
            "I1209 11:26:28.500506 139904075134848 learning.py:507] global step 1781: loss = 2.8303 (2.029 sec/step)\n",
            "INFO:tensorflow:global step 1782: loss = 1.9988 (0.743 sec/step)\n",
            "I1209 11:26:29.538184 139904075134848 learning.py:507] global step 1782: loss = 1.9988 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 1783: loss = 1.8711 (0.630 sec/step)\n",
            "I1209 11:26:30.557955 139904075134848 learning.py:507] global step 1783: loss = 1.8711 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 1784: loss = 2.4140 (0.820 sec/step)\n",
            "I1209 11:26:31.948028 139904075134848 learning.py:507] global step 1784: loss = 2.4140 (0.820 sec/step)\n",
            "INFO:tensorflow:global step 1785: loss = 3.0200 (0.643 sec/step)\n",
            "I1209 11:26:33.024121 139904075134848 learning.py:507] global step 1785: loss = 3.0200 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 1786: loss = 2.5633 (2.050 sec/step)\n",
            "I1209 11:26:35.210658 139904075134848 learning.py:507] global step 1786: loss = 2.5633 (2.050 sec/step)\n",
            "INFO:tensorflow:global step 1787: loss = 2.6388 (0.818 sec/step)\n",
            "I1209 11:26:36.030201 139904075134848 learning.py:507] global step 1787: loss = 2.6388 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 1788: loss = 2.3474 (3.171 sec/step)\n",
            "I1209 11:26:39.240875 139904075134848 learning.py:507] global step 1788: loss = 2.3474 (3.171 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1788.\n",
            "I1209 11:26:39.245354 139900543997696 supervisor.py:1050] Recording summary at step 1788.\n",
            "INFO:tensorflow:global step 1789: loss = 1.5881 (0.767 sec/step)\n",
            "I1209 11:26:40.282144 139904075134848 learning.py:507] global step 1789: loss = 1.5881 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 1790: loss = 1.8003 (3.209 sec/step)\n",
            "I1209 11:26:43.492671 139904075134848 learning.py:507] global step 1790: loss = 1.8003 (3.209 sec/step)\n",
            "INFO:tensorflow:global step 1791: loss = 1.9195 (0.599 sec/step)\n",
            "I1209 11:26:44.093222 139904075134848 learning.py:507] global step 1791: loss = 1.9195 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1792: loss = 2.7642 (1.663 sec/step)\n",
            "I1209 11:26:45.934421 139904075134848 learning.py:507] global step 1792: loss = 2.7642 (1.663 sec/step)\n",
            "INFO:tensorflow:global step 1793: loss = 2.2282 (0.762 sec/step)\n",
            "I1209 11:26:46.777015 139904075134848 learning.py:507] global step 1793: loss = 2.2282 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 1794: loss = 2.2773 (3.076 sec/step)\n",
            "I1209 11:26:50.176723 139904075134848 learning.py:507] global step 1794: loss = 2.2773 (3.076 sec/step)\n",
            "INFO:tensorflow:global step 1795: loss = 2.0259 (0.646 sec/step)\n",
            "I1209 11:26:51.200977 139904075134848 learning.py:507] global step 1795: loss = 2.0259 (0.646 sec/step)\n",
            "INFO:tensorflow:global step 1796: loss = 2.4639 (2.139 sec/step)\n",
            "I1209 11:26:53.503639 139904075134848 learning.py:507] global step 1796: loss = 2.4639 (2.139 sec/step)\n",
            "INFO:tensorflow:global step 1797: loss = 1.8889 (0.661 sec/step)\n",
            "I1209 11:26:54.166666 139904075134848 learning.py:507] global step 1797: loss = 1.8889 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1798: loss = 1.8859 (1.024 sec/step)\n",
            "I1209 11:26:55.368518 139904075134848 learning.py:507] global step 1798: loss = 1.8859 (1.024 sec/step)\n",
            "INFO:tensorflow:global step 1799: loss = 2.2679 (2.040 sec/step)\n",
            "I1209 11:26:57.562457 139904075134848 learning.py:507] global step 1799: loss = 2.2679 (2.040 sec/step)\n",
            "INFO:tensorflow:global step 1800: loss = 2.4242 (0.701 sec/step)\n",
            "I1209 11:26:58.435550 139904075134848 learning.py:507] global step 1800: loss = 2.4242 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 1801: loss = 2.0198 (2.157 sec/step)\n",
            "I1209 11:27:00.798322 139904075134848 learning.py:507] global step 1801: loss = 2.0198 (2.157 sec/step)\n",
            "INFO:tensorflow:global step 1802: loss = 2.4255 (0.999 sec/step)\n",
            "I1209 11:27:01.853214 139904075134848 learning.py:507] global step 1802: loss = 2.4255 (0.999 sec/step)\n",
            "INFO:tensorflow:global step 1803: loss = 2.8233 (1.902 sec/step)\n",
            "I1209 11:27:03.809710 139904075134848 learning.py:507] global step 1803: loss = 2.8233 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 1804: loss = 2.0375 (0.832 sec/step)\n",
            "I1209 11:27:04.821274 139904075134848 learning.py:507] global step 1804: loss = 2.0375 (0.832 sec/step)\n",
            "INFO:tensorflow:global step 1805: loss = 2.2349 (0.730 sec/step)\n",
            "I1209 11:27:06.021125 139904075134848 learning.py:507] global step 1805: loss = 2.2349 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 1806: loss = 2.7074 (2.138 sec/step)\n",
            "I1209 11:27:08.368443 139904075134848 learning.py:507] global step 1806: loss = 2.7074 (2.138 sec/step)\n",
            "INFO:tensorflow:global step 1807: loss = 2.6041 (0.610 sec/step)\n",
            "I1209 11:27:08.981220 139904075134848 learning.py:507] global step 1807: loss = 2.6041 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 1808: loss = 2.0576 (2.046 sec/step)\n",
            "I1209 11:27:11.029435 139904075134848 learning.py:507] global step 1808: loss = 2.0576 (2.046 sec/step)\n",
            "INFO:tensorflow:global step 1809: loss = 2.7337 (0.695 sec/step)\n",
            "I1209 11:27:11.726355 139904075134848 learning.py:507] global step 1809: loss = 2.7337 (0.695 sec/step)\n",
            "INFO:tensorflow:global step 1810: loss = 2.1627 (1.253 sec/step)\n",
            "I1209 11:27:13.070104 139904075134848 learning.py:507] global step 1810: loss = 2.1627 (1.253 sec/step)\n",
            "INFO:tensorflow:global step 1811: loss = 2.0858 (2.150 sec/step)\n",
            "I1209 11:27:15.369692 139904075134848 learning.py:507] global step 1811: loss = 2.0858 (2.150 sec/step)\n",
            "INFO:tensorflow:global step 1812: loss = 2.1845 (0.704 sec/step)\n",
            "I1209 11:27:16.075070 139904075134848 learning.py:507] global step 1812: loss = 2.1845 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 1813: loss = 2.1726 (1.537 sec/step)\n",
            "I1209 11:27:17.765666 139904075134848 learning.py:507] global step 1813: loss = 2.1726 (1.537 sec/step)\n",
            "INFO:tensorflow:global step 1814: loss = 2.7820 (2.036 sec/step)\n",
            "I1209 11:27:20.030496 139904075134848 learning.py:507] global step 1814: loss = 2.7820 (2.036 sec/step)\n",
            "INFO:tensorflow:global step 1815: loss = 1.6529 (0.761 sec/step)\n",
            "I1209 11:27:20.800634 139904075134848 learning.py:507] global step 1815: loss = 1.6529 (0.761 sec/step)\n",
            "INFO:tensorflow:global step 1816: loss = 2.6532 (1.848 sec/step)\n",
            "I1209 11:27:22.680986 139904075134848 learning.py:507] global step 1816: loss = 2.6532 (1.848 sec/step)\n",
            "INFO:tensorflow:global step 1817: loss = 2.8230 (0.650 sec/step)\n",
            "I1209 11:27:23.653090 139904075134848 learning.py:507] global step 1817: loss = 2.8230 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 1818: loss = 2.2100 (0.863 sec/step)\n",
            "I1209 11:27:24.938436 139904075134848 learning.py:507] global step 1818: loss = 2.2100 (0.863 sec/step)\n",
            "INFO:tensorflow:global step 1819: loss = 2.2397 (2.087 sec/step)\n",
            "I1209 11:27:27.044685 139904075134848 learning.py:507] global step 1819: loss = 2.2397 (2.087 sec/step)\n",
            "INFO:tensorflow:global step 1820: loss = 1.8848 (0.907 sec/step)\n",
            "I1209 11:27:28.255584 139904075134848 learning.py:507] global step 1820: loss = 1.8848 (0.907 sec/step)\n",
            "INFO:tensorflow:global step 1821: loss = 1.7564 (2.928 sec/step)\n",
            "I1209 11:27:31.248753 139904075134848 learning.py:507] global step 1821: loss = 1.7564 (2.928 sec/step)\n",
            "INFO:tensorflow:global step 1822: loss = 2.2333 (0.644 sec/step)\n",
            "I1209 11:27:32.221597 139904075134848 learning.py:507] global step 1822: loss = 2.2333 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1823: loss = 2.3184 (0.813 sec/step)\n",
            "I1209 11:27:33.356995 139904075134848 learning.py:507] global step 1823: loss = 2.3184 (0.813 sec/step)\n",
            "INFO:tensorflow:global step 1824: loss = 1.9230 (2.532 sec/step)\n",
            "I1209 11:27:35.890465 139904075134848 learning.py:507] global step 1824: loss = 1.9230 (2.532 sec/step)\n",
            "INFO:tensorflow:global step 1825: loss = 1.9404 (0.609 sec/step)\n",
            "I1209 11:27:36.501755 139904075134848 learning.py:507] global step 1825: loss = 1.9404 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 1826: loss = 2.3241 (1.835 sec/step)\n",
            "I1209 11:27:38.409808 139904075134848 learning.py:507] global step 1826: loss = 2.3241 (1.835 sec/step)\n",
            "INFO:tensorflow:global step 1827: loss = 1.9296 (0.903 sec/step)\n",
            "I1209 11:27:39.704991 139904075134848 learning.py:507] global step 1827: loss = 1.9296 (0.903 sec/step)\n",
            "INFO:tensorflow:global step 1828: loss = 2.4106 (1.450 sec/step)\n",
            "I1209 11:27:41.189479 139904075134848 learning.py:507] global step 1828: loss = 2.4106 (1.450 sec/step)\n",
            "INFO:tensorflow:global step 1829: loss = 1.7598 (0.562 sec/step)\n",
            "I1209 11:27:41.753301 139904075134848 learning.py:507] global step 1829: loss = 1.7598 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 1830: loss = 2.6269 (1.883 sec/step)\n",
            "I1209 11:27:43.637826 139904075134848 learning.py:507] global step 1830: loss = 2.6269 (1.883 sec/step)\n",
            "INFO:tensorflow:global step 1831: loss = 2.2677 (0.684 sec/step)\n",
            "I1209 11:27:44.597446 139904075134848 learning.py:507] global step 1831: loss = 2.2677 (0.684 sec/step)\n",
            "INFO:tensorflow:global step 1832: loss = 2.0966 (0.631 sec/step)\n",
            "I1209 11:27:45.428339 139904075134848 learning.py:507] global step 1832: loss = 2.0966 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 1833: loss = 2.6415 (1.927 sec/step)\n",
            "I1209 11:27:47.357225 139904075134848 learning.py:507] global step 1833: loss = 2.6415 (1.927 sec/step)\n",
            "INFO:tensorflow:global step 1834: loss = 2.2260 (0.628 sec/step)\n",
            "I1209 11:27:48.359900 139904075134848 learning.py:507] global step 1834: loss = 2.2260 (0.628 sec/step)\n",
            "INFO:tensorflow:global step 1835: loss = 2.0490 (0.889 sec/step)\n",
            "I1209 11:27:49.747894 139904075134848 learning.py:507] global step 1835: loss = 2.0490 (0.889 sec/step)\n",
            "INFO:tensorflow:global step 1836: loss = 1.9568 (0.713 sec/step)\n",
            "I1209 11:27:50.601224 139904075134848 learning.py:507] global step 1836: loss = 1.9568 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1837: loss = 2.2958 (2.545 sec/step)\n",
            "I1209 11:27:53.458874 139904075134848 learning.py:507] global step 1837: loss = 2.2958 (2.545 sec/step)\n",
            "INFO:tensorflow:global step 1838: loss = 2.7756 (0.592 sec/step)\n",
            "I1209 11:27:54.053004 139904075134848 learning.py:507] global step 1838: loss = 2.7756 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 1839: loss = 2.1465 (2.235 sec/step)\n",
            "I1209 11:27:56.289477 139904075134848 learning.py:507] global step 1839: loss = 2.1465 (2.235 sec/step)\n",
            "INFO:tensorflow:global step 1840: loss = 2.3139 (0.769 sec/step)\n",
            "I1209 11:27:57.137911 139904075134848 learning.py:507] global step 1840: loss = 2.3139 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 1841: loss = 2.8608 (2.085 sec/step)\n",
            "I1209 11:27:59.225954 139904075134848 learning.py:507] global step 1841: loss = 2.8608 (2.085 sec/step)\n",
            "INFO:tensorflow:global step 1842: loss = 1.8752 (1.279 sec/step)\n",
            "I1209 11:28:00.506889 139904075134848 learning.py:507] global step 1842: loss = 1.8752 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 1843: loss = 1.6673 (0.728 sec/step)\n",
            "I1209 11:28:01.456049 139904075134848 learning.py:507] global step 1843: loss = 1.6673 (0.728 sec/step)\n",
            "INFO:tensorflow:global step 1844: loss = 1.6624 (0.594 sec/step)\n",
            "I1209 11:28:02.245903 139904075134848 learning.py:507] global step 1844: loss = 1.6624 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 1845: loss = 2.3041 (1.843 sec/step)\n",
            "I1209 11:28:04.091283 139904075134848 learning.py:507] global step 1845: loss = 2.3041 (1.843 sec/step)\n",
            "INFO:tensorflow:global step 1846: loss = 2.3741 (0.892 sec/step)\n",
            "I1209 11:28:05.148677 139904075134848 learning.py:507] global step 1846: loss = 2.3741 (0.892 sec/step)\n",
            "INFO:tensorflow:global step 1847: loss = 2.2509 (0.831 sec/step)\n",
            "I1209 11:28:06.178857 139904075134848 learning.py:507] global step 1847: loss = 2.2509 (0.831 sec/step)\n",
            "INFO:tensorflow:global step 1848: loss = 2.1994 (0.719 sec/step)\n",
            "I1209 11:28:07.437458 139904075134848 learning.py:507] global step 1848: loss = 2.1994 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 1849: loss = 1.8038 (0.772 sec/step)\n",
            "I1209 11:28:08.617040 139904075134848 learning.py:507] global step 1849: loss = 1.8038 (0.772 sec/step)\n",
            "INFO:tensorflow:global step 1850: loss = 1.6730 (0.696 sec/step)\n",
            "I1209 11:28:09.322696 139904075134848 learning.py:507] global step 1850: loss = 1.6730 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 1851: loss = 1.7362 (1.304 sec/step)\n",
            "I1209 11:28:10.872779 139904075134848 learning.py:507] global step 1851: loss = 1.7362 (1.304 sec/step)\n",
            "INFO:tensorflow:global step 1852: loss = 2.0931 (2.014 sec/step)\n",
            "I1209 11:28:12.925141 139904075134848 learning.py:507] global step 1852: loss = 2.0931 (2.014 sec/step)\n",
            "INFO:tensorflow:global step 1853: loss = 2.3645 (0.580 sec/step)\n",
            "I1209 11:28:13.507269 139904075134848 learning.py:507] global step 1853: loss = 2.3645 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1854: loss = 2.2494 (1.715 sec/step)\n",
            "I1209 11:28:15.292822 139904075134848 learning.py:507] global step 1854: loss = 2.2494 (1.715 sec/step)\n",
            "INFO:tensorflow:global step 1855: loss = 2.6678 (1.410 sec/step)\n",
            "I1209 11:28:16.705360 139904075134848 learning.py:507] global step 1855: loss = 2.6678 (1.410 sec/step)\n",
            "INFO:tensorflow:global step 1856: loss = 2.3167 (0.713 sec/step)\n",
            "I1209 11:28:17.714991 139904075134848 learning.py:507] global step 1856: loss = 2.3167 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 1857: loss = 1.6071 (2.054 sec/step)\n",
            "I1209 11:28:20.012406 139904075134848 learning.py:507] global step 1857: loss = 1.6071 (2.054 sec/step)\n",
            "INFO:tensorflow:global step 1858: loss = 1.9160 (0.692 sec/step)\n",
            "I1209 11:28:20.923704 139904075134848 learning.py:507] global step 1858: loss = 1.9160 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 1859: loss = 2.0103 (3.013 sec/step)\n",
            "I1209 11:28:24.355967 139904075134848 learning.py:507] global step 1859: loss = 2.0103 (3.013 sec/step)\n",
            "INFO:tensorflow:global step 1860: loss = 1.9134 (0.676 sec/step)\n",
            "I1209 11:28:25.035330 139904075134848 learning.py:507] global step 1860: loss = 1.9134 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1861: loss = 3.0502 (2.251 sec/step)\n",
            "I1209 11:28:27.288058 139904075134848 learning.py:507] global step 1861: loss = 3.0502 (2.251 sec/step)\n",
            "INFO:tensorflow:global step 1862: loss = 2.9786 (0.919 sec/step)\n",
            "I1209 11:28:28.395740 139904075134848 learning.py:507] global step 1862: loss = 2.9786 (0.919 sec/step)\n",
            "INFO:tensorflow:global step 1863: loss = 2.0585 (0.734 sec/step)\n",
            "I1209 11:28:29.622584 139904075134848 learning.py:507] global step 1863: loss = 2.0585 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 1864: loss = 1.9043 (0.861 sec/step)\n",
            "I1209 11:28:30.684382 139904075134848 learning.py:507] global step 1864: loss = 1.9043 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 1865: loss = 2.2843 (2.260 sec/step)\n",
            "I1209 11:28:33.136379 139904075134848 learning.py:507] global step 1865: loss = 2.2843 (2.260 sec/step)\n",
            "INFO:tensorflow:global step 1866: loss = 1.8644 (0.570 sec/step)\n",
            "I1209 11:28:33.708720 139904075134848 learning.py:507] global step 1866: loss = 1.8644 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 1867: loss = 1.7789 (2.423 sec/step)\n",
            "I1209 11:28:36.133098 139904075134848 learning.py:507] global step 1867: loss = 1.7789 (2.423 sec/step)\n",
            "INFO:tensorflow:global step 1868: loss = 1.7406 (0.647 sec/step)\n",
            "I1209 11:28:36.782215 139904075134848 learning.py:507] global step 1868: loss = 1.7406 (0.647 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1868.\n",
            "I1209 11:28:39.843563 139900543997696 supervisor.py:1050] Recording summary at step 1868.\n",
            "INFO:tensorflow:global step 1869: loss = 2.3881 (3.304 sec/step)\n",
            "I1209 11:28:40.088327 139904075134848 learning.py:507] global step 1869: loss = 2.3881 (3.304 sec/step)\n",
            "INFO:tensorflow:global step 1870: loss = 2.0417 (0.811 sec/step)\n",
            "I1209 11:28:41.132141 139904075134848 learning.py:507] global step 1870: loss = 2.0417 (0.811 sec/step)\n",
            "INFO:tensorflow:global step 1871: loss = 1.8944 (1.456 sec/step)\n",
            "I1209 11:28:42.596434 139904075134848 learning.py:507] global step 1871: loss = 1.8944 (1.456 sec/step)\n",
            "INFO:tensorflow:global step 1872: loss = 1.9523 (0.702 sec/step)\n",
            "I1209 11:28:43.299795 139904075134848 learning.py:507] global step 1872: loss = 1.9523 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 1873: loss = 1.7675 (1.021 sec/step)\n",
            "I1209 11:28:44.569590 139904075134848 learning.py:507] global step 1873: loss = 1.7675 (1.021 sec/step)\n",
            "INFO:tensorflow:global step 1874: loss = 1.6255 (3.276 sec/step)\n",
            "I1209 11:28:48.136769 139904075134848 learning.py:507] global step 1874: loss = 1.6255 (3.276 sec/step)\n",
            "INFO:tensorflow:global step 1875: loss = 1.9219 (0.905 sec/step)\n",
            "I1209 11:28:49.048948 139904075134848 learning.py:507] global step 1875: loss = 1.9219 (0.905 sec/step)\n",
            "INFO:tensorflow:global step 1876: loss = 2.1691 (2.147 sec/step)\n",
            "I1209 11:28:51.236031 139904075134848 learning.py:507] global step 1876: loss = 2.1691 (2.147 sec/step)\n",
            "INFO:tensorflow:global step 1877: loss = 2.1570 (0.834 sec/step)\n",
            "I1209 11:28:52.459974 139904075134848 learning.py:507] global step 1877: loss = 2.1570 (0.834 sec/step)\n",
            "INFO:tensorflow:global step 1878: loss = 2.0969 (0.791 sec/step)\n",
            "I1209 11:28:53.500288 139904075134848 learning.py:507] global step 1878: loss = 2.0969 (0.791 sec/step)\n",
            "INFO:tensorflow:global step 1879: loss = 1.9839 (0.711 sec/step)\n",
            "I1209 11:28:54.315705 139904075134848 learning.py:507] global step 1879: loss = 1.9839 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 1880: loss = 2.0653 (0.968 sec/step)\n",
            "I1209 11:28:55.577839 139904075134848 learning.py:507] global step 1880: loss = 2.0653 (0.968 sec/step)\n",
            "INFO:tensorflow:global step 1881: loss = 2.1981 (1.905 sec/step)\n",
            "I1209 11:28:57.662919 139904075134848 learning.py:507] global step 1881: loss = 2.1981 (1.905 sec/step)\n",
            "INFO:tensorflow:global step 1882: loss = 2.1084 (0.806 sec/step)\n",
            "I1209 11:28:58.511826 139904075134848 learning.py:507] global step 1882: loss = 2.1084 (0.806 sec/step)\n",
            "INFO:tensorflow:global step 1883: loss = 1.9818 (2.167 sec/step)\n",
            "I1209 11:29:00.868346 139904075134848 learning.py:507] global step 1883: loss = 1.9818 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 1884: loss = 2.5295 (0.653 sec/step)\n",
            "I1209 11:29:01.523159 139904075134848 learning.py:507] global step 1884: loss = 2.5295 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 1885: loss = 2.2735 (2.461 sec/step)\n",
            "I1209 11:29:03.986397 139904075134848 learning.py:507] global step 1885: loss = 2.2735 (2.461 sec/step)\n",
            "INFO:tensorflow:global step 1886: loss = 2.4270 (0.720 sec/step)\n",
            "I1209 11:29:05.105378 139904075134848 learning.py:507] global step 1886: loss = 2.4270 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 1887: loss = 1.7255 (0.795 sec/step)\n",
            "I1209 11:29:06.082782 139904075134848 learning.py:507] global step 1887: loss = 1.7255 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 1888: loss = 1.8310 (3.145 sec/step)\n",
            "I1209 11:29:09.503382 139904075134848 learning.py:507] global step 1888: loss = 1.8310 (3.145 sec/step)\n",
            "INFO:tensorflow:global step 1889: loss = 2.1879 (0.851 sec/step)\n",
            "I1209 11:29:10.613843 139904075134848 learning.py:507] global step 1889: loss = 2.1879 (0.851 sec/step)\n",
            "INFO:tensorflow:global step 1890: loss = 2.1365 (1.888 sec/step)\n",
            "I1209 11:29:12.542836 139904075134848 learning.py:507] global step 1890: loss = 2.1365 (1.888 sec/step)\n",
            "INFO:tensorflow:global step 1891: loss = 2.2930 (0.819 sec/step)\n",
            "I1209 11:29:13.699730 139904075134848 learning.py:507] global step 1891: loss = 2.2930 (0.819 sec/step)\n",
            "INFO:tensorflow:global step 1892: loss = 1.7280 (1.614 sec/step)\n",
            "I1209 11:29:15.340388 139904075134848 learning.py:507] global step 1892: loss = 1.7280 (1.614 sec/step)\n",
            "INFO:tensorflow:global step 1893: loss = 1.8875 (0.686 sec/step)\n",
            "I1209 11:29:16.028458 139904075134848 learning.py:507] global step 1893: loss = 1.8875 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 1894: loss = 2.2744 (1.007 sec/step)\n",
            "I1209 11:29:17.056846 139904075134848 learning.py:507] global step 1894: loss = 2.2744 (1.007 sec/step)\n",
            "INFO:tensorflow:global step 1895: loss = 2.4009 (2.059 sec/step)\n",
            "I1209 11:29:19.309866 139904075134848 learning.py:507] global step 1895: loss = 2.4009 (2.059 sec/step)\n",
            "INFO:tensorflow:global step 1896: loss = 2.6998 (0.580 sec/step)\n",
            "I1209 11:29:19.891340 139904075134848 learning.py:507] global step 1896: loss = 2.6998 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 1897: loss = 2.0519 (2.407 sec/step)\n",
            "I1209 11:29:22.299649 139904075134848 learning.py:507] global step 1897: loss = 2.0519 (2.407 sec/step)\n",
            "INFO:tensorflow:global step 1898: loss = 2.2294 (0.945 sec/step)\n",
            "I1209 11:29:23.453496 139904075134848 learning.py:507] global step 1898: loss = 2.2294 (0.945 sec/step)\n",
            "INFO:tensorflow:global step 1899: loss = 2.0854 (0.930 sec/step)\n",
            "I1209 11:29:24.821683 139904075134848 learning.py:507] global step 1899: loss = 2.0854 (0.930 sec/step)\n",
            "INFO:tensorflow:global step 1900: loss = 2.3881 (0.870 sec/step)\n",
            "I1209 11:29:25.927779 139904075134848 learning.py:507] global step 1900: loss = 2.3881 (0.870 sec/step)\n",
            "INFO:tensorflow:global step 1901: loss = 2.5125 (0.685 sec/step)\n",
            "I1209 11:29:26.973143 139904075134848 learning.py:507] global step 1901: loss = 2.5125 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 1902: loss = 2.4160 (2.629 sec/step)\n",
            "I1209 11:29:29.886557 139904075134848 learning.py:507] global step 1902: loss = 2.4160 (2.629 sec/step)\n",
            "INFO:tensorflow:global step 1903: loss = 3.1320 (0.603 sec/step)\n",
            "I1209 11:29:31.009786 139904075134848 learning.py:507] global step 1903: loss = 3.1320 (0.603 sec/step)\n",
            "INFO:tensorflow:global step 1904: loss = 2.6031 (1.036 sec/step)\n",
            "I1209 11:29:32.325833 139904075134848 learning.py:507] global step 1904: loss = 2.6031 (1.036 sec/step)\n",
            "INFO:tensorflow:global step 1905: loss = 2.0029 (1.911 sec/step)\n",
            "I1209 11:29:34.267561 139904075134848 learning.py:507] global step 1905: loss = 2.0029 (1.911 sec/step)\n",
            "INFO:tensorflow:global step 1906: loss = 2.4393 (0.817 sec/step)\n",
            "I1209 11:29:35.090925 139904075134848 learning.py:507] global step 1906: loss = 2.4393 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 1907: loss = 2.4359 (1.757 sec/step)\n",
            "I1209 11:29:36.895492 139904075134848 learning.py:507] global step 1907: loss = 2.4359 (1.757 sec/step)\n",
            "INFO:tensorflow:global step 1908: loss = 2.6645 (0.722 sec/step)\n",
            "I1209 11:29:37.619324 139904075134848 learning.py:507] global step 1908: loss = 2.6645 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1909: loss = 2.2591 (2.109 sec/step)\n",
            "I1209 11:29:39.730662 139904075134848 learning.py:507] global step 1909: loss = 2.2591 (2.109 sec/step)\n",
            "INFO:tensorflow:global step 1910: loss = 1.9934 (0.722 sec/step)\n",
            "I1209 11:29:40.973537 139904075134848 learning.py:507] global step 1910: loss = 1.9934 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 1911: loss = 2.6022 (1.741 sec/step)\n",
            "I1209 11:29:42.837422 139904075134848 learning.py:507] global step 1911: loss = 2.6022 (1.741 sec/step)\n",
            "INFO:tensorflow:global step 1912: loss = 1.8945 (0.798 sec/step)\n",
            "I1209 11:29:43.951830 139904075134848 learning.py:507] global step 1912: loss = 1.8945 (0.798 sec/step)\n",
            "INFO:tensorflow:global step 1913: loss = 2.1498 (1.790 sec/step)\n",
            "I1209 11:29:45.774564 139904075134848 learning.py:507] global step 1913: loss = 2.1498 (1.790 sec/step)\n",
            "INFO:tensorflow:global step 1914: loss = 1.7824 (0.654 sec/step)\n",
            "I1209 11:29:46.677907 139904075134848 learning.py:507] global step 1914: loss = 1.7824 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 1915: loss = 1.8313 (1.738 sec/step)\n",
            "I1209 11:29:48.646955 139904075134848 learning.py:507] global step 1915: loss = 1.8313 (1.738 sec/step)\n",
            "INFO:tensorflow:global step 1916: loss = 1.9104 (0.732 sec/step)\n",
            "I1209 11:29:49.706590 139904075134848 learning.py:507] global step 1916: loss = 1.9104 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 1917: loss = 2.4903 (0.716 sec/step)\n",
            "I1209 11:29:50.603321 139904075134848 learning.py:507] global step 1917: loss = 2.4903 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 1918: loss = 2.3502 (1.731 sec/step)\n",
            "I1209 11:29:52.682822 139904075134848 learning.py:507] global step 1918: loss = 2.3502 (1.731 sec/step)\n",
            "INFO:tensorflow:global step 1919: loss = 2.0500 (0.846 sec/step)\n",
            "I1209 11:29:53.749696 139904075134848 learning.py:507] global step 1919: loss = 2.0500 (0.846 sec/step)\n",
            "INFO:tensorflow:global step 1920: loss = 2.2801 (2.771 sec/step)\n",
            "I1209 11:29:56.667171 139904075134848 learning.py:507] global step 1920: loss = 2.2801 (2.771 sec/step)\n",
            "INFO:tensorflow:global step 1921: loss = 1.4724 (0.662 sec/step)\n",
            "I1209 11:29:57.330798 139904075134848 learning.py:507] global step 1921: loss = 1.4724 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 1922: loss = 1.7331 (0.897 sec/step)\n",
            "I1209 11:29:58.230296 139904075134848 learning.py:507] global step 1922: loss = 1.7331 (0.897 sec/step)\n",
            "INFO:tensorflow:global step 1923: loss = 1.4925 (2.586 sec/step)\n",
            "I1209 11:30:00.817934 139904075134848 learning.py:507] global step 1923: loss = 1.4925 (2.586 sec/step)\n",
            "INFO:tensorflow:global step 1924: loss = 2.0923 (0.665 sec/step)\n",
            "I1209 11:30:01.485224 139904075134848 learning.py:507] global step 1924: loss = 2.0923 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 1925: loss = 2.2564 (1.766 sec/step)\n",
            "I1209 11:30:03.617298 139904075134848 learning.py:507] global step 1925: loss = 2.2564 (1.766 sec/step)\n",
            "INFO:tensorflow:global step 1926: loss = 1.9109 (0.698 sec/step)\n",
            "I1209 11:30:04.560968 139904075134848 learning.py:507] global step 1926: loss = 1.9109 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 1927: loss = 1.6363 (1.728 sec/step)\n",
            "I1209 11:30:06.424360 139904075134848 learning.py:507] global step 1927: loss = 1.6363 (1.728 sec/step)\n",
            "INFO:tensorflow:global step 1928: loss = 2.6750 (0.778 sec/step)\n",
            "I1209 11:30:07.461719 139904075134848 learning.py:507] global step 1928: loss = 2.6750 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 1929: loss = 1.8999 (0.731 sec/step)\n",
            "I1209 11:30:08.533267 139904075134848 learning.py:507] global step 1929: loss = 1.8999 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 1930: loss = 1.9736 (1.669 sec/step)\n",
            "I1209 11:30:10.396171 139904075134848 learning.py:507] global step 1930: loss = 1.9736 (1.669 sec/step)\n",
            "INFO:tensorflow:global step 1931: loss = 2.3237 (0.815 sec/step)\n",
            "I1209 11:30:11.388818 139904075134848 learning.py:507] global step 1931: loss = 2.3237 (0.815 sec/step)\n",
            "INFO:tensorflow:global step 1932: loss = 1.8119 (1.710 sec/step)\n",
            "I1209 11:30:13.137322 139904075134848 learning.py:507] global step 1932: loss = 1.8119 (1.710 sec/step)\n",
            "INFO:tensorflow:global step 1933: loss = 1.9041 (0.661 sec/step)\n",
            "I1209 11:30:13.799970 139904075134848 learning.py:507] global step 1933: loss = 1.9041 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 1934: loss = 2.1877 (0.897 sec/step)\n",
            "I1209 11:30:14.969774 139904075134848 learning.py:507] global step 1934: loss = 2.1877 (0.897 sec/step)\n",
            "INFO:tensorflow:global step 1935: loss = 2.0978 (2.267 sec/step)\n",
            "I1209 11:30:17.262825 139904075134848 learning.py:507] global step 1935: loss = 2.0978 (2.267 sec/step)\n",
            "INFO:tensorflow:global step 1936: loss = 2.3860 (0.791 sec/step)\n",
            "I1209 11:30:18.055700 139904075134848 learning.py:507] global step 1936: loss = 2.3860 (0.791 sec/step)\n",
            "INFO:tensorflow:global step 1937: loss = 2.4377 (2.231 sec/step)\n",
            "I1209 11:30:20.288419 139904075134848 learning.py:507] global step 1937: loss = 2.4377 (2.231 sec/step)\n",
            "INFO:tensorflow:global step 1938: loss = 2.2317 (0.894 sec/step)\n",
            "I1209 11:30:21.552750 139904075134848 learning.py:507] global step 1938: loss = 2.2317 (0.894 sec/step)\n",
            "INFO:tensorflow:global step 1939: loss = 1.8829 (0.672 sec/step)\n",
            "I1209 11:30:22.275086 139904075134848 learning.py:507] global step 1939: loss = 1.8829 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 1940: loss = 2.3071 (0.781 sec/step)\n",
            "I1209 11:30:23.058429 139904075134848 learning.py:507] global step 1940: loss = 2.3071 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 1941: loss = 1.7623 (3.223 sec/step)\n",
            "I1209 11:30:26.282770 139904075134848 learning.py:507] global step 1941: loss = 1.7623 (3.223 sec/step)\n",
            "INFO:tensorflow:global step 1942: loss = 2.1665 (0.791 sec/step)\n",
            "I1209 11:30:27.546904 139904075134848 learning.py:507] global step 1942: loss = 2.1665 (0.791 sec/step)\n",
            "INFO:tensorflow:global step 1943: loss = 2.3265 (0.775 sec/step)\n",
            "I1209 11:30:28.701336 139904075134848 learning.py:507] global step 1943: loss = 2.3265 (0.775 sec/step)\n",
            "INFO:tensorflow:global step 1944: loss = 2.0898 (2.447 sec/step)\n",
            "I1209 11:30:31.614059 139904075134848 learning.py:507] global step 1944: loss = 2.0898 (2.447 sec/step)\n",
            "INFO:tensorflow:global step 1945: loss = 2.1445 (0.712 sec/step)\n",
            "I1209 11:30:32.520874 139904075134848 learning.py:507] global step 1945: loss = 2.1445 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 1946: loss = 1.9361 (2.286 sec/step)\n",
            "I1209 11:30:34.995455 139904075134848 learning.py:507] global step 1946: loss = 1.9361 (2.286 sec/step)\n",
            "INFO:tensorflow:global step 1947: loss = 2.2739 (0.725 sec/step)\n",
            "I1209 11:30:36.088469 139904075134848 learning.py:507] global step 1947: loss = 2.2739 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 1948: loss = 2.1001 (3.121 sec/step)\n",
            "I1209 11:30:39.685687 139904075134848 learning.py:507] global step 1948: loss = 2.1001 (3.121 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 1948.\n",
            "I1209 11:30:39.940190 139900543997696 supervisor.py:1050] Recording summary at step 1948.\n",
            "INFO:tensorflow:global step 1949: loss = 1.8371 (0.727 sec/step)\n",
            "I1209 11:30:40.421252 139904075134848 learning.py:507] global step 1949: loss = 1.8371 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 1950: loss = 1.8105 (1.112 sec/step)\n",
            "I1209 11:30:41.931969 139904075134848 learning.py:507] global step 1950: loss = 1.8105 (1.112 sec/step)\n",
            "INFO:tensorflow:global step 1951: loss = 2.2073 (0.981 sec/step)\n",
            "I1209 11:30:43.351750 139904075134848 learning.py:507] global step 1951: loss = 2.2073 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 1952: loss = 1.7708 (0.997 sec/step)\n",
            "I1209 11:30:44.787833 139904075134848 learning.py:507] global step 1952: loss = 1.7708 (0.997 sec/step)\n",
            "INFO:tensorflow:global step 1953: loss = 2.1461 (0.736 sec/step)\n",
            "I1209 11:30:45.678892 139904075134848 learning.py:507] global step 1953: loss = 2.1461 (0.736 sec/step)\n",
            "INFO:tensorflow:global step 1954: loss = 2.9508 (2.021 sec/step)\n",
            "I1209 11:30:47.702371 139904075134848 learning.py:507] global step 1954: loss = 2.9508 (2.021 sec/step)\n",
            "INFO:tensorflow:global step 1955: loss = 1.8362 (0.666 sec/step)\n",
            "I1209 11:30:48.370726 139904075134848 learning.py:507] global step 1955: loss = 1.8362 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 1956: loss = 2.4564 (2.336 sec/step)\n",
            "I1209 11:30:50.708441 139904075134848 learning.py:507] global step 1956: loss = 2.4564 (2.336 sec/step)\n",
            "INFO:tensorflow:global step 1957: loss = 2.2992 (0.778 sec/step)\n",
            "I1209 11:30:51.488990 139904075134848 learning.py:507] global step 1957: loss = 2.2992 (0.778 sec/step)\n",
            "INFO:tensorflow:global step 1958: loss = 2.3034 (2.438 sec/step)\n",
            "I1209 11:30:53.929149 139904075134848 learning.py:507] global step 1958: loss = 2.3034 (2.438 sec/step)\n",
            "INFO:tensorflow:global step 1959: loss = 2.0140 (0.712 sec/step)\n",
            "I1209 11:30:54.643413 139904075134848 learning.py:507] global step 1959: loss = 2.0140 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 1960: loss = 2.6215 (1.942 sec/step)\n",
            "I1209 11:30:56.587373 139904075134848 learning.py:507] global step 1960: loss = 2.6215 (1.942 sec/step)\n",
            "INFO:tensorflow:global step 1961: loss = 1.9421 (0.792 sec/step)\n",
            "I1209 11:30:57.381753 139904075134848 learning.py:507] global step 1961: loss = 1.9421 (0.792 sec/step)\n",
            "INFO:tensorflow:global step 1962: loss = 1.7537 (2.347 sec/step)\n",
            "I1209 11:30:59.730917 139904075134848 learning.py:507] global step 1962: loss = 1.7537 (2.347 sec/step)\n",
            "INFO:tensorflow:global step 1963: loss = 1.9298 (0.625 sec/step)\n",
            "I1209 11:31:00.358263 139904075134848 learning.py:507] global step 1963: loss = 1.9298 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 1964: loss = 2.4100 (1.925 sec/step)\n",
            "I1209 11:31:02.285524 139904075134848 learning.py:507] global step 1964: loss = 2.4100 (1.925 sec/step)\n",
            "INFO:tensorflow:global step 1965: loss = 1.6725 (0.724 sec/step)\n",
            "I1209 11:31:03.337585 139904075134848 learning.py:507] global step 1965: loss = 1.6725 (0.724 sec/step)\n",
            "INFO:tensorflow:global step 1966: loss = 2.4789 (0.676 sec/step)\n",
            "I1209 11:31:04.253157 139904075134848 learning.py:507] global step 1966: loss = 2.4789 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 1967: loss = 2.1911 (1.984 sec/step)\n",
            "I1209 11:31:06.239449 139904075134848 learning.py:507] global step 1967: loss = 2.1911 (1.984 sec/step)\n",
            "INFO:tensorflow:global step 1968: loss = 2.3277 (0.829 sec/step)\n",
            "I1209 11:31:07.102833 139904075134848 learning.py:507] global step 1968: loss = 2.3277 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1969: loss = 2.0407 (1.829 sec/step)\n",
            "I1209 11:31:08.998053 139904075134848 learning.py:507] global step 1969: loss = 2.0407 (1.829 sec/step)\n",
            "INFO:tensorflow:global step 1970: loss = 2.0355 (0.752 sec/step)\n",
            "I1209 11:31:09.962089 139904075134848 learning.py:507] global step 1970: loss = 2.0355 (0.752 sec/step)\n",
            "INFO:tensorflow:global step 1971: loss = 2.5079 (0.845 sec/step)\n",
            "I1209 11:31:11.428760 139904075134848 learning.py:507] global step 1971: loss = 2.5079 (0.845 sec/step)\n",
            "INFO:tensorflow:global step 1972: loss = 2.2857 (0.797 sec/step)\n",
            "I1209 11:31:12.397832 139904075134848 learning.py:507] global step 1972: loss = 2.2857 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 1973: loss = 2.5243 (1.042 sec/step)\n",
            "I1209 11:31:13.624783 139904075134848 learning.py:507] global step 1973: loss = 2.5243 (1.042 sec/step)\n",
            "INFO:tensorflow:global step 1974: loss = 2.3397 (1.873 sec/step)\n",
            "I1209 11:31:15.820780 139904075134848 learning.py:507] global step 1974: loss = 2.3397 (1.873 sec/step)\n",
            "INFO:tensorflow:global step 1975: loss = 1.9844 (0.900 sec/step)\n",
            "I1209 11:31:16.734385 139904075134848 learning.py:507] global step 1975: loss = 1.9844 (0.900 sec/step)\n",
            "INFO:tensorflow:global step 1976: loss = 1.8971 (1.764 sec/step)\n",
            "I1209 11:31:18.609921 139904075134848 learning.py:507] global step 1976: loss = 1.8971 (1.764 sec/step)\n",
            "INFO:tensorflow:global step 1977: loss = 2.3701 (0.680 sec/step)\n",
            "I1209 11:31:19.693399 139904075134848 learning.py:507] global step 1977: loss = 2.3701 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 1978: loss = 2.5834 (1.602 sec/step)\n",
            "I1209 11:31:21.426510 139904075134848 learning.py:507] global step 1978: loss = 2.5834 (1.602 sec/step)\n",
            "INFO:tensorflow:global step 1979: loss = 2.3332 (0.599 sec/step)\n",
            "I1209 11:31:22.027432 139904075134848 learning.py:507] global step 1979: loss = 2.3332 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 1980: loss = 2.5408 (1.279 sec/step)\n",
            "I1209 11:31:23.615772 139904075134848 learning.py:507] global step 1980: loss = 2.5408 (1.279 sec/step)\n",
            "INFO:tensorflow:global step 1981: loss = 2.0840 (1.994 sec/step)\n",
            "I1209 11:31:25.709445 139904075134848 learning.py:507] global step 1981: loss = 2.0840 (1.994 sec/step)\n",
            "INFO:tensorflow:global step 1982: loss = 3.1870 (0.829 sec/step)\n",
            "I1209 11:31:26.709779 139904075134848 learning.py:507] global step 1982: loss = 3.1870 (0.829 sec/step)\n",
            "INFO:tensorflow:global step 1983: loss = 2.1126 (0.889 sec/step)\n",
            "I1209 11:31:27.866398 139904075134848 learning.py:507] global step 1983: loss = 2.1126 (0.889 sec/step)\n",
            "INFO:tensorflow:global step 1984: loss = 2.1742 (2.099 sec/step)\n",
            "I1209 11:31:30.149259 139904075134848 learning.py:507] global step 1984: loss = 2.1742 (2.099 sec/step)\n",
            "INFO:tensorflow:global step 1985: loss = 2.0953 (0.721 sec/step)\n",
            "I1209 11:31:30.872270 139904075134848 learning.py:507] global step 1985: loss = 2.0953 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 1986: loss = 2.2782 (0.611 sec/step)\n",
            "I1209 11:31:31.485318 139904075134848 learning.py:507] global step 1986: loss = 2.2782 (0.611 sec/step)\n",
            "INFO:tensorflow:global step 1987: loss = 2.5730 (1.483 sec/step)\n",
            "I1209 11:31:33.167823 139904075134848 learning.py:507] global step 1987: loss = 2.5730 (1.483 sec/step)\n",
            "INFO:tensorflow:global step 1988: loss = 2.1837 (2.561 sec/step)\n",
            "I1209 11:31:35.922110 139904075134848 learning.py:507] global step 1988: loss = 2.1837 (2.561 sec/step)\n",
            "INFO:tensorflow:global step 1989: loss = 2.3068 (0.668 sec/step)\n",
            "I1209 11:31:36.592378 139904075134848 learning.py:507] global step 1989: loss = 2.3068 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 1990: loss = 3.3457 (0.887 sec/step)\n",
            "I1209 11:31:37.699289 139904075134848 learning.py:507] global step 1990: loss = 3.3457 (0.887 sec/step)\n",
            "INFO:tensorflow:global step 1991: loss = 1.5833 (2.102 sec/step)\n",
            "I1209 11:31:40.033588 139904075134848 learning.py:507] global step 1991: loss = 1.5833 (2.102 sec/step)\n",
            "INFO:tensorflow:global step 1992: loss = 1.8872 (0.644 sec/step)\n",
            "I1209 11:31:40.917719 139904075134848 learning.py:507] global step 1992: loss = 1.8872 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 1993: loss = 2.3719 (0.710 sec/step)\n",
            "I1209 11:31:41.882144 139904075134848 learning.py:507] global step 1993: loss = 2.3719 (0.710 sec/step)\n",
            "INFO:tensorflow:global step 1994: loss = 2.0384 (2.135 sec/step)\n",
            "I1209 11:31:44.019003 139904075134848 learning.py:507] global step 1994: loss = 2.0384 (2.135 sec/step)\n",
            "INFO:tensorflow:global step 1995: loss = 2.9459 (0.729 sec/step)\n",
            "I1209 11:31:44.749745 139904075134848 learning.py:507] global step 1995: loss = 2.9459 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 1996: loss = 2.5656 (3.192 sec/step)\n",
            "I1209 11:31:47.943276 139904075134848 learning.py:507] global step 1996: loss = 2.5656 (3.192 sec/step)\n",
            "INFO:tensorflow:global step 1997: loss = 2.5291 (0.694 sec/step)\n",
            "I1209 11:31:48.639250 139904075134848 learning.py:507] global step 1997: loss = 2.5291 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 1998: loss = 1.8807 (1.277 sec/step)\n",
            "I1209 11:31:50.274330 139904075134848 learning.py:507] global step 1998: loss = 1.8807 (1.277 sec/step)\n",
            "INFO:tensorflow:global step 1999: loss = 3.0404 (2.523 sec/step)\n",
            "I1209 11:31:52.946092 139904075134848 learning.py:507] global step 1999: loss = 3.0404 (2.523 sec/step)\n",
            "INFO:tensorflow:global step 2000: loss = 1.7982 (0.585 sec/step)\n",
            "I1209 11:31:53.533013 139904075134848 learning.py:507] global step 2000: loss = 1.7982 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2001: loss = 1.8860 (0.968 sec/step)\n",
            "I1209 11:31:54.840975 139904075134848 learning.py:507] global step 2001: loss = 1.8860 (0.968 sec/step)\n",
            "INFO:tensorflow:global step 2002: loss = 2.1384 (2.755 sec/step)\n",
            "I1209 11:31:57.949711 139904075134848 learning.py:507] global step 2002: loss = 2.1384 (2.755 sec/step)\n",
            "INFO:tensorflow:global step 2003: loss = 2.4717 (0.722 sec/step)\n",
            "I1209 11:31:58.911427 139904075134848 learning.py:507] global step 2003: loss = 2.4717 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2004: loss = 2.0941 (0.781 sec/step)\n",
            "I1209 11:31:59.990942 139904075134848 learning.py:507] global step 2004: loss = 2.0941 (0.781 sec/step)\n",
            "INFO:tensorflow:global step 2005: loss = 2.1093 (1.376 sec/step)\n",
            "I1209 11:32:01.635728 139904075134848 learning.py:507] global step 2005: loss = 2.1093 (1.376 sec/step)\n",
            "INFO:tensorflow:global step 2006: loss = 2.1690 (0.687 sec/step)\n",
            "I1209 11:32:02.356794 139904075134848 learning.py:507] global step 2006: loss = 2.1690 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 2007: loss = 2.1961 (0.802 sec/step)\n",
            "I1209 11:32:03.792825 139904075134848 learning.py:507] global step 2007: loss = 2.1961 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 2008: loss = 2.3748 (0.682 sec/step)\n",
            "I1209 11:32:04.633988 139904075134848 learning.py:507] global step 2008: loss = 2.3748 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 2009: loss = 2.5040 (1.136 sec/step)\n",
            "I1209 11:32:05.969588 139904075134848 learning.py:507] global step 2009: loss = 2.5040 (1.136 sec/step)\n",
            "INFO:tensorflow:global step 2010: loss = 2.5082 (3.135 sec/step)\n",
            "I1209 11:32:09.142915 139904075134848 learning.py:507] global step 2010: loss = 2.5082 (3.135 sec/step)\n",
            "INFO:tensorflow:global step 2011: loss = 1.8783 (1.439 sec/step)\n",
            "I1209 11:32:10.583390 139904075134848 learning.py:507] global step 2011: loss = 1.8783 (1.439 sec/step)\n",
            "INFO:tensorflow:global step 2012: loss = 2.1690 (0.594 sec/step)\n",
            "I1209 11:32:11.179504 139904075134848 learning.py:507] global step 2012: loss = 2.1690 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2013: loss = 2.2754 (1.089 sec/step)\n",
            "I1209 11:32:12.479843 139904075134848 learning.py:507] global step 2013: loss = 2.2754 (1.089 sec/step)\n",
            "INFO:tensorflow:global step 2014: loss = 3.2415 (1.923 sec/step)\n",
            "I1209 11:32:14.724859 139904075134848 learning.py:507] global step 2014: loss = 3.2415 (1.923 sec/step)\n",
            "INFO:tensorflow:global step 2015: loss = 2.4120 (0.609 sec/step)\n",
            "I1209 11:32:15.335956 139904075134848 learning.py:507] global step 2015: loss = 2.4120 (0.609 sec/step)\n",
            "INFO:tensorflow:global step 2016: loss = 2.2216 (0.943 sec/step)\n",
            "I1209 11:32:16.515941 139904075134848 learning.py:507] global step 2016: loss = 2.2216 (0.943 sec/step)\n",
            "INFO:tensorflow:global step 2017: loss = 2.2655 (2.078 sec/step)\n",
            "I1209 11:32:18.724825 139904075134848 learning.py:507] global step 2017: loss = 2.2655 (2.078 sec/step)\n",
            "INFO:tensorflow:global step 2018: loss = 2.5098 (0.598 sec/step)\n",
            "I1209 11:32:19.324542 139904075134848 learning.py:507] global step 2018: loss = 2.5098 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 2019: loss = 1.9581 (1.385 sec/step)\n",
            "I1209 11:32:20.885820 139904075134848 learning.py:507] global step 2019: loss = 1.9581 (1.385 sec/step)\n",
            "INFO:tensorflow:global step 2020: loss = 1.7949 (0.947 sec/step)\n",
            "I1209 11:32:22.470175 139904075134848 learning.py:507] global step 2020: loss = 1.7949 (0.947 sec/step)\n",
            "INFO:tensorflow:global step 2021: loss = 1.6973 (0.618 sec/step)\n",
            "I1209 11:32:23.446084 139904075134848 learning.py:507] global step 2021: loss = 1.6973 (0.618 sec/step)\n",
            "INFO:tensorflow:global step 2022: loss = 2.0471 (0.660 sec/step)\n",
            "I1209 11:32:24.621842 139904075134848 learning.py:507] global step 2022: loss = 2.0471 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2023: loss = 2.4534 (2.213 sec/step)\n",
            "I1209 11:32:26.909785 139904075134848 learning.py:507] global step 2023: loss = 2.4534 (2.213 sec/step)\n",
            "INFO:tensorflow:global step 2024: loss = 2.3723 (0.607 sec/step)\n",
            "I1209 11:32:27.519276 139904075134848 learning.py:507] global step 2024: loss = 2.3723 (0.607 sec/step)\n",
            "INFO:tensorflow:global step 2025: loss = 1.5355 (2.492 sec/step)\n",
            "I1209 11:32:30.014086 139904075134848 learning.py:507] global step 2025: loss = 1.5355 (2.492 sec/step)\n",
            "INFO:tensorflow:global step 2026: loss = 2.1692 (0.663 sec/step)\n",
            "I1209 11:32:30.679268 139904075134848 learning.py:507] global step 2026: loss = 2.1692 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2027: loss = 1.7934 (1.891 sec/step)\n",
            "I1209 11:32:32.575662 139904075134848 learning.py:507] global step 2027: loss = 1.7934 (1.891 sec/step)\n",
            "INFO:tensorflow:global step 2028: loss = 2.2504 (0.899 sec/step)\n",
            "I1209 11:32:33.707308 139904075134848 learning.py:507] global step 2028: loss = 2.2504 (0.899 sec/step)\n",
            "INFO:tensorflow:global step 2029: loss = 2.4443 (0.786 sec/step)\n",
            "I1209 11:32:34.947044 139904075134848 learning.py:507] global step 2029: loss = 2.4443 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 2030: loss = 1.5827 (0.734 sec/step)\n",
            "I1209 11:32:36.239455 139904075134848 learning.py:507] global step 2030: loss = 1.5827 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 2031: loss = 1.8445 (2.257 sec/step)\n",
            "I1209 11:32:38.866760 139904075134848 learning.py:507] global step 2031: loss = 1.8445 (2.257 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2031.\n",
            "I1209 11:32:40.067397 139900543997696 supervisor.py:1050] Recording summary at step 2031.\n",
            "INFO:tensorflow:global step 2032: loss = 2.2297 (1.301 sec/step)\n",
            "I1209 11:32:40.377815 139904075134848 learning.py:507] global step 2032: loss = 2.2297 (1.301 sec/step)\n",
            "INFO:tensorflow:global step 2033: loss = 2.0574 (0.669 sec/step)\n",
            "I1209 11:32:41.048433 139904075134848 learning.py:507] global step 2033: loss = 2.0574 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 2034: loss = 2.0398 (1.955 sec/step)\n",
            "I1209 11:32:43.005044 139904075134848 learning.py:507] global step 2034: loss = 2.0398 (1.955 sec/step)\n",
            "INFO:tensorflow:global step 2035: loss = 1.9854 (0.748 sec/step)\n",
            "I1209 11:32:44.047225 139904075134848 learning.py:507] global step 2035: loss = 1.9854 (0.748 sec/step)\n",
            "INFO:tensorflow:global step 2036: loss = 2.3195 (1.824 sec/step)\n",
            "I1209 11:32:46.032585 139904075134848 learning.py:507] global step 2036: loss = 2.3195 (1.824 sec/step)\n",
            "INFO:tensorflow:global step 2037: loss = 2.1818 (0.620 sec/step)\n",
            "I1209 11:32:46.654550 139904075134848 learning.py:507] global step 2037: loss = 2.1818 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2038: loss = 1.8778 (2.282 sec/step)\n",
            "I1209 11:32:48.938841 139904075134848 learning.py:507] global step 2038: loss = 1.8778 (2.282 sec/step)\n",
            "INFO:tensorflow:global step 2039: loss = 2.2939 (0.685 sec/step)\n",
            "I1209 11:32:49.626161 139904075134848 learning.py:507] global step 2039: loss = 2.2939 (0.685 sec/step)\n",
            "INFO:tensorflow:global step 2040: loss = 2.2170 (1.375 sec/step)\n",
            "I1209 11:32:51.234087 139904075134848 learning.py:507] global step 2040: loss = 2.2170 (1.375 sec/step)\n",
            "INFO:tensorflow:global step 2041: loss = 1.8883 (2.369 sec/step)\n",
            "I1209 11:32:53.938584 139904075134848 learning.py:507] global step 2041: loss = 1.8883 (2.369 sec/step)\n",
            "INFO:tensorflow:global step 2042: loss = 2.4635 (0.741 sec/step)\n",
            "I1209 11:32:54.682385 139904075134848 learning.py:507] global step 2042: loss = 2.4635 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 2043: loss = 2.2671 (2.220 sec/step)\n",
            "I1209 11:32:56.904683 139904075134848 learning.py:507] global step 2043: loss = 2.2671 (2.220 sec/step)\n",
            "INFO:tensorflow:global step 2044: loss = 1.9154 (0.713 sec/step)\n",
            "I1209 11:32:57.620157 139904075134848 learning.py:507] global step 2044: loss = 1.9154 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 2045: loss = 2.2867 (1.717 sec/step)\n",
            "I1209 11:32:59.342721 139904075134848 learning.py:507] global step 2045: loss = 2.2867 (1.717 sec/step)\n",
            "INFO:tensorflow:global step 2046: loss = 2.4859 (1.970 sec/step)\n",
            "I1209 11:33:01.314763 139904075134848 learning.py:507] global step 2046: loss = 2.4859 (1.970 sec/step)\n",
            "INFO:tensorflow:global step 2047: loss = 2.9470 (0.840 sec/step)\n",
            "I1209 11:33:02.219680 139904075134848 learning.py:507] global step 2047: loss = 2.9470 (0.840 sec/step)\n",
            "INFO:tensorflow:global step 2048: loss = 2.5771 (0.777 sec/step)\n",
            "I1209 11:33:03.228770 139904075134848 learning.py:507] global step 2048: loss = 2.5771 (0.777 sec/step)\n",
            "INFO:tensorflow:global step 2049: loss = 2.0628 (1.057 sec/step)\n",
            "I1209 11:33:04.301412 139904075134848 learning.py:507] global step 2049: loss = 2.0628 (1.057 sec/step)\n",
            "INFO:tensorflow:global step 2050: loss = 1.9150 (1.539 sec/step)\n",
            "I1209 11:33:05.999854 139904075134848 learning.py:507] global step 2050: loss = 1.9150 (1.539 sec/step)\n",
            "INFO:tensorflow:global step 2051: loss = 2.1224 (0.892 sec/step)\n",
            "I1209 11:33:07.202996 139904075134848 learning.py:507] global step 2051: loss = 2.1224 (0.892 sec/step)\n",
            "INFO:tensorflow:global step 2052: loss = 2.8009 (2.233 sec/step)\n",
            "I1209 11:33:09.596099 139904075134848 learning.py:507] global step 2052: loss = 2.8009 (2.233 sec/step)\n",
            "INFO:tensorflow:global step 2053: loss = 1.8404 (0.730 sec/step)\n",
            "I1209 11:33:10.467266 139904075134848 learning.py:507] global step 2053: loss = 1.8404 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 2054: loss = 2.1039 (2.621 sec/step)\n",
            "I1209 11:33:13.336565 139904075134848 learning.py:507] global step 2054: loss = 2.1039 (2.621 sec/step)\n",
            "INFO:tensorflow:global step 2055: loss = 1.8366 (0.640 sec/step)\n",
            "I1209 11:33:13.978644 139904075134848 learning.py:507] global step 2055: loss = 1.8366 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2056: loss = 2.6235 (1.445 sec/step)\n",
            "I1209 11:33:15.620734 139904075134848 learning.py:507] global step 2056: loss = 2.6235 (1.445 sec/step)\n",
            "INFO:tensorflow:global step 2057: loss = 1.9384 (1.951 sec/step)\n",
            "I1209 11:33:17.857988 139904075134848 learning.py:507] global step 2057: loss = 1.9384 (1.951 sec/step)\n",
            "INFO:tensorflow:global step 2058: loss = 2.4067 (0.770 sec/step)\n",
            "I1209 11:33:19.070496 139904075134848 learning.py:507] global step 2058: loss = 2.4067 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 2059: loss = 1.8980 (0.746 sec/step)\n",
            "I1209 11:33:19.968637 139904075134848 learning.py:507] global step 2059: loss = 1.8980 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 2060: loss = 2.5170 (2.038 sec/step)\n",
            "I1209 11:33:22.009192 139904075134848 learning.py:507] global step 2060: loss = 2.5170 (2.038 sec/step)\n",
            "INFO:tensorflow:global step 2061: loss = 2.4620 (0.863 sec/step)\n",
            "I1209 11:33:23.100650 139904075134848 learning.py:507] global step 2061: loss = 2.4620 (0.863 sec/step)\n",
            "INFO:tensorflow:global step 2062: loss = 1.7596 (1.928 sec/step)\n",
            "I1209 11:33:25.288974 139904075134848 learning.py:507] global step 2062: loss = 1.7596 (1.928 sec/step)\n",
            "INFO:tensorflow:global step 2063: loss = 2.3438 (0.638 sec/step)\n",
            "I1209 11:33:25.929013 139904075134848 learning.py:507] global step 2063: loss = 2.3438 (0.638 sec/step)\n",
            "INFO:tensorflow:global step 2064: loss = 2.1913 (1.658 sec/step)\n",
            "I1209 11:33:27.898001 139904075134848 learning.py:507] global step 2064: loss = 2.1913 (1.658 sec/step)\n",
            "INFO:tensorflow:global step 2065: loss = 2.2699 (0.625 sec/step)\n",
            "I1209 11:33:28.649301 139904075134848 learning.py:507] global step 2065: loss = 2.2699 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2066: loss = 1.8211 (1.767 sec/step)\n",
            "I1209 11:33:30.422774 139904075134848 learning.py:507] global step 2066: loss = 1.8211 (1.767 sec/step)\n",
            "INFO:tensorflow:global step 2067: loss = 2.1973 (2.144 sec/step)\n",
            "I1209 11:33:32.568932 139904075134848 learning.py:507] global step 2067: loss = 2.1973 (2.144 sec/step)\n",
            "INFO:tensorflow:global step 2068: loss = 2.2044 (0.677 sec/step)\n",
            "I1209 11:33:33.247445 139904075134848 learning.py:507] global step 2068: loss = 2.2044 (0.677 sec/step)\n",
            "INFO:tensorflow:global step 2069: loss = 1.8821 (2.104 sec/step)\n",
            "I1209 11:33:35.366458 139904075134848 learning.py:507] global step 2069: loss = 1.8821 (2.104 sec/step)\n",
            "INFO:tensorflow:global step 2070: loss = 2.5188 (0.825 sec/step)\n",
            "I1209 11:33:36.260239 139904075134848 learning.py:507] global step 2070: loss = 2.5188 (0.825 sec/step)\n",
            "INFO:tensorflow:global step 2071: loss = 2.1249 (2.324 sec/step)\n",
            "I1209 11:33:38.603827 139904075134848 learning.py:507] global step 2071: loss = 2.1249 (2.324 sec/step)\n",
            "INFO:tensorflow:global step 2072: loss = 1.8548 (0.786 sec/step)\n",
            "I1209 11:33:39.905807 139904075134848 learning.py:507] global step 2072: loss = 1.8548 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 2073: loss = 2.8364 (0.754 sec/step)\n",
            "I1209 11:33:41.140116 139904075134848 learning.py:507] global step 2073: loss = 2.8364 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 2074: loss = 3.0577 (0.799 sec/step)\n",
            "I1209 11:33:42.430880 139904075134848 learning.py:507] global step 2074: loss = 3.0577 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2075: loss = 2.3925 (2.049 sec/step)\n",
            "I1209 11:33:44.571830 139904075134848 learning.py:507] global step 2075: loss = 2.3925 (2.049 sec/step)\n",
            "INFO:tensorflow:global step 2076: loss = 2.4019 (0.729 sec/step)\n",
            "I1209 11:33:45.569007 139904075134848 learning.py:507] global step 2076: loss = 2.4019 (0.729 sec/step)\n",
            "INFO:tensorflow:global step 2077: loss = 1.7570 (1.448 sec/step)\n",
            "I1209 11:33:47.266102 139904075134848 learning.py:507] global step 2077: loss = 1.7570 (1.448 sec/step)\n",
            "INFO:tensorflow:global step 2078: loss = 2.3284 (0.706 sec/step)\n",
            "I1209 11:33:48.267999 139904075134848 learning.py:507] global step 2078: loss = 2.3284 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 2079: loss = 2.5082 (1.746 sec/step)\n",
            "I1209 11:33:50.082256 139904075134848 learning.py:507] global step 2079: loss = 2.5082 (1.746 sec/step)\n",
            "INFO:tensorflow:global step 2080: loss = 2.1123 (0.610 sec/step)\n",
            "I1209 11:33:50.694306 139904075134848 learning.py:507] global step 2080: loss = 2.1123 (0.610 sec/step)\n",
            "INFO:tensorflow:global step 2081: loss = 3.1711 (0.672 sec/step)\n",
            "I1209 11:33:51.368791 139904075134848 learning.py:507] global step 2081: loss = 3.1711 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2082: loss = 2.3440 (3.267 sec/step)\n",
            "I1209 11:33:54.637409 139904075134848 learning.py:507] global step 2082: loss = 2.3440 (3.267 sec/step)\n",
            "INFO:tensorflow:global step 2083: loss = 1.9815 (0.743 sec/step)\n",
            "I1209 11:33:55.530893 139904075134848 learning.py:507] global step 2083: loss = 1.9815 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2084: loss = 2.5848 (1.799 sec/step)\n",
            "I1209 11:33:57.331555 139904075134848 learning.py:507] global step 2084: loss = 2.5848 (1.799 sec/step)\n",
            "INFO:tensorflow:global step 2085: loss = 2.1600 (0.767 sec/step)\n",
            "I1209 11:33:58.438306 139904075134848 learning.py:507] global step 2085: loss = 2.1600 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 2086: loss = 2.2883 (0.566 sec/step)\n",
            "I1209 11:33:59.387898 139904075134848 learning.py:507] global step 2086: loss = 2.2883 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 2087: loss = 1.9711 (0.899 sec/step)\n",
            "I1209 11:34:00.702205 139904075134848 learning.py:507] global step 2087: loss = 1.9711 (0.899 sec/step)\n",
            "INFO:tensorflow:global step 2088: loss = 2.3620 (2.294 sec/step)\n",
            "I1209 11:34:03.312543 139904075134848 learning.py:507] global step 2088: loss = 2.3620 (2.294 sec/step)\n",
            "INFO:tensorflow:global step 2089: loss = 2.1687 (0.717 sec/step)\n",
            "I1209 11:34:04.033291 139904075134848 learning.py:507] global step 2089: loss = 2.1687 (0.717 sec/step)\n",
            "INFO:tensorflow:global step 2090: loss = 2.1173 (2.427 sec/step)\n",
            "I1209 11:34:06.462514 139904075134848 learning.py:507] global step 2090: loss = 2.1173 (2.427 sec/step)\n",
            "INFO:tensorflow:global step 2091: loss = 2.2296 (0.837 sec/step)\n",
            "I1209 11:34:07.516216 139904075134848 learning.py:507] global step 2091: loss = 2.2296 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 2092: loss = 2.1632 (0.818 sec/step)\n",
            "I1209 11:34:08.472789 139904075134848 learning.py:507] global step 2092: loss = 2.1632 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 2093: loss = 2.1887 (2.098 sec/step)\n",
            "I1209 11:34:10.572448 139904075134848 learning.py:507] global step 2093: loss = 2.1887 (2.098 sec/step)\n",
            "INFO:tensorflow:global step 2094: loss = 2.5175 (0.715 sec/step)\n",
            "I1209 11:34:11.289743 139904075134848 learning.py:507] global step 2094: loss = 2.5175 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 2095: loss = 2.5463 (1.370 sec/step)\n",
            "I1209 11:34:13.009910 139904075134848 learning.py:507] global step 2095: loss = 2.5463 (1.370 sec/step)\n",
            "INFO:tensorflow:global step 2096: loss = 2.4962 (1.986 sec/step)\n",
            "I1209 11:34:15.213740 139904075134848 learning.py:507] global step 2096: loss = 2.4962 (1.986 sec/step)\n",
            "INFO:tensorflow:global step 2097: loss = 1.7291 (0.720 sec/step)\n",
            "I1209 11:34:15.936148 139904075134848 learning.py:507] global step 2097: loss = 1.7291 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 2098: loss = 1.8668 (2.394 sec/step)\n",
            "I1209 11:34:18.332035 139904075134848 learning.py:507] global step 2098: loss = 1.8668 (2.394 sec/step)\n",
            "INFO:tensorflow:global step 2099: loss = 2.5107 (0.786 sec/step)\n",
            "I1209 11:34:19.514892 139904075134848 learning.py:507] global step 2099: loss = 2.5107 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 2100: loss = 2.7473 (0.672 sec/step)\n",
            "I1209 11:34:20.328710 139904075134848 learning.py:507] global step 2100: loss = 2.7473 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2101: loss = 2.4924 (2.705 sec/step)\n",
            "I1209 11:34:23.077219 139904075134848 learning.py:507] global step 2101: loss = 2.4924 (2.705 sec/step)\n",
            "INFO:tensorflow:global step 2102: loss = 1.8271 (0.665 sec/step)\n",
            "I1209 11:34:23.744489 139904075134848 learning.py:507] global step 2102: loss = 1.8271 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2103: loss = 2.8112 (1.386 sec/step)\n",
            "I1209 11:34:25.400953 139904075134848 learning.py:507] global step 2103: loss = 2.8112 (1.386 sec/step)\n",
            "INFO:tensorflow:global step 2104: loss = 2.4048 (0.796 sec/step)\n",
            "I1209 11:34:26.502131 139904075134848 learning.py:507] global step 2104: loss = 2.4048 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 2105: loss = 2.2457 (1.466 sec/step)\n",
            "I1209 11:34:28.048033 139904075134848 learning.py:507] global step 2105: loss = 2.2457 (1.466 sec/step)\n",
            "INFO:tensorflow:global step 2106: loss = 1.9714 (0.804 sec/step)\n",
            "I1209 11:34:28.877370 139904075134848 learning.py:507] global step 2106: loss = 1.9714 (0.804 sec/step)\n",
            "INFO:tensorflow:global step 2107: loss = 2.5060 (0.741 sec/step)\n",
            "I1209 11:34:29.969540 139904075134848 learning.py:507] global step 2107: loss = 2.5060 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 2108: loss = 1.7727 (1.229 sec/step)\n",
            "I1209 11:34:31.205948 139904075134848 learning.py:507] global step 2108: loss = 1.7727 (1.229 sec/step)\n",
            "INFO:tensorflow:global step 2109: loss = 1.9037 (2.835 sec/step)\n",
            "I1209 11:34:34.043260 139904075134848 learning.py:507] global step 2109: loss = 1.9037 (2.835 sec/step)\n",
            "INFO:tensorflow:global step 2110: loss = 2.1383 (0.808 sec/step)\n",
            "I1209 11:34:35.247041 139904075134848 learning.py:507] global step 2110: loss = 2.1383 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 2111: loss = 1.6507 (0.790 sec/step)\n",
            "I1209 11:34:36.259533 139904075134848 learning.py:507] global step 2111: loss = 1.6507 (0.790 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 11:34:36.804867 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W1209 11:34:38.327721 139900518819584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Recording summary at step 2111.\n",
            "I1209 11:34:39.314528 139900543997696 supervisor.py:1050] Recording summary at step 2111.\n",
            "INFO:tensorflow:global step 2112: loss = 2.0134 (3.811 sec/step)\n",
            "I1209 11:34:40.172999 139904075134848 learning.py:507] global step 2112: loss = 2.0134 (3.811 sec/step)\n",
            "INFO:tensorflow:global step 2113: loss = 2.1741 (1.004 sec/step)\n",
            "I1209 11:34:42.018724 139904075134848 learning.py:507] global step 2113: loss = 2.1741 (1.004 sec/step)\n",
            "INFO:tensorflow:global step 2114: loss = 1.6112 (0.661 sec/step)\n",
            "I1209 11:34:43.080498 139904075134848 learning.py:507] global step 2114: loss = 1.6112 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2115: loss = 2.0542 (0.687 sec/step)\n",
            "I1209 11:34:43.770604 139904075134848 learning.py:507] global step 2115: loss = 2.0542 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 2116: loss = 2.5313 (2.783 sec/step)\n",
            "I1209 11:34:46.556149 139904075134848 learning.py:507] global step 2116: loss = 2.5313 (2.783 sec/step)\n",
            "INFO:tensorflow:global step 2117: loss = 2.2432 (0.600 sec/step)\n",
            "I1209 11:34:47.496838 139904075134848 learning.py:507] global step 2117: loss = 2.2432 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 2118: loss = 1.7150 (0.869 sec/step)\n",
            "I1209 11:34:48.731271 139904075134848 learning.py:507] global step 2118: loss = 1.7150 (0.869 sec/step)\n",
            "INFO:tensorflow:global step 2119: loss = 2.2152 (2.154 sec/step)\n",
            "I1209 11:34:51.035357 139904075134848 learning.py:507] global step 2119: loss = 2.2152 (2.154 sec/step)\n",
            "INFO:tensorflow:global step 2120: loss = 2.7115 (0.794 sec/step)\n",
            "I1209 11:34:51.949363 139904075134848 learning.py:507] global step 2120: loss = 2.7115 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 2121: loss = 2.2485 (0.799 sec/step)\n",
            "I1209 11:34:53.065406 139904075134848 learning.py:507] global step 2121: loss = 2.2485 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2122: loss = 2.1116 (1.878 sec/step)\n",
            "I1209 11:34:55.126139 139904075134848 learning.py:507] global step 2122: loss = 2.1116 (1.878 sec/step)\n",
            "INFO:tensorflow:global step 2123: loss = 2.4640 (2.212 sec/step)\n",
            "I1209 11:34:57.356384 139904075134848 learning.py:507] global step 2123: loss = 2.4640 (2.212 sec/step)\n",
            "INFO:tensorflow:global step 2124: loss = 2.2290 (0.833 sec/step)\n",
            "I1209 11:34:58.471170 139904075134848 learning.py:507] global step 2124: loss = 2.2290 (0.833 sec/step)\n",
            "INFO:tensorflow:global step 2125: loss = 2.0800 (0.720 sec/step)\n",
            "I1209 11:34:59.431963 139904075134848 learning.py:507] global step 2125: loss = 2.0800 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 2126: loss = 2.3746 (1.104 sec/step)\n",
            "I1209 11:35:00.776566 139904075134848 learning.py:507] global step 2126: loss = 2.3746 (1.104 sec/step)\n",
            "INFO:tensorflow:global step 2127: loss = 2.4059 (0.933 sec/step)\n",
            "I1209 11:35:02.440510 139904075134848 learning.py:507] global step 2127: loss = 2.4059 (0.933 sec/step)\n",
            "INFO:tensorflow:global step 2128: loss = 2.2244 (1.860 sec/step)\n",
            "I1209 11:35:04.306505 139904075134848 learning.py:507] global step 2128: loss = 2.2244 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 2129: loss = 2.5547 (0.615 sec/step)\n",
            "I1209 11:35:04.926548 139904075134848 learning.py:507] global step 2129: loss = 2.5547 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2130: loss = 2.4690 (1.058 sec/step)\n",
            "I1209 11:35:06.289602 139904075134848 learning.py:507] global step 2130: loss = 2.4690 (1.058 sec/step)\n",
            "INFO:tensorflow:global step 2131: loss = 1.8162 (1.688 sec/step)\n",
            "I1209 11:35:08.243392 139904075134848 learning.py:507] global step 2131: loss = 1.8162 (1.688 sec/step)\n",
            "INFO:tensorflow:global step 2132: loss = 2.2611 (0.730 sec/step)\n",
            "I1209 11:35:08.975670 139904075134848 learning.py:507] global step 2132: loss = 2.2611 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 2133: loss = 1.8892 (1.146 sec/step)\n",
            "I1209 11:35:10.370992 139904075134848 learning.py:507] global step 2133: loss = 1.8892 (1.146 sec/step)\n",
            "INFO:tensorflow:global step 2134: loss = 1.7136 (1.894 sec/step)\n",
            "I1209 11:35:12.524066 139904075134848 learning.py:507] global step 2134: loss = 1.7136 (1.894 sec/step)\n",
            "INFO:tensorflow:global step 2135: loss = 2.3435 (0.750 sec/step)\n",
            "I1209 11:35:13.367036 139904075134848 learning.py:507] global step 2135: loss = 2.3435 (0.750 sec/step)\n",
            "INFO:tensorflow:global step 2136: loss = 1.8658 (2.322 sec/step)\n",
            "I1209 11:35:15.929762 139904075134848 learning.py:507] global step 2136: loss = 1.8658 (2.322 sec/step)\n",
            "INFO:tensorflow:global step 2137: loss = 2.6502 (0.650 sec/step)\n",
            "I1209 11:35:16.582130 139904075134848 learning.py:507] global step 2137: loss = 2.6502 (0.650 sec/step)\n",
            "INFO:tensorflow:global step 2138: loss = 2.4340 (0.760 sec/step)\n",
            "I1209 11:35:17.344116 139904075134848 learning.py:507] global step 2138: loss = 2.4340 (0.760 sec/step)\n",
            "INFO:tensorflow:global step 2139: loss = 2.6670 (3.555 sec/step)\n",
            "I1209 11:35:20.900983 139904075134848 learning.py:507] global step 2139: loss = 2.6670 (3.555 sec/step)\n",
            "INFO:tensorflow:global step 2140: loss = 2.1685 (0.631 sec/step)\n",
            "I1209 11:35:21.533241 139904075134848 learning.py:507] global step 2140: loss = 2.1685 (0.631 sec/step)\n",
            "INFO:tensorflow:global step 2141: loss = 2.1668 (0.957 sec/step)\n",
            "I1209 11:35:22.754839 139904075134848 learning.py:507] global step 2141: loss = 2.1668 (0.957 sec/step)\n",
            "INFO:tensorflow:global step 2142: loss = 2.4798 (2.624 sec/step)\n",
            "I1209 11:35:25.757025 139904075134848 learning.py:507] global step 2142: loss = 2.4798 (2.624 sec/step)\n",
            "INFO:tensorflow:global step 2143: loss = 3.0591 (0.567 sec/step)\n",
            "I1209 11:35:26.326226 139904075134848 learning.py:507] global step 2143: loss = 3.0591 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 2144: loss = 2.4218 (2.075 sec/step)\n",
            "I1209 11:35:28.403411 139904075134848 learning.py:507] global step 2144: loss = 2.4218 (2.075 sec/step)\n",
            "INFO:tensorflow:global step 2145: loss = 2.1789 (0.766 sec/step)\n",
            "I1209 11:35:29.410807 139904075134848 learning.py:507] global step 2145: loss = 2.1789 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 2146: loss = 2.2983 (3.005 sec/step)\n",
            "I1209 11:35:32.598657 139904075134848 learning.py:507] global step 2146: loss = 2.2983 (3.005 sec/step)\n",
            "INFO:tensorflow:global step 2147: loss = 1.7391 (0.701 sec/step)\n",
            "I1209 11:35:33.301982 139904075134848 learning.py:507] global step 2147: loss = 1.7391 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 2148: loss = 2.5510 (1.935 sec/step)\n",
            "I1209 11:35:35.239202 139904075134848 learning.py:507] global step 2148: loss = 2.5510 (1.935 sec/step)\n",
            "INFO:tensorflow:global step 2149: loss = 2.5030 (0.676 sec/step)\n",
            "I1209 11:35:35.917047 139904075134848 learning.py:507] global step 2149: loss = 2.5030 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 2150: loss = 2.1886 (1.968 sec/step)\n",
            "I1209 11:35:37.886986 139904075134848 learning.py:507] global step 2150: loss = 2.1886 (1.968 sec/step)\n",
            "INFO:tensorflow:global step 2151: loss = 1.8177 (0.763 sec/step)\n",
            "I1209 11:35:38.897758 139904075134848 learning.py:507] global step 2151: loss = 1.8177 (0.763 sec/step)\n",
            "INFO:tensorflow:global step 2152: loss = 1.7663 (2.725 sec/step)\n",
            "I1209 11:35:41.644807 139904075134848 learning.py:507] global step 2152: loss = 1.7663 (2.725 sec/step)\n",
            "INFO:tensorflow:global step 2153: loss = 1.4960 (0.674 sec/step)\n",
            "I1209 11:35:42.320580 139904075134848 learning.py:507] global step 2153: loss = 1.4960 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2154: loss = 2.3598 (0.821 sec/step)\n",
            "I1209 11:35:43.719516 139904075134848 learning.py:507] global step 2154: loss = 2.3598 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 2155: loss = 2.8926 (1.429 sec/step)\n",
            "I1209 11:35:45.670969 139904075134848 learning.py:507] global step 2155: loss = 2.8926 (1.429 sec/step)\n",
            "INFO:tensorflow:global step 2156: loss = 2.3413 (2.018 sec/step)\n",
            "I1209 11:35:47.931506 139904075134848 learning.py:507] global step 2156: loss = 2.3413 (2.018 sec/step)\n",
            "INFO:tensorflow:global step 2157: loss = 2.7017 (0.794 sec/step)\n",
            "I1209 11:35:49.004780 139904075134848 learning.py:507] global step 2157: loss = 2.7017 (0.794 sec/step)\n",
            "INFO:tensorflow:global step 2158: loss = 1.9388 (2.033 sec/step)\n",
            "I1209 11:35:51.186521 139904075134848 learning.py:507] global step 2158: loss = 1.9388 (2.033 sec/step)\n",
            "INFO:tensorflow:global step 2159: loss = 2.1168 (0.660 sec/step)\n",
            "I1209 11:35:51.848414 139904075134848 learning.py:507] global step 2159: loss = 2.1168 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2160: loss = 2.8664 (1.177 sec/step)\n",
            "I1209 11:35:53.068346 139904075134848 learning.py:507] global step 2160: loss = 2.8664 (1.177 sec/step)\n",
            "INFO:tensorflow:global step 2161: loss = 1.6922 (2.018 sec/step)\n",
            "I1209 11:35:55.397222 139904075134848 learning.py:507] global step 2161: loss = 1.6922 (2.018 sec/step)\n",
            "INFO:tensorflow:global step 2162: loss = 2.7875 (0.730 sec/step)\n",
            "I1209 11:35:56.355530 139904075134848 learning.py:507] global step 2162: loss = 2.7875 (0.730 sec/step)\n",
            "INFO:tensorflow:global step 2163: loss = 1.8463 (0.740 sec/step)\n",
            "I1209 11:35:57.547492 139904075134848 learning.py:507] global step 2163: loss = 1.8463 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 2164: loss = 2.5407 (0.715 sec/step)\n",
            "I1209 11:35:58.629315 139904075134848 learning.py:507] global step 2164: loss = 2.5407 (0.715 sec/step)\n",
            "INFO:tensorflow:global step 2165: loss = 2.0406 (1.509 sec/step)\n",
            "I1209 11:36:00.351897 139904075134848 learning.py:507] global step 2165: loss = 2.0406 (1.509 sec/step)\n",
            "INFO:tensorflow:global step 2166: loss = 2.9491 (0.663 sec/step)\n",
            "I1209 11:36:01.295839 139904075134848 learning.py:507] global step 2166: loss = 2.9491 (0.663 sec/step)\n",
            "INFO:tensorflow:global step 2167: loss = 1.7221 (0.732 sec/step)\n",
            "I1209 11:36:02.387260 139904075134848 learning.py:507] global step 2167: loss = 1.7221 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 2168: loss = 3.1561 (0.757 sec/step)\n",
            "I1209 11:36:03.599532 139904075134848 learning.py:507] global step 2168: loss = 3.1561 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 2169: loss = 1.9323 (0.768 sec/step)\n",
            "I1209 11:36:04.476658 139904075134848 learning.py:507] global step 2169: loss = 1.9323 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 2170: loss = 2.2402 (1.817 sec/step)\n",
            "I1209 11:36:06.365898 139904075134848 learning.py:507] global step 2170: loss = 2.2402 (1.817 sec/step)\n",
            "INFO:tensorflow:global step 2171: loss = 2.0722 (1.535 sec/step)\n",
            "I1209 11:36:07.902724 139904075134848 learning.py:507] global step 2171: loss = 2.0722 (1.535 sec/step)\n",
            "INFO:tensorflow:global step 2172: loss = 2.0293 (0.696 sec/step)\n",
            "I1209 11:36:08.600765 139904075134848 learning.py:507] global step 2172: loss = 2.0293 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 2173: loss = 2.0532 (2.808 sec/step)\n",
            "I1209 11:36:11.410331 139904075134848 learning.py:507] global step 2173: loss = 2.0532 (2.808 sec/step)\n",
            "INFO:tensorflow:global step 2174: loss = 2.8179 (0.691 sec/step)\n",
            "I1209 11:36:12.381491 139904075134848 learning.py:507] global step 2174: loss = 2.8179 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 2175: loss = 2.0684 (1.712 sec/step)\n",
            "I1209 11:36:14.136959 139904075134848 learning.py:507] global step 2175: loss = 2.0684 (1.712 sec/step)\n",
            "INFO:tensorflow:global step 2176: loss = 2.4007 (0.743 sec/step)\n",
            "I1209 11:36:15.177842 139904075134848 learning.py:507] global step 2176: loss = 2.4007 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2177: loss = 2.1776 (0.908 sec/step)\n",
            "I1209 11:36:16.584497 139904075134848 learning.py:507] global step 2177: loss = 2.1776 (0.908 sec/step)\n",
            "INFO:tensorflow:global step 2178: loss = 2.7803 (1.568 sec/step)\n",
            "I1209 11:36:18.158210 139904075134848 learning.py:507] global step 2178: loss = 2.7803 (1.568 sec/step)\n",
            "INFO:tensorflow:global step 2179: loss = 1.8053 (0.641 sec/step)\n",
            "I1209 11:36:18.801466 139904075134848 learning.py:507] global step 2179: loss = 1.8053 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 2180: loss = 1.9961 (1.935 sec/step)\n",
            "I1209 11:36:20.738725 139904075134848 learning.py:507] global step 2180: loss = 1.9961 (1.935 sec/step)\n",
            "INFO:tensorflow:global step 2181: loss = 2.3638 (0.615 sec/step)\n",
            "I1209 11:36:21.355860 139904075134848 learning.py:507] global step 2181: loss = 2.3638 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2182: loss = 2.7212 (2.334 sec/step)\n",
            "I1209 11:36:23.692351 139904075134848 learning.py:507] global step 2182: loss = 2.7212 (2.334 sec/step)\n",
            "INFO:tensorflow:global step 2183: loss = 2.3620 (0.859 sec/step)\n",
            "I1209 11:36:24.755623 139904075134848 learning.py:507] global step 2183: loss = 2.3620 (0.859 sec/step)\n",
            "INFO:tensorflow:global step 2184: loss = 1.6099 (1.784 sec/step)\n",
            "I1209 11:36:26.605339 139904075134848 learning.py:507] global step 2184: loss = 1.6099 (1.784 sec/step)\n",
            "INFO:tensorflow:global step 2185: loss = 2.2553 (0.690 sec/step)\n",
            "I1209 11:36:27.407051 139904075134848 learning.py:507] global step 2185: loss = 2.2553 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 2186: loss = 2.0005 (1.382 sec/step)\n",
            "I1209 11:36:29.234800 139904075134848 learning.py:507] global step 2186: loss = 2.0005 (1.382 sec/step)\n",
            "INFO:tensorflow:global step 2187: loss = 2.1199 (0.693 sec/step)\n",
            "I1209 11:36:30.197589 139904075134848 learning.py:507] global step 2187: loss = 2.1199 (0.693 sec/step)\n",
            "INFO:tensorflow:global step 2188: loss = 2.4172 (0.766 sec/step)\n",
            "I1209 11:36:31.129659 139904075134848 learning.py:507] global step 2188: loss = 2.4172 (0.766 sec/step)\n",
            "INFO:tensorflow:global step 2189: loss = 1.9549 (1.184 sec/step)\n",
            "I1209 11:36:32.365791 139904075134848 learning.py:507] global step 2189: loss = 1.9549 (1.184 sec/step)\n",
            "INFO:tensorflow:global step 2190: loss = 2.3545 (2.153 sec/step)\n",
            "I1209 11:36:34.806692 139904075134848 learning.py:507] global step 2190: loss = 2.3545 (2.153 sec/step)\n",
            "INFO:tensorflow:global step 2191: loss = 1.6391 (0.659 sec/step)\n",
            "I1209 11:36:35.756547 139904075134848 learning.py:507] global step 2191: loss = 1.6391 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2192: loss = 2.6970 (3.380 sec/step)\n",
            "I1209 11:36:39.299491 139904075134848 learning.py:507] global step 2192: loss = 2.6970 (3.380 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2192.\n",
            "I1209 11:36:39.310714 139900543997696 supervisor.py:1050] Recording summary at step 2192.\n",
            "INFO:tensorflow:global step 2193: loss = 2.2679 (0.712 sec/step)\n",
            "I1209 11:36:40.013976 139904075134848 learning.py:507] global step 2193: loss = 2.2679 (0.712 sec/step)\n",
            "INFO:tensorflow:global step 2194: loss = 2.7221 (2.121 sec/step)\n",
            "I1209 11:36:42.137245 139904075134848 learning.py:507] global step 2194: loss = 2.7221 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 2195: loss = 2.5870 (0.727 sec/step)\n",
            "I1209 11:36:42.865758 139904075134848 learning.py:507] global step 2195: loss = 2.5870 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 2196: loss = 2.2170 (1.735 sec/step)\n",
            "I1209 11:36:44.767449 139904075134848 learning.py:507] global step 2196: loss = 2.2170 (1.735 sec/step)\n",
            "INFO:tensorflow:global step 2197: loss = 1.9985 (0.676 sec/step)\n",
            "I1209 11:36:45.792371 139904075134848 learning.py:507] global step 2197: loss = 1.9985 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 2198: loss = 1.8235 (1.581 sec/step)\n",
            "I1209 11:36:47.525624 139904075134848 learning.py:507] global step 2198: loss = 1.8235 (1.581 sec/step)\n",
            "INFO:tensorflow:global step 2199: loss = 1.8677 (0.893 sec/step)\n",
            "I1209 11:36:48.692193 139904075134848 learning.py:507] global step 2199: loss = 1.8677 (0.893 sec/step)\n",
            "INFO:tensorflow:global step 2200: loss = 1.9731 (0.680 sec/step)\n",
            "I1209 11:36:49.437806 139904075134848 learning.py:507] global step 2200: loss = 1.9731 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2201: loss = 2.2222 (1.454 sec/step)\n",
            "I1209 11:36:51.116388 139904075134848 learning.py:507] global step 2201: loss = 2.2222 (1.454 sec/step)\n",
            "INFO:tensorflow:global step 2202: loss = 1.6113 (0.839 sec/step)\n",
            "I1209 11:36:51.997841 139904075134848 learning.py:507] global step 2202: loss = 1.6113 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 2203: loss = 2.7067 (1.993 sec/step)\n",
            "I1209 11:36:54.194655 139904075134848 learning.py:507] global step 2203: loss = 2.7067 (1.993 sec/step)\n",
            "INFO:tensorflow:global step 2204: loss = 2.0341 (0.796 sec/step)\n",
            "I1209 11:36:55.129008 139904075134848 learning.py:507] global step 2204: loss = 2.0341 (0.796 sec/step)\n",
            "INFO:tensorflow:global step 2205: loss = 1.6144 (0.866 sec/step)\n",
            "I1209 11:36:56.148430 139904075134848 learning.py:507] global step 2205: loss = 1.6144 (0.866 sec/step)\n",
            "INFO:tensorflow:global step 2206: loss = 2.4367 (1.825 sec/step)\n",
            "I1209 11:36:57.975525 139904075134848 learning.py:507] global step 2206: loss = 2.4367 (1.825 sec/step)\n",
            "INFO:tensorflow:global step 2207: loss = 2.1033 (0.580 sec/step)\n",
            "I1209 11:36:58.557642 139904075134848 learning.py:507] global step 2207: loss = 2.1033 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 2208: loss = 2.2008 (0.653 sec/step)\n",
            "I1209 11:36:59.212588 139904075134848 learning.py:507] global step 2208: loss = 2.2008 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2209: loss = 2.3329 (2.950 sec/step)\n",
            "I1209 11:37:02.164376 139904075134848 learning.py:507] global step 2209: loss = 2.3329 (2.950 sec/step)\n",
            "INFO:tensorflow:global step 2210: loss = 2.2905 (1.268 sec/step)\n",
            "I1209 11:37:03.434151 139904075134848 learning.py:507] global step 2210: loss = 2.2905 (1.268 sec/step)\n",
            "INFO:tensorflow:global step 2211: loss = 1.9692 (0.694 sec/step)\n",
            "I1209 11:37:04.130295 139904075134848 learning.py:507] global step 2211: loss = 1.9692 (0.694 sec/step)\n",
            "INFO:tensorflow:global step 2212: loss = 2.2979 (1.965 sec/step)\n",
            "I1209 11:37:06.100697 139904075134848 learning.py:507] global step 2212: loss = 2.2979 (1.965 sec/step)\n",
            "INFO:tensorflow:global step 2213: loss = 1.9915 (0.835 sec/step)\n",
            "I1209 11:37:06.992722 139904075134848 learning.py:507] global step 2213: loss = 1.9915 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 2214: loss = 2.4315 (0.946 sec/step)\n",
            "I1209 11:37:08.410833 139904075134848 learning.py:507] global step 2214: loss = 2.4315 (0.946 sec/step)\n",
            "INFO:tensorflow:global step 2215: loss = 1.8104 (0.682 sec/step)\n",
            "I1209 11:37:09.243754 139904075134848 learning.py:507] global step 2215: loss = 1.8104 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 2216: loss = 2.1711 (2.405 sec/step)\n",
            "I1209 11:37:11.651002 139904075134848 learning.py:507] global step 2216: loss = 2.1711 (2.405 sec/step)\n",
            "INFO:tensorflow:global step 2217: loss = 2.0175 (0.620 sec/step)\n",
            "I1209 11:37:12.272737 139904075134848 learning.py:507] global step 2217: loss = 2.0175 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 2218: loss = 1.8490 (1.967 sec/step)\n",
            "I1209 11:37:14.242107 139904075134848 learning.py:507] global step 2218: loss = 1.8490 (1.967 sec/step)\n",
            "INFO:tensorflow:global step 2219: loss = 1.9374 (0.821 sec/step)\n",
            "I1209 11:37:15.358803 139904075134848 learning.py:507] global step 2219: loss = 1.9374 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 2220: loss = 1.8194 (0.734 sec/step)\n",
            "I1209 11:37:16.612149 139904075134848 learning.py:507] global step 2220: loss = 1.8194 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 2221: loss = 1.9857 (0.708 sec/step)\n",
            "I1209 11:37:17.657238 139904075134848 learning.py:507] global step 2221: loss = 1.9857 (0.708 sec/step)\n",
            "INFO:tensorflow:global step 2222: loss = 2.1102 (0.753 sec/step)\n",
            "I1209 11:37:18.699455 139904075134848 learning.py:507] global step 2222: loss = 2.1102 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 2223: loss = 1.5464 (2.272 sec/step)\n",
            "I1209 11:37:20.996652 139904075134848 learning.py:507] global step 2223: loss = 1.5464 (2.272 sec/step)\n",
            "INFO:tensorflow:global step 2224: loss = 3.0607 (0.779 sec/step)\n",
            "I1209 11:37:21.991847 139904075134848 learning.py:507] global step 2224: loss = 3.0607 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 2225: loss = 2.0210 (1.313 sec/step)\n",
            "I1209 11:37:23.357123 139904075134848 learning.py:507] global step 2225: loss = 2.0210 (1.313 sec/step)\n",
            "INFO:tensorflow:global step 2226: loss = 1.7917 (0.808 sec/step)\n",
            "I1209 11:37:24.368755 139904075134848 learning.py:507] global step 2226: loss = 1.7917 (0.808 sec/step)\n",
            "INFO:tensorflow:global step 2227: loss = 2.3586 (0.844 sec/step)\n",
            "I1209 11:37:25.668006 139904075134848 learning.py:507] global step 2227: loss = 2.3586 (0.844 sec/step)\n",
            "INFO:tensorflow:global step 2228: loss = 2.2749 (0.680 sec/step)\n",
            "I1209 11:37:26.506654 139904075134848 learning.py:507] global step 2228: loss = 2.2749 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2229: loss = 2.3508 (1.918 sec/step)\n",
            "I1209 11:37:28.426379 139904075134848 learning.py:507] global step 2229: loss = 2.3508 (1.918 sec/step)\n",
            "INFO:tensorflow:global step 2230: loss = 2.4712 (0.782 sec/step)\n",
            "I1209 11:37:29.363926 139904075134848 learning.py:507] global step 2230: loss = 2.4712 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 2231: loss = 1.7057 (1.877 sec/step)\n",
            "I1209 11:37:31.417489 139904075134848 learning.py:507] global step 2231: loss = 1.7057 (1.877 sec/step)\n",
            "INFO:tensorflow:global step 2232: loss = 2.2827 (0.692 sec/step)\n",
            "I1209 11:37:32.111265 139904075134848 learning.py:507] global step 2232: loss = 2.2827 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 2233: loss = 2.1018 (0.953 sec/step)\n",
            "I1209 11:37:33.189788 139904075134848 learning.py:507] global step 2233: loss = 2.1018 (0.953 sec/step)\n",
            "INFO:tensorflow:global step 2234: loss = 1.7591 (2.234 sec/step)\n",
            "I1209 11:37:35.449853 139904075134848 learning.py:507] global step 2234: loss = 1.7591 (2.234 sec/step)\n",
            "INFO:tensorflow:global step 2235: loss = 1.6942 (0.918 sec/step)\n",
            "I1209 11:37:36.398880 139904075134848 learning.py:507] global step 2235: loss = 1.6942 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 2236: loss = 1.6209 (2.392 sec/step)\n",
            "I1209 11:37:38.999922 139904075134848 learning.py:507] global step 2236: loss = 1.6209 (2.392 sec/step)\n",
            "INFO:tensorflow:global step 2237: loss = 2.6986 (0.714 sec/step)\n",
            "I1209 11:37:39.877239 139904075134848 learning.py:507] global step 2237: loss = 2.6986 (0.714 sec/step)\n",
            "INFO:tensorflow:global step 2238: loss = 2.0365 (1.512 sec/step)\n",
            "I1209 11:37:41.670407 139904075134848 learning.py:507] global step 2238: loss = 2.0365 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 2239: loss = 2.0630 (0.599 sec/step)\n",
            "I1209 11:37:42.270963 139904075134848 learning.py:507] global step 2239: loss = 2.0630 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 2240: loss = 2.5527 (2.408 sec/step)\n",
            "I1209 11:37:44.680429 139904075134848 learning.py:507] global step 2240: loss = 2.5527 (2.408 sec/step)\n",
            "INFO:tensorflow:global step 2241: loss = 1.7781 (0.624 sec/step)\n",
            "I1209 11:37:45.306129 139904075134848 learning.py:507] global step 2241: loss = 1.7781 (0.624 sec/step)\n",
            "INFO:tensorflow:global step 2242: loss = 1.8523 (0.691 sec/step)\n",
            "I1209 11:37:45.999314 139904075134848 learning.py:507] global step 2242: loss = 1.8523 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 2243: loss = 2.5045 (2.843 sec/step)\n",
            "I1209 11:37:48.844431 139904075134848 learning.py:507] global step 2243: loss = 2.5045 (2.843 sec/step)\n",
            "INFO:tensorflow:global step 2244: loss = 1.8874 (0.842 sec/step)\n",
            "I1209 11:37:49.865412 139904075134848 learning.py:507] global step 2244: loss = 1.8874 (0.842 sec/step)\n",
            "INFO:tensorflow:global step 2245: loss = 2.4636 (0.690 sec/step)\n",
            "I1209 11:37:50.943399 139904075134848 learning.py:507] global step 2245: loss = 2.4636 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 2246: loss = 1.6048 (2.300 sec/step)\n",
            "I1209 11:37:53.379175 139904075134848 learning.py:507] global step 2246: loss = 1.6048 (2.300 sec/step)\n",
            "INFO:tensorflow:global step 2247: loss = 2.1069 (0.585 sec/step)\n",
            "I1209 11:37:53.965970 139904075134848 learning.py:507] global step 2247: loss = 2.1069 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2248: loss = 2.3896 (0.622 sec/step)\n",
            "I1209 11:37:54.589713 139904075134848 learning.py:507] global step 2248: loss = 2.3896 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 2249: loss = 1.9905 (1.242 sec/step)\n",
            "I1209 11:37:56.170281 139904075134848 learning.py:507] global step 2249: loss = 1.9905 (1.242 sec/step)\n",
            "INFO:tensorflow:global step 2250: loss = 2.4811 (2.347 sec/step)\n",
            "I1209 11:37:58.646711 139904075134848 learning.py:507] global step 2250: loss = 2.4811 (2.347 sec/step)\n",
            "INFO:tensorflow:global step 2251: loss = 2.3127 (0.702 sec/step)\n",
            "I1209 11:37:59.480837 139904075134848 learning.py:507] global step 2251: loss = 2.3127 (0.702 sec/step)\n",
            "INFO:tensorflow:global step 2252: loss = 1.8363 (1.879 sec/step)\n",
            "I1209 11:38:01.566088 139904075134848 learning.py:507] global step 2252: loss = 1.8363 (1.879 sec/step)\n",
            "INFO:tensorflow:global step 2253: loss = 2.0301 (0.669 sec/step)\n",
            "I1209 11:38:02.236793 139904075134848 learning.py:507] global step 2253: loss = 2.0301 (0.669 sec/step)\n",
            "INFO:tensorflow:global step 2254: loss = 1.8455 (1.273 sec/step)\n",
            "I1209 11:38:03.811848 139904075134848 learning.py:507] global step 2254: loss = 1.8455 (1.273 sec/step)\n",
            "INFO:tensorflow:global step 2255: loss = 2.1441 (2.546 sec/step)\n",
            "I1209 11:38:06.360319 139904075134848 learning.py:507] global step 2255: loss = 2.1441 (2.546 sec/step)\n",
            "INFO:tensorflow:global step 2256: loss = 1.9036 (0.793 sec/step)\n",
            "I1209 11:38:07.305792 139904075134848 learning.py:507] global step 2256: loss = 1.9036 (0.793 sec/step)\n",
            "INFO:tensorflow:global step 2257: loss = 1.8498 (2.654 sec/step)\n",
            "I1209 11:38:10.301442 139904075134848 learning.py:507] global step 2257: loss = 1.8498 (2.654 sec/step)\n",
            "INFO:tensorflow:global step 2258: loss = 2.1459 (0.615 sec/step)\n",
            "I1209 11:38:10.918227 139904075134848 learning.py:507] global step 2258: loss = 2.1459 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 2259: loss = 2.1464 (1.902 sec/step)\n",
            "I1209 11:38:12.821858 139904075134848 learning.py:507] global step 2259: loss = 2.1464 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 2260: loss = 3.0837 (0.692 sec/step)\n",
            "I1209 11:38:13.515702 139904075134848 learning.py:507] global step 2260: loss = 3.0837 (0.692 sec/step)\n",
            "INFO:tensorflow:global step 2261: loss = 2.0213 (1.257 sec/step)\n",
            "I1209 11:38:14.811386 139904075134848 learning.py:507] global step 2261: loss = 2.0213 (1.257 sec/step)\n",
            "INFO:tensorflow:global step 2262: loss = 1.7231 (2.079 sec/step)\n",
            "I1209 11:38:17.288362 139904075134848 learning.py:507] global step 2262: loss = 1.7231 (2.079 sec/step)\n",
            "INFO:tensorflow:global step 2263: loss = 2.0332 (0.630 sec/step)\n",
            "I1209 11:38:17.919970 139904075134848 learning.py:507] global step 2263: loss = 2.0332 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2264: loss = 2.3182 (1.895 sec/step)\n",
            "I1209 11:38:19.826493 139904075134848 learning.py:507] global step 2264: loss = 2.3182 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 2265: loss = 2.0492 (0.637 sec/step)\n",
            "I1209 11:38:20.481463 139904075134848 learning.py:507] global step 2265: loss = 2.0492 (0.637 sec/step)\n",
            "INFO:tensorflow:global step 2266: loss = 1.8789 (2.198 sec/step)\n",
            "I1209 11:38:22.680888 139904075134848 learning.py:507] global step 2266: loss = 1.8789 (2.198 sec/step)\n",
            "INFO:tensorflow:global step 2267: loss = 2.0111 (0.784 sec/step)\n",
            "I1209 11:38:23.748074 139904075134848 learning.py:507] global step 2267: loss = 2.0111 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 2268: loss = 2.5780 (2.184 sec/step)\n",
            "I1209 11:38:25.933959 139904075134848 learning.py:507] global step 2268: loss = 2.5780 (2.184 sec/step)\n",
            "INFO:tensorflow:global step 2269: loss = 1.9911 (0.751 sec/step)\n",
            "I1209 11:38:26.686658 139904075134848 learning.py:507] global step 2269: loss = 1.9911 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 2270: loss = 2.4650 (1.820 sec/step)\n",
            "I1209 11:38:28.663835 139904075134848 learning.py:507] global step 2270: loss = 2.4650 (1.820 sec/step)\n",
            "INFO:tensorflow:global step 2271: loss = 1.7040 (0.738 sec/step)\n",
            "I1209 11:38:29.404651 139904075134848 learning.py:507] global step 2271: loss = 1.7040 (0.738 sec/step)\n",
            "INFO:tensorflow:global step 2272: loss = 2.3259 (2.218 sec/step)\n",
            "I1209 11:38:31.624789 139904075134848 learning.py:507] global step 2272: loss = 2.3259 (2.218 sec/step)\n",
            "INFO:tensorflow:global step 2273: loss = 2.3336 (0.783 sec/step)\n",
            "I1209 11:38:32.789872 139904075134848 learning.py:507] global step 2273: loss = 2.3336 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 2274: loss = 1.8025 (0.602 sec/step)\n",
            "I1209 11:38:33.463274 139904075134848 learning.py:507] global step 2274: loss = 1.8025 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2275: loss = 2.5352 (0.722 sec/step)\n",
            "I1209 11:38:34.669780 139904075134848 learning.py:507] global step 2275: loss = 2.5352 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2276: loss = 1.8188 (3.981 sec/step)\n",
            "I1209 11:38:38.873951 139904075134848 learning.py:507] global step 2276: loss = 1.8188 (3.981 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2276.\n",
            "I1209 11:38:39.301522 139900543997696 supervisor.py:1050] Recording summary at step 2276.\n",
            "INFO:tensorflow:global step 2277: loss = 1.7987 (0.753 sec/step)\n",
            "I1209 11:38:39.630156 139904075134848 learning.py:507] global step 2277: loss = 1.7987 (0.753 sec/step)\n",
            "INFO:tensorflow:global step 2278: loss = 2.9931 (2.220 sec/step)\n",
            "I1209 11:38:41.851534 139904075134848 learning.py:507] global step 2278: loss = 2.9931 (2.220 sec/step)\n",
            "INFO:tensorflow:global step 2279: loss = 2.2874 (0.647 sec/step)\n",
            "I1209 11:38:42.500860 139904075134848 learning.py:507] global step 2279: loss = 2.2874 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2280: loss = 2.2267 (2.227 sec/step)\n",
            "I1209 11:38:44.729379 139904075134848 learning.py:507] global step 2280: loss = 2.2267 (2.227 sec/step)\n",
            "INFO:tensorflow:global step 2281: loss = 2.3032 (0.839 sec/step)\n",
            "I1209 11:38:45.911654 139904075134848 learning.py:507] global step 2281: loss = 2.3032 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 2282: loss = 2.3941 (0.745 sec/step)\n",
            "I1209 11:38:46.999281 139904075134848 learning.py:507] global step 2282: loss = 2.3941 (0.745 sec/step)\n",
            "INFO:tensorflow:global step 2283: loss = 2.6249 (0.659 sec/step)\n",
            "I1209 11:38:47.764599 139904075134848 learning.py:507] global step 2283: loss = 2.6249 (0.659 sec/step)\n",
            "INFO:tensorflow:global step 2284: loss = 1.8588 (2.342 sec/step)\n",
            "I1209 11:38:50.173552 139904075134848 learning.py:507] global step 2284: loss = 1.8588 (2.342 sec/step)\n",
            "INFO:tensorflow:global step 2285: loss = 2.6720 (1.956 sec/step)\n",
            "I1209 11:38:52.131532 139904075134848 learning.py:507] global step 2285: loss = 2.6720 (1.956 sec/step)\n",
            "INFO:tensorflow:global step 2286: loss = 1.2950 (0.668 sec/step)\n",
            "I1209 11:38:52.802023 139904075134848 learning.py:507] global step 2286: loss = 1.2950 (0.668 sec/step)\n",
            "INFO:tensorflow:global step 2287: loss = 1.9390 (0.970 sec/step)\n",
            "I1209 11:38:53.779016 139904075134848 learning.py:507] global step 2287: loss = 1.9390 (0.970 sec/step)\n",
            "INFO:tensorflow:global step 2288: loss = 1.7766 (2.467 sec/step)\n",
            "I1209 11:38:56.328387 139904075134848 learning.py:507] global step 2288: loss = 1.7766 (2.467 sec/step)\n",
            "INFO:tensorflow:global step 2289: loss = 2.3198 (0.837 sec/step)\n",
            "I1209 11:38:57.227147 139904075134848 learning.py:507] global step 2289: loss = 2.3198 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 2290: loss = 2.1586 (0.773 sec/step)\n",
            "I1209 11:38:58.269603 139904075134848 learning.py:507] global step 2290: loss = 2.1586 (0.773 sec/step)\n",
            "INFO:tensorflow:global step 2291: loss = 2.0974 (1.080 sec/step)\n",
            "I1209 11:38:59.578067 139904075134848 learning.py:507] global step 2291: loss = 2.0974 (1.080 sec/step)\n",
            "INFO:tensorflow:global step 2292: loss = 2.6344 (1.915 sec/step)\n",
            "I1209 11:39:01.791567 139904075134848 learning.py:507] global step 2292: loss = 2.6344 (1.915 sec/step)\n",
            "INFO:tensorflow:global step 2293: loss = 2.2664 (0.633 sec/step)\n",
            "I1209 11:39:02.426278 139904075134848 learning.py:507] global step 2293: loss = 2.2664 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2294: loss = 2.2668 (1.797 sec/step)\n",
            "I1209 11:39:04.442496 139904075134848 learning.py:507] global step 2294: loss = 2.2668 (1.797 sec/step)\n",
            "INFO:tensorflow:global step 2295: loss = 2.8286 (1.508 sec/step)\n",
            "I1209 11:39:05.952061 139904075134848 learning.py:507] global step 2295: loss = 2.8286 (1.508 sec/step)\n",
            "INFO:tensorflow:global step 2296: loss = 1.8524 (0.594 sec/step)\n",
            "I1209 11:39:06.547653 139904075134848 learning.py:507] global step 2296: loss = 1.8524 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 2297: loss = 2.2532 (2.265 sec/step)\n",
            "I1209 11:39:08.813990 139904075134848 learning.py:507] global step 2297: loss = 2.2532 (2.265 sec/step)\n",
            "INFO:tensorflow:global step 2298: loss = 2.0439 (0.653 sec/step)\n",
            "I1209 11:39:09.469013 139904075134848 learning.py:507] global step 2298: loss = 2.0439 (0.653 sec/step)\n",
            "INFO:tensorflow:global step 2299: loss = 2.3778 (2.135 sec/step)\n",
            "I1209 11:39:11.605908 139904075134848 learning.py:507] global step 2299: loss = 2.3778 (2.135 sec/step)\n",
            "INFO:tensorflow:global step 2300: loss = 2.0463 (0.683 sec/step)\n",
            "I1209 11:39:12.290858 139904075134848 learning.py:507] global step 2300: loss = 2.0463 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 2301: loss = 3.4549 (2.201 sec/step)\n",
            "I1209 11:39:14.494114 139904075134848 learning.py:507] global step 2301: loss = 3.4549 (2.201 sec/step)\n",
            "INFO:tensorflow:global step 2302: loss = 2.5231 (0.689 sec/step)\n",
            "I1209 11:39:15.402698 139904075134848 learning.py:507] global step 2302: loss = 2.5231 (0.689 sec/step)\n",
            "INFO:tensorflow:global step 2303: loss = 2.1570 (0.705 sec/step)\n",
            "I1209 11:39:16.677755 139904075134848 learning.py:507] global step 2303: loss = 2.1570 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 2304: loss = 2.0198 (1.362 sec/step)\n",
            "I1209 11:39:18.208889 139904075134848 learning.py:507] global step 2304: loss = 2.0198 (1.362 sec/step)\n",
            "INFO:tensorflow:global step 2305: loss = 1.8370 (0.858 sec/step)\n",
            "I1209 11:39:19.259273 139904075134848 learning.py:507] global step 2305: loss = 1.8370 (0.858 sec/step)\n",
            "INFO:tensorflow:global step 2306: loss = 2.3763 (2.352 sec/step)\n",
            "I1209 11:39:21.628620 139904075134848 learning.py:507] global step 2306: loss = 2.3763 (2.352 sec/step)\n",
            "INFO:tensorflow:global step 2307: loss = 2.0743 (0.674 sec/step)\n",
            "I1209 11:39:22.893115 139904075134848 learning.py:507] global step 2307: loss = 2.0743 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2308: loss = 2.2210 (0.913 sec/step)\n",
            "I1209 11:39:23.854508 139904075134848 learning.py:507] global step 2308: loss = 2.2210 (0.913 sec/step)\n",
            "INFO:tensorflow:global step 2309: loss = 1.9275 (2.006 sec/step)\n",
            "I1209 11:39:25.879986 139904075134848 learning.py:507] global step 2309: loss = 1.9275 (2.006 sec/step)\n",
            "INFO:tensorflow:global step 2310: loss = 1.8863 (0.799 sec/step)\n",
            "I1209 11:39:26.892379 139904075134848 learning.py:507] global step 2310: loss = 1.8863 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2311: loss = 1.9850 (0.906 sec/step)\n",
            "I1209 11:39:28.172793 139904075134848 learning.py:507] global step 2311: loss = 1.9850 (0.906 sec/step)\n",
            "INFO:tensorflow:global step 2312: loss = 1.3682 (1.723 sec/step)\n",
            "I1209 11:39:30.147086 139904075134848 learning.py:507] global step 2312: loss = 1.3682 (1.723 sec/step)\n",
            "INFO:tensorflow:global step 2313: loss = 2.1443 (0.643 sec/step)\n",
            "I1209 11:39:30.792554 139904075134848 learning.py:507] global step 2313: loss = 2.1443 (0.643 sec/step)\n",
            "INFO:tensorflow:global step 2314: loss = 2.6326 (2.161 sec/step)\n",
            "I1209 11:39:32.957165 139904075134848 learning.py:507] global step 2314: loss = 2.6326 (2.161 sec/step)\n",
            "INFO:tensorflow:global step 2315: loss = 2.0177 (0.719 sec/step)\n",
            "I1209 11:39:34.026452 139904075134848 learning.py:507] global step 2315: loss = 2.0177 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 2316: loss = 1.9211 (2.074 sec/step)\n",
            "I1209 11:39:36.161432 139904075134848 learning.py:507] global step 2316: loss = 1.9211 (2.074 sec/step)\n",
            "INFO:tensorflow:global step 2317: loss = 1.8259 (0.713 sec/step)\n",
            "I1209 11:39:36.876598 139904075134848 learning.py:507] global step 2317: loss = 1.8259 (0.713 sec/step)\n",
            "INFO:tensorflow:global step 2318: loss = 2.2078 (1.955 sec/step)\n",
            "I1209 11:39:38.834209 139904075134848 learning.py:507] global step 2318: loss = 2.2078 (1.955 sec/step)\n",
            "INFO:tensorflow:global step 2319: loss = 1.9514 (0.795 sec/step)\n",
            "I1209 11:39:39.677252 139904075134848 learning.py:507] global step 2319: loss = 1.9514 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 2320: loss = 2.3301 (0.784 sec/step)\n",
            "I1209 11:39:40.715178 139904075134848 learning.py:507] global step 2320: loss = 2.3301 (0.784 sec/step)\n",
            "INFO:tensorflow:global step 2321: loss = 3.0688 (1.474 sec/step)\n",
            "I1209 11:39:42.581358 139904075134848 learning.py:507] global step 2321: loss = 3.0688 (1.474 sec/step)\n",
            "INFO:tensorflow:global step 2322: loss = 1.8967 (1.480 sec/step)\n",
            "I1209 11:39:44.072449 139904075134848 learning.py:507] global step 2322: loss = 1.8967 (1.480 sec/step)\n",
            "INFO:tensorflow:global step 2323: loss = 1.9777 (0.928 sec/step)\n",
            "I1209 11:39:45.333961 139904075134848 learning.py:507] global step 2323: loss = 1.9777 (0.928 sec/step)\n",
            "INFO:tensorflow:global step 2324: loss = 2.2271 (1.741 sec/step)\n",
            "I1209 11:39:47.094418 139904075134848 learning.py:507] global step 2324: loss = 2.2271 (1.741 sec/step)\n",
            "INFO:tensorflow:global step 2325: loss = 1.8984 (0.687 sec/step)\n",
            "I1209 11:39:47.783755 139904075134848 learning.py:507] global step 2325: loss = 1.8984 (0.687 sec/step)\n",
            "INFO:tensorflow:global step 2326: loss = 2.4165 (2.052 sec/step)\n",
            "I1209 11:39:49.838043 139904075134848 learning.py:507] global step 2326: loss = 2.4165 (2.052 sec/step)\n",
            "INFO:tensorflow:global step 2327: loss = 2.1115 (0.657 sec/step)\n",
            "I1209 11:39:50.719653 139904075134848 learning.py:507] global step 2327: loss = 2.1115 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2328: loss = 2.2643 (0.764 sec/step)\n",
            "I1209 11:39:52.042047 139904075134848 learning.py:507] global step 2328: loss = 2.2643 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 2329: loss = 2.3284 (0.744 sec/step)\n",
            "I1209 11:39:52.844569 139904075134848 learning.py:507] global step 2329: loss = 2.3284 (0.744 sec/step)\n",
            "INFO:tensorflow:global step 2330: loss = 2.2190 (0.836 sec/step)\n",
            "I1209 11:39:53.701100 139904075134848 learning.py:507] global step 2330: loss = 2.2190 (0.836 sec/step)\n",
            "INFO:tensorflow:global step 2331: loss = 2.2747 (2.587 sec/step)\n",
            "I1209 11:39:56.298120 139904075134848 learning.py:507] global step 2331: loss = 2.2747 (2.587 sec/step)\n",
            "INFO:tensorflow:global step 2332: loss = 1.9890 (0.827 sec/step)\n",
            "I1209 11:39:57.396228 139904075134848 learning.py:507] global step 2332: loss = 1.9890 (0.827 sec/step)\n",
            "INFO:tensorflow:global step 2333: loss = 2.0584 (0.666 sec/step)\n",
            "I1209 11:39:58.462890 139904075134848 learning.py:507] global step 2333: loss = 2.0584 (0.666 sec/step)\n",
            "INFO:tensorflow:global step 2334: loss = 1.3494 (2.311 sec/step)\n",
            "I1209 11:40:00.839161 139904075134848 learning.py:507] global step 2334: loss = 1.3494 (2.311 sec/step)\n",
            "INFO:tensorflow:global step 2335: loss = 1.8841 (0.686 sec/step)\n",
            "I1209 11:40:01.814766 139904075134848 learning.py:507] global step 2335: loss = 1.8841 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 2336: loss = 1.7573 (1.527 sec/step)\n",
            "I1209 11:40:03.529493 139904075134848 learning.py:507] global step 2336: loss = 1.7573 (1.527 sec/step)\n",
            "INFO:tensorflow:global step 2337: loss = 2.5570 (0.574 sec/step)\n",
            "I1209 11:40:04.105420 139904075134848 learning.py:507] global step 2337: loss = 2.5570 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 2338: loss = 1.8988 (0.936 sec/step)\n",
            "I1209 11:40:05.297817 139904075134848 learning.py:507] global step 2338: loss = 1.8988 (0.936 sec/step)\n",
            "INFO:tensorflow:global step 2339: loss = 2.5452 (1.870 sec/step)\n",
            "I1209 11:40:07.308960 139904075134848 learning.py:507] global step 2339: loss = 2.5452 (1.870 sec/step)\n",
            "INFO:tensorflow:global step 2340: loss = 2.6451 (0.630 sec/step)\n",
            "I1209 11:40:07.941292 139904075134848 learning.py:507] global step 2340: loss = 2.6451 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2341: loss = 2.0324 (1.997 sec/step)\n",
            "I1209 11:40:09.940701 139904075134848 learning.py:507] global step 2341: loss = 2.0324 (1.997 sec/step)\n",
            "INFO:tensorflow:global step 2342: loss = 1.9354 (0.771 sec/step)\n",
            "I1209 11:40:11.037828 139904075134848 learning.py:507] global step 2342: loss = 1.9354 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 2343: loss = 1.9383 (0.875 sec/step)\n",
            "I1209 11:40:12.388564 139904075134848 learning.py:507] global step 2343: loss = 1.9383 (0.875 sec/step)\n",
            "INFO:tensorflow:global step 2344: loss = 2.0036 (0.940 sec/step)\n",
            "I1209 11:40:13.898844 139904075134848 learning.py:507] global step 2344: loss = 2.0036 (0.940 sec/step)\n",
            "INFO:tensorflow:global step 2345: loss = 1.6981 (2.106 sec/step)\n",
            "I1209 11:40:16.021849 139904075134848 learning.py:507] global step 2345: loss = 1.6981 (2.106 sec/step)\n",
            "INFO:tensorflow:global step 2346: loss = 1.6640 (0.658 sec/step)\n",
            "I1209 11:40:16.681838 139904075134848 learning.py:507] global step 2346: loss = 1.6640 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2347: loss = 2.0077 (2.200 sec/step)\n",
            "I1209 11:40:18.884172 139904075134848 learning.py:507] global step 2347: loss = 2.0077 (2.200 sec/step)\n",
            "INFO:tensorflow:global step 2348: loss = 2.3051 (0.768 sec/step)\n",
            "I1209 11:40:20.032127 139904075134848 learning.py:507] global step 2348: loss = 2.3051 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 2349: loss = 1.8123 (0.683 sec/step)\n",
            "I1209 11:40:20.898152 139904075134848 learning.py:507] global step 2349: loss = 1.8123 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 2350: loss = 1.8824 (1.900 sec/step)\n",
            "I1209 11:40:22.951783 139904075134848 learning.py:507] global step 2350: loss = 1.8824 (1.900 sec/step)\n",
            "INFO:tensorflow:global step 2351: loss = 1.7811 (0.826 sec/step)\n",
            "I1209 11:40:23.831980 139904075134848 learning.py:507] global step 2351: loss = 1.7811 (0.826 sec/step)\n",
            "INFO:tensorflow:global step 2352: loss = 1.6033 (0.557 sec/step)\n",
            "I1209 11:40:24.572211 139904075134848 learning.py:507] global step 2352: loss = 1.6033 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 2353: loss = 2.1813 (2.538 sec/step)\n",
            "I1209 11:40:27.112902 139904075134848 learning.py:507] global step 2353: loss = 2.1813 (2.538 sec/step)\n",
            "INFO:tensorflow:global step 2354: loss = 1.8682 (0.755 sec/step)\n",
            "I1209 11:40:27.870117 139904075134848 learning.py:507] global step 2354: loss = 1.8682 (0.755 sec/step)\n",
            "INFO:tensorflow:global step 2355: loss = 2.0868 (1.572 sec/step)\n",
            "I1209 11:40:29.470217 139904075134848 learning.py:507] global step 2355: loss = 2.0868 (1.572 sec/step)\n",
            "INFO:tensorflow:global step 2356: loss = 1.8185 (1.544 sec/step)\n",
            "I1209 11:40:31.033010 139904075134848 learning.py:507] global step 2356: loss = 1.8185 (1.544 sec/step)\n",
            "INFO:tensorflow:global step 2357: loss = 2.3350 (0.690 sec/step)\n",
            "I1209 11:40:31.724818 139904075134848 learning.py:507] global step 2357: loss = 2.3350 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 2358: loss = 2.2422 (1.189 sec/step)\n",
            "I1209 11:40:32.958804 139904075134848 learning.py:507] global step 2358: loss = 2.2422 (1.189 sec/step)\n",
            "INFO:tensorflow:global step 2359: loss = 1.6821 (2.131 sec/step)\n",
            "I1209 11:40:35.387557 139904075134848 learning.py:507] global step 2359: loss = 1.6821 (2.131 sec/step)\n",
            "INFO:tensorflow:global step 2360: loss = 2.7809 (0.683 sec/step)\n",
            "I1209 11:40:36.394828 139904075134848 learning.py:507] global step 2360: loss = 2.7809 (0.683 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2360.\n",
            "I1209 11:40:40.451695 139900543997696 supervisor.py:1050] Recording summary at step 2360.\n",
            "INFO:tensorflow:global step 2361: loss = 2.0932 (4.188 sec/step)\n",
            "I1209 11:40:40.764738 139904075134848 learning.py:507] global step 2361: loss = 2.0932 (4.188 sec/step)\n",
            "INFO:tensorflow:global step 2362: loss = 2.3426 (0.630 sec/step)\n",
            "I1209 11:40:41.396770 139904075134848 learning.py:507] global step 2362: loss = 2.3426 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 2363: loss = 2.1494 (1.989 sec/step)\n",
            "I1209 11:40:43.386934 139904075134848 learning.py:507] global step 2363: loss = 2.1494 (1.989 sec/step)\n",
            "INFO:tensorflow:global step 2364: loss = 2.7945 (0.698 sec/step)\n",
            "I1209 11:40:44.087166 139904075134848 learning.py:507] global step 2364: loss = 2.7945 (0.698 sec/step)\n",
            "INFO:tensorflow:global step 2365: loss = 1.9244 (1.235 sec/step)\n",
            "I1209 11:40:45.531693 139904075134848 learning.py:507] global step 2365: loss = 1.9244 (1.235 sec/step)\n",
            "INFO:tensorflow:global step 2366: loss = 1.7623 (1.783 sec/step)\n",
            "I1209 11:40:47.690109 139904075134848 learning.py:507] global step 2366: loss = 1.7623 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 2367: loss = 2.8905 (0.770 sec/step)\n",
            "I1209 11:40:48.672483 139904075134848 learning.py:507] global step 2367: loss = 2.8905 (0.770 sec/step)\n",
            "INFO:tensorflow:global step 2368: loss = 2.3264 (2.471 sec/step)\n",
            "I1209 11:40:51.338728 139904075134848 learning.py:507] global step 2368: loss = 2.3264 (2.471 sec/step)\n",
            "INFO:tensorflow:global step 2369: loss = 2.0145 (0.820 sec/step)\n",
            "I1209 11:40:52.527117 139904075134848 learning.py:507] global step 2369: loss = 2.0145 (0.820 sec/step)\n",
            "INFO:tensorflow:global step 2370: loss = 2.7959 (0.661 sec/step)\n",
            "I1209 11:40:53.297140 139904075134848 learning.py:507] global step 2370: loss = 2.7959 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2371: loss = 1.7575 (0.974 sec/step)\n",
            "I1209 11:40:54.626891 139904075134848 learning.py:507] global step 2371: loss = 1.7575 (0.974 sec/step)\n",
            "INFO:tensorflow:global step 2372: loss = 1.8326 (1.436 sec/step)\n",
            "I1209 11:40:56.369948 139904075134848 learning.py:507] global step 2372: loss = 1.8326 (1.436 sec/step)\n",
            "INFO:tensorflow:global step 2373: loss = 2.0132 (0.699 sec/step)\n",
            "I1209 11:40:57.085484 139904075134848 learning.py:507] global step 2373: loss = 2.0132 (0.699 sec/step)\n",
            "INFO:tensorflow:global step 2374: loss = 2.4993 (1.043 sec/step)\n",
            "I1209 11:40:58.194081 139904075134848 learning.py:507] global step 2374: loss = 2.4993 (1.043 sec/step)\n",
            "INFO:tensorflow:global step 2375: loss = 1.7241 (2.142 sec/step)\n",
            "I1209 11:41:00.695014 139904075134848 learning.py:507] global step 2375: loss = 1.7241 (2.142 sec/step)\n",
            "INFO:tensorflow:global step 2376: loss = 1.8590 (0.762 sec/step)\n",
            "I1209 11:41:01.459093 139904075134848 learning.py:507] global step 2376: loss = 1.8590 (0.762 sec/step)\n",
            "INFO:tensorflow:global step 2377: loss = 2.1181 (2.018 sec/step)\n",
            "I1209 11:41:03.479378 139904075134848 learning.py:507] global step 2377: loss = 2.1181 (2.018 sec/step)\n",
            "INFO:tensorflow:global step 2378: loss = 1.6923 (0.623 sec/step)\n",
            "I1209 11:41:04.105202 139904075134848 learning.py:507] global step 2378: loss = 1.6923 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2379: loss = 2.5881 (0.941 sec/step)\n",
            "I1209 11:41:05.385227 139904075134848 learning.py:507] global step 2379: loss = 2.5881 (0.941 sec/step)\n",
            "INFO:tensorflow:global step 2380: loss = 2.4689 (1.977 sec/step)\n",
            "I1209 11:41:07.543631 139904075134848 learning.py:507] global step 2380: loss = 2.4689 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 2381: loss = 1.8954 (0.726 sec/step)\n",
            "I1209 11:41:08.461197 139904075134848 learning.py:507] global step 2381: loss = 1.8954 (0.726 sec/step)\n",
            "INFO:tensorflow:global step 2382: loss = 1.7555 (0.686 sec/step)\n",
            "I1209 11:41:09.389116 139904075134848 learning.py:507] global step 2382: loss = 1.7555 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 2383: loss = 2.1505 (1.512 sec/step)\n",
            "I1209 11:41:11.155430 139904075134848 learning.py:507] global step 2383: loss = 2.1505 (1.512 sec/step)\n",
            "INFO:tensorflow:global step 2384: loss = 1.6869 (0.889 sec/step)\n",
            "I1209 11:41:12.470554 139904075134848 learning.py:507] global step 2384: loss = 1.6869 (0.889 sec/step)\n",
            "INFO:tensorflow:global step 2385: loss = 2.4156 (0.852 sec/step)\n",
            "I1209 11:41:13.616697 139904075134848 learning.py:507] global step 2385: loss = 2.4156 (0.852 sec/step)\n",
            "INFO:tensorflow:global step 2386: loss = 1.8897 (2.501 sec/step)\n",
            "I1209 11:41:16.208474 139904075134848 learning.py:507] global step 2386: loss = 1.8897 (2.501 sec/step)\n",
            "INFO:tensorflow:global step 2387: loss = 1.6171 (0.701 sec/step)\n",
            "I1209 11:41:17.160279 139904075134848 learning.py:507] global step 2387: loss = 1.6171 (0.701 sec/step)\n",
            "INFO:tensorflow:global step 2388: loss = 2.0122 (2.013 sec/step)\n",
            "I1209 11:41:19.236634 139904075134848 learning.py:507] global step 2388: loss = 2.0122 (2.013 sec/step)\n",
            "INFO:tensorflow:global step 2389: loss = 2.1307 (0.644 sec/step)\n",
            "I1209 11:41:20.079821 139904075134848 learning.py:507] global step 2389: loss = 2.1307 (0.644 sec/step)\n",
            "INFO:tensorflow:global step 2390: loss = 2.1805 (0.678 sec/step)\n",
            "I1209 11:41:21.320830 139904075134848 learning.py:507] global step 2390: loss = 2.1805 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2391: loss = 2.6591 (2.263 sec/step)\n",
            "I1209 11:41:23.863293 139904075134848 learning.py:507] global step 2391: loss = 2.6591 (2.263 sec/step)\n",
            "INFO:tensorflow:global step 2392: loss = 2.0428 (0.732 sec/step)\n",
            "I1209 11:41:25.060687 139904075134848 learning.py:507] global step 2392: loss = 2.0428 (0.732 sec/step)\n",
            "INFO:tensorflow:global step 2393: loss = 2.4373 (0.917 sec/step)\n",
            "I1209 11:41:26.219213 139904075134848 learning.py:507] global step 2393: loss = 2.4373 (0.917 sec/step)\n",
            "INFO:tensorflow:global step 2394: loss = 1.7319 (0.835 sec/step)\n",
            "I1209 11:41:27.769338 139904075134848 learning.py:507] global step 2394: loss = 1.7319 (0.835 sec/step)\n",
            "INFO:tensorflow:global step 2395: loss = 3.2653 (0.797 sec/step)\n",
            "I1209 11:41:28.681942 139904075134848 learning.py:507] global step 2395: loss = 3.2653 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 2396: loss = 2.5590 (0.804 sec/step)\n",
            "I1209 11:41:29.752383 139904075134848 learning.py:507] global step 2396: loss = 2.5590 (0.804 sec/step)\n",
            "INFO:tensorflow:global step 2397: loss = 2.2759 (2.127 sec/step)\n",
            "I1209 11:41:31.881298 139904075134848 learning.py:507] global step 2397: loss = 2.2759 (2.127 sec/step)\n",
            "INFO:tensorflow:global step 2398: loss = 2.4634 (0.896 sec/step)\n",
            "I1209 11:41:32.913800 139904075134848 learning.py:507] global step 2398: loss = 2.4634 (0.896 sec/step)\n",
            "INFO:tensorflow:global step 2399: loss = 2.2346 (2.277 sec/step)\n",
            "I1209 11:41:35.327348 139904075134848 learning.py:507] global step 2399: loss = 2.2346 (2.277 sec/step)\n",
            "INFO:tensorflow:global step 2400: loss = 2.1533 (0.935 sec/step)\n",
            "I1209 11:41:36.277805 139904075134848 learning.py:507] global step 2400: loss = 2.1533 (0.935 sec/step)\n",
            "INFO:tensorflow:global step 2401: loss = 2.3655 (1.865 sec/step)\n",
            "I1209 11:41:38.174937 139904075134848 learning.py:507] global step 2401: loss = 2.3655 (1.865 sec/step)\n",
            "INFO:tensorflow:global step 2402: loss = 2.9002 (0.557 sec/step)\n",
            "I1209 11:41:38.733279 139904075134848 learning.py:507] global step 2402: loss = 2.9002 (0.557 sec/step)\n",
            "INFO:tensorflow:global step 2403: loss = 1.7114 (2.167 sec/step)\n",
            "I1209 11:41:40.902015 139904075134848 learning.py:507] global step 2403: loss = 1.7114 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 2404: loss = 2.0173 (0.734 sec/step)\n",
            "I1209 11:41:41.777923 139904075134848 learning.py:507] global step 2404: loss = 2.0173 (0.734 sec/step)\n",
            "INFO:tensorflow:global step 2405: loss = 2.0819 (1.923 sec/step)\n",
            "I1209 11:41:43.844482 139904075134848 learning.py:507] global step 2405: loss = 2.0819 (1.923 sec/step)\n",
            "INFO:tensorflow:global step 2406: loss = 1.9750 (0.676 sec/step)\n",
            "I1209 11:41:44.521948 139904075134848 learning.py:507] global step 2406: loss = 1.9750 (0.676 sec/step)\n",
            "INFO:tensorflow:global step 2407: loss = 2.3692 (1.219 sec/step)\n",
            "I1209 11:41:46.025121 139904075134848 learning.py:507] global step 2407: loss = 2.3692 (1.219 sec/step)\n",
            "INFO:tensorflow:global step 2408: loss = 2.2176 (0.758 sec/step)\n",
            "I1209 11:41:46.993989 139904075134848 learning.py:507] global step 2408: loss = 2.2176 (0.758 sec/step)\n",
            "INFO:tensorflow:global step 2409: loss = 2.1744 (1.137 sec/step)\n",
            "I1209 11:41:48.394547 139904075134848 learning.py:507] global step 2409: loss = 2.1744 (1.137 sec/step)\n",
            "INFO:tensorflow:global step 2410: loss = 2.1519 (1.721 sec/step)\n",
            "I1209 11:41:50.608528 139904075134848 learning.py:507] global step 2410: loss = 2.1519 (1.721 sec/step)\n",
            "INFO:tensorflow:global step 2411: loss = 1.9720 (0.783 sec/step)\n",
            "I1209 11:41:51.640873 139904075134848 learning.py:507] global step 2411: loss = 1.9720 (0.783 sec/step)\n",
            "INFO:tensorflow:global step 2412: loss = 2.1621 (2.266 sec/step)\n",
            "I1209 11:41:54.019191 139904075134848 learning.py:507] global step 2412: loss = 2.1621 (2.266 sec/step)\n",
            "INFO:tensorflow:global step 2413: loss = 1.9846 (0.769 sec/step)\n",
            "I1209 11:41:54.789979 139904075134848 learning.py:507] global step 2413: loss = 1.9846 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 2414: loss = 2.5969 (0.779 sec/step)\n",
            "I1209 11:41:55.871710 139904075134848 learning.py:507] global step 2414: loss = 2.5969 (0.779 sec/step)\n",
            "INFO:tensorflow:global step 2415: loss = 1.8589 (1.875 sec/step)\n",
            "I1209 11:41:58.100901 139904075134848 learning.py:507] global step 2415: loss = 1.8589 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 2416: loss = 1.8302 (0.709 sec/step)\n",
            "I1209 11:41:58.812166 139904075134848 learning.py:507] global step 2416: loss = 1.8302 (0.709 sec/step)\n",
            "INFO:tensorflow:global step 2417: loss = 2.0290 (2.265 sec/step)\n",
            "I1209 11:42:01.078784 139904075134848 learning.py:507] global step 2417: loss = 2.0290 (2.265 sec/step)\n",
            "INFO:tensorflow:global step 2418: loss = 1.9694 (0.765 sec/step)\n",
            "I1209 11:42:02.175672 139904075134848 learning.py:507] global step 2418: loss = 1.9694 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 2419: loss = 2.1658 (0.764 sec/step)\n",
            "I1209 11:42:03.004113 139904075134848 learning.py:507] global step 2419: loss = 2.1658 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 2420: loss = 1.6952 (2.153 sec/step)\n",
            "I1209 11:42:05.159067 139904075134848 learning.py:507] global step 2420: loss = 1.6952 (2.153 sec/step)\n",
            "INFO:tensorflow:global step 2421: loss = 1.7415 (0.633 sec/step)\n",
            "I1209 11:42:05.794029 139904075134848 learning.py:507] global step 2421: loss = 1.7415 (0.633 sec/step)\n",
            "INFO:tensorflow:global step 2422: loss = 2.1278 (1.901 sec/step)\n",
            "I1209 11:42:07.697262 139904075134848 learning.py:507] global step 2422: loss = 2.1278 (1.901 sec/step)\n",
            "INFO:tensorflow:global step 2423: loss = 1.5186 (0.805 sec/step)\n",
            "I1209 11:42:08.768877 139904075134848 learning.py:507] global step 2423: loss = 1.5186 (0.805 sec/step)\n",
            "INFO:tensorflow:global step 2424: loss = 1.7126 (2.294 sec/step)\n",
            "I1209 11:42:11.347387 139904075134848 learning.py:507] global step 2424: loss = 1.7126 (2.294 sec/step)\n",
            "INFO:tensorflow:global step 2425: loss = 2.5315 (0.657 sec/step)\n",
            "I1209 11:42:12.006367 139904075134848 learning.py:507] global step 2425: loss = 2.5315 (0.657 sec/step)\n",
            "INFO:tensorflow:global step 2426: loss = 1.4781 (1.828 sec/step)\n",
            "I1209 11:42:13.837357 139904075134848 learning.py:507] global step 2426: loss = 1.4781 (1.828 sec/step)\n",
            "INFO:tensorflow:global step 2427: loss = 2.3391 (0.897 sec/step)\n",
            "I1209 11:42:15.036406 139904075134848 learning.py:507] global step 2427: loss = 2.3391 (0.897 sec/step)\n",
            "INFO:tensorflow:global step 2428: loss = 1.8953 (0.721 sec/step)\n",
            "I1209 11:42:16.055396 139904075134848 learning.py:507] global step 2428: loss = 1.8953 (0.721 sec/step)\n",
            "INFO:tensorflow:global step 2429: loss = 1.5423 (1.895 sec/step)\n",
            "I1209 11:42:18.167151 139904075134848 learning.py:507] global step 2429: loss = 1.5423 (1.895 sec/step)\n",
            "INFO:tensorflow:global step 2430: loss = 2.0339 (0.654 sec/step)\n",
            "I1209 11:42:18.824257 139904075134848 learning.py:507] global step 2430: loss = 2.0339 (0.654 sec/step)\n",
            "INFO:tensorflow:global step 2431: loss = 1.9301 (2.461 sec/step)\n",
            "I1209 11:42:21.287820 139904075134848 learning.py:507] global step 2431: loss = 1.9301 (2.461 sec/step)\n",
            "INFO:tensorflow:global step 2432: loss = 1.9581 (0.856 sec/step)\n",
            "I1209 11:42:22.418061 139904075134848 learning.py:507] global step 2432: loss = 1.9581 (0.856 sec/step)\n",
            "INFO:tensorflow:global step 2433: loss = 1.7957 (0.797 sec/step)\n",
            "I1209 11:42:23.334169 139904075134848 learning.py:507] global step 2433: loss = 1.7957 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 2434: loss = 1.9258 (1.010 sec/step)\n",
            "I1209 11:42:24.541680 139904075134848 learning.py:507] global step 2434: loss = 1.9258 (1.010 sec/step)\n",
            "INFO:tensorflow:global step 2435: loss = 3.1698 (2.085 sec/step)\n",
            "I1209 11:42:26.647045 139904075134848 learning.py:507] global step 2435: loss = 3.1698 (2.085 sec/step)\n",
            "INFO:tensorflow:global step 2436: loss = 1.5330 (0.690 sec/step)\n",
            "I1209 11:42:27.513383 139904075134848 learning.py:507] global step 2436: loss = 1.5330 (0.690 sec/step)\n",
            "INFO:tensorflow:global step 2437: loss = 2.3024 (0.795 sec/step)\n",
            "I1209 11:42:28.775771 139904075134848 learning.py:507] global step 2437: loss = 2.3024 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 2438: loss = 2.3905 (1.477 sec/step)\n",
            "I1209 11:42:30.352080 139904075134848 learning.py:507] global step 2438: loss = 2.3905 (1.477 sec/step)\n",
            "INFO:tensorflow:global step 2439: loss = 1.7346 (0.741 sec/step)\n",
            "I1209 11:42:31.114419 139904075134848 learning.py:507] global step 2439: loss = 1.7346 (0.741 sec/step)\n",
            "INFO:tensorflow:global step 2440: loss = 2.2513 (1.421 sec/step)\n",
            "I1209 11:42:32.770902 139904075134848 learning.py:507] global step 2440: loss = 2.2513 (1.421 sec/step)\n",
            "INFO:tensorflow:global step 2441: loss = 2.3341 (0.672 sec/step)\n",
            "I1209 11:42:33.444356 139904075134848 learning.py:507] global step 2441: loss = 2.3341 (0.672 sec/step)\n",
            "INFO:tensorflow:global step 2442: loss = 1.9384 (2.099 sec/step)\n",
            "I1209 11:42:35.545324 139904075134848 learning.py:507] global step 2442: loss = 1.9384 (2.099 sec/step)\n",
            "INFO:tensorflow:global step 2443: loss = 2.1461 (0.768 sec/step)\n",
            "I1209 11:42:36.714349 139904075134848 learning.py:507] global step 2443: loss = 2.1461 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 2444: loss = 2.1133 (2.454 sec/step)\n",
            "I1209 11:42:39.231912 139904075134848 learning.py:507] global step 2444: loss = 2.1133 (2.454 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2444.\n",
            "I1209 11:42:39.914652 139900543997696 supervisor.py:1050] Recording summary at step 2444.\n",
            "INFO:tensorflow:global step 2445: loss = 2.0975 (0.980 sec/step)\n",
            "I1209 11:42:40.517705 139904075134848 learning.py:507] global step 2445: loss = 2.0975 (0.980 sec/step)\n",
            "INFO:tensorflow:global step 2446: loss = 2.4690 (0.665 sec/step)\n",
            "I1209 11:42:41.430658 139904075134848 learning.py:507] global step 2446: loss = 2.4690 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2447: loss = 1.7809 (2.004 sec/step)\n",
            "I1209 11:42:43.436788 139904075134848 learning.py:507] global step 2447: loss = 1.7809 (2.004 sec/step)\n",
            "INFO:tensorflow:global step 2448: loss = 2.4135 (0.802 sec/step)\n",
            "I1209 11:42:44.385406 139904075134848 learning.py:507] global step 2448: loss = 2.4135 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 2449: loss = 1.6312 (2.031 sec/step)\n",
            "I1209 11:42:46.566173 139904075134848 learning.py:507] global step 2449: loss = 1.6312 (2.031 sec/step)\n",
            "INFO:tensorflow:global step 2450: loss = 1.6948 (0.602 sec/step)\n",
            "I1209 11:42:47.170395 139904075134848 learning.py:507] global step 2450: loss = 1.6948 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 2451: loss = 2.1528 (2.032 sec/step)\n",
            "I1209 11:42:49.204198 139904075134848 learning.py:507] global step 2451: loss = 2.1528 (2.032 sec/step)\n",
            "INFO:tensorflow:global step 2452: loss = 2.1449 (0.640 sec/step)\n",
            "I1209 11:42:49.951846 139904075134848 learning.py:507] global step 2452: loss = 2.1449 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2453: loss = 2.5034 (1.854 sec/step)\n",
            "I1209 11:42:51.967128 139904075134848 learning.py:507] global step 2453: loss = 2.5034 (1.854 sec/step)\n",
            "INFO:tensorflow:global step 2454: loss = 2.7224 (0.634 sec/step)\n",
            "I1209 11:42:52.603019 139904075134848 learning.py:507] global step 2454: loss = 2.7224 (0.634 sec/step)\n",
            "INFO:tensorflow:global step 2455: loss = 2.6607 (1.054 sec/step)\n",
            "I1209 11:42:53.952026 139904075134848 learning.py:507] global step 2455: loss = 2.6607 (1.054 sec/step)\n",
            "INFO:tensorflow:global step 2456: loss = 2.0175 (1.782 sec/step)\n",
            "I1209 11:42:55.764491 139904075134848 learning.py:507] global step 2456: loss = 2.0175 (1.782 sec/step)\n",
            "INFO:tensorflow:global step 2457: loss = 2.0808 (0.716 sec/step)\n",
            "I1209 11:42:56.639442 139904075134848 learning.py:507] global step 2457: loss = 2.0808 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 2458: loss = 1.8061 (1.586 sec/step)\n",
            "I1209 11:42:58.687336 139904075134848 learning.py:507] global step 2458: loss = 1.8061 (1.586 sec/step)\n",
            "INFO:tensorflow:global step 2459: loss = 2.2919 (0.591 sec/step)\n",
            "I1209 11:42:59.696376 139904075134848 learning.py:507] global step 2459: loss = 2.2919 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 2460: loss = 1.7439 (0.604 sec/step)\n",
            "I1209 11:43:00.441114 139904075134848 learning.py:507] global step 2460: loss = 1.7439 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2461: loss = 2.0898 (1.893 sec/step)\n",
            "I1209 11:43:02.335632 139904075134848 learning.py:507] global step 2461: loss = 2.0898 (1.893 sec/step)\n",
            "INFO:tensorflow:global step 2462: loss = 1.8704 (0.593 sec/step)\n",
            "I1209 11:43:03.085262 139904075134848 learning.py:507] global step 2462: loss = 1.8704 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 2463: loss = 1.8243 (2.121 sec/step)\n",
            "I1209 11:43:05.482450 139904075134848 learning.py:507] global step 2463: loss = 1.8243 (2.121 sec/step)\n",
            "INFO:tensorflow:global step 2464: loss = 1.9628 (0.727 sec/step)\n",
            "I1209 11:43:06.780758 139904075134848 learning.py:507] global step 2464: loss = 1.9628 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 2465: loss = 1.9603 (0.751 sec/step)\n",
            "I1209 11:43:07.631183 139904075134848 learning.py:507] global step 2465: loss = 1.9603 (0.751 sec/step)\n",
            "INFO:tensorflow:global step 2466: loss = 1.4864 (1.772 sec/step)\n",
            "I1209 11:43:09.745107 139904075134848 learning.py:507] global step 2466: loss = 1.4864 (1.772 sec/step)\n",
            "INFO:tensorflow:global step 2467: loss = 2.1746 (0.661 sec/step)\n",
            "I1209 11:43:10.800033 139904075134848 learning.py:507] global step 2467: loss = 2.1746 (0.661 sec/step)\n",
            "INFO:tensorflow:global step 2468: loss = 2.1400 (1.411 sec/step)\n",
            "I1209 11:43:12.416663 139904075134848 learning.py:507] global step 2468: loss = 2.1400 (1.411 sec/step)\n",
            "INFO:tensorflow:global step 2469: loss = 1.7223 (0.576 sec/step)\n",
            "I1209 11:43:12.994256 139904075134848 learning.py:507] global step 2469: loss = 1.7223 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 2470: loss = 1.7196 (2.184 sec/step)\n",
            "I1209 11:43:15.179779 139904075134848 learning.py:507] global step 2470: loss = 1.7196 (2.184 sec/step)\n",
            "INFO:tensorflow:global step 2471: loss = 2.2067 (0.739 sec/step)\n",
            "I1209 11:43:15.990460 139904075134848 learning.py:507] global step 2471: loss = 2.2067 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 2472: loss = 1.7389 (2.133 sec/step)\n",
            "I1209 11:43:18.157196 139904075134848 learning.py:507] global step 2472: loss = 1.7389 (2.133 sec/step)\n",
            "INFO:tensorflow:global step 2473: loss = 2.2963 (0.652 sec/step)\n",
            "I1209 11:43:18.811295 139904075134848 learning.py:507] global step 2473: loss = 2.2963 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2474: loss = 1.8809 (2.213 sec/step)\n",
            "I1209 11:43:21.026920 139904075134848 learning.py:507] global step 2474: loss = 1.8809 (2.213 sec/step)\n",
            "INFO:tensorflow:global step 2475: loss = 2.4222 (0.802 sec/step)\n",
            "I1209 11:43:21.934602 139904075134848 learning.py:507] global step 2475: loss = 2.4222 (0.802 sec/step)\n",
            "INFO:tensorflow:global step 2476: loss = 2.6682 (1.805 sec/step)\n",
            "I1209 11:43:23.873940 139904075134848 learning.py:507] global step 2476: loss = 2.6682 (1.805 sec/step)\n",
            "INFO:tensorflow:global step 2477: loss = 2.1827 (0.852 sec/step)\n",
            "I1209 11:43:25.063492 139904075134848 learning.py:507] global step 2477: loss = 2.1827 (0.852 sec/step)\n",
            "INFO:tensorflow:global step 2478: loss = 2.4315 (1.501 sec/step)\n",
            "I1209 11:43:26.615885 139904075134848 learning.py:507] global step 2478: loss = 2.4315 (1.501 sec/step)\n",
            "INFO:tensorflow:global step 2479: loss = 1.8690 (0.821 sec/step)\n",
            "I1209 11:43:27.497150 139904075134848 learning.py:507] global step 2479: loss = 1.8690 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 2480: loss = 1.9683 (0.662 sec/step)\n",
            "I1209 11:43:28.638146 139904075134848 learning.py:507] global step 2480: loss = 1.9683 (0.662 sec/step)\n",
            "INFO:tensorflow:global step 2481: loss = 1.7346 (1.671 sec/step)\n",
            "I1209 11:43:30.406752 139904075134848 learning.py:507] global step 2481: loss = 1.7346 (1.671 sec/step)\n",
            "INFO:tensorflow:global step 2482: loss = 1.6934 (0.565 sec/step)\n",
            "I1209 11:43:30.973018 139904075134848 learning.py:507] global step 2482: loss = 1.6934 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 2483: loss = 2.9554 (1.649 sec/step)\n",
            "I1209 11:43:32.725452 139904075134848 learning.py:507] global step 2483: loss = 2.9554 (1.649 sec/step)\n",
            "INFO:tensorflow:global step 2484: loss = 2.4084 (0.803 sec/step)\n",
            "I1209 11:43:34.092193 139904075134848 learning.py:507] global step 2484: loss = 2.4084 (0.803 sec/step)\n",
            "INFO:tensorflow:global step 2485: loss = 1.2779 (2.101 sec/step)\n",
            "I1209 11:43:36.331562 139904075134848 learning.py:507] global step 2485: loss = 1.2779 (2.101 sec/step)\n",
            "INFO:tensorflow:global step 2486: loss = 1.9320 (0.674 sec/step)\n",
            "I1209 11:43:37.007532 139904075134848 learning.py:507] global step 2486: loss = 1.9320 (0.674 sec/step)\n",
            "INFO:tensorflow:global step 2487: loss = 2.0148 (2.159 sec/step)\n",
            "I1209 11:43:39.168514 139904075134848 learning.py:507] global step 2487: loss = 2.0148 (2.159 sec/step)\n",
            "INFO:tensorflow:global step 2488: loss = 1.3394 (0.601 sec/step)\n",
            "I1209 11:43:39.772119 139904075134848 learning.py:507] global step 2488: loss = 1.3394 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 2489: loss = 1.4579 (0.999 sec/step)\n",
            "I1209 11:43:40.790414 139904075134848 learning.py:507] global step 2489: loss = 1.4579 (0.999 sec/step)\n",
            "INFO:tensorflow:global step 2490: loss = 1.7246 (2.172 sec/step)\n",
            "I1209 11:43:43.086848 139904075134848 learning.py:507] global step 2490: loss = 1.7246 (2.172 sec/step)\n",
            "INFO:tensorflow:global step 2491: loss = 2.3437 (0.679 sec/step)\n",
            "I1209 11:43:44.140388 139904075134848 learning.py:507] global step 2491: loss = 2.3437 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 2492: loss = 2.1562 (1.756 sec/step)\n",
            "I1209 11:43:46.024892 139904075134848 learning.py:507] global step 2492: loss = 2.1562 (1.756 sec/step)\n",
            "INFO:tensorflow:global step 2493: loss = 2.2383 (0.774 sec/step)\n",
            "I1209 11:43:46.898525 139904075134848 learning.py:507] global step 2493: loss = 2.2383 (0.774 sec/step)\n",
            "INFO:tensorflow:global step 2494: loss = 1.8001 (0.747 sec/step)\n",
            "I1209 11:43:48.253942 139904075134848 learning.py:507] global step 2494: loss = 1.8001 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 2495: loss = 2.4475 (1.658 sec/step)\n",
            "I1209 11:43:49.930979 139904075134848 learning.py:507] global step 2495: loss = 2.4475 (1.658 sec/step)\n",
            "INFO:tensorflow:global step 2496: loss = 2.4671 (0.577 sec/step)\n",
            "I1209 11:43:50.509721 139904075134848 learning.py:507] global step 2496: loss = 2.4671 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 2497: loss = 1.9294 (2.048 sec/step)\n",
            "I1209 11:43:52.559090 139904075134848 learning.py:507] global step 2497: loss = 1.9294 (2.048 sec/step)\n",
            "INFO:tensorflow:global step 2498: loss = 1.7304 (0.757 sec/step)\n",
            "I1209 11:43:53.573295 139904075134848 learning.py:507] global step 2498: loss = 1.7304 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 2499: loss = 2.1116 (0.757 sec/step)\n",
            "I1209 11:43:54.710878 139904075134848 learning.py:507] global step 2499: loss = 2.1116 (0.757 sec/step)\n",
            "INFO:tensorflow:global step 2500: loss = 1.9300 (1.413 sec/step)\n",
            "I1209 11:43:56.251013 139904075134848 learning.py:507] global step 2500: loss = 1.9300 (1.413 sec/step)\n",
            "INFO:tensorflow:global step 2501: loss = 1.7734 (0.742 sec/step)\n",
            "I1209 11:43:57.223410 139904075134848 learning.py:507] global step 2501: loss = 1.7734 (0.742 sec/step)\n",
            "INFO:tensorflow:global step 2502: loss = 2.5504 (0.771 sec/step)\n",
            "I1209 11:43:58.134263 139904075134848 learning.py:507] global step 2502: loss = 2.5504 (0.771 sec/step)\n",
            "INFO:tensorflow:global step 2503: loss = 1.7934 (2.364 sec/step)\n",
            "I1209 11:44:00.500283 139904075134848 learning.py:507] global step 2503: loss = 1.7934 (2.364 sec/step)\n",
            "INFO:tensorflow:global step 2504: loss = 2.1691 (0.787 sec/step)\n",
            "I1209 11:44:01.537924 139904075134848 learning.py:507] global step 2504: loss = 2.1691 (0.787 sec/step)\n",
            "INFO:tensorflow:global step 2505: loss = 1.9187 (1.171 sec/step)\n",
            "I1209 11:44:02.784736 139904075134848 learning.py:507] global step 2505: loss = 1.9187 (1.171 sec/step)\n",
            "INFO:tensorflow:global step 2506: loss = 1.9426 (1.945 sec/step)\n",
            "I1209 11:44:04.763628 139904075134848 learning.py:507] global step 2506: loss = 1.9426 (1.945 sec/step)\n",
            "INFO:tensorflow:global step 2507: loss = 1.9196 (0.696 sec/step)\n",
            "I1209 11:44:05.533104 139904075134848 learning.py:507] global step 2507: loss = 1.9196 (0.696 sec/step)\n",
            "INFO:tensorflow:global step 2508: loss = 2.0155 (0.918 sec/step)\n",
            "I1209 11:44:06.558805 139904075134848 learning.py:507] global step 2508: loss = 2.0155 (0.918 sec/step)\n",
            "INFO:tensorflow:global step 2509: loss = 1.7071 (1.961 sec/step)\n",
            "I1209 11:44:08.586462 139904075134848 learning.py:507] global step 2509: loss = 1.7071 (1.961 sec/step)\n",
            "INFO:tensorflow:global step 2510: loss = 1.9094 (0.782 sec/step)\n",
            "I1209 11:44:09.477515 139904075134848 learning.py:507] global step 2510: loss = 1.9094 (0.782 sec/step)\n",
            "INFO:tensorflow:global step 2511: loss = 2.9785 (0.636 sec/step)\n",
            "I1209 11:44:10.348421 139904075134848 learning.py:507] global step 2511: loss = 2.9785 (0.636 sec/step)\n",
            "INFO:tensorflow:global step 2512: loss = 2.2267 (2.238 sec/step)\n",
            "I1209 11:44:12.588978 139904075134848 learning.py:507] global step 2512: loss = 2.2267 (2.238 sec/step)\n",
            "INFO:tensorflow:global step 2513: loss = 2.5162 (0.671 sec/step)\n",
            "I1209 11:44:13.263183 139904075134848 learning.py:507] global step 2513: loss = 2.5162 (0.671 sec/step)\n",
            "INFO:tensorflow:global step 2514: loss = 1.7873 (1.103 sec/step)\n",
            "I1209 11:44:14.574017 139904075134848 learning.py:507] global step 2514: loss = 1.7873 (1.103 sec/step)\n",
            "INFO:tensorflow:global step 2515: loss = 1.8202 (1.902 sec/step)\n",
            "I1209 11:44:16.627448 139904075134848 learning.py:507] global step 2515: loss = 1.8202 (1.902 sec/step)\n",
            "INFO:tensorflow:global step 2516: loss = 1.7154 (0.867 sec/step)\n",
            "I1209 11:44:17.848573 139904075134848 learning.py:507] global step 2516: loss = 1.7154 (0.867 sec/step)\n",
            "INFO:tensorflow:global step 2517: loss = 1.8423 (1.613 sec/step)\n",
            "I1209 11:44:19.523490 139904075134848 learning.py:507] global step 2517: loss = 1.8423 (1.613 sec/step)\n",
            "INFO:tensorflow:global step 2518: loss = 1.4472 (0.883 sec/step)\n",
            "I1209 11:44:20.624954 139904075134848 learning.py:507] global step 2518: loss = 1.4472 (0.883 sec/step)\n",
            "INFO:tensorflow:global step 2519: loss = 2.1449 (2.043 sec/step)\n",
            "I1209 11:44:22.669863 139904075134848 learning.py:507] global step 2519: loss = 2.1449 (2.043 sec/step)\n",
            "INFO:tensorflow:global step 2520: loss = 2.2875 (0.683 sec/step)\n",
            "I1209 11:44:23.354670 139904075134848 learning.py:507] global step 2520: loss = 2.2875 (0.683 sec/step)\n",
            "INFO:tensorflow:global step 2521: loss = 2.5091 (1.492 sec/step)\n",
            "I1209 11:44:25.086687 139904075134848 learning.py:507] global step 2521: loss = 2.5091 (1.492 sec/step)\n",
            "INFO:tensorflow:global step 2522: loss = 1.7920 (0.731 sec/step)\n",
            "I1209 11:44:26.007697 139904075134848 learning.py:507] global step 2522: loss = 1.7920 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 2523: loss = 1.8730 (1.841 sec/step)\n",
            "I1209 11:44:27.935903 139904075134848 learning.py:507] global step 2523: loss = 1.8730 (1.841 sec/step)\n",
            "INFO:tensorflow:global step 2524: loss = 1.6455 (0.652 sec/step)\n",
            "I1209 11:44:28.959975 139904075134848 learning.py:507] global step 2524: loss = 1.6455 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2525: loss = 2.2062 (1.358 sec/step)\n",
            "I1209 11:44:30.645433 139904075134848 learning.py:507] global step 2525: loss = 2.2062 (1.358 sec/step)\n",
            "INFO:tensorflow:global step 2526: loss = 2.4341 (0.705 sec/step)\n",
            "I1209 11:44:31.640543 139904075134848 learning.py:507] global step 2526: loss = 2.4341 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 2527: loss = 1.8437 (1.599 sec/step)\n",
            "I1209 11:44:33.310443 139904075134848 learning.py:507] global step 2527: loss = 1.8437 (1.599 sec/step)\n",
            "INFO:tensorflow:global step 2528: loss = 2.3683 (0.585 sec/step)\n",
            "I1209 11:44:33.896987 139904075134848 learning.py:507] global step 2528: loss = 2.3683 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 2529: loss = 1.6470 (2.108 sec/step)\n",
            "I1209 11:44:36.006890 139904075134848 learning.py:507] global step 2529: loss = 1.6470 (2.108 sec/step)\n",
            "INFO:tensorflow:Saving checkpoint to path training/model.ckpt\n",
            "I1209 11:44:36.805090 139900518819584 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "INFO:tensorflow:global step 2530: loss = 1.7626 (0.659 sec/step)\n",
            "I1209 11:44:37.091587 139904075134848 learning.py:507] global step 2530: loss = 1.7626 (0.659 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2530.\n",
            "I1209 11:44:40.365972 139900543997696 supervisor.py:1050] Recording summary at step 2530.\n",
            "INFO:tensorflow:global step 2531: loss = 1.7191 (2.835 sec/step)\n",
            "I1209 11:44:40.963690 139904075134848 learning.py:507] global step 2531: loss = 1.7191 (2.835 sec/step)\n",
            "INFO:tensorflow:global step 2532: loss = 2.1029 (1.185 sec/step)\n",
            "I1209 11:44:42.167291 139904075134848 learning.py:507] global step 2532: loss = 2.1029 (1.185 sec/step)\n",
            "INFO:tensorflow:global step 2533: loss = 2.1767 (1.148 sec/step)\n",
            "I1209 11:44:43.452785 139904075134848 learning.py:507] global step 2533: loss = 2.1767 (1.148 sec/step)\n",
            "INFO:tensorflow:global step 2534: loss = 2.0527 (1.931 sec/step)\n",
            "I1209 11:44:45.650036 139904075134848 learning.py:507] global step 2534: loss = 2.0527 (1.931 sec/step)\n",
            "INFO:tensorflow:global step 2535: loss = 2.1213 (0.786 sec/step)\n",
            "I1209 11:44:46.861236 139904075134848 learning.py:507] global step 2535: loss = 2.1213 (0.786 sec/step)\n",
            "INFO:tensorflow:global step 2536: loss = 1.5573 (0.600 sec/step)\n",
            "I1209 11:44:47.464492 139904075134848 learning.py:507] global step 2536: loss = 1.5573 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 2537: loss = 1.8643 (1.052 sec/step)\n",
            "I1209 11:44:48.524143 139904075134848 learning.py:507] global step 2537: loss = 1.8643 (1.052 sec/step)\n",
            "INFO:tensorflow:global step 2538: loss = 1.9035 (2.174 sec/step)\n",
            "I1209 11:44:50.700154 139904075134848 learning.py:507] global step 2538: loss = 1.9035 (2.174 sec/step)\n",
            "INFO:tensorflow:global step 2539: loss = 2.0316 (0.596 sec/step)\n",
            "I1209 11:44:51.297516 139904075134848 learning.py:507] global step 2539: loss = 2.0316 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 2540: loss = 1.8477 (1.040 sec/step)\n",
            "I1209 11:44:52.431085 139904075134848 learning.py:507] global step 2540: loss = 1.8477 (1.040 sec/step)\n",
            "INFO:tensorflow:global step 2541: loss = 2.1238 (2.185 sec/step)\n",
            "I1209 11:44:54.768336 139904075134848 learning.py:507] global step 2541: loss = 2.1238 (2.185 sec/step)\n",
            "INFO:tensorflow:global step 2542: loss = 1.8136 (0.665 sec/step)\n",
            "I1209 11:44:55.694794 139904075134848 learning.py:507] global step 2542: loss = 1.8136 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2543: loss = 3.0188 (0.722 sec/step)\n",
            "I1209 11:44:56.642968 139904075134848 learning.py:507] global step 2543: loss = 3.0188 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2544: loss = 1.4536 (1.331 sec/step)\n",
            "I1209 11:44:58.118654 139904075134848 learning.py:507] global step 2544: loss = 1.4536 (1.331 sec/step)\n",
            "INFO:tensorflow:global step 2545: loss = 1.8539 (2.160 sec/step)\n",
            "I1209 11:45:00.297059 139904075134848 learning.py:507] global step 2545: loss = 1.8539 (2.160 sec/step)\n",
            "INFO:tensorflow:global step 2546: loss = 2.3522 (0.888 sec/step)\n",
            "I1209 11:45:01.445445 139904075134848 learning.py:507] global step 2546: loss = 2.3522 (0.888 sec/step)\n",
            "INFO:tensorflow:global step 2547: loss = 2.1519 (0.875 sec/step)\n",
            "I1209 11:45:02.781927 139904075134848 learning.py:507] global step 2547: loss = 2.1519 (0.875 sec/step)\n",
            "INFO:tensorflow:global step 2548: loss = 1.9489 (2.084 sec/step)\n",
            "I1209 11:45:04.872856 139904075134848 learning.py:507] global step 2548: loss = 1.9489 (2.084 sec/step)\n",
            "INFO:tensorflow:global step 2549: loss = 1.7262 (0.679 sec/step)\n",
            "I1209 11:45:05.553330 139904075134848 learning.py:507] global step 2549: loss = 1.7262 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 2550: loss = 1.9100 (1.030 sec/step)\n",
            "I1209 11:45:06.805272 139904075134848 learning.py:507] global step 2550: loss = 1.9100 (1.030 sec/step)\n",
            "INFO:tensorflow:global step 2551: loss = 1.5555 (1.783 sec/step)\n",
            "I1209 11:45:08.909409 139904075134848 learning.py:507] global step 2551: loss = 1.5555 (1.783 sec/step)\n",
            "INFO:tensorflow:global step 2552: loss = 1.6307 (0.900 sec/step)\n",
            "I1209 11:45:09.908792 139904075134848 learning.py:507] global step 2552: loss = 1.6307 (0.900 sec/step)\n",
            "INFO:tensorflow:global step 2553: loss = 2.2305 (1.018 sec/step)\n",
            "I1209 11:45:11.241088 139904075134848 learning.py:507] global step 2553: loss = 2.2305 (1.018 sec/step)\n",
            "INFO:tensorflow:global step 2554: loss = 1.8561 (1.809 sec/step)\n",
            "I1209 11:45:13.203988 139904075134848 learning.py:507] global step 2554: loss = 1.8561 (1.809 sec/step)\n",
            "INFO:tensorflow:global step 2555: loss = 2.2860 (0.648 sec/step)\n",
            "I1209 11:45:14.075676 139904075134848 learning.py:507] global step 2555: loss = 2.2860 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 2556: loss = 2.0506 (0.700 sec/step)\n",
            "I1209 11:45:15.150197 139904075134848 learning.py:507] global step 2556: loss = 2.0506 (0.700 sec/step)\n",
            "INFO:tensorflow:global step 2557: loss = 2.3128 (2.138 sec/step)\n",
            "I1209 11:45:17.289947 139904075134848 learning.py:507] global step 2557: loss = 2.3128 (2.138 sec/step)\n",
            "INFO:tensorflow:global step 2558: loss = 2.3296 (0.623 sec/step)\n",
            "I1209 11:45:17.914454 139904075134848 learning.py:507] global step 2558: loss = 2.3296 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 2559: loss = 2.1717 (0.849 sec/step)\n",
            "I1209 11:45:18.826296 139904075134848 learning.py:507] global step 2559: loss = 2.1717 (0.849 sec/step)\n",
            "INFO:tensorflow:global step 2560: loss = 1.9785 (2.158 sec/step)\n",
            "I1209 11:45:21.156944 139904075134848 learning.py:507] global step 2560: loss = 1.9785 (2.158 sec/step)\n",
            "INFO:tensorflow:global step 2561: loss = 1.7092 (0.678 sec/step)\n",
            "I1209 11:45:21.836770 139904075134848 learning.py:507] global step 2561: loss = 1.7092 (0.678 sec/step)\n",
            "INFO:tensorflow:global step 2562: loss = 1.5344 (2.206 sec/step)\n",
            "I1209 11:45:24.044193 139904075134848 learning.py:507] global step 2562: loss = 1.5344 (2.206 sec/step)\n",
            "INFO:tensorflow:global step 2563: loss = 1.9164 (0.711 sec/step)\n",
            "I1209 11:45:24.757960 139904075134848 learning.py:507] global step 2563: loss = 1.9164 (0.711 sec/step)\n",
            "INFO:tensorflow:global step 2564: loss = 3.0867 (0.845 sec/step)\n",
            "I1209 11:45:26.702358 139904075134848 learning.py:507] global step 2564: loss = 3.0867 (0.845 sec/step)\n",
            "INFO:tensorflow:global step 2565: loss = 1.6006 (2.710 sec/step)\n",
            "I1209 11:45:29.621247 139904075134848 learning.py:507] global step 2565: loss = 1.6006 (2.710 sec/step)\n",
            "INFO:tensorflow:global step 2566: loss = 2.8537 (0.720 sec/step)\n",
            "I1209 11:45:30.728717 139904075134848 learning.py:507] global step 2566: loss = 2.8537 (0.720 sec/step)\n",
            "INFO:tensorflow:global step 2567: loss = 2.6023 (0.749 sec/step)\n",
            "I1209 11:45:31.577876 139904075134848 learning.py:507] global step 2567: loss = 2.6023 (0.749 sec/step)\n",
            "INFO:tensorflow:global step 2568: loss = 1.3687 (2.275 sec/step)\n",
            "I1209 11:45:33.855382 139904075134848 learning.py:507] global step 2568: loss = 1.3687 (2.275 sec/step)\n",
            "INFO:tensorflow:global step 2569: loss = 2.5336 (0.733 sec/step)\n",
            "I1209 11:45:34.633583 139904075134848 learning.py:507] global step 2569: loss = 2.5336 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 2570: loss = 2.7831 (2.262 sec/step)\n",
            "I1209 11:45:36.970283 139904075134848 learning.py:507] global step 2570: loss = 2.7831 (2.262 sec/step)\n",
            "INFO:tensorflow:global step 2571: loss = 1.7738 (0.768 sec/step)\n",
            "I1209 11:45:38.088844 139904075134848 learning.py:507] global step 2571: loss = 1.7738 (0.768 sec/step)\n",
            "INFO:tensorflow:global step 2572: loss = 1.7772 (0.830 sec/step)\n",
            "I1209 11:45:39.031551 139904075134848 learning.py:507] global step 2572: loss = 1.7772 (0.830 sec/step)\n",
            "INFO:tensorflow:global step 2573: loss = 1.9228 (2.466 sec/step)\n",
            "I1209 11:45:41.499704 139904075134848 learning.py:507] global step 2573: loss = 1.9228 (2.466 sec/step)\n",
            "INFO:tensorflow:global step 2574: loss = 1.8788 (0.640 sec/step)\n",
            "I1209 11:45:42.141235 139904075134848 learning.py:507] global step 2574: loss = 1.8788 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 2575: loss = 3.1011 (0.731 sec/step)\n",
            "I1209 11:45:42.874519 139904075134848 learning.py:507] global step 2575: loss = 3.1011 (0.731 sec/step)\n",
            "INFO:tensorflow:global step 2576: loss = 2.2773 (3.257 sec/step)\n",
            "I1209 11:45:46.139938 139904075134848 learning.py:507] global step 2576: loss = 2.2773 (3.257 sec/step)\n",
            "INFO:tensorflow:global step 2577: loss = 1.8552 (0.868 sec/step)\n",
            "I1209 11:45:47.061366 139904075134848 learning.py:507] global step 2577: loss = 1.8552 (0.868 sec/step)\n",
            "INFO:tensorflow:global step 2578: loss = 2.0320 (2.379 sec/step)\n",
            "I1209 11:45:49.578089 139904075134848 learning.py:507] global step 2578: loss = 2.0320 (2.379 sec/step)\n",
            "INFO:tensorflow:global step 2579: loss = 2.0876 (0.839 sec/step)\n",
            "I1209 11:45:50.672734 139904075134848 learning.py:507] global step 2579: loss = 2.0876 (0.839 sec/step)\n",
            "INFO:tensorflow:global step 2580: loss = 2.2494 (0.553 sec/step)\n",
            "I1209 11:45:51.407553 139904075134848 learning.py:507] global step 2580: loss = 2.2494 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 2581: loss = 1.8884 (1.716 sec/step)\n",
            "I1209 11:45:53.211816 139904075134848 learning.py:507] global step 2581: loss = 1.8884 (1.716 sec/step)\n",
            "INFO:tensorflow:global step 2582: loss = 2.1641 (0.560 sec/step)\n",
            "I1209 11:45:53.778635 139904075134848 learning.py:507] global step 2582: loss = 2.1641 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 2583: loss = 1.8228 (1.318 sec/step)\n",
            "I1209 11:45:55.273408 139904075134848 learning.py:507] global step 2583: loss = 1.8228 (1.318 sec/step)\n",
            "INFO:tensorflow:global step 2584: loss = 2.1155 (2.764 sec/step)\n",
            "I1209 11:45:58.138465 139904075134848 learning.py:507] global step 2584: loss = 2.1155 (2.764 sec/step)\n",
            "INFO:tensorflow:global step 2585: loss = 1.7474 (0.743 sec/step)\n",
            "I1209 11:45:59.114920 139904075134848 learning.py:507] global step 2585: loss = 1.7474 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2586: loss = 1.4662 (2.128 sec/step)\n",
            "I1209 11:46:01.388288 139904075134848 learning.py:507] global step 2586: loss = 1.4662 (2.128 sec/step)\n",
            "INFO:tensorflow:global step 2587: loss = 1.6449 (0.769 sec/step)\n",
            "I1209 11:46:02.564494 139904075134848 learning.py:507] global step 2587: loss = 1.6449 (0.769 sec/step)\n",
            "INFO:tensorflow:global step 2588: loss = 2.6196 (0.639 sec/step)\n",
            "I1209 11:46:03.331954 139904075134848 learning.py:507] global step 2588: loss = 2.6196 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 2589: loss = 1.7811 (1.961 sec/step)\n",
            "I1209 11:46:05.333868 139904075134848 learning.py:507] global step 2589: loss = 1.7811 (1.961 sec/step)\n",
            "INFO:tensorflow:global step 2590: loss = 2.6213 (0.747 sec/step)\n",
            "I1209 11:46:06.413453 139904075134848 learning.py:507] global step 2590: loss = 2.6213 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 2591: loss = 2.0766 (0.861 sec/step)\n",
            "I1209 11:46:07.422215 139904075134848 learning.py:507] global step 2591: loss = 2.0766 (0.861 sec/step)\n",
            "INFO:tensorflow:global step 2592: loss = 2.0084 (2.102 sec/step)\n",
            "I1209 11:46:09.767703 139904075134848 learning.py:507] global step 2592: loss = 2.0084 (2.102 sec/step)\n",
            "INFO:tensorflow:global step 2593: loss = 1.7742 (0.780 sec/step)\n",
            "I1209 11:46:10.806551 139904075134848 learning.py:507] global step 2593: loss = 1.7742 (0.780 sec/step)\n",
            "INFO:tensorflow:global step 2594: loss = 1.7444 (2.551 sec/step)\n",
            "I1209 11:46:13.441875 139904075134848 learning.py:507] global step 2594: loss = 1.7444 (2.551 sec/step)\n",
            "INFO:tensorflow:global step 2595: loss = 1.9338 (0.818 sec/step)\n",
            "I1209 11:46:14.668318 139904075134848 learning.py:507] global step 2595: loss = 1.9338 (0.818 sec/step)\n",
            "INFO:tensorflow:global step 2596: loss = 1.9900 (0.719 sec/step)\n",
            "I1209 11:46:15.488040 139904075134848 learning.py:507] global step 2596: loss = 1.9900 (0.719 sec/step)\n",
            "INFO:tensorflow:global step 2597: loss = 2.1334 (0.981 sec/step)\n",
            "I1209 11:46:16.678968 139904075134848 learning.py:507] global step 2597: loss = 2.1334 (0.981 sec/step)\n",
            "INFO:tensorflow:global step 2598: loss = 1.6267 (1.654 sec/step)\n",
            "I1209 11:46:18.644457 139904075134848 learning.py:507] global step 2598: loss = 1.6267 (1.654 sec/step)\n",
            "INFO:tensorflow:global step 2599: loss = 2.2289 (0.797 sec/step)\n",
            "I1209 11:46:19.710172 139904075134848 learning.py:507] global step 2599: loss = 2.2289 (0.797 sec/step)\n",
            "INFO:tensorflow:global step 2600: loss = 1.8169 (0.579 sec/step)\n",
            "I1209 11:46:20.431404 139904075134848 learning.py:507] global step 2600: loss = 1.8169 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 2601: loss = 2.0432 (0.706 sec/step)\n",
            "I1209 11:46:21.140002 139904075134848 learning.py:507] global step 2601: loss = 2.0432 (0.706 sec/step)\n",
            "INFO:tensorflow:global step 2602: loss = 1.9416 (1.457 sec/step)\n",
            "I1209 11:46:22.655453 139904075134848 learning.py:507] global step 2602: loss = 1.9416 (1.457 sec/step)\n",
            "INFO:tensorflow:global step 2603: loss = 1.6845 (2.666 sec/step)\n",
            "I1209 11:46:25.431989 139904075134848 learning.py:507] global step 2603: loss = 1.6845 (2.666 sec/step)\n",
            "INFO:tensorflow:global step 2604: loss = 2.1674 (0.767 sec/step)\n",
            "I1209 11:46:26.201205 139904075134848 learning.py:507] global step 2604: loss = 2.1674 (0.767 sec/step)\n",
            "INFO:tensorflow:global step 2605: loss = 1.4854 (1.983 sec/step)\n",
            "I1209 11:46:28.185786 139904075134848 learning.py:507] global step 2605: loss = 1.4854 (1.983 sec/step)\n",
            "INFO:tensorflow:global step 2606: loss = 1.8635 (0.681 sec/step)\n",
            "I1209 11:46:29.140909 139904075134848 learning.py:507] global step 2606: loss = 1.8635 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2607: loss = 2.1642 (1.561 sec/step)\n",
            "I1209 11:46:30.800011 139904075134848 learning.py:507] global step 2607: loss = 2.1642 (1.561 sec/step)\n",
            "INFO:tensorflow:global step 2608: loss = 2.2148 (0.880 sec/step)\n",
            "I1209 11:46:31.788740 139904075134848 learning.py:507] global step 2608: loss = 2.2148 (0.880 sec/step)\n",
            "INFO:tensorflow:global step 2609: loss = 1.5994 (0.746 sec/step)\n",
            "I1209 11:46:33.030429 139904075134848 learning.py:507] global step 2609: loss = 1.5994 (0.746 sec/step)\n",
            "INFO:tensorflow:global step 2610: loss = 2.4457 (1.389 sec/step)\n",
            "I1209 11:46:34.467738 139904075134848 learning.py:507] global step 2610: loss = 2.4457 (1.389 sec/step)\n",
            "INFO:tensorflow:global step 2611: loss = 2.4817 (0.650 sec/step)\n",
            "I1209 11:46:35.411422 139904075134848 learning.py:507] global step 2611: loss = 2.4817 (0.650 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2611.\n",
            "I1209 11:46:39.239376 139900543997696 supervisor.py:1050] Recording summary at step 2611.\n",
            "INFO:tensorflow:global step 2612: loss = 2.0268 (4.023 sec/step)\n",
            "I1209 11:46:39.564159 139904075134848 learning.py:507] global step 2612: loss = 2.0268 (4.023 sec/step)\n",
            "INFO:tensorflow:global step 2613: loss = 2.3247 (0.645 sec/step)\n",
            "I1209 11:46:40.210706 139904075134848 learning.py:507] global step 2613: loss = 2.3247 (0.645 sec/step)\n",
            "INFO:tensorflow:global step 2614: loss = 3.1695 (1.772 sec/step)\n",
            "I1209 11:46:41.984774 139904075134848 learning.py:507] global step 2614: loss = 3.1695 (1.772 sec/step)\n",
            "INFO:tensorflow:global step 2615: loss = 1.9458 (0.704 sec/step)\n",
            "I1209 11:46:43.085488 139904075134848 learning.py:507] global step 2615: loss = 1.9458 (0.704 sec/step)\n",
            "INFO:tensorflow:global step 2616: loss = 2.5171 (0.759 sec/step)\n",
            "I1209 11:46:44.068427 139904075134848 learning.py:507] global step 2616: loss = 2.5171 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 2617: loss = 2.7395 (0.733 sec/step)\n",
            "I1209 11:46:44.935348 139904075134848 learning.py:507] global step 2617: loss = 2.7395 (0.733 sec/step)\n",
            "INFO:tensorflow:global step 2618: loss = 1.5898 (0.681 sec/step)\n",
            "I1209 11:46:46.219687 139904075134848 learning.py:507] global step 2618: loss = 1.5898 (0.681 sec/step)\n",
            "INFO:tensorflow:global step 2619: loss = 2.0347 (2.069 sec/step)\n",
            "I1209 11:46:48.462274 139904075134848 learning.py:507] global step 2619: loss = 2.0347 (2.069 sec/step)\n",
            "INFO:tensorflow:global step 2620: loss = 1.5269 (0.765 sec/step)\n",
            "I1209 11:46:49.457361 139904075134848 learning.py:507] global step 2620: loss = 1.5269 (0.765 sec/step)\n",
            "INFO:tensorflow:global step 2621: loss = 1.7034 (0.697 sec/step)\n",
            "I1209 11:46:50.231707 139904075134848 learning.py:507] global step 2621: loss = 1.7034 (0.697 sec/step)\n",
            "INFO:tensorflow:global step 2622: loss = 1.4925 (2.167 sec/step)\n",
            "I1209 11:46:52.439212 139904075134848 learning.py:507] global step 2622: loss = 1.4925 (2.167 sec/step)\n",
            "INFO:tensorflow:global step 2623: loss = 2.4950 (0.847 sec/step)\n",
            "I1209 11:46:53.523288 139904075134848 learning.py:507] global step 2623: loss = 2.4950 (0.847 sec/step)\n",
            "INFO:tensorflow:global step 2624: loss = 2.0772 (1.558 sec/step)\n",
            "I1209 11:46:55.166671 139904075134848 learning.py:507] global step 2624: loss = 2.0772 (1.558 sec/step)\n",
            "INFO:tensorflow:global step 2625: loss = 1.7086 (0.647 sec/step)\n",
            "I1209 11:46:55.815082 139904075134848 learning.py:507] global step 2625: loss = 1.7086 (0.647 sec/step)\n",
            "INFO:tensorflow:global step 2626: loss = 2.4765 (1.838 sec/step)\n",
            "I1209 11:46:57.834916 139904075134848 learning.py:507] global step 2626: loss = 2.4765 (1.838 sec/step)\n",
            "INFO:tensorflow:global step 2627: loss = 1.2827 (0.588 sec/step)\n",
            "I1209 11:46:58.424843 139904075134848 learning.py:507] global step 2627: loss = 1.2827 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 2628: loss = 1.5672 (1.920 sec/step)\n",
            "I1209 11:47:00.346427 139904075134848 learning.py:507] global step 2628: loss = 1.5672 (1.920 sec/step)\n",
            "INFO:tensorflow:global step 2629: loss = 2.5985 (0.651 sec/step)\n",
            "I1209 11:47:00.999619 139904075134848 learning.py:507] global step 2629: loss = 2.5985 (0.651 sec/step)\n",
            "INFO:tensorflow:global step 2630: loss = 1.9832 (1.802 sec/step)\n",
            "I1209 11:47:02.803596 139904075134848 learning.py:507] global step 2630: loss = 1.9832 (1.802 sec/step)\n",
            "INFO:tensorflow:global step 2631: loss = 2.6332 (0.727 sec/step)\n",
            "I1209 11:47:03.630149 139904075134848 learning.py:507] global step 2631: loss = 2.6332 (0.727 sec/step)\n",
            "INFO:tensorflow:global step 2632: loss = 3.1318 (2.187 sec/step)\n",
            "I1209 11:47:06.005096 139904075134848 learning.py:507] global step 2632: loss = 3.1318 (2.187 sec/step)\n",
            "INFO:tensorflow:global step 2633: loss = 1.8653 (0.682 sec/step)\n",
            "I1209 11:47:06.991943 139904075134848 learning.py:507] global step 2633: loss = 1.8653 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 2634: loss = 1.3996 (0.625 sec/step)\n",
            "I1209 11:47:07.786715 139904075134848 learning.py:507] global step 2634: loss = 1.3996 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 2635: loss = 1.4999 (2.049 sec/step)\n",
            "I1209 11:47:09.837396 139904075134848 learning.py:507] global step 2635: loss = 1.4999 (2.049 sec/step)\n",
            "INFO:tensorflow:global step 2636: loss = 1.5887 (0.605 sec/step)\n",
            "I1209 11:47:10.443822 139904075134848 learning.py:507] global step 2636: loss = 1.5887 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 2637: loss = 1.6213 (1.875 sec/step)\n",
            "I1209 11:47:12.320998 139904075134848 learning.py:507] global step 2637: loss = 1.6213 (1.875 sec/step)\n",
            "INFO:tensorflow:global step 2638: loss = 2.3013 (0.747 sec/step)\n",
            "I1209 11:47:13.311430 139904075134848 learning.py:507] global step 2638: loss = 2.3013 (0.747 sec/step)\n",
            "INFO:tensorflow:global step 2639: loss = 2.3633 (1.536 sec/step)\n",
            "I1209 11:47:15.014553 139904075134848 learning.py:507] global step 2639: loss = 2.3633 (1.536 sec/step)\n",
            "INFO:tensorflow:global step 2640: loss = 1.8351 (0.754 sec/step)\n",
            "I1209 11:47:16.102823 139904075134848 learning.py:507] global step 2640: loss = 1.8351 (0.754 sec/step)\n",
            "INFO:tensorflow:global step 2641: loss = 1.4693 (0.989 sec/step)\n",
            "I1209 11:47:17.494494 139904075134848 learning.py:507] global step 2641: loss = 1.4693 (0.989 sec/step)\n",
            "INFO:tensorflow:global step 2642: loss = 2.4419 (0.686 sec/step)\n",
            "I1209 11:47:18.236309 139904075134848 learning.py:507] global step 2642: loss = 2.4419 (0.686 sec/step)\n",
            "INFO:tensorflow:global step 2643: loss = 1.8826 (0.971 sec/step)\n",
            "I1209 11:47:19.468930 139904075134848 learning.py:507] global step 2643: loss = 1.8826 (0.971 sec/step)\n",
            "INFO:tensorflow:global step 2644: loss = 2.1232 (1.860 sec/step)\n",
            "I1209 11:47:21.587468 139904075134848 learning.py:507] global step 2644: loss = 2.1232 (1.860 sec/step)\n",
            "INFO:tensorflow:global step 2645: loss = 1.7422 (0.680 sec/step)\n",
            "I1209 11:47:22.269581 139904075134848 learning.py:507] global step 2645: loss = 1.7422 (0.680 sec/step)\n",
            "INFO:tensorflow:global step 2646: loss = 1.5247 (1.925 sec/step)\n",
            "I1209 11:47:24.197718 139904075134848 learning.py:507] global step 2646: loss = 1.5247 (1.925 sec/step)\n",
            "INFO:tensorflow:global step 2647: loss = 2.3336 (0.560 sec/step)\n",
            "I1209 11:47:24.760161 139904075134848 learning.py:507] global step 2647: loss = 2.3336 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 2648: loss = 1.4079 (1.930 sec/step)\n",
            "I1209 11:47:26.692751 139904075134848 learning.py:507] global step 2648: loss = 1.4079 (1.930 sec/step)\n",
            "INFO:tensorflow:global step 2649: loss = 1.5134 (0.703 sec/step)\n",
            "I1209 11:47:27.542524 139904075134848 learning.py:507] global step 2649: loss = 1.5134 (0.703 sec/step)\n",
            "INFO:tensorflow:global step 2650: loss = 1.5620 (1.359 sec/step)\n",
            "I1209 11:47:29.170118 139904075134848 learning.py:507] global step 2650: loss = 1.5620 (1.359 sec/step)\n",
            "INFO:tensorflow:global step 2651: loss = 1.8115 (0.542 sec/step)\n",
            "I1209 11:47:29.713805 139904075134848 learning.py:507] global step 2651: loss = 1.8115 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 2652: loss = 1.5771 (2.020 sec/step)\n",
            "I1209 11:47:31.735855 139904075134848 learning.py:507] global step 2652: loss = 1.5771 (2.020 sec/step)\n",
            "INFO:tensorflow:global step 2653: loss = 2.1763 (0.682 sec/step)\n",
            "I1209 11:47:32.616542 139904075134848 learning.py:507] global step 2653: loss = 2.1763 (0.682 sec/step)\n",
            "INFO:tensorflow:global step 2654: loss = 2.2779 (2.052 sec/step)\n",
            "I1209 11:47:34.876349 139904075134848 learning.py:507] global step 2654: loss = 2.2779 (2.052 sec/step)\n",
            "INFO:tensorflow:global step 2655: loss = 2.1769 (0.949 sec/step)\n",
            "I1209 11:47:35.960950 139904075134848 learning.py:507] global step 2655: loss = 2.1769 (0.949 sec/step)\n",
            "INFO:tensorflow:global step 2656: loss = 2.1581 (1.619 sec/step)\n",
            "I1209 11:47:37.582102 139904075134848 learning.py:507] global step 2656: loss = 2.1581 (1.619 sec/step)\n",
            "INFO:tensorflow:global step 2657: loss = 1.5018 (0.740 sec/step)\n",
            "I1209 11:47:38.629628 139904075134848 learning.py:507] global step 2657: loss = 1.5018 (0.740 sec/step)\n",
            "INFO:tensorflow:global step 2658: loss = 1.8738 (1.555 sec/step)\n",
            "I1209 11:47:40.325839 139904075134848 learning.py:507] global step 2658: loss = 1.8738 (1.555 sec/step)\n",
            "INFO:tensorflow:global step 2659: loss = 2.1028 (0.705 sec/step)\n",
            "I1209 11:47:41.212874 139904075134848 learning.py:507] global step 2659: loss = 2.1028 (0.705 sec/step)\n",
            "INFO:tensorflow:global step 2660: loss = 1.6993 (2.298 sec/step)\n",
            "I1209 11:47:43.721450 139904075134848 learning.py:507] global step 2660: loss = 1.6993 (2.298 sec/step)\n",
            "INFO:tensorflow:global step 2661: loss = 1.7417 (0.652 sec/step)\n",
            "I1209 11:47:44.682445 139904075134848 learning.py:507] global step 2661: loss = 1.7417 (0.652 sec/step)\n",
            "INFO:tensorflow:global step 2662: loss = 2.0519 (1.977 sec/step)\n",
            "I1209 11:47:46.711400 139904075134848 learning.py:507] global step 2662: loss = 2.0519 (1.977 sec/step)\n",
            "INFO:tensorflow:global step 2663: loss = 1.6035 (0.627 sec/step)\n",
            "I1209 11:47:47.340687 139904075134848 learning.py:507] global step 2663: loss = 1.6035 (0.627 sec/step)\n",
            "INFO:tensorflow:global step 2664: loss = 1.5670 (1.634 sec/step)\n",
            "I1209 11:47:49.054108 139904075134848 learning.py:507] global step 2664: loss = 1.5670 (1.634 sec/step)\n",
            "INFO:tensorflow:global step 2665: loss = 2.6730 (0.691 sec/step)\n",
            "I1209 11:47:49.988180 139904075134848 learning.py:507] global step 2665: loss = 2.6730 (0.691 sec/step)\n",
            "INFO:tensorflow:global step 2666: loss = 1.7883 (2.084 sec/step)\n",
            "I1209 11:47:52.073910 139904075134848 learning.py:507] global step 2666: loss = 1.7883 (2.084 sec/step)\n",
            "INFO:tensorflow:global step 2667: loss = 1.8874 (0.658 sec/step)\n",
            "I1209 11:47:52.734267 139904075134848 learning.py:507] global step 2667: loss = 1.8874 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 2668: loss = 1.7527 (2.396 sec/step)\n",
            "I1209 11:47:55.131978 139904075134848 learning.py:507] global step 2668: loss = 1.7527 (2.396 sec/step)\n",
            "INFO:tensorflow:global step 2669: loss = 2.0906 (0.716 sec/step)\n",
            "I1209 11:47:56.179512 139904075134848 learning.py:507] global step 2669: loss = 2.0906 (0.716 sec/step)\n",
            "INFO:tensorflow:global step 2670: loss = 2.3060 (2.311 sec/step)\n",
            "I1209 11:47:58.584601 139904075134848 learning.py:507] global step 2670: loss = 2.3060 (2.311 sec/step)\n",
            "INFO:tensorflow:global step 2671: loss = 2.3346 (0.799 sec/step)\n",
            "I1209 11:47:59.495008 139904075134848 learning.py:507] global step 2671: loss = 2.3346 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2672: loss = 1.7350 (1.972 sec/step)\n",
            "I1209 11:48:01.710742 139904075134848 learning.py:507] global step 2672: loss = 1.7350 (1.972 sec/step)\n",
            "INFO:tensorflow:global step 2673: loss = 2.6837 (0.817 sec/step)\n",
            "I1209 11:48:02.539645 139904075134848 learning.py:507] global step 2673: loss = 2.6837 (0.817 sec/step)\n",
            "INFO:tensorflow:global step 2674: loss = 2.6703 (1.686 sec/step)\n",
            "I1209 11:48:04.449346 139904075134848 learning.py:507] global step 2674: loss = 2.6703 (1.686 sec/step)\n",
            "INFO:tensorflow:global step 2675: loss = 1.9761 (0.795 sec/step)\n",
            "I1209 11:48:05.246306 139904075134848 learning.py:507] global step 2675: loss = 1.9761 (0.795 sec/step)\n",
            "INFO:tensorflow:global step 2676: loss = 2.8932 (1.677 sec/step)\n",
            "I1209 11:48:07.046025 139904075134848 learning.py:507] global step 2676: loss = 2.8932 (1.677 sec/step)\n",
            "INFO:tensorflow:global step 2677: loss = 2.5498 (0.739 sec/step)\n",
            "I1209 11:48:08.075367 139904075134848 learning.py:507] global step 2677: loss = 2.5498 (0.739 sec/step)\n",
            "INFO:tensorflow:global step 2678: loss = 2.0201 (0.665 sec/step)\n",
            "I1209 11:48:08.877195 139904075134848 learning.py:507] global step 2678: loss = 2.0201 (0.665 sec/step)\n",
            "INFO:tensorflow:global step 2679: loss = 2.0798 (1.104 sec/step)\n",
            "I1209 11:48:10.018482 139904075134848 learning.py:507] global step 2679: loss = 2.0798 (1.104 sec/step)\n",
            "INFO:tensorflow:global step 2680: loss = 2.6716 (2.621 sec/step)\n",
            "I1209 11:48:12.794919 139904075134848 learning.py:507] global step 2680: loss = 2.6716 (2.621 sec/step)\n",
            "INFO:tensorflow:global step 2681: loss = 2.1503 (0.799 sec/step)\n",
            "I1209 11:48:14.032988 139904075134848 learning.py:507] global step 2681: loss = 2.1503 (0.799 sec/step)\n",
            "INFO:tensorflow:global step 2682: loss = 2.4778 (0.837 sec/step)\n",
            "I1209 11:48:14.904681 139904075134848 learning.py:507] global step 2682: loss = 2.4778 (0.837 sec/step)\n",
            "INFO:tensorflow:global step 2683: loss = 1.7504 (1.896 sec/step)\n",
            "I1209 11:48:16.803043 139904075134848 learning.py:507] global step 2683: loss = 1.7504 (1.896 sec/step)\n",
            "INFO:tensorflow:global step 2684: loss = 1.6365 (0.604 sec/step)\n",
            "I1209 11:48:17.408861 139904075134848 learning.py:507] global step 2684: loss = 1.6365 (0.604 sec/step)\n",
            "INFO:tensorflow:global step 2685: loss = 2.2996 (1.014 sec/step)\n",
            "I1209 11:48:18.598064 139904075134848 learning.py:507] global step 2685: loss = 2.2996 (1.014 sec/step)\n",
            "INFO:tensorflow:global step 2686: loss = 1.9733 (2.017 sec/step)\n",
            "I1209 11:48:20.735751 139904075134848 learning.py:507] global step 2686: loss = 1.9733 (2.017 sec/step)\n",
            "INFO:tensorflow:global step 2687: loss = 1.9420 (0.722 sec/step)\n",
            "I1209 11:48:21.777121 139904075134848 learning.py:507] global step 2687: loss = 1.9420 (0.722 sec/step)\n",
            "INFO:tensorflow:global step 2688: loss = 1.7381 (2.879 sec/step)\n",
            "I1209 11:48:24.721940 139904075134848 learning.py:507] global step 2688: loss = 1.7381 (2.879 sec/step)\n",
            "INFO:tensorflow:global step 2689: loss = 1.9950 (0.862 sec/step)\n",
            "I1209 11:48:25.613531 139904075134848 learning.py:507] global step 2689: loss = 1.9950 (0.862 sec/step)\n",
            "INFO:tensorflow:global step 2690: loss = 2.0776 (0.821 sec/step)\n",
            "I1209 11:48:27.150720 139904075134848 learning.py:507] global step 2690: loss = 2.0776 (0.821 sec/step)\n",
            "INFO:tensorflow:global step 2691: loss = 2.0859 (2.212 sec/step)\n",
            "I1209 11:48:29.432569 139904075134848 learning.py:507] global step 2691: loss = 2.0859 (2.212 sec/step)\n",
            "INFO:tensorflow:global step 2692: loss = 1.8594 (0.660 sec/step)\n",
            "I1209 11:48:30.094554 139904075134848 learning.py:507] global step 2692: loss = 1.8594 (0.660 sec/step)\n",
            "INFO:tensorflow:global step 2693: loss = 2.4111 (2.312 sec/step)\n",
            "I1209 11:48:32.408419 139904075134848 learning.py:507] global step 2693: loss = 2.4111 (2.312 sec/step)\n",
            "INFO:tensorflow:global step 2694: loss = 2.0041 (0.679 sec/step)\n",
            "I1209 11:48:33.089725 139904075134848 learning.py:507] global step 2694: loss = 2.0041 (0.679 sec/step)\n",
            "INFO:tensorflow:global step 2695: loss = 2.2531 (1.228 sec/step)\n",
            "I1209 11:48:34.337079 139904075134848 learning.py:507] global step 2695: loss = 2.2531 (1.228 sec/step)\n",
            "INFO:tensorflow:global step 2696: loss = 2.1812 (1.981 sec/step)\n",
            "I1209 11:48:36.589153 139904075134848 learning.py:507] global step 2696: loss = 2.1812 (1.981 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 2696.\n",
            "I1209 11:48:38.929585 139900543997696 supervisor.py:1050] Recording summary at step 2696.\n",
            "INFO:tensorflow:global step 2697: loss = 2.0844 (2.643 sec/step)\n",
            "I1209 11:48:39.233625 139904075134848 learning.py:507] global step 2697: loss = 2.0844 (2.643 sec/step)\n",
            "INFO:tensorflow:global step 2698: loss = 1.8422 (0.869 sec/step)\n",
            "I1209 11:48:40.272334 139904075134848 learning.py:507] global step 2698: loss = 1.8422 (0.869 sec/step)\n",
            "INFO:tensorflow:global step 2699: loss = 1.5912 (0.743 sec/step)\n",
            "I1209 11:48:41.414804 139904075134848 learning.py:507] global step 2699: loss = 1.5912 (0.743 sec/step)\n",
            "INFO:tensorflow:global step 2700: loss = 1.8283 (2.253 sec/step)\n",
            "I1209 11:48:43.700156 139904075134848 learning.py:507] global step 2700: loss = 1.8283 (2.253 sec/step)\n",
            "INFO:tensorflow:global step 2701: loss = 1.8794 (0.656 sec/step)\n",
            "I1209 11:48:44.358716 139904075134848 learning.py:507] global step 2701: loss = 1.8794 (0.656 sec/step)\n",
            "INFO:tensorflow:global step 2702: loss = 1.4899 (2.185 sec/step)\n",
            "I1209 11:48:46.546256 139904075134848 learning.py:507] global step 2702: loss = 1.4899 (2.185 sec/step)\n",
            "INFO:tensorflow:global step 2703: loss = 1.5651 (0.725 sec/step)\n",
            "I1209 11:48:47.637486 139904075134848 learning.py:507] global step 2703: loss = 1.5651 (0.725 sec/step)\n",
            "INFO:tensorflow:global step 2704: loss = 1.6512 (2.282 sec/step)\n",
            "I1209 11:48:49.955861 139904075134848 learning.py:507] global step 2704: loss = 1.6512 (2.282 sec/step)\n",
            "INFO:tensorflow:global step 2705: loss = 2.0192 (0.764 sec/step)\n",
            "I1209 11:48:50.977677 139904075134848 learning.py:507] global step 2705: loss = 2.0192 (0.764 sec/step)\n",
            "INFO:tensorflow:global step 2706: loss = 1.8421 (0.743 sec/step)\n",
            "I1209 11:48:52.190167 139904075134848 learning.py:507] global step 2706: loss = 1.8421 (0.743 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 185, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 181, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 417, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/slim/python/slim/learning.py\", line 523, in train_step\n",
            "    should_stop = sess.run(train_step_kwargs['should_stop'])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYXafYtY5Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WzWDHjZQ-5",
        "colab_type": "code",
        "outputId": "67a31645-fa99-4b5b-b5c9-540933a6b819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-2530 --output_directory inference_graph"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1209 11:49:24.064886 139857130162048 module_wrapper.py:139] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W1209 11:49:24.072052 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1209 11:49:24.072314 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W1209 11:49:24.110965 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1209 11:49:24.142318 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1209 11:49:24.144896 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W1209 11:49:26.400856 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W1209 11:49:26.411982 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.412199 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.506636 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.603739 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.815351 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.901861 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1209 11:49:26.985798 139857130162048 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1209 11:49:27.288218 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W1209 11:49:27.634526 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W1209 11:49:27.634854 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1209 11:49:27.638831 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W1209 11:49:27.639064 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W1209 11:49:27.640503 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/2.99m params)\n",
            "  BoxPredictor_0 (--/20.75k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/69.16k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/27.68k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/13.86k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/13.86k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/6.95k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/13.71k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1209 11:49:28.783544 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1209 11:49:29.846166 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-12-09 11:49:29.864082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-09 11:49:29.922284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:29.922901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-09 11:49:29.925629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 11:49:29.947566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-09 11:49:30.046836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-09 11:49:30.056127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-09 11:49:30.074285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-09 11:49:30.089357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-09 11:49:30.154903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 11:49:30.155068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.155756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.156256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-09 11:49:30.156853: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-09 11:49:30.166528: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000134999 Hz\n",
            "2019-12-09 11:49:30.167658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1fd9100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-09 11:49:30.167691: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-09 11:49:30.305917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.306587: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1fd9640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-09 11:49:30.306632: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-12-09 11:49:30.306883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.307424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-09 11:49:30.307519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 11:49:30.307545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-09 11:49:30.307570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-09 11:49:30.307592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-09 11:49:30.307633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-09 11:49:30.307658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-09 11:49:30.307689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 11:49:30.307762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.308320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.308848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-09 11:49:30.311549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 11:49:30.313494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-09 11:49:30.313525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-09 11:49:30.313535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-09 11:49:30.313772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.314345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:30.314920: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-09 11:49:30.314969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2530\n",
            "I1209 11:49:30.316905 139857130162048 saver.py:1284] Restoring parameters from training/model.ckpt-2530\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W1209 11:49:31.659363 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-12-09 11:49:32.416796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:32.417366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-09 11:49:32.417455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 11:49:32.417470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-09 11:49:32.417483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-09 11:49:32.417496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-09 11:49:32.417508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-09 11:49:32.417519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-09 11:49:32.417532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 11:49:32.417597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:32.418137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:32.418598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-09 11:49:32.418645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-09 11:49:32.418655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-09 11:49:32.418664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-09 11:49:32.418741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:32.419230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:32.419707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2530\n",
            "I1209 11:49:32.420884 139857130162048 saver.py:1284] Restoring parameters from training/model.ckpt-2530\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W1209 11:49:35.327843 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W1209 11:49:35.328108 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 404 variables.\n",
            "I1209 11:49:35.751618 139857130162048 graph_util_impl.py:334] Froze 404 variables.\n",
            "INFO:tensorflow:Converted 404 variables to const ops.\n",
            "I1209 11:49:35.821470 139857130162048 graph_util_impl.py:394] Converted 404 variables to const ops.\n",
            "2019-12-09 11:49:35.964487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:35.965131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-09 11:49:35.965254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-09 11:49:35.965283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-09 11:49:35.965308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-09 11:49:35.965333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-09 11:49:35.965356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-09 11:49:35.965385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-09 11:49:35.965409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-09 11:49:35.965503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:35.966096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:35.966592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-09 11:49:35.966649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-09 11:49:35.966666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-09 11:49:35.966680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-09 11:49:35.966783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:35.967320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-09 11:49:35.967824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W1209 11:49:36.356432 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W1209 11:49:36.361007 139857130162048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W1209 11:49:36.361498 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W1209 11:49:36.361702 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W1209 11:49:36.361943 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W1209 11:49:36.362079 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I1209 11:49:36.362341 139857130162048 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I1209 11:49:36.362431 139857130162048 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "I1209 11:49:36.595653 139857130162048 builder_impl.py:425] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W1209 11:49:36.618473 139857130162048 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to inference_graph/pipeline.config\n",
            "I1209 11:49:36.618715 139857130162048 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGhSchrjUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -r /content/models/research/object_detection/inference_graph /gdrive/My\\ Drive/colabfiles/lektion20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owJO0_cqhSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasenbilder190726_01/bild101.jpg /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8-djxitLZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp /gdrive/My\\ Drive/colabfiles/lektion31/rasen.mov /content/models/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcwxa8EfjeBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Image Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/15/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on an image.\n",
        "# It draws boxes and scores around the objects of interest in the image.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "IMAGE_NAME = 'bild101.jpg'\n",
        "#IMAGE_NAME = 'ch (8).jpg'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(\"/gdrive/My Drive/colabfiles/lektion20/DSC01015.JPG\")\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs2cfgcr1AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "b94461f0-e2bd-4123-e57b-fd2016199b43"
      },
      "source": [
        "######## Video Object Detection Using Tensorflow-trained Classifier #########\n",
        "#\n",
        "# Author: Evan Juras\n",
        "# Date: 1/16/18\n",
        "# Description: \n",
        "# This program uses a TensorFlow-trained classifier to perform object detection.\n",
        "# It loads the classifier uses it to perform object detection on a video.\n",
        "# It draws boxes and scores around the objects of interest in each frame\n",
        "# of the video.\n",
        "\n",
        "## Some of the code is copied from Google's example at\n",
        "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "\n",
        "## and some is copied from Dat Tran's example at\n",
        "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\n",
        "\n",
        "## but I changed it to make it more understandable to me.\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import imutils\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "# Name of the directory containing the object detection module we're using\n",
        "MODEL_NAME = 'inference_graph'\n",
        "VIDEO_NAME = 'rasen.mov'\n",
        "\n",
        "# Grab path to current working directory\n",
        "CWD_PATH = os.getcwd()\n",
        "\n",
        "print(CWD_PATH)\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','label_map.pbtxt')\n",
        "\n",
        "# Path to video\n",
        "PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Open video file\n",
        "video = cv2.VideoCapture(PATH_TO_VIDEO)\n",
        "print(\"ok\")\n",
        "i = 0\n",
        "while(video.isOpened()):\n",
        "\n",
        "    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]\n",
        "    # i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "    ret, frame = video.read()\n",
        "    if ret==True:\n",
        "        \n",
        "        frame = cv2.imread(\"/gdrive/My Drive/colabfiles/lektion20/DSC01015.JPG\")\n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={image_tensor: frame_expanded})\n",
        "\n",
        "        # Draw the results of the detection (aka 'visulaize the results')\n",
        "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            frame,\n",
        "            np.squeeze(boxes),\n",
        "            np.squeeze(classes).astype(np.int32),\n",
        "            np.squeeze(scores),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.70)\n",
        "\n",
        "        # All the results have been drawn on the frame, so it's time to display it.\n",
        "        \n",
        "        frame = imutils.resize(frame, 400)\n",
        "        cv2_imshow(frame)\n",
        "        #print(boxes)\n",
        "        time.sleep(1)\n",
        "        clear_output()\n",
        "        \n",
        "\n",
        "        # Press 'q' to quit\n",
        "        #print(i)\n",
        "        #i = i + 1\n",
        "        #if cv2.waitKey(1) == ord('q'):\n",
        "        #    break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Clean up\n",
        "print(\"end1\")\n",
        "#video.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(\"end2\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEsCAIAAABi1XKVAAEAAElEQVR4nDz9Sa9v3ZbmB41iVmut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\nTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x300 at 0x7F039CD46748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e6720a9cd5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m#print(boxes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdH-jDqafF5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}